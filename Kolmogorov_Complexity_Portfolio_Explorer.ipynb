{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "42e494058727406fb8cc931aa4e323cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7505a29c9664b8f8d9754cb2f39b146",
              "IPY_MODEL_efbec998ff02489cb4f17af55e621a4c",
              "IPY_MODEL_77478c5f5e754f3ebc210ded5daa29d9"
            ],
            "layout": "IPY_MODEL_d7361cf01c9e41e2846f95c91cf0f0d1"
          }
        },
        "e7505a29c9664b8f8d9754cb2f39b146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54cd229859c64ef69db75e765d089aa3",
            "placeholder": "​",
            "style": "IPY_MODEL_3f0d04561a1c40a688f4f78961604e8c",
            "value": "100%"
          }
        },
        "efbec998ff02489cb4f17af55e621a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c9b8559566b48468b1e4fd639131514",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1503a261fc9e4c2eacaf151035ab1c6d",
            "value": 1000
          }
        },
        "77478c5f5e754f3ebc210ded5daa29d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0d4c1f572484821ba7966551e31f8af",
            "placeholder": "​",
            "style": "IPY_MODEL_3e40f457f05540e39f646b0eee927a32",
            "value": " 1000/1000 [00:11&lt;00:00, 118.27it/s]"
          }
        },
        "d7361cf01c9e41e2846f95c91cf0f0d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54cd229859c64ef69db75e765d089aa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f0d04561a1c40a688f4f78961604e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c9b8559566b48468b1e4fd639131514": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1503a261fc9e4c2eacaf151035ab1c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0d4c1f572484821ba7966551e31f8af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e40f457f05540e39f646b0eee927a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "_8EjARTYpOqH",
        "outputId": "5920cca1-9542-44dc-abf2-29f15106da3d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Weights: [0.30638402 0.28064122 0.41297473]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Kolmogorov Complexity Penalty Function\n",
        "def kc_penalty(w, gamma=1e-2, eta=1e-2):\n",
        "    l0 = np.count_nonzero(w)  # L0 norm (sparsity)\n",
        "    l1 = np.sum(np.abs(w))    # L1 norm (weight magnitude)\n",
        "    return gamma * l0 + eta * l1\n",
        "\n",
        "# Objective Function: Risk-Return with KC Penalty\n",
        "def objective(w, Sigma, mu, lam, gamma, eta):\n",
        "    risk = w.T @ Sigma @ w\n",
        "    ret = w.T @ mu\n",
        "    penalty = kc_penalty(w, gamma, eta)\n",
        "    return risk - lam * ret + penalty\n",
        "\n",
        "# Gradient of the Objective Function\n",
        "def grad_objective(w, Sigma, mu, lam, eta):\n",
        "    grad_risk = 2 * Sigma @ w\n",
        "    grad_ret = lam * mu\n",
        "    grad_l1 = eta * np.sign(w)\n",
        "    return grad_risk - grad_ret + grad_l1\n",
        "\n",
        "# Portfolio Optimization using Gradient Descent\n",
        "def optimize_weights(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, lr=1e-2, epochs=500):\n",
        "    w = np.ones_like(mu) / len(mu)  # Initial equal weights\n",
        "    for epoch in range(epochs):\n",
        "        grad = grad_objective(w, Sigma, mu, lam, eta)\n",
        "        w -= lr * grad\n",
        "        # Apply Proximal Operator for L0 (sparsity)\n",
        "        w[np.abs(w) < gamma] = 0\n",
        "        # Ensure non-negative weights and sum to 1\n",
        "        w = np.maximum(w, 0)\n",
        "        w /= np.sum(w + 1e-8)\n",
        "\n",
        "    return w\n",
        "\n",
        "# Example Data (Covariance matrix and expected returns)\n",
        "Sigma = np.array([[0.02, 0.015, -0.005], [0.015, 0.03, -0.008], [-0.005, -0.008, 0.01]])\n",
        "mu = np.array([0.01, 0.005, -0.002])  # Expected returns\n",
        "\n",
        "# Run optimization\n",
        "optimized_weights = optimize_weights(Sigma, mu)\n",
        "\n",
        "# Plotting the final sparse weights\n",
        "plt.bar(range(len(optimized_weights)), optimized_weights)\n",
        "plt.title(\"Optimized Portfolio Weights (with Kolmogorov Complexity Penalty)\")\n",
        "plt.xlabel(\"Asset Index\")\n",
        "plt.ylabel(\"Weight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Optimized Weights:\", optimized_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.covariance import EmpiricalCovariance\n",
        "from scipy.optimize import minimize\n",
        "import heapq\n",
        "import random\n",
        "\n",
        "# -------------------------------\n",
        "# Compression-based Regularization (Huffman Encoding)\n",
        "# -------------------------------\n",
        "\n",
        "# Function to calculate Huffman encoding penalty\n",
        "def huffman_penalty(w):\n",
        "    # Huffman coding approximation: Treat weights as a sequence of bits (binary representation)\n",
        "    weights_abs = np.abs(w)\n",
        "    symbols = list(zip(range(len(weights_abs)), weights_abs))\n",
        "    heap = [[weight, [symbol]] for symbol, weight in symbols]\n",
        "    heapq.heapify(heap)\n",
        "\n",
        "    while len(heap) > 1:\n",
        "        lo = heapq.heappop(heap)\n",
        "        hi = heapq.heappop(heap)\n",
        "        heapq.heappush(heap, [lo[0] + hi[0], lo[1] + hi[1]])\n",
        "\n",
        "    # Calculate total length of the encoded weights\n",
        "    huffman_length = 0\n",
        "    for symbol, weight in symbols:\n",
        "        huffman_length += weight * np.log2(weight + 1e-8)  # Encoding cost based on frequency (simplified)\n",
        "    return huffman_length\n",
        "\n",
        "# -------------------------------\n",
        "# Robust Mean-Variance Optimization (Robust Optimization)\n",
        "# -------------------------------\n",
        "\n",
        "# Function to implement robust mean-variance optimization\n",
        "def robust_mean_variance(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, epsilon=0.05):\n",
        "    \"\"\"Solve for a robust portfolio by considering uncertainties in the covariance matrix.\"\"\"\n",
        "    robust_Sigma = Sigma + epsilon * np.identity(Sigma.shape[0])  # Adding uncertainty\n",
        "    return optimize_weights(robust_Sigma, mu, lam, gamma, eta)\n",
        "\n",
        "# -------------------------------\n",
        "# Stress Testing\n",
        "# -------------------------------\n",
        "\n",
        "def stress_test(Sigma, mu, stress_scenarios, lam=0.1, gamma=1e-2, eta=1e-2):\n",
        "    \"\"\"Simulate portfolio performance under various stress scenarios.\"\"\"\n",
        "    results = {}\n",
        "    for scenario_name, scenario in stress_scenarios.items():\n",
        "        # Adjust Sigma and mu according to the stress scenario\n",
        "        stressed_Sigma = Sigma + scenario['sigma_change']\n",
        "        stressed_mu = mu + scenario['mu_change']\n",
        "\n",
        "        # Optimize portfolio\n",
        "        optimized_weights = optimize_weights(stressed_Sigma, stressed_mu, lam, gamma, eta)\n",
        "        results[scenario_name] = optimized_weights\n",
        "    return results\n",
        "\n",
        "# -------------------------------\n",
        "# Sensitivity Analysis\n",
        "# -------------------------------\n",
        "\n",
        "def sensitivity_analysis(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, perturbation=0.05):\n",
        "    \"\"\"Perform sensitivity analysis by perturbing risk parameters.\"\"\"\n",
        "    original_weights = optimize_weights(Sigma, mu, lam, gamma, eta)\n",
        "\n",
        "    sensitivities = {}\n",
        "    for i in range(len(mu)):\n",
        "        perturbed_mu = mu.copy()\n",
        "        perturbed_mu[i] += perturbation  # Perturb the i-th expected return\n",
        "        weights_perturbed = optimize_weights(Sigma, perturbed_mu, lam, gamma, eta)\n",
        "        sensitivity = np.abs(weights_perturbed - original_weights)\n",
        "        sensitivities[i] = sensitivity\n",
        "\n",
        "    return sensitivities\n",
        "\n",
        "# -------------------------------\n",
        "# Benchmarking\n",
        "# -------------------------------\n",
        "\n",
        "def benchmark(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2):\n",
        "    \"\"\"Compare different portfolio optimization strategies.\"\"\"\n",
        "    # Classical Mean-Variance Optimization\n",
        "    mv_weights = optimize_weights(Sigma, mu, lam)\n",
        "\n",
        "    # Equal Weight Portfolio\n",
        "    equal_weight = np.ones(len(mu)) / len(mu)\n",
        "\n",
        "    # Robust Mean-Variance Optimization\n",
        "    robust_weights = robust_mean_variance(Sigma, mu, lam, gamma, eta)\n",
        "\n",
        "    return mv_weights, equal_weight, robust_weights\n",
        "\n",
        "# -------------------------------\n",
        "# Portfolio Optimization with Kolmogorov Complexity Penalty\n",
        "# -------------------------------\n",
        "\n",
        "# Kolmogorov Complexity Penalty Function\n",
        "def kc_penalty(w, gamma=1e-2, eta=1e-2):\n",
        "    l0 = np.count_nonzero(w)  # L0 norm (sparsity)\n",
        "    l1 = np.sum(np.abs(w))    # L1 norm (weight magnitude)\n",
        "    return gamma * l0 + eta * l1\n",
        "\n",
        "# Objective Function: Risk-Return with KC Penalty\n",
        "def objective(w, Sigma, mu, lam, gamma, eta):\n",
        "    risk = w.T @ Sigma @ w\n",
        "    ret = w.T @ mu\n",
        "    penalty = kc_penalty(w, gamma, eta)\n",
        "    return risk - lam * ret + penalty\n",
        "\n",
        "# Gradient of the Objective Function\n",
        "def grad_objective(w, Sigma, mu, lam, eta):\n",
        "    grad_risk = 2 * Sigma @ w\n",
        "    grad_ret = lam * mu\n",
        "    grad_l1 = eta * np.sign(w)\n",
        "    return grad_risk - grad_ret + grad_l1\n",
        "\n",
        "# Portfolio Optimization using Gradient Descent\n",
        "def optimize_weights(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, lr=1e-2, epochs=500):\n",
        "    w = np.ones_like(mu) / len(mu)  # Initial equal weights\n",
        "    for epoch in range(epochs):\n",
        "        grad = grad_objective(w, Sigma, mu, lam, eta)\n",
        "        w -= lr * grad\n",
        "        # Apply Proximal Operator for L0 (sparsity)\n",
        "        w[np.abs(w) < gamma] = 0\n",
        "        # Ensure non-negative weights and sum to 1\n",
        "        w = np.maximum(w, 0)\n",
        "        w /= np.sum(w + 1e-8)\n",
        "\n",
        "    return w\n",
        "\n",
        "# -------------------------------\n",
        "# Example Data (Covariance matrix and expected returns)\n",
        "# -------------------------------\n",
        "\n",
        "Sigma = np.array([[0.02, 0.015, -0.005], [0.015, 0.03, -0.008], [-0.005, -0.008, 0.01]])\n",
        "mu = np.array([0.01, 0.005, -0.002])  # Expected returns\n",
        "\n",
        "# -------------------------------\n",
        "# Run Optimization, Stress Test, Sensitivity Analysis, and Benchmarking\n",
        "# -------------------------------\n",
        "\n",
        "# Optimize Portfolio\n",
        "optimized_weights = optimize_weights(Sigma, mu)\n",
        "\n",
        "# Define Stress Scenarios\n",
        "stress_scenarios = {\n",
        "    'Market Crash': {'sigma_change': 0.05 * np.identity(3), 'mu_change': -0.01 * np.ones(3)},\n",
        "    'Volatility Surge': {'sigma_change': 0.1 * np.identity(3), 'mu_change': 0.0 * np.ones(3)},\n",
        "}\n",
        "\n",
        "# Perform Stress Test\n",
        "stress_test_results = stress_test(Sigma, mu, stress_scenarios)\n",
        "\n",
        "# Perform Sensitivity Analysis\n",
        "sensitivity_results = sensitivity_analysis(Sigma, mu)\n",
        "\n",
        "# Perform Benchmarking\n",
        "mv_weights, equal_weight, robust_weights = benchmark(Sigma, mu)\n",
        "\n",
        "# -------------------------------\n",
        "# Plotting Results\n",
        "# -------------------------------\n",
        "\n",
        "# Plot Optimized Weights\n",
        "plt.bar(range(len(optimized_weights)), optimized_weights)\n",
        "plt.title(\"Optimized Portfolio Weights (with Kolmogorov Complexity Penalty)\")\n",
        "plt.xlabel(\"Asset Index\")\n",
        "plt.ylabel(\"Weight\")\n",
        "plt.show()\n",
        "\n",
        "# Display Results\n",
        "print(\"Optimized Weights:\", optimized_weights)\n",
        "print(\"Stress Test Results:\", stress_test_results)\n",
        "print(\"Sensitivity Analysis:\", sensitivity_results)\n",
        "print(\"Mean-Variance Optimized Weights:\", mv_weights)\n",
        "print(\"Equal Weight Portfolio Weights:\", equal_weight)\n",
        "print(\"Robust Optimized Weights:\", robust_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "V6bWG_OOqSIP",
        "outputId": "a3262953-d1ca-4413-d214-c4ea909d05c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Weights: [0.30638402 0.28064122 0.41297473]\n",
            "Stress Test Results: {'Market Crash': array([0.30615139, 0.28018993, 0.41365865]), 'Volatility Surge': array([0.30632868, 0.28053511, 0.41313618])}\n",
            "Sensitivity Analysis: {0: array([0.01885387, 0.00910504, 0.00974883]), 1: array([0.00942463, 0.01841016, 0.00898552]), 2: array([0.00840111, 0.00731177, 0.01571289])}\n",
            "Mean-Variance Optimized Weights: [0.30638402 0.28064122 0.41297473]\n",
            "Equal Weight Portfolio Weights: [0.33333333 0.33333333 0.33333333]\n",
            "Robust Optimized Weights: [0.30635638 0.28058822 0.41305537]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import t, norm\n",
        "from scipy.optimize import minimize\n",
        "import heapq\n",
        "\n",
        "# -------------------------------\n",
        "# Compression-based Regularization (Huffman Encoding)\n",
        "# -------------------------------\n",
        "\n",
        "# Function to calculate Huffman encoding penalty\n",
        "def huffman_penalty(w):\n",
        "    weights_abs = np.abs(w)\n",
        "    symbols = list(zip(range(len(weights_abs)), weights_abs))\n",
        "    heap = [[weight, [symbol]] for symbol, weight in symbols]\n",
        "    heapq.heapify(heap)\n",
        "\n",
        "    while len(heap) > 1:\n",
        "        lo = heapq.heappop(heap)\n",
        "        hi = heapq.heappop(heap)\n",
        "        heapq.heappush(heap, [lo[0] + hi[0], lo[1] + hi[1]])\n",
        "\n",
        "    huffman_length = 0\n",
        "    for symbol, weight in symbols:\n",
        "        huffman_length += weight * np.log2(weight + 1e-8)\n",
        "    return huffman_length\n",
        "\n",
        "# -------------------------------\n",
        "# Robust Mean-Variance Optimization (Robust Optimization)\n",
        "# -------------------------------\n",
        "\n",
        "# Function to implement robust mean-variance optimization\n",
        "def robust_mean_variance(Sigma, lam=0.1, gamma=1e-2, eta=1e-2, epsilon=0.05):\n",
        "    robust_Sigma = Sigma + epsilon * np.identity(Sigma.shape[0])  # Adding uncertainty\n",
        "    return optimize_weights(robust_Sigma, lam, gamma, eta)\n",
        "\n",
        "# -------------------------------\n",
        "# Stress Testing: Black Swan & Extreme Scenarios\n",
        "# -------------------------------\n",
        "\n",
        "def extreme_market_scenarios(Sigma, scenarios):\n",
        "    results = {}\n",
        "    for scenario in scenarios:\n",
        "        stressed_Sigma = Sigma + scenario['sigma_change']\n",
        "        optimized_weights = optimize_weights(stressed_Sigma)\n",
        "        results[scenario['name']] = optimized_weights\n",
        "    return results\n",
        "\n",
        "# -------------------------------\n",
        "# Sensitivity Analysis: Higher-Order Sensitivity\n",
        "# -------------------------------\n",
        "\n",
        "def higher_order_sensitivity(Sigma, lam=0.1, gamma=1e-2, eta=1e-2, perturbation=0.05):\n",
        "    original_weights = optimize_weights(Sigma, lam, gamma, eta)\n",
        "\n",
        "    sensitivities = {}\n",
        "    for i in range(Sigma.shape[0]):\n",
        "        perturbed_Sigma = Sigma.copy()\n",
        "        perturbed_Sigma[i, i] += perturbation\n",
        "        weights_perturbed = optimize_weights(perturbed_Sigma, lam, gamma, eta)\n",
        "        sensitivity = np.abs(weights_perturbed - original_weights)\n",
        "        sensitivities[i] = sensitivity\n",
        "\n",
        "    cov_sensitivities = {}\n",
        "    for i in range(Sigma.shape[0]):\n",
        "        for j in range(Sigma.shape[1]):\n",
        "            perturbed_Sigma = Sigma.copy()\n",
        "            perturbed_Sigma[i, j] += perturbation\n",
        "            weights_perturbed = optimize_weights(perturbed_Sigma, lam, gamma, eta)\n",
        "            cov_sensitivities[(i, j)] = np.abs(weights_perturbed - original_weights)\n",
        "\n",
        "    return sensitivities, cov_sensitivities\n",
        "\n",
        "# -------------------------------\n",
        "# Cross-validation for Out-of-Sample Testing\n",
        "# -------------------------------\n",
        "\n",
        "def cross_validate_portfolio(Sigma, lam=0.1, gamma=1e-2, eta=1e-2, n_splits=5):\n",
        "    \"\"\"Perform cross-validation to test the model's out-of-sample performance.\"\"\"\n",
        "\n",
        "    # Verbosity: Check the dimensions of Sigma\n",
        "    print(f\"Before Cross-validation:\")\n",
        "    print(f\"Sigma shape: {Sigma.shape}\")\n",
        "\n",
        "    # Manually split `mu` to ensure compatibility with Sigma\n",
        "    print(f\"mu before splitting: {mu}\")\n",
        "\n",
        "    # Manually split `mu` into `mu_train` and `mu_test` with correct lengths\n",
        "    mu_train = mu[:2]  # First 2 values as training\n",
        "    mu_test = mu[2:]   # Last value as test\n",
        "\n",
        "    # Verbosity: Output the dimensions of `mu_train` and `mu_test`\n",
        "    print(f\"mu_train before optimization: {mu_train}, mu_test before optimization: {mu_test}\")\n",
        "\n",
        "    validation_errors = []\n",
        "\n",
        "    # Ensure that `Sigma` and `mu` have compatible dimensions\n",
        "    assert Sigma.shape[0] == len(mu), f\"Sigma rows ({Sigma.shape[0]}) must match length of mu ({len(mu)})\"\n",
        "    print(f\"Dimensions of Sigma and mu match. Proceeding with optimization.\")\n",
        "\n",
        "    # Perform cross-validation and calculate error using `mu_train` for training\n",
        "    optimized_weights = optimize_weights(Sigma, lam, gamma, eta)\n",
        "\n",
        "    # Check the dimensions of optimized weights\n",
        "    print(f\"Optimized weights shape: {optimized_weights.shape}\")\n",
        "\n",
        "    # Validate that optimized weights match the number of assets\n",
        "    assert optimized_weights.shape[0] == Sigma.shape[0], f\"Shape mismatch: {optimized_weights.shape[0]} != {Sigma.shape[0]}\"\n",
        "\n",
        "    # Calculate the validation error using Mean Squared Error (MSE) and `mu_test`\n",
        "    validation_error = np.sum((Sigma @ optimized_weights - mu_test) ** 2)  # MSE as error\n",
        "    validation_errors.append(validation_error)\n",
        "\n",
        "    return np.mean(validation_errors)\n",
        "\n",
        "# -------------------------------\n",
        "# Portfolio Optimization with Kolmogorov Complexity Penalty\n",
        "# -------------------------------\n",
        "\n",
        "# Kolmogorov Complexity Penalty Function\n",
        "def kc_penalty(w, gamma=1e-2, eta=1e-2):\n",
        "    l0 = np.count_nonzero(w)  # L0 norm (sparsity)\n",
        "    l1 = np.sum(np.abs(w))    # L1 norm (weight magnitude)\n",
        "    return gamma * l0 + eta * l1\n",
        "\n",
        "# Objective Function: Risk-Return with KC Penalty\n",
        "def objective(w, Sigma, lam, gamma, eta):\n",
        "    risk = w.T @ Sigma @ w\n",
        "    penalty = kc_penalty(w, gamma, eta)\n",
        "    return risk + penalty\n",
        "\n",
        "# Gradient of the Objective Function\n",
        "def grad_objective(w, Sigma, lam, eta):\n",
        "    grad_risk = 2 * Sigma @ w  # Correct matrix multiplication\n",
        "    grad_l1 = eta * np.sign(w)\n",
        "    return grad_risk + grad_l1\n",
        "\n",
        "# Portfolio Optimization using Gradient Descent\n",
        "def optimize_weights(Sigma, lam=0.1, gamma=1e-2, eta=1e-2, lr=1e-2, epochs=500):\n",
        "    # Ensure `w` has the same length as `Sigma`\n",
        "    w = np.ones(Sigma.shape[0]) / Sigma.shape[0]  # Initialize weights (equal distribution)\n",
        "\n",
        "    # Ensure that the shape of `Sigma` is valid for optimization\n",
        "    assert Sigma.shape[0] == len(w), f\"Sigma rows ({Sigma.shape[0]}) must match length of weights ({len(w)})\"\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        grad = grad_objective(w, Sigma, lam, eta)\n",
        "        w -= lr * grad\n",
        "        # Apply Proximal Operator for L0 (sparsity)\n",
        "        w[np.abs(w) < gamma] = 0\n",
        "        # Ensure non-negative weights and sum to 1\n",
        "        w = np.maximum(w, 0)\n",
        "        w /= np.sum(w + 1e-8)\n",
        "\n",
        "    return w\n",
        "\n",
        "# -------------------------------\n",
        "# Simulate Non-Normal Return Distributions (Fat-Tailed)\n",
        "# -------------------------------\n",
        "\n",
        "def simulate_fat_tailed_returns(Sigma, mu, dof=4, n_samples=1000):\n",
        "    \"\"\"Simulate returns from a fat-tailed distribution (Student's t-distribution).\"\"\"\n",
        "    t_returns = t.rvs(dof, size=(n_samples, len(mu)))\n",
        "    fat_tailed_returns = np.dot(t_returns, Sigma) + mu\n",
        "    return fat_tailed_returns\n",
        "\n",
        "# -------------------------------\n",
        "# Running the Tests\n",
        "# -------------------------------\n",
        "\n",
        "# Example Data (Covariance matrix and expected returns)\n",
        "Sigma = np.array([[0.02, 0.015, -0.005], [0.015, 0.03, -0.008], [-0.005, -0.008, 0.01]])\n",
        "\n",
        "# Corrected expected returns (for 3 assets)\n",
        "mu = np.array([0.01, 0.005, -0.002])  # Expected returns for 3 assets\n",
        "\n",
        "# Cross-validation for Out-of-Sample Testing\n",
        "validation_error = cross_validate_portfolio(Sigma, lam=0.1, gamma=1e-2, eta=1e-2)\n",
        "\n",
        "print(\"Validation Error (Cross-validation):\", validation_error)\n",
        "\n",
        "# Stress test scenarios (extreme market events)\n",
        "extreme_scenarios = [\n",
        "    {'name': 'Black Swan', 'sigma_change': 0.1 * np.identity(3)},\n",
        "    {'name': 'Liquidity Crisis', 'sigma_change': 0.2 * np.identity(3)},\n",
        "]\n",
        "\n",
        "# Stress Test Results\n",
        "stress_test_results = extreme_market_scenarios(Sigma, extreme_scenarios)\n",
        "\n",
        "# Higher-Order Sensitivity Analysis\n",
        "sensitivity_results, cov_sensitivity_results = higher_order_sensitivity(Sigma)\n",
        "\n",
        "# Simulate Fat-Tailed Returns\n",
        "fat_tailed_returns = simulate_fat_tailed_returns(Sigma, mu)\n",
        "\n",
        "# -------------------------------\n",
        "# Results Display\n",
        "# -------------------------------\n",
        "\n",
        "print(\"Stress Test Results:\", stress_test_results)\n",
        "print(\"Sensitivity Analysis:\", sensitivity_results)\n",
        "print(\"Covariance Sensitivity Analysis:\", cov_sensitivity_results)\n",
        "print(\"Validation Error (Cross-validation):\", validation_error)\n",
        "print(\"Fat-Tailed Returns Simulation (First 5 Samples):\", fat_tailed_returns[:5])\n",
        "\n",
        "# -------------------------------\n",
        "# Plotting Results (Stress Test Comparison)\n",
        "# -------------------------------\n",
        "\n",
        "for scenario in extreme_scenarios:\n",
        "    plt.bar(range(len(stress_test_results[scenario['name']])), stress_test_results[scenario['name']])\n",
        "    plt.title(f\"Portfolio Weights - {scenario['name']}\")\n",
        "    plt.xlabel(\"Asset Index\")\n",
        "    plt.ylabel(\"Weight\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gYsu24JXrQ-N",
        "outputId": "026f8621-cfe4-46b5-a595-92f64bae9cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Cross-validation:\n",
            "Sigma shape: (3, 3)\n",
            "mu before splitting: [ 0.01   0.005 -0.002]\n",
            "mu_train before optimization: [0.01  0.005], mu_test before optimization: [-0.002]\n",
            "Dimensions of Sigma and mu match. Proceeding with optimization.\n",
            "Optimized weights shape: (3,)\n",
            "Validation Error (Cross-validation): 0.0002446900115481169\n",
            "Stress Test Results: {'Black Swan': array([0.30310816, 0.28021959, 0.41667223]), 'Liquidity Crisis': array([0.30304539, 0.28011253, 0.41684204])}\n",
            "Sensitivity Analysis: {0: array([0.10859313, 0.05322174, 0.05537139]), 1: array([0.05257926, 0.10096275, 0.04838349]), 2: array([0.05992316, 0.05228222, 0.11220538])}\n",
            "Covariance Sensitivity Analysis: {(0, 0): array([0.10859313, 0.05322174, 0.05537139]), (0, 1): array([0.14100508, 0.06819476, 0.07281032]), (0, 2): array([0.17426175, 0.08355192, 0.09070983]), (1, 0): array([0.07264126, 0.14217716, 0.0695359 ]), (1, 1): array([0.05257926, 0.10096275, 0.04838349]), (1, 2): array([0.08533267, 0.16824985, 0.08291717]), (2, 0): array([0.06607221, 0.05773897, 0.12381117]), (2, 1): array([0.06309595, 0.05509467, 0.11819063]), (2, 2): array([0.05992316, 0.05228222, 0.11220538])}\n",
            "Validation Error (Cross-validation): 0.0002446900115481169\n",
            "Fat-Tailed Returns Simulation (First 5 Samples): [[ 0.01456007  0.00563276 -0.00457251]\n",
            " [-0.01111152 -0.02367062  0.00684722]\n",
            " [-0.04026053 -0.08955682  0.02971102]\n",
            " [ 0.00090405 -0.01250533  0.00745507]\n",
            " [-0.04388913 -0.08049871  0.03439348]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import t, norm\n",
        "from scipy.optimize import minimize\n",
        "import heapq\n",
        "\n",
        "# -------------------------------\n",
        "# Compression-based Regularization (Huffman Encoding)\n",
        "# -------------------------------\n",
        "\n",
        "# Function to calculate Huffman encoding penalty\n",
        "def huffman_penalty(w):\n",
        "    weights_abs = np.abs(w)\n",
        "    symbols = list(zip(range(len(weights_abs)), weights_abs))\n",
        "    heap = [[weight, [symbol]] for symbol, weight in symbols]\n",
        "    heapq.heapify(heap)\n",
        "\n",
        "    while len(heap) > 1:\n",
        "        lo = heapq.heappop(heap)\n",
        "        hi = heapq.heappop(heap)\n",
        "        heapq.heappush(heap, [lo[0] + hi[0], lo[1] + hi[1]])\n",
        "\n",
        "    huffman_length = 0\n",
        "    for symbol, weight in symbols:\n",
        "        huffman_length += weight * np.log2(weight + 1e-8)\n",
        "    return huffman_length\n",
        "\n",
        "# -------------------------------\n",
        "# Robust Mean-Variance Optimization (Robust Optimization)\n",
        "# -------------------------------\n",
        "\n",
        "# Function to implement robust mean-variance optimization\n",
        "def robust_mean_variance(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, epsilon=0.05):\n",
        "    robust_Sigma = Sigma + epsilon * np.identity(Sigma.shape[0])  # Adding uncertainty\n",
        "    return optimize_weights(robust_Sigma, mu, lam, gamma, eta)\n",
        "\n",
        "# -------------------------------\n",
        "# Stress Testing: Black Swan & Extreme Scenarios\n",
        "# -------------------------------\n",
        "\n",
        "def extreme_market_scenarios(Sigma, mu, scenarios):\n",
        "    results = {}\n",
        "    for scenario in scenarios:\n",
        "        stressed_Sigma = Sigma + scenario['sigma_change']\n",
        "        # Ensure mu is passed as the second argument to optimize_weights\n",
        "        optimized_weights = optimize_weights(stressed_Sigma, mu)  # Passing mu here\n",
        "        results[scenario['name']] = optimized_weights\n",
        "    return results\n",
        "\n",
        "# -------------------------------\n",
        "# Sensitivity Analysis: Higher-Order Sensitivity\n",
        "# -------------------------------\n",
        "\n",
        "def higher_order_sensitivity(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, perturbation=0.05):\n",
        "    original_weights = optimize_weights(Sigma, mu, lam, gamma, eta)\n",
        "\n",
        "    sensitivities = {}\n",
        "    for i in range(len(mu)):\n",
        "        perturbed_mu = mu.copy()\n",
        "        perturbed_mu[i] += perturbation\n",
        "        weights_perturbed = optimize_weights(Sigma, perturbed_mu, lam, gamma, eta)\n",
        "        sensitivity = np.abs(weights_perturbed - original_weights)\n",
        "        sensitivities[i] = sensitivity\n",
        "\n",
        "    cov_sensitivities = {}\n",
        "    for i in range(Sigma.shape[0]):\n",
        "        for j in range(Sigma.shape[1]):\n",
        "            perturbed_Sigma = Sigma.copy()\n",
        "            perturbed_Sigma[i, j] += perturbation\n",
        "            weights_perturbed = optimize_weights(perturbed_Sigma, mu, lam, gamma, eta)\n",
        "            cov_sensitivities[(i, j)] = np.abs(weights_perturbed - original_weights)\n",
        "\n",
        "    return sensitivities, cov_sensitivities\n",
        "\n",
        "# -------------------------------\n",
        "# Cross-validation for Out-of-Sample Testing\n",
        "# -------------------------------\n",
        "\n",
        "def cross_validate_portfolio(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, n_splits=5):\n",
        "    \"\"\"Perform cross-validation to test the model's out-of-sample performance.\"\"\"\n",
        "\n",
        "    # Verbosity: Check the dimensions of Sigma and mu before proceeding\n",
        "    print(f\"Before Cross-validation:\")\n",
        "    print(f\"Sigma shape: {Sigma.shape}, mu length: {len(mu)}\")\n",
        "\n",
        "    # Manually split `mu` to ensure compatibility with Sigma\n",
        "    print(f\"mu before splitting: {mu}\")\n",
        "\n",
        "    # Manually split `mu` into `mu_train` and `mu_test` with correct lengths\n",
        "    mu_train = mu[:2]  # First 2 values for training\n",
        "    mu_test = mu[2:]   # Last value for testing\n",
        "\n",
        "    # Verbosity: Output the dimensions of `mu_train` and `mu_test`\n",
        "    print(f\"mu_train before optimization: {mu_train}, mu_test before optimization: {mu_test}\")\n",
        "\n",
        "    validation_errors = []\n",
        "\n",
        "    # Ensure that `Sigma` and `mu` have compatible dimensions\n",
        "    assert Sigma.shape[0] == len(mu), f\"Sigma rows ({Sigma.shape[0]}) must match length of mu ({len(mu)})\"\n",
        "    print(f\"Dimensions of Sigma and mu match. Proceeding with optimization.\")\n",
        "\n",
        "    # Pad `mu_train` with zeros to match the number of assets in Sigma\n",
        "    if len(mu_train) < Sigma.shape[0]:\n",
        "        mu_train = np.pad(mu_train, (0, Sigma.shape[0] - len(mu_train)), 'constant', constant_values=0)\n",
        "        print(f\"mu_train after padding: {mu_train}\")\n",
        "\n",
        "    # Perform cross-validation and calculate error using `mu_train` for training\n",
        "    optimized_weights = optimize_weights(Sigma, mu_train, lam, gamma, eta)\n",
        "\n",
        "    # Check the dimensions of optimized weights\n",
        "    print(f\"Optimized weights shape: {optimized_weights.shape}\")\n",
        "\n",
        "    # Validate that optimized weights match the number of assets\n",
        "    assert optimized_weights.shape[0] == Sigma.shape[0], f\"Shape mismatch: {optimized_weights.shape[0]} != {Sigma.shape[0]}\"\n",
        "\n",
        "    # Calculate the validation error using Mean Squared Error (MSE) and `mu_test`\n",
        "    validation_error = np.sum((Sigma @ optimized_weights - mu_test) ** 2)  # MSE as error\n",
        "    validation_errors.append(validation_error)\n",
        "\n",
        "    return np.mean(validation_errors)\n",
        "\n",
        "# -------------------------------\n",
        "# Portfolio Optimization with Kolmogorov Complexity Penalty\n",
        "# -------------------------------\n",
        "\n",
        "def kc_penalty(w, gamma=1e-2, eta=1e-2):\n",
        "    l0 = np.count_nonzero(w)  # L0 norm (sparsity)\n",
        "    l1 = np.sum(np.abs(w))    # L1 norm (weight magnitude)\n",
        "    return gamma * l0 + eta * l1\n",
        "\n",
        "# Objective Function: Risk-Return with KC Penalty\n",
        "def objective(w, Sigma, lam, gamma, eta):\n",
        "    risk = w.T @ Sigma @ w\n",
        "    penalty = kc_penalty(w, gamma, eta)\n",
        "    return risk + penalty\n",
        "\n",
        "# Gradient of the Objective Function\n",
        "def grad_objective(w, Sigma, lam, eta):\n",
        "    grad_risk = 2 * Sigma @ w  # Correct matrix multiplication\n",
        "    grad_l1 = eta * np.sign(w)\n",
        "    return grad_risk + grad_l1\n",
        "\n",
        "# Portfolio Optimization using Gradient Descent\n",
        "def optimize_weights(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, lr=1e-2, epochs=500):\n",
        "    w = np.ones(Sigma.shape[0]) / Sigma.shape[0]  # Equal distribution of weights\n",
        "    for epoch in range(epochs):\n",
        "        grad = grad_objective(w, Sigma, lam, eta)\n",
        "        w -= lr * grad\n",
        "        w = np.maximum(w, 0)  # Ensure non-negative weights\n",
        "        w /= np.sum(w + 1e-8)\n",
        "\n",
        "    return w\n",
        "\n",
        "# -------------------------------\n",
        "# Simulate Non-Normal Return Distributions (Fat-Tailed)\n",
        "# -------------------------------\n",
        "\n",
        "def simulate_fat_tailed_returns(Sigma, mu, dof=4, n_samples=1000):\n",
        "    \"\"\"Simulate returns from a fat-tailed distribution (Student's t-distribution).\"\"\"\n",
        "    t_returns = t.rvs(dof, size=(n_samples, len(mu)))\n",
        "    fat_tailed_returns = np.dot(t_returns, Sigma) + mu\n",
        "    return fat_tailed_returns\n",
        "\n",
        "# -------------------------------\n",
        "# Running the Tests\n",
        "# -------------------------------\n",
        "\n",
        "# Example Data (Covariance matrix and expected returns)\n",
        "Sigma = np.array([[0.02, 0.015, -0.005], [0.015, 0.03, -0.008], [-0.005, -0.008, 0.01]])\n",
        "mu = np.array([0.01, 0.005, -0.002])  # Expected returns for 3 assets\n",
        "\n",
        "# Cross-validation for Out-of-Sample Testing\n",
        "validation_error = cross_validate_portfolio(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2)\n",
        "\n",
        "print(\"Validation Error (Cross-validation):\", validation_error)\n",
        "\n",
        "# Stress test scenarios (extreme market events)\n",
        "extreme_scenarios = [\n",
        "    {'name': 'Black Swan', 'sigma_change': 0.1 * np.identity(3)},\n",
        "    {'name': 'Liquidity Crisis', 'sigma_change': 0.2 * np.identity(3)},\n",
        "]\n",
        "\n",
        "# Stress Test Results\n",
        "stress_test_results = extreme_market_scenarios(Sigma, mu, extreme_scenarios)\n",
        "\n",
        "# Higher-Order Sensitivity Analysis\n",
        "sensitivity_results, cov_sensitivity_results = higher_order_sensitivity(Sigma, mu)\n",
        "\n",
        "# Simulate Fat-Tailed Returns\n",
        "fat_tailed_returns = simulate_fat_tailed_returns(Sigma, mu)\n",
        "\n",
        "# -------------------------------\n",
        "# Results Display\n",
        "# -------------------------------\n",
        "\n",
        "print(\"Stress Test Results:\", stress_test_results)\n",
        "print(\"Sensitivity Analysis:\", sensitivity_results)\n",
        "print(\"Covariance Sensitivity Analysis:\", cov_sensitivity_results)\n",
        "print(\"Validation Error (Cross-validation):\", validation_error)\n",
        "print(\"Fat-Tailed Returns Simulation (First 5 Samples):\", fat_tailed_returns[:5])\n",
        "\n",
        "# -------------------------------\n",
        "# Monte Carlo Simulation Testing\n",
        "# -------------------------------\n",
        "\n",
        "def monte_carlo_testing(Sigma, mu, n_samples=1000):\n",
        "    \"\"\"Run Monte Carlo simulations for stress testing\"\"\"\n",
        "    results = []\n",
        "    for _ in range(n_samples):\n",
        "        simulated_Sigma = Sigma * np.random.uniform(0.8, 1.2)  # Vary covariance matrix\n",
        "        optimized_weights = optimize_weights(simulated_Sigma, mu)\n",
        "        results.append(optimized_weights)\n",
        "\n",
        "    return np.mean(results, axis=0)\n",
        "\n",
        "monte_carlo_results = monte_carlo_testing(Sigma, mu)\n",
        "\n",
        "print(\"Monte Carlo Simulation Results:\", monte_carlo_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQJr9T_QrmOn",
        "outputId": "df6dc55b-e2f0-49b2-ba99-9d70ad1eb2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Cross-validation:\n",
            "Sigma shape: (3, 3), mu length: 3\n",
            "mu before splitting: [ 0.01   0.005 -0.002]\n",
            "mu_train before optimization: [0.01  0.005], mu_test before optimization: [-0.002]\n",
            "Dimensions of Sigma and mu match. Proceeding with optimization.\n",
            "mu_train after padding: [0.01  0.005 0.   ]\n",
            "Optimized weights shape: (3,)\n",
            "Validation Error (Cross-validation): 0.0002446900115481169\n",
            "Stress Test Results: {'Black Swan': array([0.30310816, 0.28021959, 0.41667223]), 'Liquidity Crisis': array([0.30304539, 0.28011253, 0.41684204])}\n",
            "Sensitivity Analysis: {0: array([0., 0., 0.]), 1: array([0., 0., 0.]), 2: array([0., 0., 0.])}\n",
            "Covariance Sensitivity Analysis: {(0, 0): array([0.10859313, 0.05322174, 0.05537139]), (0, 1): array([0.14100508, 0.06819476, 0.07281032]), (0, 2): array([0.17426175, 0.08355192, 0.09070983]), (1, 0): array([0.07264126, 0.14217716, 0.0695359 ]), (1, 1): array([0.05257926, 0.10096275, 0.04838349]), (1, 2): array([0.08533267, 0.16824985, 0.08291717]), (2, 0): array([0.06607221, 0.05773897, 0.12381117]), (2, 1): array([0.06309595, 0.05509467, 0.11819063]), (2, 2): array([0.05992316, 0.05228222, 0.11220538])}\n",
            "Validation Error (Cross-validation): 0.0002446900115481169\n",
            "Fat-Tailed Returns Simulation (First 5 Samples): [[ 0.00841959 -0.00341667  0.00409977]\n",
            " [-0.00600577 -0.02460773  0.01510783]\n",
            " [-0.00260823  0.00783201 -0.01644742]\n",
            " [ 0.0412953   0.05968044 -0.05258912]\n",
            " [-0.01127372 -0.01964692  0.01557373]]\n",
            "Monte Carlo Simulation Results: [0.30331578 0.2805949  0.41608929]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import t, norm\n",
        "from scipy.optimize import minimize\n",
        "import heapq\n",
        "\n",
        "# Create a text file to save all the outputs\n",
        "log_file = open(\"portfolio_optimization_results.txt\", \"w\")\n",
        "\n",
        "# Function to log output to the text file\n",
        "def log_output(output):\n",
        "    print(output)\n",
        "    log_file.write(output + \"\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# Compression-based Regularization (Huffman Encoding)\n",
        "# -------------------------------\n",
        "\n",
        "# Function to calculate Huffman encoding penalty\n",
        "def huffman_penalty(w):\n",
        "    weights_abs = np.abs(w)\n",
        "    symbols = list(zip(range(len(weights_abs)), weights_abs))\n",
        "    heap = [[weight, [symbol]] for symbol, weight in symbols]\n",
        "    heapq.heapify(heap)\n",
        "\n",
        "    while len(heap) > 1:\n",
        "        lo = heapq.heappop(heap)\n",
        "        hi = heapq.heappop(heap)\n",
        "        heapq.heappush(heap, [lo[0] + hi[0], lo[1] + hi[1]])\n",
        "\n",
        "    huffman_length = 0\n",
        "    for symbol, weight in symbols:\n",
        "        huffman_length += weight * np.log2(weight + 1e-8)\n",
        "    return huffman_length\n",
        "\n",
        "# -------------------------------\n",
        "# Robust Mean-Variance Optimization (Robust Optimization)\n",
        "# -------------------------------\n",
        "\n",
        "# Function to implement robust mean-variance optimization\n",
        "def robust_mean_variance(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, epsilon=0.05):\n",
        "    robust_Sigma = Sigma + epsilon * np.identity(Sigma.shape[0])  # Adding uncertainty\n",
        "    return optimize_weights(robust_Sigma, mu, lam, gamma, eta)\n",
        "\n",
        "# -------------------------------\n",
        "# Stress Testing: Black Swan & Extreme Scenarios\n",
        "# -------------------------------\n",
        "\n",
        "def extreme_market_scenarios(Sigma, mu, scenarios):\n",
        "    results = {}\n",
        "    for scenario in scenarios:\n",
        "        stressed_Sigma = Sigma + scenario['sigma_change']\n",
        "        # Ensure mu is passed as the second argument to optimize_weights\n",
        "        optimized_weights = optimize_weights(stressed_Sigma, mu)  # Passing mu here\n",
        "        results[scenario['name']] = optimized_weights\n",
        "    return results\n",
        "\n",
        "# -------------------------------\n",
        "# Sensitivity Analysis: Higher-Order Sensitivity\n",
        "# -------------------------------\n",
        "\n",
        "def higher_order_sensitivity(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, perturbation=0.05):\n",
        "    original_weights = optimize_weights(Sigma, mu, lam, gamma, eta)\n",
        "\n",
        "    sensitivities = {}\n",
        "    for i in range(len(mu)):\n",
        "        perturbed_mu = mu.copy()\n",
        "        perturbed_mu[i] += perturbation\n",
        "        weights_perturbed = optimize_weights(Sigma, perturbed_mu, lam, gamma, eta)\n",
        "        sensitivity = np.abs(weights_perturbed - original_weights)\n",
        "        sensitivities[i] = sensitivity\n",
        "\n",
        "    cov_sensitivities = {}\n",
        "    for i in range(Sigma.shape[0]):\n",
        "        for j in range(Sigma.shape[1]):\n",
        "            perturbed_Sigma = Sigma.copy()\n",
        "            perturbed_Sigma[i, j] += perturbation\n",
        "            weights_perturbed = optimize_weights(perturbed_Sigma, mu, lam, gamma, eta)\n",
        "            cov_sensitivities[(i, j)] = np.abs(weights_perturbed - original_weights)\n",
        "\n",
        "    return sensitivities, cov_sensitivities\n",
        "\n",
        "# -------------------------------\n",
        "# Cross-validation for Out-of-Sample Testing\n",
        "# -------------------------------\n",
        "\n",
        "def cross_validate_portfolio(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, n_splits=5):\n",
        "    \"\"\"Perform cross-validation to test the model's out-of-sample performance.\"\"\"\n",
        "\n",
        "    log_output(f\"Before Cross-validation:\")\n",
        "    log_output(f\"Sigma shape: {Sigma.shape}, mu length: {len(mu)}\")\n",
        "\n",
        "    # Manually split `mu` into `mu_train` and `mu_test`\n",
        "    log_output(f\"mu before splitting: {mu}\")\n",
        "    mu_train = mu[:2]  # First 2 values for training\n",
        "    mu_test = mu[2:]   # Last value for testing\n",
        "\n",
        "    # Verbosity: Output the dimensions of `mu_train` and `mu_test`\n",
        "    log_output(f\"mu_train before optimization: {mu_train}, mu_test before optimization: {mu_test}\")\n",
        "\n",
        "    validation_errors = []\n",
        "\n",
        "    # Perform cross-validation and calculate error using `mu_train` for training\n",
        "    optimized_weights = optimize_weights(Sigma, mu_train, lam, gamma, eta)\n",
        "\n",
        "    # Calculate the validation error (MSE)\n",
        "    validation_error = np.sum((Sigma @ optimized_weights - mu_test) ** 2)  # MSE as error\n",
        "    validation_errors.append(validation_error)\n",
        "\n",
        "    return np.mean(validation_errors)\n",
        "\n",
        "# -------------------------------\n",
        "# Portfolio Optimization with Kolmogorov Complexity Penalty\n",
        "# -------------------------------\n",
        "\n",
        "def kc_penalty(w, gamma=1e-2, eta=1e-2):\n",
        "    l0 = np.count_nonzero(w)  # L0 norm (sparsity)\n",
        "    l1 = np.sum(np.abs(w))    # L1 norm (weight magnitude)\n",
        "    return gamma * l0 + eta * l1\n",
        "\n",
        "# Objective Function: Risk-Return with KC Penalty\n",
        "def objective(w, Sigma, lam, gamma, eta):\n",
        "    risk = w.T @ Sigma @ w\n",
        "    penalty = kc_penalty(w, gamma, eta)\n",
        "    return risk + penalty\n",
        "\n",
        "# Gradient of the Objective Function\n",
        "def grad_objective(w, Sigma, lam, eta):\n",
        "    grad_risk = 2 * Sigma @ w  # Correct matrix multiplication\n",
        "    grad_l1 = eta * np.sign(w)\n",
        "    return grad_risk + grad_l1\n",
        "\n",
        "# Portfolio Optimization using Gradient Descent\n",
        "def optimize_weights(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, lr=1e-2, epochs=500):\n",
        "    w = np.ones(Sigma.shape[0]) / Sigma.shape[0]  # Equal distribution of weights\n",
        "    for epoch in range(epochs):\n",
        "        grad = grad_objective(w, Sigma, lam, eta)\n",
        "        w -= lr * grad\n",
        "        w = np.maximum(w, 0)  # Ensure non-negative weights\n",
        "        w /= np.sum(w + 1e-8)\n",
        "\n",
        "    return w\n",
        "\n",
        "# -------------------------------\n",
        "# Simulate Non-Normal Return Distributions (Fat-Tailed)\n",
        "# -------------------------------\n",
        "\n",
        "def simulate_fat_tailed_returns(Sigma, mu, dof=4, n_samples=1000):\n",
        "    \"\"\"Simulate returns from a fat-tailed distribution (Student's t-distribution).\"\"\"\n",
        "    t_returns = t.rvs(dof, size=(n_samples, len(mu)))\n",
        "    fat_tailed_returns = np.dot(t_returns, Sigma) + mu\n",
        "    return fat_tailed_returns\n",
        "\n",
        "# -------------------------------\n",
        "# Running the Tests\n",
        "# -------------------------------\n",
        "\n",
        "# Example Data (Covariance matrix and expected returns)\n",
        "Sigma = np.array([[0.02, 0.015, -0.005], [0.015, 0.03, -0.008], [-0.005, -0.008, 0.01]])\n",
        "mu = np.array([0.01, 0.005, -0.002])  # Expected returns for 3 assets\n",
        "\n",
        "# Cross-validation for Out-of-Sample Testing\n",
        "validation_error = cross_validate_portfolio(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2)\n",
        "\n",
        "log_output(f\"Validation Error (Cross-validation): {validation_error}\")\n",
        "\n",
        "# Stress test scenarios (extreme market events)\n",
        "extreme_scenarios = [\n",
        "    {'name': 'Black Swan', 'sigma_change': 0.1 * np.identity(3)},\n",
        "    {'name': 'Liquidity Crisis', 'sigma_change': 0.2 * np.identity(3)},\n",
        "]\n",
        "\n",
        "# Stress Test Results\n",
        "stress_test_results = extreme_market_scenarios(Sigma, mu, extreme_scenarios)\n",
        "\n",
        "log_output(f\"Stress Test Results: {stress_test_results}\")\n",
        "\n",
        "# Higher-Order Sensitivity Analysis\n",
        "sensitivity_results, cov_sensitivity_results = higher_order_sensitivity(Sigma, mu)\n",
        "\n",
        "log_output(f\"Sensitivity Analysis: {sensitivity_results}\")\n",
        "log_output(f\"Covariance Sensitivity Analysis: {cov_sensitivity_results}\")\n",
        "\n",
        "# Simulate Fat-Tailed Returns\n",
        "fat_tailed_returns = simulate_fat_tailed_returns(Sigma, mu)\n",
        "\n",
        "log_output(f\"Fat-Tailed Returns Simulation (First 5 Samples): {fat_tailed_returns[:5]}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Monte Carlo Simulation Testing\n",
        "# -------------------------------\n",
        "\n",
        "def monte_carlo_testing(Sigma, mu, n_samples=1000):\n",
        "    \"\"\"Run Monte Carlo simulations for stress testing\"\"\"\n",
        "    results = []\n",
        "    for _ in range(n_samples):\n",
        "        simulated_Sigma = Sigma * np.random.uniform(0.8, 1.2)  # Vary covariance matrix\n",
        "        optimized_weights = optimize_weights(simulated_Sigma, mu)\n",
        "        results.append(optimized_weights)\n",
        "\n",
        "    return np.mean(results, axis=0)\n",
        "\n",
        "monte_carlo_results = monte_carlo_testing(Sigma, mu)\n",
        "\n",
        "log_output(f\"Monte Carlo Simulation Results: {monte_carlo_results}\")\n",
        "\n",
        "# Close the log file after writing all results\n",
        "log_file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trLElOqIw7AD",
        "outputId": "02ee6e93-05d7-456c-a81f-b19f23f1ee48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Cross-validation:\n",
            "Sigma shape: (3, 3), mu length: 3\n",
            "mu before splitting: [ 0.01   0.005 -0.002]\n",
            "mu_train before optimization: [0.01  0.005], mu_test before optimization: [-0.002]\n",
            "Validation Error (Cross-validation): 0.0002446900115481169\n",
            "Stress Test Results: {'Black Swan': array([0.30310816, 0.28021959, 0.41667223]), 'Liquidity Crisis': array([0.30304539, 0.28011253, 0.41684204])}\n",
            "Sensitivity Analysis: {0: array([0., 0., 0.]), 1: array([0., 0., 0.]), 2: array([0., 0., 0.])}\n",
            "Covariance Sensitivity Analysis: {(0, 0): array([0.10859313, 0.05322174, 0.05537139]), (0, 1): array([0.14100508, 0.06819476, 0.07281032]), (0, 2): array([0.17426175, 0.08355192, 0.09070983]), (1, 0): array([0.07264126, 0.14217716, 0.0695359 ]), (1, 1): array([0.05257926, 0.10096275, 0.04838349]), (1, 2): array([0.08533267, 0.16824985, 0.08291717]), (2, 0): array([0.06607221, 0.05773897, 0.12381117]), (2, 1): array([0.06309595, 0.05509467, 0.11819063]), (2, 2): array([0.05992316, 0.05228222, 0.11220538])}\n",
            "Fat-Tailed Returns Simulation (First 5 Samples): [[ 0.03304776  0.04298981 -0.01324711]\n",
            " [ 0.03510689  0.00356124  0.00312203]\n",
            " [ 0.01100999  0.012915   -0.00546602]\n",
            " [-0.06112611 -0.05106247  0.01789927]\n",
            " [ 0.01794002  0.00205534 -0.00093991]]\n",
            "Monte Carlo Simulation Results: [0.30333957 0.28063563 0.41602477]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Example synthetic data with a protected attribute (e.g., 'gender')\n",
        "data = {\n",
        "    'feature1': np.random.randn(1000),\n",
        "    'feature2': np.random.randn(1000),\n",
        "    'protected_attribute': np.random.choice([0, 1], size=1000),  # 0 or 1 (e.g., male/female)\n",
        "    'target': np.random.choice([0, 1], size=1000)  # 0 or 1 (e.g., approval or denial)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Train/test split\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standard scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train a logistic regression classifier\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate baseline performance\n",
        "print(f\"Initial Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Fairness metrics: Demographic parity (difference in approval rates)\n",
        "# Boolean indexing on the test set\n",
        "protected_group_0_mask = (df.iloc[y_test.index]['protected_attribute'] == 0)\n",
        "protected_group_1_mask = (df.iloc[y_test.index]['protected_attribute'] == 1)\n",
        "\n",
        "# Calculate approval rates for each group\n",
        "approval_rate_group_0 = y_pred[protected_group_0_mask].mean()\n",
        "approval_rate_group_1 = y_pred[protected_group_1_mask].mean()\n",
        "\n",
        "demographic_parity = np.abs(approval_rate_group_0 - approval_rate_group_1)\n",
        "print(f\"Demographic Parity: {demographic_parity:.4f}\")\n",
        "\n",
        "# Reweighting strategy: Adjust weights to correct for demographic parity\n",
        "# The weight adjustment factor is inversely proportional to the group's approval rate\n",
        "weights = np.where(df['protected_attribute'] == 0, approval_rate_group_1 / approval_rate_group_0, 1)\n",
        "print(f\"Adjusted Weights: {weights[:10]}\")  # Show first 10 adjusted weights\n",
        "\n",
        "# Since we're only fitting on the training set, we need the weights for the training set\n",
        "weights_train = weights[:len(y_train)]  # Adjust weights to match training set size\n",
        "\n",
        "# Refit the model with adjusted weights\n",
        "clf.fit(X_train, y_train, sample_weight=weights_train)\n",
        "\n",
        "# Make new predictions with corrected model\n",
        "y_pred_corrected = clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance after fairness adjustment\n",
        "print(f\"Corrected Accuracy: {accuracy_score(y_test, y_pred_corrected):.4f}\")\n",
        "\n",
        "# Recalculate demographic parity after correction\n",
        "approval_rate_group_0_corrected = y_pred_corrected[protected_group_0_mask].mean()\n",
        "approval_rate_group_1_corrected = y_pred_corrected[protected_group_1_mask].mean()\n",
        "\n",
        "demographic_parity_corrected = np.abs(approval_rate_group_0_corrected - approval_rate_group_1_corrected)\n",
        "print(f\"Corrected Demographic Parity: {demographic_parity_corrected:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfu_im3jxsyW",
        "outputId": "3fa980e0-6f8b-4fdc-b023-4ebaf95f2bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Accuracy: 0.4900\n",
            "Demographic Parity: 0.0141\n",
            "Adjusted Weights: [1.01515079 1.01515079 1.01515079 1.01515079 1.         1.\n",
            " 1.01515079 1.01515079 1.         1.        ]\n",
            "Corrected Accuracy: 0.4933\n",
            "Corrected Demographic Parity: 0.0074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Example synthetic data with a protected attribute (e.g., 'gender')\n",
        "data = {\n",
        "    'feature1': np.random.randn(1000),\n",
        "    'feature2': np.random.randn(1000),\n",
        "    'protected_attribute': np.random.choice([0, 1], size=1000),  # 0 or 1 (e.g., male/female)\n",
        "    'target': np.random.choice([0, 1], size=1000)  # 0 or 1 (e.g., approval or denial)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Train/test split\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standard scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train a logistic regression classifier\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate baseline performance\n",
        "print(f\"Initial Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Fairness metrics: Demographic parity (difference in approval rates)\n",
        "# Boolean indexing on the test set\n",
        "protected_group_0_mask = (df.iloc[y_test.index]['protected_attribute'] == 0)\n",
        "protected_group_1_mask = (df.iloc[y_test.index]['protected_attribute'] == 1)\n",
        "\n",
        "# Calculate approval rates for each group\n",
        "approval_rate_group_0 = y_pred[protected_group_0_mask].mean()\n",
        "approval_rate_group_1 = y_pred[protected_group_1_mask].mean()\n",
        "\n",
        "demographic_parity = np.abs(approval_rate_group_0 - approval_rate_group_1)\n",
        "print(f\"Demographic Parity: {demographic_parity:.4f}\")\n",
        "\n",
        "# Additional fairness metrics\n",
        "\n",
        "# 1. Equal Opportunity (True Positive Rate)\n",
        "TPR_group_0 = (y_pred[protected_group_0_mask] == 1) & (y_test[protected_group_0_mask] == 1)\n",
        "TPR_group_1 = (y_pred[protected_group_1_mask] == 1) & (y_test[protected_group_1_mask] == 1)\n",
        "\n",
        "TPR_0 = TPR_group_0.sum() / protected_group_0_mask.sum()\n",
        "TPR_1 = TPR_group_1.sum() / protected_group_1_mask.sum()\n",
        "\n",
        "equal_opportunity = np.abs(TPR_0 - TPR_1)\n",
        "print(f\"Equal Opportunity: {equal_opportunity:.4f}\")\n",
        "\n",
        "# 2. Disparate Impact (Approval Rate Ratio)\n",
        "disparate_impact = approval_rate_group_1 / approval_rate_group_0\n",
        "print(f\"Disparate Impact: {disparate_impact:.4f}\")\n",
        "\n",
        "# Reweighting strategy: Adjust weights to correct for demographic parity\n",
        "# The weight adjustment factor is inversely proportional to the group's approval rate\n",
        "weights = np.where(df['protected_attribute'] == 0, approval_rate_group_1 / approval_rate_group_0, 1)\n",
        "print(f\"Adjusted Weights: {weights[:10]}\")  # Show first 10 adjusted weights\n",
        "\n",
        "# Since we're only fitting on the training set, we need the weights for the training set\n",
        "weights_train = weights[:len(y_train)]  # Adjust weights to match training set size\n",
        "\n",
        "# Refit the model with adjusted weights\n",
        "clf.fit(X_train, y_train, sample_weight=weights_train)\n",
        "\n",
        "# Make new predictions with corrected model\n",
        "y_pred_corrected = clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance after fairness adjustment\n",
        "print(f\"Corrected Accuracy: {accuracy_score(y_test, y_pred_corrected):.4f}\")\n",
        "\n",
        "# Recalculate fairness metrics after correction\n",
        "approval_rate_group_0_corrected = y_pred_corrected[protected_group_0_mask].mean()\n",
        "approval_rate_group_1_corrected = y_pred_corrected[protected_group_1_mask].mean()\n",
        "\n",
        "demographic_parity_corrected = np.abs(approval_rate_group_0_corrected - approval_rate_group_1_corrected)\n",
        "print(f\"Corrected Demographic Parity: {demographic_parity_corrected:.4f}\")\n",
        "\n",
        "# Recalculate Equal Opportunity (TPR)\n",
        "TPR_group_0_corrected = (y_pred_corrected[protected_group_0_mask] == 1) & (y_test[protected_group_0_mask] == 1)\n",
        "TPR_group_1_corrected = (y_pred_corrected[protected_group_1_mask] == 1) & (y_test[protected_group_1_mask] == 1)\n",
        "\n",
        "TPR_0_corrected = TPR_group_0_corrected.sum() / protected_group_0_mask.sum()\n",
        "TPR_1_corrected = TPR_group_1_corrected.sum() / protected_group_1_mask.sum()\n",
        "\n",
        "equal_opportunity_corrected = np.abs(TPR_0_corrected - TPR_1_corrected)\n",
        "print(f\"Corrected Equal Opportunity: {equal_opportunity_corrected:.4f}\")\n",
        "\n",
        "# Recalculate Disparate Impact after correction\n",
        "disparate_impact_corrected = approval_rate_group_1_corrected / approval_rate_group_0_corrected\n",
        "print(f\"Corrected Disparate Impact: {disparate_impact_corrected:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Is4cvl6yL9V",
        "outputId": "fb58840f-c874-4339-bcdb-eea28f8dc855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Accuracy: 0.5033\n",
            "Demographic Parity: 0.0448\n",
            "Equal Opportunity: 0.0035\n",
            "Disparate Impact: 0.9428\n",
            "Adjusted Weights: [0.9428278 1.        0.9428278 0.9428278 0.9428278 0.9428278 0.9428278\n",
            " 0.9428278 1.        1.       ]\n",
            "Corrected Accuracy: 0.5067\n",
            "Corrected Demographic Parity: 0.0521\n",
            "Corrected Equal Opportunity: 0.0035\n",
            "Corrected Disparate Impact: 0.9336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Example synthetic data with a protected attribute (e.g., 'gender')\n",
        "data = {\n",
        "    'feature1': np.random.randn(1000),\n",
        "    'feature2': np.random.randn(1000),\n",
        "    'protected_attribute': np.random.choice([0, 1], size=1000),  # 0 or 1 (e.g., male/female)\n",
        "    'target': np.random.choice([0, 1], size=1000)  # 0 or 1 (e.g., approval or denial)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Train/test split (Stratified to preserve the class distribution)\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Standard scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train a logistic regression classifier\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate baseline performance\n",
        "print(f\"Initial Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Fairness metrics: Demographic parity (difference in approval rates)\n",
        "# Boolean indexing on the test set\n",
        "protected_group_0_mask = (df.iloc[y_test.index]['protected_attribute'] == 0)\n",
        "protected_group_1_mask = (df.iloc[y_test.index]['protected_attribute'] == 1)\n",
        "\n",
        "# Calculate approval rates for each group\n",
        "approval_rate_group_0 = y_pred[protected_group_0_mask].mean()\n",
        "approval_rate_group_1 = y_pred[protected_group_1_mask].mean()\n",
        "\n",
        "demographic_parity = np.abs(approval_rate_group_0 - approval_rate_group_1)\n",
        "print(f\"Demographic Parity: {demographic_parity:.4f}\")\n",
        "\n",
        "# Additional fairness metrics\n",
        "\n",
        "# 1. Equal Opportunity (True Positive Rate)\n",
        "TPR_group_0 = (y_pred[protected_group_0_mask] == 1) & (y_test[protected_group_0_mask] == 1)\n",
        "TPR_group_1 = (y_pred[protected_group_1_mask] == 1) & (y_test[protected_group_1_mask] == 1)\n",
        "\n",
        "TPR_0 = TPR_group_0.sum() / protected_group_0_mask.sum()\n",
        "TPR_1 = TPR_group_1.sum() / protected_group_1_mask.sum()\n",
        "\n",
        "equal_opportunity = np.abs(TPR_0 - TPR_1)\n",
        "print(f\"Equal Opportunity: {equal_opportunity:.4f}\")\n",
        "\n",
        "# 2. Disparate Impact (Approval Rate Ratio)\n",
        "disparate_impact = approval_rate_group_1 / approval_rate_group_0\n",
        "print(f\"Disparate Impact: {disparate_impact:.4f}\")\n",
        "\n",
        "# Reweighting strategy: Adjust weights to correct for demographic parity\n",
        "# The weight adjustment factor is inversely proportional to the group's approval rate\n",
        "weights = np.where(df['protected_attribute'] == 0, approval_rate_group_1 / approval_rate_group_0, 1)\n",
        "print(f\"Adjusted Weights: {weights[:10]}\")  # Show first 10 adjusted weights\n",
        "\n",
        "# Since we're only fitting on the training set, we need the weights for the training set\n",
        "weights_train = weights[:len(y_train)]  # Adjust weights to match training set size\n",
        "\n",
        "# Refit the model with adjusted weights\n",
        "clf.fit(X_train, y_train, sample_weight=weights_train)\n",
        "\n",
        "# Make new predictions with corrected model\n",
        "y_pred_corrected = clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance after fairness adjustment\n",
        "print(f\"Corrected Accuracy: {accuracy_score(y_test, y_pred_corrected):.4f}\")\n",
        "\n",
        "# Recalculate fairness metrics after correction\n",
        "approval_rate_group_0_corrected = y_pred_corrected[protected_group_0_mask].mean()\n",
        "approval_rate_group_1_corrected = y_pred_corrected[protected_group_1_mask].mean()\n",
        "\n",
        "demographic_parity_corrected = np.abs(approval_rate_group_0_corrected - approval_rate_group_1_corrected)\n",
        "print(f\"Corrected Demographic Parity: {demographic_parity_corrected:.4f}\")\n",
        "\n",
        "# Recalculate Equal Opportunity (TPR)\n",
        "TPR_group_0_corrected = (y_pred_corrected[protected_group_0_mask] == 1) & (y_test[protected_group_0_mask] == 1)\n",
        "TPR_group_1_corrected = (y_pred_corrected[protected_group_1_mask] == 1) & (y_test[protected_group_1_mask] == 1)\n",
        "\n",
        "TPR_0_corrected = TPR_group_0_corrected.sum() / protected_group_0_mask.sum()\n",
        "TPR_1_corrected = TPR_group_1_corrected.sum() / protected_group_1_mask.sum()\n",
        "\n",
        "equal_opportunity_corrected = np.abs(TPR_0_corrected - TPR_1_corrected)\n",
        "print(f\"Corrected Equal Opportunity: {equal_opportunity_corrected:.4f}\")\n",
        "\n",
        "# Recalculate Disparate Impact after correction\n",
        "disparate_impact_corrected = approval_rate_group_1_corrected / approval_rate_group_0_corrected\n",
        "print(f\"Corrected Disparate Impact: {disparate_impact_corrected:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0Yrb_t0PpKW",
        "outputId": "2d9f6186-fae6-4e97-b6a2-83479b684e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Accuracy: 0.5100\n",
            "Demographic Parity: 0.0115\n",
            "Equal Opportunity: 0.0330\n",
            "Disparate Impact: 1.0183\n",
            "Adjusted Weights: [1.01830566 1.01830566 1.01830566 1.01830566 1.01830566 1.01830566\n",
            " 1.01830566 1.         1.         1.        ]\n",
            "Corrected Accuracy: 0.5100\n",
            "Corrected Demographic Parity: 0.0115\n",
            "Corrected Equal Opportunity: 0.0330\n",
            "Corrected Disparate Impact: 1.0183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define adversary network to predict the protected attribute\n",
        "class Adversary(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Adversary, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, 1)  # Predicting protected attribute\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Train the adversary to predict protected attribute\n",
        "def train_adversary(X_train, protected_attribute, epochs=100, lr=0.001):\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    protected_attribute_tensor = torch.tensor(protected_attribute, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    adversary = Adversary(X_train.shape[1])\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(adversary.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        output = adversary(X_train_tensor)\n",
        "        loss = criterion(output, protected_attribute_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return adversary\n",
        "\n",
        "# Train a reweighted model using adversarial debiasing\n",
        "def adversarial_debiasing(X_train, y_train, protected_attribute, epochs=100, lr=0.001):\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(X_train, y_train)  # Initial model fit\n",
        "\n",
        "    adversary = train_adversary(X_train, protected_attribute, epochs, lr)\n",
        "\n",
        "    # Reweight samples based on adversarial feedback\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    weights = np.ones(len(X_train))  # Initialize with equal weights\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        adversary_loss = adversary(X_train_tensor)  # Adversarial loss\n",
        "        loss = torch.mean(adversary_loss)  # Mean loss\n",
        "        clf.fit(X_train, y_train, sample_weight=loss.detach().numpy())  # Refit model with loss as weights\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "    return clf\n",
        "\n",
        "# Reject Option Classification\n",
        "def reject_option_classification(y_pred, X, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Rejects predictions in regions where the model is uncertain.\n",
        "    For example, if the model prediction is within the threshold margin, it won't be used.\n",
        "    \"\"\"\n",
        "    uncertain_mask = np.abs(y_pred - 0.5) < threshold  # Reject predictions close to 0.5\n",
        "    y_pred[uncertain_mask] = -1  # Mark those predictions as rejected (e.g., -1)\n",
        "    return y_pred\n",
        "\n",
        "# Apply reject option classification after predictions\n",
        "y_pred_adjusted = reject_option_classification(y_pred, X_test, threshold=0.2)\n",
        "\n",
        "# Evaluate performance after reject option classification\n",
        "print(f\"Accuracy after rejection: {accuracy_score(y_test, y_pred_adjusted):.4f}\")\n",
        "\n",
        "# Additional fairness metrics: Equalized Odds & Treatment Equality\n",
        "\n",
        "# 1. Equalized Odds (True Positive Rate & False Positive Rate)\n",
        "TPR_group_0 = (y_pred[protected_group_0_mask] == 1) & (y_test[protected_group_0_mask] == 1)\n",
        "TPR_group_1 = (y_pred[protected_group_1_mask] == 1) & (y_test[protected_group_1_mask] == 1)\n",
        "\n",
        "FPR_group_0 = (y_pred[protected_group_0_mask] == 1) & (y_test[protected_group_0_mask] == 0)\n",
        "FPR_group_1 = (y_pred[protected_group_1_mask] == 1) & (y_test[protected_group_1_mask] == 0)\n",
        "\n",
        "# Calculate TPR and FPR\n",
        "TPR_0_rate = TPR_group_0.sum() / protected_group_0_mask.sum()\n",
        "TPR_1_rate = TPR_group_1.sum() / protected_group_1_mask.sum()\n",
        "\n",
        "FPR_0_rate = FPR_group_0.sum() / protected_group_0_mask.sum()\n",
        "FPR_1_rate = FPR_group_1.sum() / protected_group_1_mask.sum()\n",
        "\n",
        "# Equalized Odds\n",
        "equalized_odds = np.abs(TPR_0_rate - TPR_1_rate) + np.abs(FPR_0_rate - FPR_1_rate)\n",
        "print(f\"Equalized Odds: {equalized_odds:.4f}\")\n",
        "\n",
        "# 2. Treatment Equality (Approval Rate Difference)\n",
        "treatment_equality = np.abs(approval_rate_group_0 - approval_rate_group_1)\n",
        "print(f\"Treatment Equality: {treatment_equality:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWynFl2sQDAy",
        "outputId": "4064482e-cff3-41ce-9586-b43edfd40292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy after rejection: 0.5100\n",
            "Equalized Odds: 0.0776\n",
            "Treatment Equality: 0.0115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the adversary model\n",
        "class Adversary(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Adversary, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, 1)  # Predicting protected attribute\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Adversarial Debiasing: Main Model + Adversary\n",
        "def adversarial_debiasing(X_train, y_train, protected_attribute, epochs=100, lr=0.001, lam=0.1):\n",
        "    # Initialize the main model (Logistic Regression)\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(X_train, y_train)  # Train the main model initially\n",
        "\n",
        "    # Define the adversary model\n",
        "    adversary = Adversary(1)  # Adversary will receive 1D predictions (probabilities) as input\n",
        "    criterion = nn.BCEWithLogitsLoss()  # For binary classification in adversary\n",
        "    optimizer = optim.Adam(adversary.parameters(), lr=lr)\n",
        "\n",
        "    # Convert data to tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "\n",
        "    # Only pass the protected attribute for the training samples (700 samples)\n",
        "    protected_attribute_tensor = torch.tensor(protected_attribute[:len(y_train)], dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # Training the adversary and model\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions using the logistic regression model\n",
        "        model_predictions = clf.predict_proba(X_train)[:, 1]  # Get predicted probabilities for class 1\n",
        "\n",
        "        # Ensure model predictions are reshaped to (n_samples, 1) for adversary\n",
        "        model_predictions_tensor = torch.tensor(model_predictions, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "        # Train the adversary to predict the protected attribute\n",
        "        adversary_predictions = adversary(model_predictions_tensor)\n",
        "\n",
        "        # Compute adversarial loss\n",
        "        adversary_loss = criterion(adversary_predictions, protected_attribute_tensor)\n",
        "\n",
        "        # Minimize adversarial loss\n",
        "        adversary_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Add regularization term for fairness (L1 penalty)\n",
        "        clf_weights = torch.tensor(clf.coef_.flatten(), dtype=torch.float32)\n",
        "        fairness_penalty = lam * torch.sum(torch.abs(clf_weights))  # L1 regularization for fairness\n",
        "\n",
        "        # Create weights based on the adversary loss for each sample\n",
        "        sample_weights = adversary_loss.detach().numpy() * np.ones(len(y_train))  # Expand loss for all samples\n",
        "\n",
        "        # Refit the main model with adversarial feedback as sample weights\n",
        "        clf.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Adversary Loss: {adversary_loss.item():.4f}\")\n",
        "\n",
        "    return clf, adversary\n",
        "\n",
        "# Example usage (using a simple synthetic dataset):\n",
        "# Example synthetic data with a protected attribute (e.g., 'gender')\n",
        "data = {\n",
        "    'feature1': np.random.randn(1000),\n",
        "    'feature2': np.random.randn(1000),\n",
        "    'protected_attribute': np.random.choice([0, 1], size=1000),  # 0 or 1 (e.g., male/female)\n",
        "    'target': np.random.choice([0, 1], size=1000)  # 0 or 1 (e.g., approval or denial)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "protected_attribute = df['protected_attribute']\n",
        "\n",
        "# Train/test split (Stratified to preserve the class distribution)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Standard scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Apply adversarial debiasing\n",
        "clf, adversary = adversarial_debiasing(X_train, y_train, protected_attribute, epochs=500, lr=0.001, lam=0.1)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f\"Accuracy after Adversarial Debiasing: {accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm3-zJxZQY-A",
        "outputId": "cc864025-545f-46db-b510-c12a5e0cf38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Adversary Loss: 0.7149\n",
            "Epoch 2/500, Adversary Loss: 0.7147\n",
            "Epoch 3/500, Adversary Loss: 0.7145\n",
            "Epoch 4/500, Adversary Loss: 0.7144\n",
            "Epoch 5/500, Adversary Loss: 0.7142\n",
            "Epoch 6/500, Adversary Loss: 0.7140\n",
            "Epoch 7/500, Adversary Loss: 0.7139\n",
            "Epoch 8/500, Adversary Loss: 0.7137\n",
            "Epoch 9/500, Adversary Loss: 0.7136\n",
            "Epoch 10/500, Adversary Loss: 0.7134\n",
            "Epoch 11/500, Adversary Loss: 0.7133\n",
            "Epoch 12/500, Adversary Loss: 0.7131\n",
            "Epoch 13/500, Adversary Loss: 0.7129\n",
            "Epoch 14/500, Adversary Loss: 0.7128\n",
            "Epoch 15/500, Adversary Loss: 0.7126\n",
            "Epoch 16/500, Adversary Loss: 0.7125\n",
            "Epoch 17/500, Adversary Loss: 0.7123\n",
            "Epoch 18/500, Adversary Loss: 0.7122\n",
            "Epoch 19/500, Adversary Loss: 0.7120\n",
            "Epoch 20/500, Adversary Loss: 0.7119\n",
            "Epoch 21/500, Adversary Loss: 0.7117\n",
            "Epoch 22/500, Adversary Loss: 0.7116\n",
            "Epoch 23/500, Adversary Loss: 0.7114\n",
            "Epoch 24/500, Adversary Loss: 0.7113\n",
            "Epoch 25/500, Adversary Loss: 0.7111\n",
            "Epoch 26/500, Adversary Loss: 0.7110\n",
            "Epoch 27/500, Adversary Loss: 0.7108\n",
            "Epoch 28/500, Adversary Loss: 0.7107\n",
            "Epoch 29/500, Adversary Loss: 0.7105\n",
            "Epoch 30/500, Adversary Loss: 0.7104\n",
            "Epoch 31/500, Adversary Loss: 0.7102\n",
            "Epoch 32/500, Adversary Loss: 0.7101\n",
            "Epoch 33/500, Adversary Loss: 0.7100\n",
            "Epoch 34/500, Adversary Loss: 0.7098\n",
            "Epoch 35/500, Adversary Loss: 0.7097\n",
            "Epoch 36/500, Adversary Loss: 0.7095\n",
            "Epoch 37/500, Adversary Loss: 0.7094\n",
            "Epoch 38/500, Adversary Loss: 0.7093\n",
            "Epoch 39/500, Adversary Loss: 0.7091\n",
            "Epoch 40/500, Adversary Loss: 0.7090\n",
            "Epoch 41/500, Adversary Loss: 0.7088\n",
            "Epoch 42/500, Adversary Loss: 0.7087\n",
            "Epoch 43/500, Adversary Loss: 0.7086\n",
            "Epoch 44/500, Adversary Loss: 0.7084\n",
            "Epoch 45/500, Adversary Loss: 0.7083\n",
            "Epoch 46/500, Adversary Loss: 0.7082\n",
            "Epoch 47/500, Adversary Loss: 0.7080\n",
            "Epoch 48/500, Adversary Loss: 0.7079\n",
            "Epoch 49/500, Adversary Loss: 0.7078\n",
            "Epoch 50/500, Adversary Loss: 0.7077\n",
            "Epoch 51/500, Adversary Loss: 0.7075\n",
            "Epoch 52/500, Adversary Loss: 0.7074\n",
            "Epoch 53/500, Adversary Loss: 0.7073\n",
            "Epoch 54/500, Adversary Loss: 0.7071\n",
            "Epoch 55/500, Adversary Loss: 0.7070\n",
            "Epoch 56/500, Adversary Loss: 0.7069\n",
            "Epoch 57/500, Adversary Loss: 0.7068\n",
            "Epoch 58/500, Adversary Loss: 0.7066\n",
            "Epoch 59/500, Adversary Loss: 0.7065\n",
            "Epoch 60/500, Adversary Loss: 0.7064\n",
            "Epoch 61/500, Adversary Loss: 0.7063\n",
            "Epoch 62/500, Adversary Loss: 0.7062\n",
            "Epoch 63/500, Adversary Loss: 0.7060\n",
            "Epoch 64/500, Adversary Loss: 0.7059\n",
            "Epoch 65/500, Adversary Loss: 0.7058\n",
            "Epoch 66/500, Adversary Loss: 0.7057\n",
            "Epoch 67/500, Adversary Loss: 0.7056\n",
            "Epoch 68/500, Adversary Loss: 0.7054\n",
            "Epoch 69/500, Adversary Loss: 0.7053\n",
            "Epoch 70/500, Adversary Loss: 0.7052\n",
            "Epoch 71/500, Adversary Loss: 0.7051\n",
            "Epoch 72/500, Adversary Loss: 0.7050\n",
            "Epoch 73/500, Adversary Loss: 0.7049\n",
            "Epoch 74/500, Adversary Loss: 0.7048\n",
            "Epoch 75/500, Adversary Loss: 0.7046\n",
            "Epoch 76/500, Adversary Loss: 0.7045\n",
            "Epoch 77/500, Adversary Loss: 0.7044\n",
            "Epoch 78/500, Adversary Loss: 0.7043\n",
            "Epoch 79/500, Adversary Loss: 0.7042\n",
            "Epoch 80/500, Adversary Loss: 0.7041\n",
            "Epoch 81/500, Adversary Loss: 0.7040\n",
            "Epoch 82/500, Adversary Loss: 0.7039\n",
            "Epoch 83/500, Adversary Loss: 0.7038\n",
            "Epoch 84/500, Adversary Loss: 0.7037\n",
            "Epoch 85/500, Adversary Loss: 0.7036\n",
            "Epoch 86/500, Adversary Loss: 0.7035\n",
            "Epoch 87/500, Adversary Loss: 0.7034\n",
            "Epoch 88/500, Adversary Loss: 0.7033\n",
            "Epoch 89/500, Adversary Loss: 0.7032\n",
            "Epoch 90/500, Adversary Loss: 0.7031\n",
            "Epoch 91/500, Adversary Loss: 0.7030\n",
            "Epoch 92/500, Adversary Loss: 0.7029\n",
            "Epoch 93/500, Adversary Loss: 0.7028\n",
            "Epoch 94/500, Adversary Loss: 0.7027\n",
            "Epoch 95/500, Adversary Loss: 0.7026\n",
            "Epoch 96/500, Adversary Loss: 0.7025\n",
            "Epoch 97/500, Adversary Loss: 0.7024\n",
            "Epoch 98/500, Adversary Loss: 0.7023\n",
            "Epoch 99/500, Adversary Loss: 0.7022\n",
            "Epoch 100/500, Adversary Loss: 0.7021\n",
            "Epoch 101/500, Adversary Loss: 0.7020\n",
            "Epoch 102/500, Adversary Loss: 0.7019\n",
            "Epoch 103/500, Adversary Loss: 0.7018\n",
            "Epoch 104/500, Adversary Loss: 0.7017\n",
            "Epoch 105/500, Adversary Loss: 0.7016\n",
            "Epoch 106/500, Adversary Loss: 0.7015\n",
            "Epoch 107/500, Adversary Loss: 0.7014\n",
            "Epoch 108/500, Adversary Loss: 0.7013\n",
            "Epoch 109/500, Adversary Loss: 0.7012\n",
            "Epoch 110/500, Adversary Loss: 0.7012\n",
            "Epoch 111/500, Adversary Loss: 0.7011\n",
            "Epoch 112/500, Adversary Loss: 0.7010\n",
            "Epoch 113/500, Adversary Loss: 0.7009\n",
            "Epoch 114/500, Adversary Loss: 0.7008\n",
            "Epoch 115/500, Adversary Loss: 0.7007\n",
            "Epoch 116/500, Adversary Loss: 0.7006\n",
            "Epoch 117/500, Adversary Loss: 0.7005\n",
            "Epoch 118/500, Adversary Loss: 0.7005\n",
            "Epoch 119/500, Adversary Loss: 0.7004\n",
            "Epoch 120/500, Adversary Loss: 0.7003\n",
            "Epoch 121/500, Adversary Loss: 0.7002\n",
            "Epoch 122/500, Adversary Loss: 0.7001\n",
            "Epoch 123/500, Adversary Loss: 0.7001\n",
            "Epoch 124/500, Adversary Loss: 0.7000\n",
            "Epoch 125/500, Adversary Loss: 0.6999\n",
            "Epoch 126/500, Adversary Loss: 0.6998\n",
            "Epoch 127/500, Adversary Loss: 0.6997\n",
            "Epoch 128/500, Adversary Loss: 0.6997\n",
            "Epoch 129/500, Adversary Loss: 0.6996\n",
            "Epoch 130/500, Adversary Loss: 0.6995\n",
            "Epoch 131/500, Adversary Loss: 0.6994\n",
            "Epoch 132/500, Adversary Loss: 0.6993\n",
            "Epoch 133/500, Adversary Loss: 0.6993\n",
            "Epoch 134/500, Adversary Loss: 0.6992\n",
            "Epoch 135/500, Adversary Loss: 0.6991\n",
            "Epoch 136/500, Adversary Loss: 0.6990\n",
            "Epoch 137/500, Adversary Loss: 0.6990\n",
            "Epoch 138/500, Adversary Loss: 0.6989\n",
            "Epoch 139/500, Adversary Loss: 0.6988\n",
            "Epoch 140/500, Adversary Loss: 0.6988\n",
            "Epoch 141/500, Adversary Loss: 0.6987\n",
            "Epoch 142/500, Adversary Loss: 0.6986\n",
            "Epoch 143/500, Adversary Loss: 0.6985\n",
            "Epoch 144/500, Adversary Loss: 0.6985\n",
            "Epoch 145/500, Adversary Loss: 0.6984\n",
            "Epoch 146/500, Adversary Loss: 0.6983\n",
            "Epoch 147/500, Adversary Loss: 0.6983\n",
            "Epoch 148/500, Adversary Loss: 0.6982\n",
            "Epoch 149/500, Adversary Loss: 0.6981\n",
            "Epoch 150/500, Adversary Loss: 0.6981\n",
            "Epoch 151/500, Adversary Loss: 0.6980\n",
            "Epoch 152/500, Adversary Loss: 0.6979\n",
            "Epoch 153/500, Adversary Loss: 0.6979\n",
            "Epoch 154/500, Adversary Loss: 0.6978\n",
            "Epoch 155/500, Adversary Loss: 0.6977\n",
            "Epoch 156/500, Adversary Loss: 0.6977\n",
            "Epoch 157/500, Adversary Loss: 0.6976\n",
            "Epoch 158/500, Adversary Loss: 0.6975\n",
            "Epoch 159/500, Adversary Loss: 0.6975\n",
            "Epoch 160/500, Adversary Loss: 0.6974\n",
            "Epoch 161/500, Adversary Loss: 0.6974\n",
            "Epoch 162/500, Adversary Loss: 0.6973\n",
            "Epoch 163/500, Adversary Loss: 0.6972\n",
            "Epoch 164/500, Adversary Loss: 0.6972\n",
            "Epoch 165/500, Adversary Loss: 0.6971\n",
            "Epoch 166/500, Adversary Loss: 0.6971\n",
            "Epoch 167/500, Adversary Loss: 0.6970\n",
            "Epoch 168/500, Adversary Loss: 0.6969\n",
            "Epoch 169/500, Adversary Loss: 0.6969\n",
            "Epoch 170/500, Adversary Loss: 0.6968\n",
            "Epoch 171/500, Adversary Loss: 0.6968\n",
            "Epoch 172/500, Adversary Loss: 0.6967\n",
            "Epoch 173/500, Adversary Loss: 0.6966\n",
            "Epoch 174/500, Adversary Loss: 0.6966\n",
            "Epoch 175/500, Adversary Loss: 0.6965\n",
            "Epoch 176/500, Adversary Loss: 0.6965\n",
            "Epoch 177/500, Adversary Loss: 0.6964\n",
            "Epoch 178/500, Adversary Loss: 0.6964\n",
            "Epoch 179/500, Adversary Loss: 0.6963\n",
            "Epoch 180/500, Adversary Loss: 0.6963\n",
            "Epoch 181/500, Adversary Loss: 0.6962\n",
            "Epoch 182/500, Adversary Loss: 0.6962\n",
            "Epoch 183/500, Adversary Loss: 0.6961\n",
            "Epoch 184/500, Adversary Loss: 0.6961\n",
            "Epoch 185/500, Adversary Loss: 0.6960\n",
            "Epoch 186/500, Adversary Loss: 0.6960\n",
            "Epoch 187/500, Adversary Loss: 0.6959\n",
            "Epoch 188/500, Adversary Loss: 0.6959\n",
            "Epoch 189/500, Adversary Loss: 0.6958\n",
            "Epoch 190/500, Adversary Loss: 0.6958\n",
            "Epoch 191/500, Adversary Loss: 0.6957\n",
            "Epoch 192/500, Adversary Loss: 0.6957\n",
            "Epoch 193/500, Adversary Loss: 0.6956\n",
            "Epoch 194/500, Adversary Loss: 0.6956\n",
            "Epoch 195/500, Adversary Loss: 0.6955\n",
            "Epoch 196/500, Adversary Loss: 0.6955\n",
            "Epoch 197/500, Adversary Loss: 0.6954\n",
            "Epoch 198/500, Adversary Loss: 0.6954\n",
            "Epoch 199/500, Adversary Loss: 0.6953\n",
            "Epoch 200/500, Adversary Loss: 0.6953\n",
            "Epoch 201/500, Adversary Loss: 0.6952\n",
            "Epoch 202/500, Adversary Loss: 0.6952\n",
            "Epoch 203/500, Adversary Loss: 0.6952\n",
            "Epoch 204/500, Adversary Loss: 0.6951\n",
            "Epoch 205/500, Adversary Loss: 0.6951\n",
            "Epoch 206/500, Adversary Loss: 0.6950\n",
            "Epoch 207/500, Adversary Loss: 0.6950\n",
            "Epoch 208/500, Adversary Loss: 0.6949\n",
            "Epoch 209/500, Adversary Loss: 0.6949\n",
            "Epoch 210/500, Adversary Loss: 0.6949\n",
            "Epoch 211/500, Adversary Loss: 0.6948\n",
            "Epoch 212/500, Adversary Loss: 0.6948\n",
            "Epoch 213/500, Adversary Loss: 0.6947\n",
            "Epoch 214/500, Adversary Loss: 0.6947\n",
            "Epoch 215/500, Adversary Loss: 0.6947\n",
            "Epoch 216/500, Adversary Loss: 0.6946\n",
            "Epoch 217/500, Adversary Loss: 0.6946\n",
            "Epoch 218/500, Adversary Loss: 0.6945\n",
            "Epoch 219/500, Adversary Loss: 0.6945\n",
            "Epoch 220/500, Adversary Loss: 0.6945\n",
            "Epoch 221/500, Adversary Loss: 0.6944\n",
            "Epoch 222/500, Adversary Loss: 0.6944\n",
            "Epoch 223/500, Adversary Loss: 0.6944\n",
            "Epoch 224/500, Adversary Loss: 0.6943\n",
            "Epoch 225/500, Adversary Loss: 0.6943\n",
            "Epoch 226/500, Adversary Loss: 0.6942\n",
            "Epoch 227/500, Adversary Loss: 0.6942\n",
            "Epoch 228/500, Adversary Loss: 0.6942\n",
            "Epoch 229/500, Adversary Loss: 0.6941\n",
            "Epoch 230/500, Adversary Loss: 0.6941\n",
            "Epoch 231/500, Adversary Loss: 0.6941\n",
            "Epoch 232/500, Adversary Loss: 0.6940\n",
            "Epoch 233/500, Adversary Loss: 0.6940\n",
            "Epoch 234/500, Adversary Loss: 0.6940\n",
            "Epoch 235/500, Adversary Loss: 0.6939\n",
            "Epoch 236/500, Adversary Loss: 0.6939\n",
            "Epoch 237/500, Adversary Loss: 0.6939\n",
            "Epoch 238/500, Adversary Loss: 0.6938\n",
            "Epoch 239/500, Adversary Loss: 0.6938\n",
            "Epoch 240/500, Adversary Loss: 0.6938\n",
            "Epoch 241/500, Adversary Loss: 0.6937\n",
            "Epoch 242/500, Adversary Loss: 0.6937\n",
            "Epoch 243/500, Adversary Loss: 0.6937\n",
            "Epoch 244/500, Adversary Loss: 0.6937\n",
            "Epoch 245/500, Adversary Loss: 0.6936\n",
            "Epoch 246/500, Adversary Loss: 0.6936\n",
            "Epoch 247/500, Adversary Loss: 0.6936\n",
            "Epoch 248/500, Adversary Loss: 0.6935\n",
            "Epoch 249/500, Adversary Loss: 0.6935\n",
            "Epoch 250/500, Adversary Loss: 0.6935\n",
            "Epoch 251/500, Adversary Loss: 0.6934\n",
            "Epoch 252/500, Adversary Loss: 0.6934\n",
            "Epoch 253/500, Adversary Loss: 0.6934\n",
            "Epoch 254/500, Adversary Loss: 0.6934\n",
            "Epoch 255/500, Adversary Loss: 0.6933\n",
            "Epoch 256/500, Adversary Loss: 0.6933\n",
            "Epoch 257/500, Adversary Loss: 0.6933\n",
            "Epoch 258/500, Adversary Loss: 0.6933\n",
            "Epoch 259/500, Adversary Loss: 0.6932\n",
            "Epoch 260/500, Adversary Loss: 0.6932\n",
            "Epoch 261/500, Adversary Loss: 0.6932\n",
            "Epoch 262/500, Adversary Loss: 0.6932\n",
            "Epoch 263/500, Adversary Loss: 0.6931\n",
            "Epoch 264/500, Adversary Loss: 0.6931\n",
            "Epoch 265/500, Adversary Loss: 0.6931\n",
            "Epoch 266/500, Adversary Loss: 0.6931\n",
            "Epoch 267/500, Adversary Loss: 0.6930\n",
            "Epoch 268/500, Adversary Loss: 0.6930\n",
            "Epoch 269/500, Adversary Loss: 0.6930\n",
            "Epoch 270/500, Adversary Loss: 0.6930\n",
            "Epoch 271/500, Adversary Loss: 0.6929\n",
            "Epoch 272/500, Adversary Loss: 0.6929\n",
            "Epoch 273/500, Adversary Loss: 0.6929\n",
            "Epoch 274/500, Adversary Loss: 0.6929\n",
            "Epoch 275/500, Adversary Loss: 0.6928\n",
            "Epoch 276/500, Adversary Loss: 0.6928\n",
            "Epoch 277/500, Adversary Loss: 0.6928\n",
            "Epoch 278/500, Adversary Loss: 0.6928\n",
            "Epoch 279/500, Adversary Loss: 0.6927\n",
            "Epoch 280/500, Adversary Loss: 0.6927\n",
            "Epoch 281/500, Adversary Loss: 0.6927\n",
            "Epoch 282/500, Adversary Loss: 0.6927\n",
            "Epoch 283/500, Adversary Loss: 0.6927\n",
            "Epoch 284/500, Adversary Loss: 0.6926\n",
            "Epoch 285/500, Adversary Loss: 0.6926\n",
            "Epoch 286/500, Adversary Loss: 0.6926\n",
            "Epoch 287/500, Adversary Loss: 0.6926\n",
            "Epoch 288/500, Adversary Loss: 0.6926\n",
            "Epoch 289/500, Adversary Loss: 0.6925\n",
            "Epoch 290/500, Adversary Loss: 0.6925\n",
            "Epoch 291/500, Adversary Loss: 0.6925\n",
            "Epoch 292/500, Adversary Loss: 0.6925\n",
            "Epoch 293/500, Adversary Loss: 0.6925\n",
            "Epoch 294/500, Adversary Loss: 0.6924\n",
            "Epoch 295/500, Adversary Loss: 0.6924\n",
            "Epoch 296/500, Adversary Loss: 0.6924\n",
            "Epoch 297/500, Adversary Loss: 0.6924\n",
            "Epoch 298/500, Adversary Loss: 0.6924\n",
            "Epoch 299/500, Adversary Loss: 0.6924\n",
            "Epoch 300/500, Adversary Loss: 0.6923\n",
            "Epoch 301/500, Adversary Loss: 0.6923\n",
            "Epoch 302/500, Adversary Loss: 0.6923\n",
            "Epoch 303/500, Adversary Loss: 0.6923\n",
            "Epoch 304/500, Adversary Loss: 0.6923\n",
            "Epoch 305/500, Adversary Loss: 0.6923\n",
            "Epoch 306/500, Adversary Loss: 0.6922\n",
            "Epoch 307/500, Adversary Loss: 0.6922\n",
            "Epoch 308/500, Adversary Loss: 0.6922\n",
            "Epoch 309/500, Adversary Loss: 0.6922\n",
            "Epoch 310/500, Adversary Loss: 0.6922\n",
            "Epoch 311/500, Adversary Loss: 0.6922\n",
            "Epoch 312/500, Adversary Loss: 0.6921\n",
            "Epoch 313/500, Adversary Loss: 0.6921\n",
            "Epoch 314/500, Adversary Loss: 0.6921\n",
            "Epoch 315/500, Adversary Loss: 0.6921\n",
            "Epoch 316/500, Adversary Loss: 0.6921\n",
            "Epoch 317/500, Adversary Loss: 0.6921\n",
            "Epoch 318/500, Adversary Loss: 0.6921\n",
            "Epoch 319/500, Adversary Loss: 0.6920\n",
            "Epoch 320/500, Adversary Loss: 0.6920\n",
            "Epoch 321/500, Adversary Loss: 0.6920\n",
            "Epoch 322/500, Adversary Loss: 0.6920\n",
            "Epoch 323/500, Adversary Loss: 0.6920\n",
            "Epoch 324/500, Adversary Loss: 0.6920\n",
            "Epoch 325/500, Adversary Loss: 0.6920\n",
            "Epoch 326/500, Adversary Loss: 0.6919\n",
            "Epoch 327/500, Adversary Loss: 0.6919\n",
            "Epoch 328/500, Adversary Loss: 0.6919\n",
            "Epoch 329/500, Adversary Loss: 0.6919\n",
            "Epoch 330/500, Adversary Loss: 0.6919\n",
            "Epoch 331/500, Adversary Loss: 0.6919\n",
            "Epoch 332/500, Adversary Loss: 0.6919\n",
            "Epoch 333/500, Adversary Loss: 0.6919\n",
            "Epoch 334/500, Adversary Loss: 0.6918\n",
            "Epoch 335/500, Adversary Loss: 0.6918\n",
            "Epoch 336/500, Adversary Loss: 0.6918\n",
            "Epoch 337/500, Adversary Loss: 0.6918\n",
            "Epoch 338/500, Adversary Loss: 0.6918\n",
            "Epoch 339/500, Adversary Loss: 0.6918\n",
            "Epoch 340/500, Adversary Loss: 0.6918\n",
            "Epoch 341/500, Adversary Loss: 0.6918\n",
            "Epoch 342/500, Adversary Loss: 0.6918\n",
            "Epoch 343/500, Adversary Loss: 0.6917\n",
            "Epoch 344/500, Adversary Loss: 0.6917\n",
            "Epoch 345/500, Adversary Loss: 0.6917\n",
            "Epoch 346/500, Adversary Loss: 0.6917\n",
            "Epoch 347/500, Adversary Loss: 0.6917\n",
            "Epoch 348/500, Adversary Loss: 0.6917\n",
            "Epoch 349/500, Adversary Loss: 0.6917\n",
            "Epoch 350/500, Adversary Loss: 0.6917\n",
            "Epoch 351/500, Adversary Loss: 0.6917\n",
            "Epoch 352/500, Adversary Loss: 0.6917\n",
            "Epoch 353/500, Adversary Loss: 0.6916\n",
            "Epoch 354/500, Adversary Loss: 0.6916\n",
            "Epoch 355/500, Adversary Loss: 0.6916\n",
            "Epoch 356/500, Adversary Loss: 0.6916\n",
            "Epoch 357/500, Adversary Loss: 0.6916\n",
            "Epoch 358/500, Adversary Loss: 0.6916\n",
            "Epoch 359/500, Adversary Loss: 0.6916\n",
            "Epoch 360/500, Adversary Loss: 0.6916\n",
            "Epoch 361/500, Adversary Loss: 0.6916\n",
            "Epoch 362/500, Adversary Loss: 0.6916\n",
            "Epoch 363/500, Adversary Loss: 0.6916\n",
            "Epoch 364/500, Adversary Loss: 0.6915\n",
            "Epoch 365/500, Adversary Loss: 0.6915\n",
            "Epoch 366/500, Adversary Loss: 0.6915\n",
            "Epoch 367/500, Adversary Loss: 0.6915\n",
            "Epoch 368/500, Adversary Loss: 0.6915\n",
            "Epoch 369/500, Adversary Loss: 0.6915\n",
            "Epoch 370/500, Adversary Loss: 0.6915\n",
            "Epoch 371/500, Adversary Loss: 0.6915\n",
            "Epoch 372/500, Adversary Loss: 0.6915\n",
            "Epoch 373/500, Adversary Loss: 0.6915\n",
            "Epoch 374/500, Adversary Loss: 0.6915\n",
            "Epoch 375/500, Adversary Loss: 0.6915\n",
            "Epoch 376/500, Adversary Loss: 0.6914\n",
            "Epoch 377/500, Adversary Loss: 0.6914\n",
            "Epoch 378/500, Adversary Loss: 0.6914\n",
            "Epoch 379/500, Adversary Loss: 0.6914\n",
            "Epoch 380/500, Adversary Loss: 0.6914\n",
            "Epoch 381/500, Adversary Loss: 0.6914\n",
            "Epoch 382/500, Adversary Loss: 0.6914\n",
            "Epoch 383/500, Adversary Loss: 0.6914\n",
            "Epoch 384/500, Adversary Loss: 0.6914\n",
            "Epoch 385/500, Adversary Loss: 0.6914\n",
            "Epoch 386/500, Adversary Loss: 0.6914\n",
            "Epoch 387/500, Adversary Loss: 0.6914\n",
            "Epoch 388/500, Adversary Loss: 0.6914\n",
            "Epoch 389/500, Adversary Loss: 0.6914\n",
            "Epoch 390/500, Adversary Loss: 0.6914\n",
            "Epoch 391/500, Adversary Loss: 0.6913\n",
            "Epoch 392/500, Adversary Loss: 0.6913\n",
            "Epoch 393/500, Adversary Loss: 0.6913\n",
            "Epoch 394/500, Adversary Loss: 0.6913\n",
            "Epoch 395/500, Adversary Loss: 0.6913\n",
            "Epoch 396/500, Adversary Loss: 0.6913\n",
            "Epoch 397/500, Adversary Loss: 0.6913\n",
            "Epoch 398/500, Adversary Loss: 0.6913\n",
            "Epoch 399/500, Adversary Loss: 0.6913\n",
            "Epoch 400/500, Adversary Loss: 0.6913\n",
            "Epoch 401/500, Adversary Loss: 0.6913\n",
            "Epoch 402/500, Adversary Loss: 0.6913\n",
            "Epoch 403/500, Adversary Loss: 0.6913\n",
            "Epoch 404/500, Adversary Loss: 0.6913\n",
            "Epoch 405/500, Adversary Loss: 0.6913\n",
            "Epoch 406/500, Adversary Loss: 0.6913\n",
            "Epoch 407/500, Adversary Loss: 0.6913\n",
            "Epoch 408/500, Adversary Loss: 0.6913\n",
            "Epoch 409/500, Adversary Loss: 0.6913\n",
            "Epoch 410/500, Adversary Loss: 0.6912\n",
            "Epoch 411/500, Adversary Loss: 0.6912\n",
            "Epoch 412/500, Adversary Loss: 0.6912\n",
            "Epoch 413/500, Adversary Loss: 0.6912\n",
            "Epoch 414/500, Adversary Loss: 0.6912\n",
            "Epoch 415/500, Adversary Loss: 0.6912\n",
            "Epoch 416/500, Adversary Loss: 0.6912\n",
            "Epoch 417/500, Adversary Loss: 0.6912\n",
            "Epoch 418/500, Adversary Loss: 0.6912\n",
            "Epoch 419/500, Adversary Loss: 0.6912\n",
            "Epoch 420/500, Adversary Loss: 0.6912\n",
            "Epoch 421/500, Adversary Loss: 0.6912\n",
            "Epoch 422/500, Adversary Loss: 0.6912\n",
            "Epoch 423/500, Adversary Loss: 0.6912\n",
            "Epoch 424/500, Adversary Loss: 0.6912\n",
            "Epoch 425/500, Adversary Loss: 0.6912\n",
            "Epoch 426/500, Adversary Loss: 0.6912\n",
            "Epoch 427/500, Adversary Loss: 0.6912\n",
            "Epoch 428/500, Adversary Loss: 0.6912\n",
            "Epoch 429/500, Adversary Loss: 0.6912\n",
            "Epoch 430/500, Adversary Loss: 0.6912\n",
            "Epoch 431/500, Adversary Loss: 0.6912\n",
            "Epoch 432/500, Adversary Loss: 0.6912\n",
            "Epoch 433/500, Adversary Loss: 0.6912\n",
            "Epoch 434/500, Adversary Loss: 0.6911\n",
            "Epoch 435/500, Adversary Loss: 0.6911\n",
            "Epoch 436/500, Adversary Loss: 0.6911\n",
            "Epoch 437/500, Adversary Loss: 0.6911\n",
            "Epoch 438/500, Adversary Loss: 0.6911\n",
            "Epoch 439/500, Adversary Loss: 0.6911\n",
            "Epoch 440/500, Adversary Loss: 0.6911\n",
            "Epoch 441/500, Adversary Loss: 0.6911\n",
            "Epoch 442/500, Adversary Loss: 0.6911\n",
            "Epoch 443/500, Adversary Loss: 0.6911\n",
            "Epoch 444/500, Adversary Loss: 0.6911\n",
            "Epoch 445/500, Adversary Loss: 0.6911\n",
            "Epoch 446/500, Adversary Loss: 0.6911\n",
            "Epoch 447/500, Adversary Loss: 0.6911\n",
            "Epoch 448/500, Adversary Loss: 0.6911\n",
            "Epoch 449/500, Adversary Loss: 0.6911\n",
            "Epoch 450/500, Adversary Loss: 0.6911\n",
            "Epoch 451/500, Adversary Loss: 0.6911\n",
            "Epoch 452/500, Adversary Loss: 0.6911\n",
            "Epoch 453/500, Adversary Loss: 0.6911\n",
            "Epoch 454/500, Adversary Loss: 0.6911\n",
            "Epoch 455/500, Adversary Loss: 0.6911\n",
            "Epoch 456/500, Adversary Loss: 0.6911\n",
            "Epoch 457/500, Adversary Loss: 0.6911\n",
            "Epoch 458/500, Adversary Loss: 0.6911\n",
            "Epoch 459/500, Adversary Loss: 0.6911\n",
            "Epoch 460/500, Adversary Loss: 0.6911\n",
            "Epoch 461/500, Adversary Loss: 0.6911\n",
            "Epoch 462/500, Adversary Loss: 0.6911\n",
            "Epoch 463/500, Adversary Loss: 0.6911\n",
            "Epoch 464/500, Adversary Loss: 0.6911\n",
            "Epoch 465/500, Adversary Loss: 0.6911\n",
            "Epoch 466/500, Adversary Loss: 0.6911\n",
            "Epoch 467/500, Adversary Loss: 0.6911\n",
            "Epoch 468/500, Adversary Loss: 0.6911\n",
            "Epoch 469/500, Adversary Loss: 0.6911\n",
            "Epoch 470/500, Adversary Loss: 0.6911\n",
            "Epoch 471/500, Adversary Loss: 0.6910\n",
            "Epoch 472/500, Adversary Loss: 0.6910\n",
            "Epoch 473/500, Adversary Loss: 0.6910\n",
            "Epoch 474/500, Adversary Loss: 0.6910\n",
            "Epoch 475/500, Adversary Loss: 0.6910\n",
            "Epoch 476/500, Adversary Loss: 0.6910\n",
            "Epoch 477/500, Adversary Loss: 0.6910\n",
            "Epoch 478/500, Adversary Loss: 0.6910\n",
            "Epoch 479/500, Adversary Loss: 0.6910\n",
            "Epoch 480/500, Adversary Loss: 0.6910\n",
            "Epoch 481/500, Adversary Loss: 0.6910\n",
            "Epoch 482/500, Adversary Loss: 0.6910\n",
            "Epoch 483/500, Adversary Loss: 0.6910\n",
            "Epoch 484/500, Adversary Loss: 0.6910\n",
            "Epoch 485/500, Adversary Loss: 0.6910\n",
            "Epoch 486/500, Adversary Loss: 0.6910\n",
            "Epoch 487/500, Adversary Loss: 0.6910\n",
            "Epoch 488/500, Adversary Loss: 0.6910\n",
            "Epoch 489/500, Adversary Loss: 0.6910\n",
            "Epoch 490/500, Adversary Loss: 0.6910\n",
            "Epoch 491/500, Adversary Loss: 0.6910\n",
            "Epoch 492/500, Adversary Loss: 0.6910\n",
            "Epoch 493/500, Adversary Loss: 0.6910\n",
            "Epoch 494/500, Adversary Loss: 0.6910\n",
            "Epoch 495/500, Adversary Loss: 0.6910\n",
            "Epoch 496/500, Adversary Loss: 0.6910\n",
            "Epoch 497/500, Adversary Loss: 0.6910\n",
            "Epoch 498/500, Adversary Loss: 0.6910\n",
            "Epoch 499/500, Adversary Loss: 0.6910\n",
            "Epoch 500/500, Adversary Loss: 0.6910\n",
            "Accuracy after Adversarial Debiasing: 0.4667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the adversary model with a deeper neural network\n",
        "class Adversary(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Adversary, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)  # Hidden layer with 64 units\n",
        "        self.fc2 = nn.Linear(64, 32)          # Another hidden layer with 32 units\n",
        "        self.fc3 = nn.Linear(32, 1)           # Output layer predicting the protected attribute\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # Apply ReLU activation function\n",
        "        x = torch.relu(self.fc2(x))  # Apply ReLU activation function\n",
        "        return self.fc3(x)           # Output the final prediction\n",
        "\n",
        "# Adversarial Debiasing: Main Model + Adversary\n",
        "def adversarial_debiasing(X_train, y_train, protected_attribute, epochs=100, lr_main=0.001, lr_adv=0.001, lam=0.1):\n",
        "    # Initialize the main model (Logistic Regression)\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(X_train, y_train)  # Train the main model initially\n",
        "\n",
        "    # Define the adversary model (Deeper neural network)\n",
        "    adversary = Adversary(1)  # Adversary will receive 1D predictions (probabilities) as input\n",
        "    criterion = nn.BCEWithLogitsLoss()  # For binary classification in adversary\n",
        "    optimizer_adv = optim.Adam(adversary.parameters(), lr=lr_adv)  # Only optimize the adversary\n",
        "\n",
        "    # Convert data to tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    protected_attribute_tensor = torch.tensor(protected_attribute[:len(y_train)], dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # Training the adversary and model\n",
        "    for epoch in range(epochs):\n",
        "        optimizer_adv.zero_grad()\n",
        "\n",
        "        # Make predictions using the logistic regression model\n",
        "        model_predictions = clf.predict_proba(X_train)[:, 1]  # Get predicted probabilities for class 1\n",
        "\n",
        "        # Ensure model predictions are reshaped to (n_samples, 1) for adversary\n",
        "        model_predictions_tensor = torch.tensor(model_predictions, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "        # Train the adversary to predict the protected attribute\n",
        "        adversary_predictions = adversary(model_predictions_tensor)\n",
        "\n",
        "        # Compute adversarial loss\n",
        "        adversary_loss = criterion(adversary_predictions, protected_attribute_tensor)\n",
        "\n",
        "        # Apply L2 regularization to adversary\n",
        "        l2_reg = sum(param.pow(2).sum() for param in adversary.parameters())\n",
        "        total_loss = adversary_loss + lam * l2_reg\n",
        "\n",
        "        # Minimize adversarial loss\n",
        "        total_loss.backward()\n",
        "        optimizer_adv.step()\n",
        "\n",
        "        # Use adversary loss as sample weights (broadcast to the whole training set)\n",
        "        sample_weights = adversary_loss.detach().numpy() * np.ones(len(y_train))  # Create a sample weights array\n",
        "\n",
        "        # Refit the main model with adversarial feedback as sample weights\n",
        "        clf.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "\n",
        "        # Add fairness penalty for main model (L1 regularization)\n",
        "        clf_weights = torch.tensor(clf.coef_.flatten(), dtype=torch.float32)\n",
        "        fairness_penalty = lam * torch.sum(torch.abs(clf_weights))  # L1 regularization for fairness\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Adversary Loss: {adversary_loss.item():.4f}, Fairness Penalty: {fairness_penalty.item():.4f}\")\n",
        "\n",
        "    return clf, adversary\n",
        "\n",
        "# Example usage (using a simple synthetic dataset):\n",
        "# Example synthetic data with a protected attribute (e.g., 'gender')\n",
        "data = {\n",
        "    'feature1': np.random.randn(1000),\n",
        "    'feature2': np.random.randn(1000),\n",
        "    'protected_attribute': np.random.choice([0, 1], size=1000),  # 0 or 1 (e.g., male/female)\n",
        "    'target': np.random.choice([0, 1], size=1000)  # 0 or 1 (e.g., approval or denial)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "protected_attribute = df['protected_attribute']\n",
        "\n",
        "# Train/test split (Stratified to preserve the class distribution)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Standard scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Apply adversarial debiasing\n",
        "clf, adversary = adversarial_debiasing(X_train, y_train, protected_attribute, epochs=500, lr_main=0.001, lr_adv=0.001, lam=0.1)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f\"Accuracy after Adversarial Debiasing: {accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI7nEu7jQo7I",
        "outputId": "925cd288-d71b-4950-947a-ce7d58277b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Adversary Loss: 0.6930, Fairness Penalty: 0.0024\n",
            "Epoch 2/500, Adversary Loss: 0.6929, Fairness Penalty: 0.0024\n",
            "Epoch 3/500, Adversary Loss: 0.6928, Fairness Penalty: 0.0024\n",
            "Epoch 4/500, Adversary Loss: 0.6928, Fairness Penalty: 0.0024\n",
            "Epoch 5/500, Adversary Loss: 0.6927, Fairness Penalty: 0.0024\n",
            "Epoch 6/500, Adversary Loss: 0.6927, Fairness Penalty: 0.0024\n",
            "Epoch 7/500, Adversary Loss: 0.6927, Fairness Penalty: 0.0024\n",
            "Epoch 8/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0024\n",
            "Epoch 9/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0024\n",
            "Epoch 10/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0024\n",
            "Epoch 11/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0024\n",
            "Epoch 12/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0024\n",
            "Epoch 13/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0024\n",
            "Epoch 14/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0024\n",
            "Epoch 15/500, Adversary Loss: 0.6924, Fairness Penalty: 0.0024\n",
            "Epoch 16/500, Adversary Loss: 0.6924, Fairness Penalty: 0.0024\n",
            "Epoch 17/500, Adversary Loss: 0.6924, Fairness Penalty: 0.0024\n",
            "Epoch 18/500, Adversary Loss: 0.6924, Fairness Penalty: 0.0024\n",
            "Epoch 19/500, Adversary Loss: 0.6924, Fairness Penalty: 0.0024\n",
            "Epoch 20/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 21/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 22/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 23/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 24/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 25/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 26/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 27/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 28/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 29/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 30/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 31/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 32/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 33/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 34/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 35/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 36/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 37/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 38/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 39/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 40/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 41/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 42/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 43/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 44/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 45/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 46/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 47/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 48/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 49/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 50/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 51/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 52/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 53/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 54/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 55/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 56/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 57/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 58/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 59/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 60/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 61/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 62/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 63/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 64/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 65/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 66/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 67/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 68/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 69/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 70/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 71/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 72/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 73/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 74/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 75/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 76/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 77/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 78/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 79/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 80/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 81/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 82/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 83/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 84/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 85/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 86/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 87/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 88/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 89/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 90/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 91/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 92/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 93/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 94/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 95/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 96/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 97/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 98/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 99/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 100/500, Adversary Loss: 0.6921, Fairness Penalty: 0.0024\n",
            "Epoch 101/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 102/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 103/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 104/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 105/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 106/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 107/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 108/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 109/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 110/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 111/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 112/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 113/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 114/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 115/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 116/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 117/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 118/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 119/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 120/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 121/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 122/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 123/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 124/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 125/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 126/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 127/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 128/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 129/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 130/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 131/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 132/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 133/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 134/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 135/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 136/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 137/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 138/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 139/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 140/500, Adversary Loss: 0.6922, Fairness Penalty: 0.0024\n",
            "Epoch 141/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 142/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 143/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 144/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 145/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 146/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 147/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 148/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 149/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 150/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 151/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 152/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 153/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 154/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 155/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 156/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 157/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 158/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 159/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 160/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 161/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 162/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 163/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 164/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 165/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 166/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 167/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 168/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 169/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 170/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 171/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 172/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 173/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 174/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 175/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 176/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 177/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 178/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 179/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 180/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 181/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 182/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 183/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 184/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 185/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 186/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 187/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 188/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 189/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 190/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 191/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 192/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 193/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 194/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 195/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 196/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 197/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 198/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 199/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 200/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 201/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 202/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 203/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 204/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 205/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 206/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 207/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 208/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 209/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 210/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 211/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 212/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 213/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 214/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 215/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 216/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 217/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 218/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 219/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 220/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 221/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 222/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 223/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 224/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 225/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 226/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 227/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 228/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 229/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 230/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 231/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 232/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 233/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 234/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 235/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 236/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 237/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 238/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 239/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 240/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 241/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 242/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 243/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 244/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 245/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 246/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 247/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 248/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 249/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 250/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 251/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 252/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 253/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 254/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 255/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 256/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 257/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 258/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 259/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 260/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 261/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 262/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 263/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 264/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 265/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 266/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 267/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 268/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 269/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 270/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 271/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 272/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 273/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 274/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 275/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 276/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 277/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 278/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 279/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 280/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 281/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 282/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 283/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 284/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 285/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 286/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 287/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 288/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 289/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 290/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 291/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 292/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 293/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 294/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 295/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 296/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 297/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 298/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 299/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 300/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 301/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 302/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 303/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 304/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 305/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 306/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 307/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 308/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 309/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 310/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 311/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 312/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 313/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 314/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 315/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 316/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 317/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 318/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 319/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 320/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 321/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 322/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 323/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 324/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 325/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 326/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 327/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 328/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 329/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 330/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 331/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 332/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 333/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 334/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 335/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 336/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 337/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 338/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 339/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 340/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 341/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 342/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 343/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 344/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 345/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 346/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 347/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 348/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 349/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 350/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 351/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 352/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 353/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 354/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 355/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 356/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 357/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 358/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 359/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 360/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 361/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 362/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 363/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 364/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 365/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 366/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 367/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 368/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 369/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 370/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 371/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 372/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 373/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 374/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 375/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 376/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 377/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 378/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 379/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 380/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 381/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 382/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 383/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 384/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 385/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 386/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 387/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 388/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 389/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 390/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 391/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 392/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 393/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 394/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 395/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 396/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 397/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 398/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 399/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 400/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 401/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 402/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 403/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 404/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 405/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 406/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 407/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 408/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 409/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 410/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 411/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 412/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 413/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 414/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 415/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 416/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 417/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 418/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 419/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 420/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 421/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 422/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 423/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 424/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 425/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 426/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 427/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 428/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 429/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 430/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 431/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 432/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 433/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 434/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 435/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 436/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 437/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 438/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 439/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 440/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 441/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 442/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 443/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 444/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 445/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 446/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 447/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 448/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 449/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 450/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 451/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 452/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 453/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 454/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 455/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 456/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 457/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 458/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 459/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 460/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 461/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 462/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 463/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 464/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 465/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 466/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 467/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 468/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 469/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 470/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 471/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 472/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 473/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 474/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 475/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 476/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 477/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 478/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 479/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 480/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 481/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 482/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 483/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 484/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 485/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 486/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 487/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 488/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 489/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 490/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 491/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 492/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 493/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 494/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 495/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 496/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 497/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 498/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 499/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Epoch 500/500, Adversary Loss: 0.6923, Fairness Penalty: 0.0024\n",
            "Accuracy after Adversarial Debiasing: 0.5033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F  # Importing the functional module for LeakyReLU\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the adversary model with more layers and Leaky ReLU activation\n",
        "class Adversary(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Adversary, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)  # Increase hidden units to 128\n",
        "        self.fc2 = nn.Linear(128, 64)          # Another hidden layer with 64 units\n",
        "        self.fc3 = nn.Linear(64, 32)           # Another hidden layer with 32 units\n",
        "        self.fc4 = nn.Linear(32, 1)            # Output layer predicting the protected attribute\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), negative_slope=0.01)  # Use Leaky ReLU\n",
        "        x = F.leaky_relu(self.fc2(x), negative_slope=0.01)\n",
        "        x = F.leaky_relu(self.fc3(x), negative_slope=0.01)\n",
        "        return self.fc4(x)  # Output the final prediction\n",
        "\n",
        "# Adversarial Debiasing: Main Model + Adversary\n",
        "def adversarial_debiasing(X_train, y_train, protected_attribute, epochs=100, lr_main=0.001, lr_adv=0.001, lam=0.1):\n",
        "    # Initialize the main model (Logistic Regression)\n",
        "    clf = LogisticRegression(max_iter=1000)  # Ensure enough iterations for convergence\n",
        "    clf.fit(X_train, y_train)  # Train the main model initially\n",
        "\n",
        "    # Define the adversary model (Deeper neural network with more layers)\n",
        "    adversary = Adversary(1)  # Adversary will receive 1D predictions (probabilities) as input\n",
        "    criterion = nn.BCEWithLogitsLoss()  # For binary classification in adversary\n",
        "    optimizer_adv = optim.Adam(adversary.parameters(), lr=lr_adv)  # Only optimize the adversary\n",
        "\n",
        "    # Convert data to tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    protected_attribute_tensor = torch.tensor(protected_attribute[:len(y_train)], dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # Training the adversary and model\n",
        "    for epoch in range(epochs):\n",
        "        optimizer_adv.zero_grad()\n",
        "\n",
        "        # Make predictions using the logistic regression model\n",
        "        model_predictions = clf.predict_proba(X_train)[:, 1]  # Get predicted probabilities for class 1\n",
        "\n",
        "        # Ensure model predictions are reshaped to (n_samples, 1) for adversary\n",
        "        model_predictions_tensor = torch.tensor(model_predictions, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "        # Train the adversary to predict the protected attribute\n",
        "        adversary_predictions = adversary(model_predictions_tensor)\n",
        "\n",
        "        # Compute adversarial loss\n",
        "        adversary_loss = criterion(adversary_predictions, protected_attribute_tensor)\n",
        "\n",
        "        # Apply L2 regularization to adversary\n",
        "        l2_reg = sum(param.pow(2).sum() for param in adversary.parameters())\n",
        "        total_loss = adversary_loss + lam * l2_reg\n",
        "\n",
        "        # Minimize adversarial loss\n",
        "        total_loss.backward()\n",
        "        optimizer_adv.step()\n",
        "\n",
        "        # Use adversary loss as sample weights (broadcast to the whole training set)\n",
        "        sample_weights = adversary_loss.detach().numpy() * np.ones(len(y_train))  # Create a sample weights array\n",
        "\n",
        "        # Refit the main model with adversarial feedback as sample weights\n",
        "        clf.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "\n",
        "        # Add fairness penalty for main model (L1 regularization)\n",
        "        clf_weights = torch.tensor(clf.coef_.flatten(), dtype=torch.float32)\n",
        "        fairness_penalty = lam * torch.sum(torch.abs(clf_weights))  # L1 regularization for fairness\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Adversary Loss: {adversary_loss.item():.4f}, Fairness Penalty: {fairness_penalty.item():.4f}\")\n",
        "\n",
        "    return clf, adversary\n",
        "\n",
        "# Example usage (using a simple synthetic dataset):\n",
        "# Example synthetic data with a protected attribute (e.g., 'gender')\n",
        "data = {\n",
        "    'feature1': np.random.randn(1000),\n",
        "    'feature2': np.random.randn(1000),\n",
        "    'protected_attribute': np.random.choice([0, 1], size=1000),  # 0 or 1 (e.g., male/female)\n",
        "    'target': np.random.choice([0, 1], size=1000)  # 0 or 1 (e.g., approval or denial)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "protected_attribute = df['protected_attribute']\n",
        "\n",
        "# Train/test split (Stratified to preserve the class distribution)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Standard scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Apply adversarial debiasing\n",
        "clf, adversary = adversarial_debiasing(X_train, y_train, protected_attribute, epochs=500, lr_main=0.001, lr_adv=0.001, lam=0.1)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f\"Accuracy after Adversarial Debiasing: {accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb_xoKDfRqIR",
        "outputId": "d9fe5c1f-7948-4862-8a49-fa148aed23b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 2/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 3/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 4/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 5/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 6/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 7/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 8/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 9/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 10/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 11/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 12/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 13/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 14/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 15/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 16/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 17/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 18/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 19/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 20/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 21/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 22/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 23/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 24/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 25/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 26/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 27/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 28/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 29/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 30/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 31/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 32/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 33/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 34/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 35/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 36/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 37/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 38/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 39/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 40/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 41/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 42/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 43/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 44/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 45/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 46/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 47/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 48/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 49/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 50/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 51/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 52/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 53/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 54/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 55/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 56/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 57/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 58/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 59/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 60/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 61/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 62/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 63/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 64/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 65/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 66/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 67/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 68/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 69/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 70/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 71/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 72/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 73/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 74/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 75/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 76/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 77/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 78/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 79/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 80/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 81/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 82/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 83/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 84/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 85/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 86/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 87/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 88/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 89/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 90/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 91/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 92/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 93/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 94/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 95/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 96/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 97/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 98/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 99/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 100/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 101/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 102/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 103/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 104/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 105/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 106/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 107/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 108/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 109/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 110/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 111/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 112/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 113/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 114/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 115/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 116/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 117/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 118/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 119/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 120/500, Adversary Loss: 0.6925, Fairness Penalty: 0.0025\n",
            "Epoch 121/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 122/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 123/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 124/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 125/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 126/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 127/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 128/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 129/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 130/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 131/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 132/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 133/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 134/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 135/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 136/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 137/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 138/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 139/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 140/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 141/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 142/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 143/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 144/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 145/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 146/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 147/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 148/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 149/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 150/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 151/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 152/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 153/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 154/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 155/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 156/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 157/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 158/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 159/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 160/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 161/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 162/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 163/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 164/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 165/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 166/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 167/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 168/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 169/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 170/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 171/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 172/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 173/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 174/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 175/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 176/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 177/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 178/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 179/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 180/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 181/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 182/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 183/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 184/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 185/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 186/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 187/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 188/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 189/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 190/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 191/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 192/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 193/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 194/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 195/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 196/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 197/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 198/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 199/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 200/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 201/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 202/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 203/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 204/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 205/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 206/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 207/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 208/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 209/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 210/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 211/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 212/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 213/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 214/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 215/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 216/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 217/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 218/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 219/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 220/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 221/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 222/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 223/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 224/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 225/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 226/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 227/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 228/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 229/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 230/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 231/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 232/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 233/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 234/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 235/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 236/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 237/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 238/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 239/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 240/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 241/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 242/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 243/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 244/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 245/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 246/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 247/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 248/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 249/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 250/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 251/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 252/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 253/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 254/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 255/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 256/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 257/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 258/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 259/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 260/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 261/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 262/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 263/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 264/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 265/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 266/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 267/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 268/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 269/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 270/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 271/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 272/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 273/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 274/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 275/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 276/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 277/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 278/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 279/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 280/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 281/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 282/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 283/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 284/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 285/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 286/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 287/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 288/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 289/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 290/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 291/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 292/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 293/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 294/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 295/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 296/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 297/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 298/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 299/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 300/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 301/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 302/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 303/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 304/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 305/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 306/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 307/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 308/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 309/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 310/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 311/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 312/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 313/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 314/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 315/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 316/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 317/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 318/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 319/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 320/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 321/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 322/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 323/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 324/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 325/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 326/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 327/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 328/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 329/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 330/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 331/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 332/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 333/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 334/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 335/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 336/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 337/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 338/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 339/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 340/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 341/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 342/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 343/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 344/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 345/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 346/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 347/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 348/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 349/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 350/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 351/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 352/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 353/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 354/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 355/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 356/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 357/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 358/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 359/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 360/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 361/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 362/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 363/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 364/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 365/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 366/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 367/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 368/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 369/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 370/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 371/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 372/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 373/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 374/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 375/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 376/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 377/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 378/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 379/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 380/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 381/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 382/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 383/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 384/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 385/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 386/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 387/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 388/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 389/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 390/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 391/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 392/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 393/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 394/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 395/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 396/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 397/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 398/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 399/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 400/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 401/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 402/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 403/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 404/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 405/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 406/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 407/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 408/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 409/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 410/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 411/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 412/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 413/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 414/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 415/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 416/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 417/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 418/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 419/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 420/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 421/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 422/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 423/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 424/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 425/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 426/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 427/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 428/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 429/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 430/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 431/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 432/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 433/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 434/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 435/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 436/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 437/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 438/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 439/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 440/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 441/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 442/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 443/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 444/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 445/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 446/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 447/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 448/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 449/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 450/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 451/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 452/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 453/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 454/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 455/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 456/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 457/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 458/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 459/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 460/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 461/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 462/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 463/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 464/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 465/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 466/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 467/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 468/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 469/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 470/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 471/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 472/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 473/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 474/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 475/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 476/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 477/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 478/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 479/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 480/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 481/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 482/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 483/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 484/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 485/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 486/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 487/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 488/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 489/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 490/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 491/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 492/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 493/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 494/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 495/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 496/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 497/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 498/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 499/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Epoch 500/500, Adversary Loss: 0.6926, Fairness Penalty: 0.0025\n",
            "Accuracy after Adversarial Debiasing: 0.5267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------\n",
        "# Define Main Model (Classifier)\n",
        "# -------------------\n",
        "\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MainModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)  # Output class probabilities (before softmax)\n",
        "\n",
        "# -------------------\n",
        "# Define Adversary Network\n",
        "# -------------------\n",
        "\n",
        "class Adversary(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(Adversary, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # Takes output of the main model\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Output: Bias prediction for protected attribute\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return torch.sigmoid(self.fc2(x))  # Sigmoid output for binary prediction (protected attribute)\n",
        "\n",
        "# -------------------\n",
        "# Adversarial Debiasing Function\n",
        "# -------------------\n",
        "\n",
        "def adversarial_debiasing(X_train, y_train, protected_attribute, epochs=500, lr_main=0.001, lr_adv=0.001, lam=0.1):\n",
        "    # Convert numpy arrays to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Reshaping for binary classification\n",
        "    protected_attribute = torch.tensor(protected_attribute, dtype=torch.float32).view(-1, 1)  # Binary protected attribute\n",
        "\n",
        "    # Initialize models\n",
        "    input_dim = X_train.shape[1]\n",
        "    hidden_dim = 128\n",
        "    main_model = MainModel(input_dim, hidden_dim, 1)  # Main model outputs 1 value\n",
        "    adversary = Adversary(1, hidden_dim, 1)  # Adversary outputs 1 value (protected attribute prediction)\n",
        "\n",
        "    # Loss functions\n",
        "    criterion_main = nn.BCEWithLogitsLoss()  # For main model (binary classification)\n",
        "    criterion_adv = nn.BCELoss()  # For adversary (binary cross-entropy loss)\n",
        "\n",
        "    # Optimizers\n",
        "    optimizer_main = optim.Adam(main_model.parameters(), lr=lr_main)\n",
        "    optimizer_adv = optim.Adam(adversary.parameters(), lr=lr_adv)\n",
        "\n",
        "    # DataLoader\n",
        "    train_data = TensorDataset(X_train, y_train, protected_attribute)\n",
        "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        main_model.train()\n",
        "        adversary.train()\n",
        "\n",
        "        running_loss_main = 0.0\n",
        "        running_loss_adv = 0.0\n",
        "        running_fairness_penalty = 0.0\n",
        "\n",
        "        for batch_idx, (inputs, targets, protected) in enumerate(train_loader):\n",
        "            # Forward pass through the main model\n",
        "            optimizer_main.zero_grad()\n",
        "            optimizer_adv.zero_grad()\n",
        "\n",
        "            main_output = main_model(inputs)  # Main model output (logits)\n",
        "            main_loss = criterion_main(main_output, targets)  # Main loss\n",
        "\n",
        "            # Forward pass through the adversary\n",
        "            adversary_output = adversary(main_output)  # Adversary input is the output of the main model\n",
        "            adversary_loss = criterion_adv(adversary_output, protected)  # Adversary loss\n",
        "\n",
        "            # Fairness penalty (the adversary's objective is to predict the protected attribute)\n",
        "            fairness_penalty = torch.mean((adversary_output - protected) ** 2)\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = main_loss + lam * adversary_loss + fairness_penalty\n",
        "\n",
        "            # Backpropagation\n",
        "            total_loss.backward()\n",
        "\n",
        "            # Optimizers step\n",
        "            optimizer_main.step()\n",
        "            optimizer_adv.step()\n",
        "\n",
        "            # Track losses\n",
        "            running_loss_main += main_loss.item()\n",
        "            running_loss_adv += adversary_loss.item()\n",
        "            running_fairness_penalty += fairness_penalty.item()\n",
        "\n",
        "        # Print losses\n",
        "        if epoch % 50 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Main Loss: {running_loss_main / len(train_loader):.4f}, '\n",
        "                  f'Adversary Loss: {running_loss_adv / len(train_loader):.4f}, '\n",
        "                  f'Fairness Penalty: {running_fairness_penalty / len(train_loader):.4f}')\n",
        "\n",
        "    return main_model, adversary\n",
        "\n",
        "# -------------------\n",
        "# Generate Synthetic Data\n",
        "# -------------------\n",
        "\n",
        "# Generate synthetic data for illustration\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "n_features = 5\n",
        "\n",
        "X = np.random.randn(n_samples, n_features)  # Random features\n",
        "y = np.random.choice([0, 1], size=n_samples)  # Random binary target (0 or 1)\n",
        "protected_attribute = np.random.choice([0, 1], size=n_samples)  # Binary protected attribute (e.g., gender)\n",
        "\n",
        "# -------------------\n",
        "# Preprocess the Data\n",
        "# -------------------\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test, protected_train, protected_test = train_test_split(X, y, protected_attribute, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# -------------------\n",
        "# Apply Adversarial Debiasing\n",
        "# -------------------\n",
        "\n",
        "# Apply adversarial debiasing\n",
        "clf, adversary = adversarial_debiasing(X_train, y_train, protected_train, epochs=500, lr_main=0.001, lr_adv=0.001, lam=0.1)\n",
        "\n",
        "# -------------------\n",
        "# Evaluate the model performance\n",
        "# -------------------\n",
        "\n",
        "# Evaluation on the test set\n",
        "clf.eval()\n",
        "adversary.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred = torch.sigmoid(clf(torch.tensor(X_test, dtype=torch.float32)))  # Predict using the main model\n",
        "    y_pred = (y_pred > 0.5).float()\n",
        "\n",
        "    accuracy = (y_pred.view(-1) == torch.tensor(y_test, dtype=torch.float32)).float().mean()\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRfQpFxMSPnY",
        "outputId": "e5311c56-af0c-4247-dfea-bfb117c5b76f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Main Loss: 0.6950, Adversary Loss: 0.6940, Fairness Penalty: 0.2504\n",
            "Epoch 51/500, Main Loss: 0.6705, Adversary Loss: 0.6824, Fairness Penalty: 0.2447\n",
            "Epoch 101/500, Main Loss: 0.6629, Adversary Loss: 0.6412, Fairness Penalty: 0.2255\n",
            "Epoch 151/500, Main Loss: 0.6595, Adversary Loss: 0.6040, Fairness Penalty: 0.2078\n",
            "Epoch 201/500, Main Loss: 0.6589, Adversary Loss: 0.5471, Fairness Penalty: 0.1842\n",
            "Epoch 251/500, Main Loss: 0.6571, Adversary Loss: 0.4942, Fairness Penalty: 0.1614\n",
            "Epoch 301/500, Main Loss: 0.6564, Adversary Loss: 0.4563, Fairness Penalty: 0.1453\n",
            "Epoch 351/500, Main Loss: 0.6570, Adversary Loss: 0.4244, Fairness Penalty: 0.1323\n",
            "Epoch 401/500, Main Loss: 0.6572, Adversary Loss: 0.3905, Fairness Penalty: 0.1188\n",
            "Epoch 451/500, Main Loss: 0.6575, Adversary Loss: 0.3672, Fairness Penalty: 0.1086\n",
            "Test Accuracy: 0.5233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the main model\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MainModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)  # Hidden layer for adversary\n",
        "        self.fc3 = nn.Linear(32, 1)   # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        hidden_output = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(hidden_output)\n",
        "        return x, hidden_output  # Main output and hidden output for adversary\n",
        "\n",
        "# Define the adversary model\n",
        "class Adversary(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Adversary, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 2)  # Binary classification (protected attribute)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Function to apply adversarial debiasing\n",
        "def adversarial_debiasing(X_train, y_train, protected_attribute, epochs=500, lr_main=0.001, lr_adv=0.001, lam=0.1):\n",
        "    # Prepare data\n",
        "    train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "    # Convert protected_attribute to torch tensor\n",
        "    protected_attribute = torch.tensor(protected_attribute, dtype=torch.long)  # Ensure it's a tensor\n",
        "\n",
        "    # Initialize models\n",
        "    main_model = MainModel(input_dim=X_train.shape[1])\n",
        "    adversary = Adversary(input_dim=32)  # Hidden output dimension from main model\n",
        "\n",
        "    # Optimizers\n",
        "    optimizer_main = optim.AdamW(main_model.parameters(), lr=lr_main, weight_decay=1e-4)\n",
        "    optimizer_adv = optim.AdamW(adversary.parameters(), lr=lr_adv, weight_decay=1e-4)\n",
        "\n",
        "    # Loss functions\n",
        "    criterion_main = nn.BCEWithLogitsLoss()  # Main loss function\n",
        "    criterion_adv = nn.CrossEntropyLoss()  # Adversary loss function\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        main_model.train()\n",
        "        adversary.train()\n",
        "\n",
        "        total_main_loss = 0.0\n",
        "        total_adv_loss = 0.0\n",
        "        total_fairness_penalty = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (data, target) in enumerate(train_loader):\n",
        "            batch_size = data.size(0)\n",
        "\n",
        "            # Ensure adversary target matches the batch size\n",
        "            adv_target = protected_attribute[i*batch_size:(i+1)*batch_size]\n",
        "\n",
        "            # Forward pass through main model\n",
        "            optimizer_main.zero_grad()\n",
        "            optimizer_adv.zero_grad()\n",
        "\n",
        "            # Main model output and intermediate hidden output for adversary\n",
        "            main_output, hidden_output = main_model(data)\n",
        "\n",
        "            adversary_output = adversary(hidden_output)\n",
        "\n",
        "            # Compute adversary loss (cross entropy)\n",
        "            adv_loss = criterion_adv(adversary_output, adv_target)\n",
        "\n",
        "            # Compute fairness penalty\n",
        "            fairness_penalty = adv_loss * lam\n",
        "\n",
        "            # Compute main model loss (BCE)\n",
        "            main_loss = criterion_main(main_output.squeeze(), target)\n",
        "\n",
        "            # Backpropagate main loss\n",
        "            main_loss.backward(retain_graph=True)  # Retain graph for the adversary backward pass\n",
        "\n",
        "            # Backpropagate adversary loss\n",
        "            adv_loss.backward()  # No need to retain graph for the main model loss anymore\n",
        "\n",
        "            optimizer_main.step()\n",
        "            optimizer_adv.step()\n",
        "\n",
        "            total_main_loss += main_loss.item()\n",
        "            total_adv_loss += adv_loss.item()\n",
        "            total_fairness_penalty += fairness_penalty.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            pred = (main_output.squeeze() > 0.5).float()\n",
        "            correct += (pred == target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "        # Print loss statistics\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Main Loss: {total_main_loss/len(train_loader)}, Adversary Loss: {total_adv_loss/len(train_loader)}, Fairness Penalty: {total_fairness_penalty/len(train_loader)}\")\n",
        "\n",
        "    return main_model, adversary\n",
        "\n",
        "# Apply adversarial debiasing\n",
        "main_model, adversary = adversarial_debiasing(X_train, y_train, protected_attribute, epochs=500, lr_main=0.001, lr_adv=0.001, lam=0.1)\n",
        "\n",
        "# Test the trained model\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "        output, _ = model(X_test_tensor)  # Extract only the main model's output\n",
        "        predictions = (output.squeeze() > 0.5).float()  # Apply squeeze and threshold for binary classification\n",
        "        accuracy = accuracy_score(y_test_tensor, predictions)\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate on test set\n",
        "test_accuracy = evaluate_model(main_model, X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL7W1M1bSpr9",
        "outputId": "26b6cbeb-fd20-4715-ad00-5a3e1a677d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/500, Main Loss: 0.6373029026118192, Adversary Loss: 0.6931596127423373, Fairness Penalty: 0.06931596317074516\n",
            "Epoch 100/500, Main Loss: 0.566653620113026, Adversary Loss: 0.6926060535691001, Fairness Penalty: 0.06926060671156103\n",
            "Epoch 150/500, Main Loss: 0.5131757855415344, Adversary Loss: 0.6940768740393899, Fairness Penalty: 0.06940768930045041\n",
            "Epoch 200/500, Main Loss: 0.47360814159566705, Adversary Loss: 0.6937413920055736, Fairness Penalty: 0.06937414001334798\n",
            "Epoch 250/500, Main Loss: 0.43007502230730926, Adversary Loss: 0.6931272149085999, Fairness Penalty: 0.06931272352283652\n",
            "Epoch 300/500, Main Loss: 0.3877239227294922, Adversary Loss: 0.6927694353190336, Fairness Penalty: 0.06927694448015907\n",
            "Epoch 350/500, Main Loss: 0.3526949042623693, Adversary Loss: 0.6930981440977617, Fairness Penalty: 0.0693098164417527\n",
            "Epoch 400/500, Main Loss: 0.31459875811230054, Adversary Loss: 0.6929379268126055, Fairness Penalty: 0.06929379430684177\n",
            "Epoch 450/500, Main Loss: 0.28530093214728613, Adversary Loss: 0.6930021372708407, Fairness Penalty: 0.0693002153526653\n",
            "Epoch 500/500, Main Loss: 0.2521738044240258, Adversary Loss: 0.6928804300048135, Fairness Penalty: 0.06928804449059746\n",
            "Test Accuracy: 0.4867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define Main Model\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MainModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)  # Expecting input of shape (batch_size, input_dim)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
        "        x = self.fc2(x)  # Output layer\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# Define Adversary Model\n",
        "class AdversaryModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(AdversaryModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)  # Adversary expects an input of size 128 (from the main model)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
        "        x = self.fc2(x)  # Output layer for adversary\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "# Define the adversarial debiasing function\n",
        "def adversarial_debiasing(X_train, y_train, protected_attribute, epochs=500, lr_main=0.001, lr_adv=0.001, lam=0.1):\n",
        "    # Prepare data\n",
        "    train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
        "                                torch.tensor(y_train, dtype=torch.float32),\n",
        "                                torch.tensor(protected_attribute, dtype=torch.float32))\n",
        "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "    # Initialize Models\n",
        "    input_dim = X_train.shape[1]  # Get number of features from X_train shape\n",
        "\n",
        "    # Define models\n",
        "    main_model = MainModel(input_dim)\n",
        "    adversary = AdversaryModel(1)  # Adversary receives output size 1 from main model (we will add transformation)\n",
        "\n",
        "    # Modify adversary to handle the 1-dimensional input from the main model\n",
        "    adversary.fc1 = nn.Linear(1, 128)  # Project from size (batch_size, 1) to (batch_size, 128)\n",
        "\n",
        "    # Optimizers\n",
        "    optimizer_main = optim.Adam(main_model.parameters(), lr=lr_main)\n",
        "    optimizer_adv = optim.Adam(adversary.parameters(), lr=lr_adv)\n",
        "\n",
        "    # Loss Functions\n",
        "    criterion_main = nn.BCELoss()\n",
        "    criterion_adv = nn.BCELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        main_model.train()\n",
        "        adversary.train()\n",
        "\n",
        "        total_loss_main = 0\n",
        "        total_loss_adv = 0\n",
        "        total_fairness_penalty = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            X_batch, y_batch, protected_batch = batch\n",
        "\n",
        "            # Ensure the input has the correct shape (batch_size, input_dim)\n",
        "            X_batch = X_batch.view(X_batch.size(0), -1)  # Flatten input if necessary\n",
        "\n",
        "            optimizer_main.zero_grad()\n",
        "            optimizer_adv.zero_grad()\n",
        "\n",
        "            # Forward pass through the main model\n",
        "            main_output = main_model(X_batch)\n",
        "            main_loss = criterion_main(main_output.squeeze(), y_batch)\n",
        "\n",
        "            # Forward pass through the adversary\n",
        "            # Pass the output of the main model (which has size 1) through the adversary's layers\n",
        "            adversary_output = adversary(main_output)\n",
        "            adv_loss = criterion_adv(adversary_output.squeeze(), protected_batch)\n",
        "\n",
        "            # Compute fairness penalty\n",
        "            fairness_penalty = lam * adv_loss\n",
        "\n",
        "            # Backward pass through main model\n",
        "            main_loss.backward(retain_graph=True)  # Retain graph for the adversary's backward pass\n",
        "\n",
        "            # Backward pass through adversary (Gradient Reversal Layer)\n",
        "            adversary_loss = adv_loss + fairness_penalty\n",
        "            adversary_loss.backward()\n",
        "\n",
        "            # Optimizer steps\n",
        "            optimizer_main.step()\n",
        "            optimizer_adv.step()\n",
        "\n",
        "            # Track losses\n",
        "            total_loss_main += main_loss.item()\n",
        "            total_loss_adv += adv_loss.item()\n",
        "            total_fairness_penalty += fairness_penalty.item()\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f\"Epoch {epoch}/{epochs}, Main Loss: {total_loss_main/len(train_loader)}, \"\n",
        "                  f\"Adversary Loss: {total_loss_adv/len(train_loader)}, \"\n",
        "                  f\"Fairness Penalty: {total_fairness_penalty/len(train_loader)}\")\n",
        "\n",
        "    return main_model, adversary\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "        output = model(X_test_tensor)\n",
        "        predictions = (output.squeeze() > 0.5).float()\n",
        "        accuracy = accuracy_score(y_test_tensor, predictions)\n",
        "    return accuracy\n",
        "\n",
        "# Apply adversarial debiasing\n",
        "main_model, adversary = adversarial_debiasing(X_train, y_train, protected_attribute_train, epochs=500,\n",
        "                                              lr_main=0.001, lr_adv=0.001, lam=0.1)\n",
        "\n",
        "# Test the trained model\n",
        "test_accuracy = evaluate_model(main_model, X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDjANesSTnL1",
        "outputId": "82d463e6-06c1-45c6-8221-af67b0d5c877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/500, Main Loss: 0.6931309654162481, Adversary Loss: 0.6959471198228689, Fairness Penalty: 0.06959471393090028\n",
            "Epoch 50/500, Main Loss: 0.6665085324874291, Adversary Loss: 0.6918841783816998, Fairness Penalty: 0.06918841886978883\n",
            "Epoch 100/500, Main Loss: 0.6542460643328153, Adversary Loss: 0.6854917819683368, Fairness Penalty: 0.0685491796869498\n",
            "Epoch 150/500, Main Loss: 0.6498469435251676, Adversary Loss: 0.660951880308298, Fairness Penalty: 0.0660951894063216\n",
            "Epoch 200/500, Main Loss: 0.6461124374316289, Adversary Loss: 0.6238565490796015, Fairness Penalty: 0.0623856562261398\n",
            "Epoch 250/500, Main Loss: 0.6510215768447289, Adversary Loss: 0.5891423133703378, Fairness Penalty: 0.058914232139404006\n",
            "Epoch 300/500, Main Loss: 0.6476664634851309, Adversary Loss: 0.5524538457393646, Fairness Penalty: 0.05524538572017963\n",
            "Epoch 350/500, Main Loss: 0.654588928589454, Adversary Loss: 0.5060261167012728, Fairness Penalty: 0.050602612587121815\n",
            "Epoch 400/500, Main Loss: 0.6588509953938998, Adversary Loss: 0.4818666371015402, Fairness Penalty: 0.048186664684460714\n",
            "Epoch 450/500, Main Loss: 0.6594340938788193, Adversary Loss: 0.44494945957110477, Fairness Penalty: 0.04449494698872933\n",
            "Test Accuracy: 0.4800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Set up the device (CPU for now)\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Define a simplified main model and adversary model\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=8, output_dim=1):\n",
        "        super(MainModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "class AdversaryModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=8, output_dim=1):\n",
        "        super(AdversaryModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def adversarial_debiasing(X_train, y_train, protected_attribute_train, epochs=5, lr_main=0.001, lr_adv=0.001, lam=0.1):\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs('/content/adversarial_debiasing_output', exist_ok=True)\n",
        "    output_file_path = '/content/adversarial_debiasing_output/adversarial_debiasing_output.txt'\n",
        "\n",
        "    # Open the file to write output\n",
        "    with open(output_file_path, 'w') as f:\n",
        "        sys.stdout = f  # Redirect output to file\n",
        "        print(\"Starting Adversarial Debiasing Training...\\n\")\n",
        "\n",
        "        # Initialize models with smaller dimensions\n",
        "        input_dim = X_train.shape[1]\n",
        "        main_model = MainModel(input_dim).to(device)\n",
        "        adversary = AdversaryModel(input_dim).to(device)\n",
        "\n",
        "        # Define loss functions and optimizers\n",
        "        criterion_main = nn.BCELoss()\n",
        "        criterion_adv = nn.BCEWithLogitsLoss()\n",
        "        optimizer_main = optim.Adam(main_model.parameters(), lr=lr_main)\n",
        "        optimizer_adv = optim.Adam(adversary.parameters(), lr=lr_adv)\n",
        "\n",
        "        # Prepare data (using only 10% of the dataset for faster testing)\n",
        "        train_data = TensorDataset(torch.tensor(X_train[:int(0.1*len(X_train))], dtype=torch.float32).to(device),\n",
        "                                   torch.tensor(y_train[:int(0.1*len(y_train))], dtype=torch.float32).to(device))\n",
        "        train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
        "\n",
        "        # Start training\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            for i, (inputs, labels) in enumerate(train_loader):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer_main.zero_grad()\n",
        "                optimizer_adv.zero_grad()\n",
        "\n",
        "                # Forward pass through the main model\n",
        "                main_output = main_model(inputs)\n",
        "\n",
        "                # Forward pass through the adversary model\n",
        "                adversary_output = adversary(main_output)\n",
        "\n",
        "                # Reshape labels if necessary\n",
        "                labels = labels.float().view(-1, 1)\n",
        "                adversary_output = adversary_output.view(-1, 1)\n",
        "\n",
        "                # Compute losses\n",
        "                main_loss = criterion_main(main_output, labels)\n",
        "                adversary_loss = criterion_adv(adversary_output, protected_attribute_train[i:i + len(labels)].float())\n",
        "\n",
        "                # Backpropagation for main model\n",
        "                main_loss.backward(retain_graph=True)\n",
        "                optimizer_main.step()\n",
        "\n",
        "                # Backpropagation for adversary\n",
        "                adversary_loss.backward()\n",
        "                optimizer_adv.step()\n",
        "\n",
        "                # Fairness penalty\n",
        "                fairness_penalty = lam * adversary_loss.item()\n",
        "\n",
        "                # Print verbose output for debugging\n",
        "                print(f\"Batch {i+1}/{len(train_loader)} - Main Loss: {main_loss.item()}, \"\n",
        "                      f\"Adversary Loss: {adversary_loss.item()}, Fairness Penalty: {fairness_penalty}\")\n",
        "\n",
        "        print(f\"Training complete. Final Main Loss: {main_loss.item()}, Final Adversary Loss: {adversary_loss.item()}\")\n",
        "        return main_model, adversary\n",
        "\n",
        "\n",
        "# Simulating the data (use your actual data here)\n",
        "np.random.seed(0)\n",
        "X_train = np.random.rand(160, 5)  # Example feature data (using only 10% of the original data for quicker testing)\n",
        "y_train = np.random.randint(0, 2, size=160)  # Example binary labels\n",
        "protected_attribute_train = np.random.randint(0, 2, size=160)  # Example protected attribute\n",
        "\n",
        "# Run training with the function\n",
        "main_model, adversary = adversarial_debiasing(X_train, y_train, protected_attribute_train, epochs=5,\n",
        "                                              lr_main=0.001, lr_adv=0.001, lam=0.1)\n",
        "\n",
        "# Simulate testing data (use actual test data)\n",
        "X_test = np.random.rand(40, 5)  # Example test feature data (using only 10% of the original data)\n",
        "y_test = np.random.randint(0, 2, size=40)  # Example test binary labels\n",
        "\n",
        "# Evaluate the model (modify for your use case)\n",
        "def test_model(model, X_test, y_test):\n",
        "    model.eval()\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "\n",
        "    output = model(X_test_tensor)\n",
        "    predictions = (output.squeeze() > 0.5).float()\n",
        "    accuracy = (predictions == y_test_tensor).float().mean()\n",
        "    return accuracy.item()\n",
        "\n",
        "# Evaluate the main model\n",
        "test_accuracy = test_model(main_model, X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "d6L7_diwWUXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# 1. Data Generation (Fix the error by adjusting the number of informative, redundant, and repeated features)\n",
        "X, y = make_classification(n_samples=800, n_features=6, n_informative=5, n_redundant=0, n_repeated=0, n_classes=2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the protected attribute (this is just a binary attribute for testing)\n",
        "protected_attribute_train = np.random.choice([0, 1], size=len(y_train))\n",
        "\n",
        "# Convert to torch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "protected_attribute_train = torch.tensor(protected_attribute_train, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# DataLoader\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# 2. Define the Model\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MainModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "class Adversary(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Adversary, self).__init__()\n",
        "        self.fc1 = nn.Linear(1, 64)  # Predicting the protected attribute\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 3. Adversarial Debiasing Process\n",
        "def adversarial_debiasing(X_train, y_train, protected_attribute_train, epochs=500, lr_main=0.001, lr_adv=0.001, lam=0.1):\n",
        "    # Initialize models and optimizers\n",
        "    main_model = MainModel(input_dim=X_train.shape[1])\n",
        "    adversary = Adversary()\n",
        "\n",
        "    optimizer_main = optim.Adam(main_model.parameters(), lr=lr_main)\n",
        "    optimizer_adv = optim.Adam(adversary.parameters(), lr=lr_adv)\n",
        "\n",
        "    criterion_main = nn.BCEWithLogitsLoss()  # Loss function for the main model\n",
        "    criterion_adv = nn.BCEWithLogitsLoss()   # Loss function for adversary\n",
        "\n",
        "    # Start training\n",
        "    for epoch in range(epochs):\n",
        "        main_model.train()\n",
        "        adversary.train()\n",
        "        running_main_loss = 0.0\n",
        "        running_adv_loss = 0.0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            optimizer_main.zero_grad()\n",
        "            optimizer_adv.zero_grad()\n",
        "\n",
        "            # Main model forward pass\n",
        "            main_output = main_model(inputs)\n",
        "            main_loss = criterion_main(main_output, labels)\n",
        "\n",
        "            # Adversary forward pass (predicting protected attribute)\n",
        "            adversary_output = adversary(main_output)\n",
        "            adversary_loss = criterion_adv(adversary_output, protected_attribute_train[i * 64: (i + 1) * 64].float())\n",
        "\n",
        "            # Compute fairness penalty (adversarial loss)\n",
        "            fairness_penalty = lam * adversary_loss\n",
        "\n",
        "            # Backpropagation for main model and adversary\n",
        "            loss = main_loss + fairness_penalty\n",
        "            loss.backward()\n",
        "            optimizer_main.step()\n",
        "            optimizer_adv.step()\n",
        "\n",
        "            # Track losses\n",
        "            running_main_loss += main_loss.item()\n",
        "            running_adv_loss += adversary_loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch}/{epochs} - Main Loss: {running_main_loss/len(train_loader)}, Adversary Loss: {running_adv_loss/len(train_loader)}\")\n",
        "\n",
        "    return main_model, adversary\n",
        "\n",
        "# 4. Model Evaluation (Accuracy & Fairness Metrics)\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(X_test)\n",
        "        predictions = (output > 0.5).float()\n",
        "        accuracy = accuracy_score(y_test, predictions)\n",
        "        return accuracy\n",
        "\n",
        "# 5. Hyperparameter Tuning\n",
        "lr_main_values = [0.0001, 0.001, 0.01]\n",
        "lr_adv_values = [0.0001, 0.001, 0.01]\n",
        "best_accuracy = 0\n",
        "best_lr_main = 0\n",
        "best_lr_adv = 0\n",
        "\n",
        "for lr_main in lr_main_values:\n",
        "    for lr_adv in lr_adv_values:\n",
        "        print(f\"Testing with lr_main={lr_main}, lr_adv={lr_adv}\")\n",
        "        main_model, adversary = adversarial_debiasing(X_train, y_train, protected_attribute_train, epochs=100,\n",
        "                                                     lr_main=lr_main, lr_adv=lr_adv, lam=0.1)\n",
        "        test_accuracy = evaluate_model(main_model, X_test, y_test)\n",
        "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "        if test_accuracy > best_accuracy:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_lr_main = lr_main\n",
        "            best_lr_adv = lr_adv\n",
        "\n",
        "print(f\"Best Test Accuracy: {best_accuracy:.4f} with lr_main={best_lr_main}, lr_adv={best_lr_adv}\")\n",
        "\n",
        "# 6. Adversarial Robustness Testing\n",
        "# Here, you can use adversarial attacks such as FGSM or PGD to test the model's robustness.\n",
        "# For simplicity, we'll skip this step here but you can implement it using libraries like `torchattacks`.\n",
        "\n",
        "# 7. Fairness Metrics (for example, Demographic Parity)\n",
        "def fairness_metrics(model, X_test, y_test, protected_attribute_test):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(X_test)\n",
        "        predictions = (output > 0.5).float()\n",
        "        # Compute Demographic Parity or other fairness metrics\n",
        "        demographic_parity = (predictions[protected_attribute_test == 1].mean() - predictions[protected_attribute_test == 0].mean()).abs()\n",
        "        print(f\"Demographic Parity: {demographic_parity:.4f}\")\n",
        "\n",
        "# Assuming you have a `protected_attribute_test` tensor for the test set:\n",
        "# protected_attribute_test = ...\n",
        "\n",
        "# Example usage of fairness metrics\n",
        "# fairness_metrics(main_model, X_test, y_test, protected_attribute_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_u_qeVdYLCx",
        "outputId": "852ef2fa-eb81-428f-c7fc-69319e89d402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with lr_main=0.0001, lr_adv=0.0001\n",
            "Epoch 0/100 - Main Loss: 0.7205706655979156, Adversary Loss: 0.7131814539432526\n",
            "Epoch 1/100 - Main Loss: 0.7195731222629547, Adversary Loss: 0.7103110373020172\n",
            "Epoch 2/100 - Main Loss: 0.718675309419632, Adversary Loss: 0.70895916223526\n",
            "Epoch 3/100 - Main Loss: 0.7177384555339813, Adversary Loss: 0.707073712348938\n",
            "Epoch 4/100 - Main Loss: 0.7168073415756225, Adversary Loss: 0.7053982257843018\n",
            "Epoch 5/100 - Main Loss: 0.7158985376358032, Adversary Loss: 0.7042194783687592\n",
            "Epoch 6/100 - Main Loss: 0.7149682939052582, Adversary Loss: 0.7028839886188507\n",
            "Epoch 7/100 - Main Loss: 0.714087063074112, Adversary Loss: 0.7011624693870544\n",
            "Epoch 8/100 - Main Loss: 0.7132363259792328, Adversary Loss: 0.6998951613903046\n",
            "Epoch 9/100 - Main Loss: 0.7123435258865356, Adversary Loss: 0.699499249458313\n",
            "Epoch 10/100 - Main Loss: 0.7114299237728119, Adversary Loss: 0.6989599883556366\n",
            "Epoch 11/100 - Main Loss: 0.7106115162372589, Adversary Loss: 0.6982430398464203\n",
            "Epoch 12/100 - Main Loss: 0.7097713112831116, Adversary Loss: 0.6970868527889251\n",
            "Epoch 13/100 - Main Loss: 0.7089151680469513, Adversary Loss: 0.6968345761299133\n",
            "Epoch 14/100 - Main Loss: 0.7080786824226379, Adversary Loss: 0.6959735214710235\n",
            "Epoch 15/100 - Main Loss: 0.7072589278221131, Adversary Loss: 0.6959498822689056\n",
            "Epoch 16/100 - Main Loss: 0.7064256370067596, Adversary Loss: 0.6951736509799957\n",
            "Epoch 17/100 - Main Loss: 0.7056353569030762, Adversary Loss: 0.6941023409366608\n",
            "Epoch 18/100 - Main Loss: 0.704802805185318, Adversary Loss: 0.6946214973926544\n",
            "Epoch 19/100 - Main Loss: 0.703984135389328, Adversary Loss: 0.6940164566040039\n",
            "Epoch 20/100 - Main Loss: 0.7031825661659241, Adversary Loss: 0.6944243371486664\n",
            "Epoch 21/100 - Main Loss: 0.7023800611495972, Adversary Loss: 0.6939219415187836\n",
            "Epoch 22/100 - Main Loss: 0.7016167879104614, Adversary Loss: 0.6937571823596954\n",
            "Epoch 23/100 - Main Loss: 0.7007927596569061, Adversary Loss: 0.6939844250679016\n",
            "Epoch 24/100 - Main Loss: 0.7000293850898742, Adversary Loss: 0.6939923286437988\n",
            "Epoch 25/100 - Main Loss: 0.6992380142211914, Adversary Loss: 0.6932874023914337\n",
            "Epoch 26/100 - Main Loss: 0.6984795093536377, Adversary Loss: 0.6932667851448059\n",
            "Epoch 27/100 - Main Loss: 0.6977245032787323, Adversary Loss: 0.6924286425113678\n",
            "Epoch 28/100 - Main Loss: 0.6969358444213867, Adversary Loss: 0.6927998483180999\n",
            "Epoch 29/100 - Main Loss: 0.696214884519577, Adversary Loss: 0.6924538791179657\n",
            "Epoch 30/100 - Main Loss: 0.6954542577266694, Adversary Loss: 0.6927726805210114\n",
            "Epoch 31/100 - Main Loss: 0.6946873068809509, Adversary Loss: 0.6928556263446808\n",
            "Epoch 32/100 - Main Loss: 0.6939677357673645, Adversary Loss: 0.6930650293827056\n",
            "Epoch 33/100 - Main Loss: 0.6932061731815338, Adversary Loss: 0.6930697202682495\n",
            "Epoch 34/100 - Main Loss: 0.6924807548522949, Adversary Loss: 0.6934703290462494\n",
            "Epoch 35/100 - Main Loss: 0.6917590081691742, Adversary Loss: 0.6936625421047211\n",
            "Epoch 36/100 - Main Loss: 0.6910135686397553, Adversary Loss: 0.6929233849048615\n",
            "Epoch 37/100 - Main Loss: 0.6903248012065888, Adversary Loss: 0.692176342010498\n",
            "Epoch 38/100 - Main Loss: 0.6896022617816925, Adversary Loss: 0.6934055685997009\n",
            "Epoch 39/100 - Main Loss: 0.6888718008995056, Adversary Loss: 0.6932752192020416\n",
            "Epoch 40/100 - Main Loss: 0.6881658017635346, Adversary Loss: 0.6937223732471466\n",
            "Epoch 41/100 - Main Loss: 0.687459409236908, Adversary Loss: 0.6935830593109131\n",
            "Epoch 42/100 - Main Loss: 0.6867550432682037, Adversary Loss: 0.6929978489875793\n",
            "Epoch 43/100 - Main Loss: 0.6860671520233155, Adversary Loss: 0.693221914768219\n",
            "Epoch 44/100 - Main Loss: 0.6853704810142517, Adversary Loss: 0.6938039541244507\n",
            "Epoch 45/100 - Main Loss: 0.6846983253955841, Adversary Loss: 0.6930066585540772\n",
            "Epoch 46/100 - Main Loss: 0.684000986814499, Adversary Loss: 0.6942866742610931\n",
            "Epoch 47/100 - Main Loss: 0.6833244323730469, Adversary Loss: 0.6923049986362457\n",
            "Epoch 48/100 - Main Loss: 0.6826584756374359, Adversary Loss: 0.6928083240985871\n",
            "Epoch 49/100 - Main Loss: 0.6819817960262299, Adversary Loss: 0.6922145068645478\n",
            "Epoch 50/100 - Main Loss: 0.6813349545001983, Adversary Loss: 0.6936598420143127\n",
            "Epoch 51/100 - Main Loss: 0.680648148059845, Adversary Loss: 0.6931901335716247\n",
            "Epoch 52/100 - Main Loss: 0.6799922406673431, Adversary Loss: 0.6930480778217316\n",
            "Epoch 53/100 - Main Loss: 0.6793579459190369, Adversary Loss: 0.6927315711975097\n",
            "Epoch 54/100 - Main Loss: 0.6786931812763214, Adversary Loss: 0.6933446884155273\n",
            "Epoch 55/100 - Main Loss: 0.6780228674411773, Adversary Loss: 0.6926800727844238\n",
            "Epoch 56/100 - Main Loss: 0.677385663986206, Adversary Loss: 0.6938406646251678\n",
            "Epoch 57/100 - Main Loss: 0.676745080947876, Adversary Loss: 0.6938016772270202\n",
            "Epoch 58/100 - Main Loss: 0.6761101722717285, Adversary Loss: 0.692578774690628\n",
            "Epoch 59/100 - Main Loss: 0.6754609227180481, Adversary Loss: 0.6924589455127717\n",
            "Epoch 60/100 - Main Loss: 0.67484832406044, Adversary Loss: 0.6922799646854401\n",
            "Epoch 61/100 - Main Loss: 0.6741981863975525, Adversary Loss: 0.692488694190979\n",
            "Epoch 62/100 - Main Loss: 0.6735740303993225, Adversary Loss: 0.6925004661083222\n",
            "Epoch 63/100 - Main Loss: 0.6729629218578339, Adversary Loss: 0.6926685273647308\n",
            "Epoch 64/100 - Main Loss: 0.6723221838474274, Adversary Loss: 0.6927089691162109\n",
            "Epoch 65/100 - Main Loss: 0.6717200577259064, Adversary Loss: 0.6941694557666779\n",
            "Epoch 66/100 - Main Loss: 0.6710992813110351, Adversary Loss: 0.6932843148708343\n",
            "Epoch 67/100 - Main Loss: 0.6704894065856933, Adversary Loss: 0.6917981266975403\n",
            "Epoch 68/100 - Main Loss: 0.6698994576931, Adversary Loss: 0.6940138220787049\n",
            "Epoch 69/100 - Main Loss: 0.6692789316177368, Adversary Loss: 0.6931307315826416\n",
            "Epoch 70/100 - Main Loss: 0.6686832785606385, Adversary Loss: 0.6925890266895294\n",
            "Epoch 71/100 - Main Loss: 0.6681017756462098, Adversary Loss: 0.6923786580562592\n",
            "Epoch 72/100 - Main Loss: 0.6675040185451507, Adversary Loss: 0.6931489646434784\n",
            "Epoch 73/100 - Main Loss: 0.6668909251689911, Adversary Loss: 0.6936968028545379\n",
            "Epoch 74/100 - Main Loss: 0.6663134038448334, Adversary Loss: 0.6922384083271027\n",
            "Epoch 75/100 - Main Loss: 0.6657186210155487, Adversary Loss: 0.693239563703537\n",
            "Epoch 76/100 - Main Loss: 0.6651575207710266, Adversary Loss: 0.6949091851711273\n",
            "Epoch 77/100 - Main Loss: 0.6645698785781861, Adversary Loss: 0.6934613764286042\n",
            "Epoch 78/100 - Main Loss: 0.6639904260635376, Adversary Loss: 0.6934485256671905\n",
            "Epoch 79/100 - Main Loss: 0.6634079813957214, Adversary Loss: 0.6932126939296722\n",
            "Epoch 80/100 - Main Loss: 0.6628487586975098, Adversary Loss: 0.6927638649940491\n",
            "Epoch 81/100 - Main Loss: 0.6622717499732971, Adversary Loss: 0.6930507123470306\n",
            "Epoch 82/100 - Main Loss: 0.6617166936397553, Adversary Loss: 0.6916811764240265\n",
            "Epoch 83/100 - Main Loss: 0.6611530005931854, Adversary Loss: 0.6931491494178772\n",
            "Epoch 84/100 - Main Loss: 0.6605998277664185, Adversary Loss: 0.6921823441982269\n",
            "Epoch 85/100 - Main Loss: 0.6600428104400635, Adversary Loss: 0.6930111587047577\n",
            "Epoch 86/100 - Main Loss: 0.6594747006893158, Adversary Loss: 0.6936981678009033\n",
            "Epoch 87/100 - Main Loss: 0.6589327335357666, Adversary Loss: 0.6932464957237243\n",
            "Epoch 88/100 - Main Loss: 0.6583938539028168, Adversary Loss: 0.693478399515152\n",
            "Epoch 89/100 - Main Loss: 0.6578468441963196, Adversary Loss: 0.6932377755641937\n",
            "Epoch 90/100 - Main Loss: 0.6573160588741302, Adversary Loss: 0.6915476858615875\n",
            "Epoch 91/100 - Main Loss: 0.6567528545856476, Adversary Loss: 0.6936498284339905\n",
            "Epoch 92/100 - Main Loss: 0.6562339723110199, Adversary Loss: 0.6937088549137116\n",
            "Epoch 93/100 - Main Loss: 0.6556989252567291, Adversary Loss: 0.693436723947525\n",
            "Epoch 94/100 - Main Loss: 0.6551659762859344, Adversary Loss: 0.6935862004756927\n",
            "Epoch 95/100 - Main Loss: 0.6546485900878907, Adversary Loss: 0.6921942949295044\n",
            "Epoch 96/100 - Main Loss: 0.6541261374950409, Adversary Loss: 0.6933940887451172\n",
            "Epoch 97/100 - Main Loss: 0.6536039173603058, Adversary Loss: 0.6928877234458923\n",
            "Epoch 98/100 - Main Loss: 0.653085458278656, Adversary Loss: 0.693317973613739\n",
            "Epoch 99/100 - Main Loss: 0.652578455209732, Adversary Loss: 0.6946523666381836\n",
            "Test Accuracy: 0.7063\n",
            "Testing with lr_main=0.0001, lr_adv=0.001\n",
            "Epoch 0/100 - Main Loss: 0.7178931772708893, Adversary Loss: 0.6975599527359009\n",
            "Epoch 1/100 - Main Loss: 0.7170110821723938, Adversary Loss: 0.6941600501537323\n",
            "Epoch 2/100 - Main Loss: 0.7161464929580689, Adversary Loss: 0.6932901859283447\n",
            "Epoch 3/100 - Main Loss: 0.7152981400489807, Adversary Loss: 0.6931366324424744\n",
            "Epoch 4/100 - Main Loss: 0.7144473016262054, Adversary Loss: 0.6931316912174225\n",
            "Epoch 5/100 - Main Loss: 0.7135959148406983, Adversary Loss: 0.6929494619369507\n",
            "Epoch 6/100 - Main Loss: 0.7127408504486084, Adversary Loss: 0.693046098947525\n",
            "Epoch 7/100 - Main Loss: 0.7119162261486054, Adversary Loss: 0.6930768251419067\n",
            "Epoch 8/100 - Main Loss: 0.7110731422901153, Adversary Loss: 0.6930685758590698\n",
            "Epoch 9/100 - Main Loss: 0.7102111995220184, Adversary Loss: 0.6929216623306275\n",
            "Epoch 10/100 - Main Loss: 0.7093929827213288, Adversary Loss: 0.6929754614830017\n",
            "Epoch 11/100 - Main Loss: 0.7085629224777221, Adversary Loss: 0.6928966403007507\n",
            "Epoch 12/100 - Main Loss: 0.7077335834503173, Adversary Loss: 0.6932080149650574\n",
            "Epoch 13/100 - Main Loss: 0.7069275617599488, Adversary Loss: 0.6928021490573884\n",
            "Epoch 14/100 - Main Loss: 0.706087076663971, Adversary Loss: 0.6928362786769867\n",
            "Epoch 15/100 - Main Loss: 0.7052856862545014, Adversary Loss: 0.6928686022758483\n",
            "Epoch 16/100 - Main Loss: 0.7044712722301483, Adversary Loss: 0.693131023645401\n",
            "Epoch 17/100 - Main Loss: 0.7036422371864319, Adversary Loss: 0.6930440425872803\n",
            "Epoch 18/100 - Main Loss: 0.7028421044349671, Adversary Loss: 0.6929610133171081\n",
            "Epoch 19/100 - Main Loss: 0.7020462810993194, Adversary Loss: 0.6931506812572479\n",
            "Epoch 20/100 - Main Loss: 0.7012441754341125, Adversary Loss: 0.6929838240146637\n",
            "Epoch 21/100 - Main Loss: 0.7004384875297547, Adversary Loss: 0.6928662717342376\n",
            "Epoch 22/100 - Main Loss: 0.699636024236679, Adversary Loss: 0.6932406485080719\n",
            "Epoch 23/100 - Main Loss: 0.6988500952720642, Adversary Loss: 0.6930363953113556\n",
            "Epoch 24/100 - Main Loss: 0.6980528414249421, Adversary Loss: 0.6929042160511016\n",
            "Epoch 25/100 - Main Loss: 0.697258597612381, Adversary Loss: 0.6928462743759155\n",
            "Epoch 26/100 - Main Loss: 0.6964671730995178, Adversary Loss: 0.693191510438919\n",
            "Epoch 27/100 - Main Loss: 0.6956806540489197, Adversary Loss: 0.6928505897521973\n",
            "Epoch 28/100 - Main Loss: 0.6948932409286499, Adversary Loss: 0.6931383490562439\n",
            "Epoch 29/100 - Main Loss: 0.6941267192363739, Adversary Loss: 0.6935211241245269\n",
            "Epoch 30/100 - Main Loss: 0.6933315217494964, Adversary Loss: 0.692890727519989\n",
            "Epoch 31/100 - Main Loss: 0.6925614655017853, Adversary Loss: 0.6931775152683258\n",
            "Epoch 32/100 - Main Loss: 0.6917952418327331, Adversary Loss: 0.6930902659893036\n",
            "Epoch 33/100 - Main Loss: 0.6910107433795929, Adversary Loss: 0.6932420909404755\n",
            "Epoch 34/100 - Main Loss: 0.6902663171291351, Adversary Loss: 0.6928789556026459\n",
            "Epoch 35/100 - Main Loss: 0.6894883751869202, Adversary Loss: 0.6932264626026153\n",
            "Epoch 36/100 - Main Loss: 0.6887083947658539, Adversary Loss: 0.6932032525539398\n",
            "Epoch 37/100 - Main Loss: 0.6879818439483643, Adversary Loss: 0.6930810451507569\n",
            "Epoch 38/100 - Main Loss: 0.6872161328792572, Adversary Loss: 0.6928510129451751\n",
            "Epoch 39/100 - Main Loss: 0.6864570319652558, Adversary Loss: 0.6931234419345855\n",
            "Epoch 40/100 - Main Loss: 0.6857025504112244, Adversary Loss: 0.6931005120277405\n",
            "Epoch 41/100 - Main Loss: 0.6849669277667999, Adversary Loss: 0.6930034816265106\n",
            "Epoch 42/100 - Main Loss: 0.6842261135578156, Adversary Loss: 0.6931731641292572\n",
            "Epoch 43/100 - Main Loss: 0.6834832012653351, Adversary Loss: 0.6930046796798706\n",
            "Epoch 44/100 - Main Loss: 0.6827491462230683, Adversary Loss: 0.6932686686515808\n",
            "Epoch 45/100 - Main Loss: 0.682036018371582, Adversary Loss: 0.6928798913955688\n",
            "Epoch 46/100 - Main Loss: 0.6812711954116821, Adversary Loss: 0.6931480407714844\n",
            "Epoch 47/100 - Main Loss: 0.6805703341960907, Adversary Loss: 0.6929296910762787\n",
            "Epoch 48/100 - Main Loss: 0.6798208653926849, Adversary Loss: 0.6931643128395081\n",
            "Epoch 49/100 - Main Loss: 0.6791118204593658, Adversary Loss: 0.6930738747119903\n",
            "Epoch 50/100 - Main Loss: 0.6783941447734833, Adversary Loss: 0.6930360317230224\n",
            "Epoch 51/100 - Main Loss: 0.67769655585289, Adversary Loss: 0.6930334270000458\n",
            "Epoch 52/100 - Main Loss: 0.6769829869270325, Adversary Loss: 0.6930540859699249\n",
            "Epoch 53/100 - Main Loss: 0.6762675523757935, Adversary Loss: 0.6931921601295471\n",
            "Epoch 54/100 - Main Loss: 0.6755618453025818, Adversary Loss: 0.6932581543922425\n",
            "Epoch 55/100 - Main Loss: 0.674871689081192, Adversary Loss: 0.693138188123703\n",
            "Epoch 56/100 - Main Loss: 0.6741910934448242, Adversary Loss: 0.6930926144123077\n",
            "Epoch 57/100 - Main Loss: 0.6734992325305938, Adversary Loss: 0.6930232107639313\n",
            "Epoch 58/100 - Main Loss: 0.6728035271167755, Adversary Loss: 0.6928314745426178\n",
            "Epoch 59/100 - Main Loss: 0.6721221685409546, Adversary Loss: 0.6929972350597382\n",
            "Epoch 60/100 - Main Loss: 0.6714493870735169, Adversary Loss: 0.6928685247898102\n",
            "Epoch 61/100 - Main Loss: 0.6707707643508911, Adversary Loss: 0.6928427696228028\n",
            "Epoch 62/100 - Main Loss: 0.6701197564601898, Adversary Loss: 0.6929379284381867\n",
            "Epoch 63/100 - Main Loss: 0.6694318890571594, Adversary Loss: 0.6929975748062134\n",
            "Epoch 64/100 - Main Loss: 0.6687920987606049, Adversary Loss: 0.6929576992988586\n",
            "Epoch 65/100 - Main Loss: 0.6681244552135468, Adversary Loss: 0.6930294632911682\n",
            "Epoch 66/100 - Main Loss: 0.6674764335155488, Adversary Loss: 0.692740923166275\n",
            "Epoch 67/100 - Main Loss: 0.6668305575847626, Adversary Loss: 0.6935090541839599\n",
            "Epoch 68/100 - Main Loss: 0.6661954402923584, Adversary Loss: 0.6929317951202393\n",
            "Epoch 69/100 - Main Loss: 0.6655393898487091, Adversary Loss: 0.6931157350540161\n",
            "Epoch 70/100 - Main Loss: 0.6649186551570893, Adversary Loss: 0.6927287399768829\n",
            "Epoch 71/100 - Main Loss: 0.6642852187156677, Adversary Loss: 0.6932860612869263\n",
            "Epoch 72/100 - Main Loss: 0.6636595845222473, Adversary Loss: 0.6929359674453736\n",
            "Epoch 73/100 - Main Loss: 0.6630341529846191, Adversary Loss: 0.6931130468845368\n",
            "Epoch 74/100 - Main Loss: 0.6624163389205933, Adversary Loss: 0.6929818987846375\n",
            "Epoch 75/100 - Main Loss: 0.6618072390556335, Adversary Loss: 0.6927770376205444\n",
            "Epoch 76/100 - Main Loss: 0.6611918389797211, Adversary Loss: 0.6934382796287537\n",
            "Epoch 77/100 - Main Loss: 0.6605960488319397, Adversary Loss: 0.6930027723312377\n",
            "Epoch 78/100 - Main Loss: 0.6599826097488404, Adversary Loss: 0.693393486738205\n",
            "Epoch 79/100 - Main Loss: 0.6593852698802948, Adversary Loss: 0.6924912869930268\n",
            "Epoch 80/100 - Main Loss: 0.6587861537933349, Adversary Loss: 0.6936731398105621\n",
            "Epoch 81/100 - Main Loss: 0.6582235932350159, Adversary Loss: 0.6928503036499023\n",
            "Epoch 82/100 - Main Loss: 0.6576254963874817, Adversary Loss: 0.6931655585765839\n",
            "Epoch 83/100 - Main Loss: 0.6570481896400452, Adversary Loss: 0.6931850492954255\n",
            "Epoch 84/100 - Main Loss: 0.6564653396606446, Adversary Loss: 0.6933919370174408\n",
            "Epoch 85/100 - Main Loss: 0.6558904707431793, Adversary Loss: 0.6932461619377136\n",
            "Epoch 86/100 - Main Loss: 0.6553220331668854, Adversary Loss: 0.6929351687431335\n",
            "Epoch 87/100 - Main Loss: 0.6547668814659119, Adversary Loss: 0.6932533562183381\n",
            "Epoch 88/100 - Main Loss: 0.6542058289051056, Adversary Loss: 0.6932520866394043\n",
            "Epoch 89/100 - Main Loss: 0.6536532878875733, Adversary Loss: 0.6927229046821595\n",
            "Epoch 90/100 - Main Loss: 0.6531009435653686, Adversary Loss: 0.6932145237922669\n",
            "Epoch 91/100 - Main Loss: 0.6525461554527283, Adversary Loss: 0.6929630935192108\n",
            "Epoch 92/100 - Main Loss: 0.6520066142082215, Adversary Loss: 0.6931339085102082\n",
            "Epoch 93/100 - Main Loss: 0.6514844775199891, Adversary Loss: 0.6933303415775299\n",
            "Epoch 94/100 - Main Loss: 0.6509437024593353, Adversary Loss: 0.6934231698513031\n",
            "Epoch 95/100 - Main Loss: 0.6504067957401276, Adversary Loss: 0.6931335031986237\n",
            "Epoch 96/100 - Main Loss: 0.6498804926872254, Adversary Loss: 0.6929042875766754\n",
            "Epoch 97/100 - Main Loss: 0.6493545413017273, Adversary Loss: 0.6928087115287781\n",
            "Epoch 98/100 - Main Loss: 0.6488318860530853, Adversary Loss: 0.6930815994739532\n",
            "Epoch 99/100 - Main Loss: 0.6483242988586426, Adversary Loss: 0.6929099321365356\n",
            "Test Accuracy: 0.7500\n",
            "Testing with lr_main=0.0001, lr_adv=0.01\n",
            "Epoch 0/100 - Main Loss: 0.7263949513435364, Adversary Loss: 0.6970929205417633\n",
            "Epoch 1/100 - Main Loss: 0.725597482919693, Adversary Loss: 0.6994740426540375\n",
            "Epoch 2/100 - Main Loss: 0.7247799634933472, Adversary Loss: 0.6954002857208252\n",
            "Epoch 3/100 - Main Loss: 0.7239952445030212, Adversary Loss: 0.6930485546588898\n",
            "Epoch 4/100 - Main Loss: 0.7231966495513916, Adversary Loss: 0.6947385609149933\n",
            "Epoch 5/100 - Main Loss: 0.7224223911762238, Adversary Loss: 0.6940918743610383\n",
            "Epoch 6/100 - Main Loss: 0.7216424107551574, Adversary Loss: 0.6938401222229004\n",
            "Epoch 7/100 - Main Loss: 0.7208694279193878, Adversary Loss: 0.6941179275512696\n",
            "Epoch 8/100 - Main Loss: 0.7201087832450866, Adversary Loss: 0.6940378189086914\n",
            "Epoch 9/100 - Main Loss: 0.7193175852298737, Adversary Loss: 0.6939825713634491\n",
            "Epoch 10/100 - Main Loss: 0.7185552239418029, Adversary Loss: 0.6940695583820343\n",
            "Epoch 11/100 - Main Loss: 0.7177862226963043, Adversary Loss: 0.694044542312622\n",
            "Epoch 12/100 - Main Loss: 0.7170506894588471, Adversary Loss: 0.6939671516418457\n",
            "Epoch 13/100 - Main Loss: 0.7163057565689087, Adversary Loss: 0.6940430283546448\n",
            "Epoch 14/100 - Main Loss: 0.7155637502670288, Adversary Loss: 0.6939735293388367\n",
            "Epoch 15/100 - Main Loss: 0.7147911489009857, Adversary Loss: 0.6939348161220551\n",
            "Epoch 16/100 - Main Loss: 0.7140606582164765, Adversary Loss: 0.6939275681972503\n",
            "Epoch 17/100 - Main Loss: 0.7133150279521943, Adversary Loss: 0.6940099239349365\n",
            "Epoch 18/100 - Main Loss: 0.7125577509403229, Adversary Loss: 0.6939846098423004\n",
            "Epoch 19/100 - Main Loss: 0.7118392705917358, Adversary Loss: 0.6939117550849915\n",
            "Epoch 20/100 - Main Loss: 0.711080527305603, Adversary Loss: 0.6939098119735718\n",
            "Epoch 21/100 - Main Loss: 0.7103528141975403, Adversary Loss: 0.6938398659229279\n",
            "Epoch 22/100 - Main Loss: 0.7096062481403351, Adversary Loss: 0.6938353061676026\n",
            "Epoch 23/100 - Main Loss: 0.7088829219341278, Adversary Loss: 0.693903648853302\n",
            "Epoch 24/100 - Main Loss: 0.7081543564796448, Adversary Loss: 0.6939369797706604\n",
            "Epoch 25/100 - Main Loss: 0.7074133217334747, Adversary Loss: 0.6938490271568298\n",
            "Epoch 26/100 - Main Loss: 0.7066688239574432, Adversary Loss: 0.6940363764762878\n",
            "Epoch 27/100 - Main Loss: 0.70596342086792, Adversary Loss: 0.6937613666057587\n",
            "Epoch 28/100 - Main Loss: 0.7052296817302703, Adversary Loss: 0.6935765385627747\n",
            "Epoch 29/100 - Main Loss: 0.704505467414856, Adversary Loss: 0.6938688576221466\n",
            "Epoch 30/100 - Main Loss: 0.7037777900695801, Adversary Loss: 0.6937866926193237\n",
            "Epoch 31/100 - Main Loss: 0.7030696868896484, Adversary Loss: 0.6940089583396911\n",
            "Epoch 32/100 - Main Loss: 0.7023182690143586, Adversary Loss: 0.6937041759490967\n",
            "Epoch 33/100 - Main Loss: 0.7016200602054596, Adversary Loss: 0.6940890729427338\n",
            "Epoch 34/100 - Main Loss: 0.7008916437625885, Adversary Loss: 0.6937526226043701\n",
            "Epoch 35/100 - Main Loss: 0.7001638352870941, Adversary Loss: 0.6938268780708313\n",
            "Epoch 36/100 - Main Loss: 0.699450820684433, Adversary Loss: 0.6938028395175934\n",
            "Epoch 37/100 - Main Loss: 0.6987392604351044, Adversary Loss: 0.6936839520931244\n",
            "Epoch 38/100 - Main Loss: 0.6980294525623322, Adversary Loss: 0.6936246871948242\n",
            "Epoch 39/100 - Main Loss: 0.6972995400428772, Adversary Loss: 0.6938105940818786\n",
            "Epoch 40/100 - Main Loss: 0.6965802311897278, Adversary Loss: 0.6936752140522003\n",
            "Epoch 41/100 - Main Loss: 0.695889002084732, Adversary Loss: 0.6937590658664703\n",
            "Epoch 42/100 - Main Loss: 0.6951426446437836, Adversary Loss: 0.6937522709369659\n",
            "Epoch 43/100 - Main Loss: 0.6944464445114136, Adversary Loss: 0.6936990261077881\n",
            "Epoch 44/100 - Main Loss: 0.6937486946582794, Adversary Loss: 0.6937410414218903\n",
            "Epoch 45/100 - Main Loss: 0.6930180072784424, Adversary Loss: 0.6936367392539978\n",
            "Epoch 46/100 - Main Loss: 0.6922976970672607, Adversary Loss: 0.6936479806900024\n",
            "Epoch 47/100 - Main Loss: 0.6916223287582397, Adversary Loss: 0.6937794268131257\n",
            "Epoch 48/100 - Main Loss: 0.6908966660499573, Adversary Loss: 0.6937319278717041\n",
            "Epoch 49/100 - Main Loss: 0.6902034699916839, Adversary Loss: 0.6935096800327301\n",
            "Epoch 50/100 - Main Loss: 0.6894907116889953, Adversary Loss: 0.6936887741088867\n",
            "Epoch 51/100 - Main Loss: 0.6887901425361633, Adversary Loss: 0.6935244202613831\n",
            "Epoch 52/100 - Main Loss: 0.6881045758724212, Adversary Loss: 0.6938649475574493\n",
            "Epoch 53/100 - Main Loss: 0.6873881638050079, Adversary Loss: 0.6934188485145569\n",
            "Epoch 54/100 - Main Loss: 0.6867066919803619, Adversary Loss: 0.6940440118312836\n",
            "Epoch 55/100 - Main Loss: 0.6860022187232971, Adversary Loss: 0.6938223600387573\n",
            "Epoch 56/100 - Main Loss: 0.6853089988231659, Adversary Loss: 0.6935490965843201\n",
            "Epoch 57/100 - Main Loss: 0.6846290707588196, Adversary Loss: 0.6936112940311432\n",
            "Epoch 58/100 - Main Loss: 0.6839212894439697, Adversary Loss: 0.6935494184494019\n",
            "Epoch 59/100 - Main Loss: 0.6832431316375732, Adversary Loss: 0.6932984173297883\n",
            "Epoch 60/100 - Main Loss: 0.6825608611106873, Adversary Loss: 0.6933836698532104\n",
            "Epoch 61/100 - Main Loss: 0.6818907141685486, Adversary Loss: 0.6935575246810913\n",
            "Epoch 62/100 - Main Loss: 0.6811905443668366, Adversary Loss: 0.6933787405490875\n",
            "Epoch 63/100 - Main Loss: 0.6805183827877045, Adversary Loss: 0.6941456735134125\n",
            "Epoch 64/100 - Main Loss: 0.6798431277275085, Adversary Loss: 0.6939991474151611\n",
            "Epoch 65/100 - Main Loss: 0.6791792750358582, Adversary Loss: 0.6936972200870514\n",
            "Epoch 66/100 - Main Loss: 0.6784954786300659, Adversary Loss: 0.6935253322124482\n",
            "Epoch 67/100 - Main Loss: 0.677834403514862, Adversary Loss: 0.6935481429100037\n",
            "Epoch 68/100 - Main Loss: 0.6771555304527282, Adversary Loss: 0.6934141874313354\n",
            "Epoch 69/100 - Main Loss: 0.6765036702156066, Adversary Loss: 0.6935684859752655\n",
            "Epoch 70/100 - Main Loss: 0.675841236114502, Adversary Loss: 0.6934166252613068\n",
            "Epoch 71/100 - Main Loss: 0.6751732885837555, Adversary Loss: 0.693993890285492\n",
            "Epoch 72/100 - Main Loss: 0.6745073139667511, Adversary Loss: 0.6934693694114685\n",
            "Epoch 73/100 - Main Loss: 0.6738710939884186, Adversary Loss: 0.6938101887702942\n",
            "Epoch 74/100 - Main Loss: 0.6732056379318238, Adversary Loss: 0.6935725808143616\n",
            "Epoch 75/100 - Main Loss: 0.6725631654262543, Adversary Loss: 0.6934744834899902\n",
            "Epoch 76/100 - Main Loss: 0.6718940794467926, Adversary Loss: 0.6935852289199829\n",
            "Epoch 77/100 - Main Loss: 0.6712759912014008, Adversary Loss: 0.6935439229011535\n",
            "Epoch 78/100 - Main Loss: 0.6705985069274902, Adversary Loss: 0.6935371041297913\n",
            "Epoch 79/100 - Main Loss: 0.6699746251106262, Adversary Loss: 0.6935005784034729\n",
            "Epoch 80/100 - Main Loss: 0.6693244874477386, Adversary Loss: 0.6932380557060241\n",
            "Epoch 81/100 - Main Loss: 0.6686907231807708, Adversary Loss: 0.6933518767356872\n",
            "Epoch 82/100 - Main Loss: 0.6680465757846832, Adversary Loss: 0.6933867871761322\n",
            "Epoch 83/100 - Main Loss: 0.6674340426921844, Adversary Loss: 0.6931678593158722\n",
            "Epoch 84/100 - Main Loss: 0.6667879045009613, Adversary Loss: 0.6928688585758209\n",
            "Epoch 85/100 - Main Loss: 0.6661553859710694, Adversary Loss: 0.6935072004795074\n",
            "Epoch 86/100 - Main Loss: 0.6655311465263367, Adversary Loss: 0.6934169590473175\n",
            "Epoch 87/100 - Main Loss: 0.6649203658103943, Adversary Loss: 0.6935824751853943\n",
            "Epoch 88/100 - Main Loss: 0.664295208454132, Adversary Loss: 0.6928794741630554\n",
            "Epoch 89/100 - Main Loss: 0.6636788666248321, Adversary Loss: 0.6933411180973053\n",
            "Epoch 90/100 - Main Loss: 0.6630714416503907, Adversary Loss: 0.6941341161727905\n",
            "Epoch 91/100 - Main Loss: 0.6624539375305176, Adversary Loss: 0.6940059721469879\n",
            "Epoch 92/100 - Main Loss: 0.6618480265140534, Adversary Loss: 0.6930017352104187\n",
            "Epoch 93/100 - Main Loss: 0.6612341940402985, Adversary Loss: 0.6948311150074005\n",
            "Epoch 94/100 - Main Loss: 0.6606312990188599, Adversary Loss: 0.6932013213634491\n",
            "Epoch 95/100 - Main Loss: 0.6600293517112732, Adversary Loss: 0.6939803242683411\n",
            "Epoch 96/100 - Main Loss: 0.6594474673271179, Adversary Loss: 0.6934380292892456\n",
            "Epoch 97/100 - Main Loss: 0.6588512182235717, Adversary Loss: 0.693519389629364\n",
            "Epoch 98/100 - Main Loss: 0.6582474648952484, Adversary Loss: 0.6937479019165039\n",
            "Epoch 99/100 - Main Loss: 0.6576701402664185, Adversary Loss: 0.6934985339641571\n",
            "Test Accuracy: 0.7125\n",
            "Testing with lr_main=0.001, lr_adv=0.0001\n",
            "Epoch 0/100 - Main Loss: 0.7049953639507294, Adversary Loss: 0.6977979898452759\n",
            "Epoch 1/100 - Main Loss: 0.6969409763813019, Adversary Loss: 0.6975991427898407\n",
            "Epoch 2/100 - Main Loss: 0.6897987961769104, Adversary Loss: 0.6972995817661285\n",
            "Epoch 3/100 - Main Loss: 0.6829064905643463, Adversary Loss: 0.696178525686264\n",
            "Epoch 4/100 - Main Loss: 0.6766333878040314, Adversary Loss: 0.6960505902767181\n",
            "Epoch 5/100 - Main Loss: 0.6705853462219238, Adversary Loss: 0.6952589333057404\n",
            "Epoch 6/100 - Main Loss: 0.6651141107082367, Adversary Loss: 0.6953444123268128\n",
            "Epoch 7/100 - Main Loss: 0.6596644639968872, Adversary Loss: 0.6959602415561676\n",
            "Epoch 8/100 - Main Loss: 0.6546805083751679, Adversary Loss: 0.6940188944339752\n",
            "Epoch 9/100 - Main Loss: 0.6497312426567078, Adversary Loss: 0.6932393074035644\n",
            "Epoch 10/100 - Main Loss: 0.6452296674251556, Adversary Loss: 0.6938141703605651\n",
            "Epoch 11/100 - Main Loss: 0.6407033443450928, Adversary Loss: 0.6931190371513367\n",
            "Epoch 12/100 - Main Loss: 0.6365701258182526, Adversary Loss: 0.6935206890106201\n",
            "Epoch 13/100 - Main Loss: 0.6326115012168885, Adversary Loss: 0.6941835403442382\n",
            "Epoch 14/100 - Main Loss: 0.6289842367172241, Adversary Loss: 0.6933862328529358\n",
            "Epoch 15/100 - Main Loss: 0.625635689496994, Adversary Loss: 0.6938389062881469\n",
            "Epoch 16/100 - Main Loss: 0.6225716233253479, Adversary Loss: 0.6920916736125946\n",
            "Epoch 17/100 - Main Loss: 0.619587379693985, Adversary Loss: 0.6936221122741699\n",
            "Epoch 18/100 - Main Loss: 0.6169412314891816, Adversary Loss: 0.6940894484519958\n",
            "Epoch 19/100 - Main Loss: 0.6145649433135987, Adversary Loss: 0.6930413842201233\n",
            "Epoch 20/100 - Main Loss: 0.6123873114585876, Adversary Loss: 0.693884402513504\n",
            "Epoch 21/100 - Main Loss: 0.6102652490139008, Adversary Loss: 0.692662262916565\n",
            "Epoch 22/100 - Main Loss: 0.6084783613681793, Adversary Loss: 0.6926691055297851\n",
            "Epoch 23/100 - Main Loss: 0.6066706240177154, Adversary Loss: 0.6936554789543152\n",
            "Epoch 24/100 - Main Loss: 0.6050864219665527, Adversary Loss: 0.6941056191921234\n",
            "Epoch 25/100 - Main Loss: 0.6035947680473328, Adversary Loss: 0.6923221707344055\n",
            "Epoch 26/100 - Main Loss: 0.6022605955600738, Adversary Loss: 0.6929352164268494\n",
            "Epoch 27/100 - Main Loss: 0.6008794128894805, Adversary Loss: 0.6947105467319489\n",
            "Epoch 28/100 - Main Loss: 0.5996786832809449, Adversary Loss: 0.6940326452255249\n",
            "Epoch 29/100 - Main Loss: 0.598581051826477, Adversary Loss: 0.6934964597225189\n",
            "Epoch 30/100 - Main Loss: 0.5974928200244903, Adversary Loss: 0.693278843164444\n",
            "Epoch 31/100 - Main Loss: 0.5965008497238159, Adversary Loss: 0.6930100500583649\n",
            "Epoch 32/100 - Main Loss: 0.5955216825008393, Adversary Loss: 0.6926185488700867\n",
            "Epoch 33/100 - Main Loss: 0.5946058571338654, Adversary Loss: 0.6946971952915192\n",
            "Epoch 34/100 - Main Loss: 0.593720155954361, Adversary Loss: 0.693320232629776\n",
            "Epoch 35/100 - Main Loss: 0.5928982019424438, Adversary Loss: 0.6923160314559936\n",
            "Epoch 36/100 - Main Loss: 0.5920984148979187, Adversary Loss: 0.6925205171108246\n",
            "Epoch 37/100 - Main Loss: 0.5913293778896331, Adversary Loss: 0.6932452082633972\n",
            "Epoch 38/100 - Main Loss: 0.5906294882297516, Adversary Loss: 0.6926218807697296\n",
            "Epoch 39/100 - Main Loss: 0.5899063289165497, Adversary Loss: 0.6944490909576416\n",
            "Epoch 40/100 - Main Loss: 0.5892304599285125, Adversary Loss: 0.6937564313411713\n",
            "Epoch 41/100 - Main Loss: 0.5885276556015014, Adversary Loss: 0.6924717843532562\n",
            "Epoch 42/100 - Main Loss: 0.587908285856247, Adversary Loss: 0.6924549221992493\n",
            "Epoch 43/100 - Main Loss: 0.5873256742954254, Adversary Loss: 0.6939671754837036\n",
            "Epoch 44/100 - Main Loss: 0.5867254793643951, Adversary Loss: 0.6940112590789795\n",
            "Epoch 45/100 - Main Loss: 0.5861217021942139, Adversary Loss: 0.6922552108764648\n",
            "Epoch 46/100 - Main Loss: 0.585564810037613, Adversary Loss: 0.694518095254898\n",
            "Epoch 47/100 - Main Loss: 0.5850021123886109, Adversary Loss: 0.6923272967338562\n",
            "Epoch 48/100 - Main Loss: 0.5845791161060333, Adversary Loss: 0.6930158615112305\n",
            "Epoch 49/100 - Main Loss: 0.584044462442398, Adversary Loss: 0.691787588596344\n",
            "Epoch 50/100 - Main Loss: 0.5835166573524475, Adversary Loss: 0.6935750544071198\n",
            "Epoch 51/100 - Main Loss: 0.5831166267395019, Adversary Loss: 0.6928125023841858\n",
            "Epoch 52/100 - Main Loss: 0.5826556026935578, Adversary Loss: 0.693520051240921\n",
            "Epoch 53/100 - Main Loss: 0.5822126805782318, Adversary Loss: 0.691894245147705\n",
            "Epoch 54/100 - Main Loss: 0.5818032205104828, Adversary Loss: 0.6930811524391174\n",
            "Epoch 55/100 - Main Loss: 0.5814011931419373, Adversary Loss: 0.692992240190506\n",
            "Epoch 56/100 - Main Loss: 0.581043928861618, Adversary Loss: 0.6926925957202912\n",
            "Epoch 57/100 - Main Loss: 0.5806264996528625, Adversary Loss: 0.6932640016078949\n",
            "Epoch 58/100 - Main Loss: 0.5803087651729584, Adversary Loss: 0.6929649770259857\n",
            "Epoch 59/100 - Main Loss: 0.5799718201160431, Adversary Loss: 0.6932065963745118\n",
            "Epoch 60/100 - Main Loss: 0.579631793498993, Adversary Loss: 0.6927936434745788\n",
            "Epoch 61/100 - Main Loss: 0.5793224096298217, Adversary Loss: 0.6935038387775421\n",
            "Epoch 62/100 - Main Loss: 0.579009884595871, Adversary Loss: 0.692441213130951\n",
            "Epoch 63/100 - Main Loss: 0.5786810994148255, Adversary Loss: 0.693434226512909\n",
            "Epoch 64/100 - Main Loss: 0.5783640146255493, Adversary Loss: 0.6940808653831482\n",
            "Epoch 65/100 - Main Loss: 0.5780942618846894, Adversary Loss: 0.6926608681678772\n",
            "Epoch 66/100 - Main Loss: 0.5778406918048858, Adversary Loss: 0.6937515616416932\n",
            "Epoch 67/100 - Main Loss: 0.5775574207305908, Adversary Loss: 0.6925725996494293\n",
            "Epoch 68/100 - Main Loss: 0.577275025844574, Adversary Loss: 0.692919385433197\n",
            "Epoch 69/100 - Main Loss: 0.5770329594612121, Adversary Loss: 0.6927326500415802\n",
            "Epoch 70/100 - Main Loss: 0.5767585277557373, Adversary Loss: 0.6925413608551025\n",
            "Epoch 71/100 - Main Loss: 0.5765361428260803, Adversary Loss: 0.6932321190834045\n",
            "Epoch 72/100 - Main Loss: 0.5763240456581116, Adversary Loss: 0.6920337617397309\n",
            "Epoch 73/100 - Main Loss: 0.5760319471359253, Adversary Loss: 0.6937094151973724\n",
            "Epoch 74/100 - Main Loss: 0.5758673310279846, Adversary Loss: 0.6934577345848083\n",
            "Epoch 75/100 - Main Loss: 0.5756230294704437, Adversary Loss: 0.6923645377159119\n",
            "Epoch 76/100 - Main Loss: 0.5753950715065003, Adversary Loss: 0.6933107197284698\n",
            "Epoch 77/100 - Main Loss: 0.5751601099967957, Adversary Loss: 0.692739850282669\n",
            "Epoch 78/100 - Main Loss: 0.57494335770607, Adversary Loss: 0.6933956623077393\n",
            "Epoch 79/100 - Main Loss: 0.5747281730175018, Adversary Loss: 0.6921990752220154\n",
            "Epoch 80/100 - Main Loss: 0.5745208024978637, Adversary Loss: 0.6930042266845703\n",
            "Epoch 81/100 - Main Loss: 0.574363899230957, Adversary Loss: 0.6940731763839721\n",
            "Epoch 82/100 - Main Loss: 0.5741243958473206, Adversary Loss: 0.6932339668273926\n",
            "Epoch 83/100 - Main Loss: 0.573916894197464, Adversary Loss: 0.6923422694206238\n",
            "Epoch 84/100 - Main Loss: 0.5737511277198791, Adversary Loss: 0.693128502368927\n",
            "Epoch 85/100 - Main Loss: 0.5735626697540284, Adversary Loss: 0.6934980392456055\n",
            "Epoch 86/100 - Main Loss: 0.5733615040779114, Adversary Loss: 0.6928886651992798\n",
            "Epoch 87/100 - Main Loss: 0.5731898963451385, Adversary Loss: 0.6932756960391998\n",
            "Epoch 88/100 - Main Loss: 0.5730201601982117, Adversary Loss: 0.6923730850219727\n",
            "Epoch 89/100 - Main Loss: 0.5727960228919983, Adversary Loss: 0.6929029583930969\n",
            "Epoch 90/100 - Main Loss: 0.5727067351341247, Adversary Loss: 0.6930859208106994\n",
            "Epoch 91/100 - Main Loss: 0.5724613785743713, Adversary Loss: 0.6935540676116944\n",
            "Epoch 92/100 - Main Loss: 0.5722828686237336, Adversary Loss: 0.6926924109458923\n",
            "Epoch 93/100 - Main Loss: 0.5720785319805145, Adversary Loss: 0.6927140891551972\n",
            "Epoch 94/100 - Main Loss: 0.5719148397445679, Adversary Loss: 0.6928905487060547\n",
            "Epoch 95/100 - Main Loss: 0.5717671334743499, Adversary Loss: 0.6934854507446289\n",
            "Epoch 96/100 - Main Loss: 0.5716420829296112, Adversary Loss: 0.6930997312068939\n",
            "Epoch 97/100 - Main Loss: 0.5713919281959534, Adversary Loss: 0.6926422357559204\n",
            "Epoch 98/100 - Main Loss: 0.5712813556194305, Adversary Loss: 0.6929175913333893\n",
            "Epoch 99/100 - Main Loss: 0.5710857391357422, Adversary Loss: 0.6931318461894989\n",
            "Test Accuracy: 0.8562\n",
            "Testing with lr_main=0.001, lr_adv=0.001\n",
            "Epoch 0/100 - Main Loss: 0.7156491458415986, Adversary Loss: 0.6953891932964325\n",
            "Epoch 1/100 - Main Loss: 0.7058516502380371, Adversary Loss: 0.6934747397899628\n",
            "Epoch 2/100 - Main Loss: 0.6968595921993256, Adversary Loss: 0.6932884752750397\n",
            "Epoch 3/100 - Main Loss: 0.6887068688869477, Adversary Loss: 0.6929890215396881\n",
            "Epoch 4/100 - Main Loss: 0.6814617931842804, Adversary Loss: 0.6929916858673095\n",
            "Epoch 5/100 - Main Loss: 0.6747771680355072, Adversary Loss: 0.6932642340660096\n",
            "Epoch 6/100 - Main Loss: 0.6686826586723328, Adversary Loss: 0.6931180417537689\n",
            "Epoch 7/100 - Main Loss: 0.662909996509552, Adversary Loss: 0.6929883778095245\n",
            "Epoch 8/100 - Main Loss: 0.6575051307678222, Adversary Loss: 0.692832064628601\n",
            "Epoch 9/100 - Main Loss: 0.6521981239318848, Adversary Loss: 0.6931389033794403\n",
            "Epoch 10/100 - Main Loss: 0.647372704744339, Adversary Loss: 0.693554812669754\n",
            "Epoch 11/100 - Main Loss: 0.6426042497158051, Adversary Loss: 0.6931893348693847\n",
            "Epoch 12/100 - Main Loss: 0.6381588399410247, Adversary Loss: 0.6926770269870758\n",
            "Epoch 13/100 - Main Loss: 0.6340772688388825, Adversary Loss: 0.6931354105472565\n",
            "Epoch 14/100 - Main Loss: 0.6303158581256867, Adversary Loss: 0.6930249154567718\n",
            "Epoch 15/100 - Main Loss: 0.6267411947250366, Adversary Loss: 0.693536502122879\n",
            "Epoch 16/100 - Main Loss: 0.6236543357372284, Adversary Loss: 0.693511837720871\n",
            "Epoch 17/100 - Main Loss: 0.620608127117157, Adversary Loss: 0.6932566165924072\n",
            "Epoch 18/100 - Main Loss: 0.6179762780666351, Adversary Loss: 0.6929612278938293\n",
            "Epoch 19/100 - Main Loss: 0.6155367434024811, Adversary Loss: 0.6934077680110932\n",
            "Epoch 20/100 - Main Loss: 0.6133349418640137, Adversary Loss: 0.6932288527488708\n",
            "Epoch 21/100 - Main Loss: 0.6113105237483978, Adversary Loss: 0.6931950449943542\n",
            "Epoch 22/100 - Main Loss: 0.6094301760196685, Adversary Loss: 0.6933316826820374\n",
            "Epoch 23/100 - Main Loss: 0.607772433757782, Adversary Loss: 0.6931402087211609\n",
            "Epoch 24/100 - Main Loss: 0.6062526524066925, Adversary Loss: 0.6930051386356354\n",
            "Epoch 25/100 - Main Loss: 0.604787963628769, Adversary Loss: 0.6930251061916352\n",
            "Epoch 26/100 - Main Loss: 0.6034994184970855, Adversary Loss: 0.6929994583129883\n",
            "Epoch 27/100 - Main Loss: 0.6022784054279328, Adversary Loss: 0.6929926693439483\n",
            "Epoch 28/100 - Main Loss: 0.6011293113231659, Adversary Loss: 0.6913614273071289\n",
            "Epoch 29/100 - Main Loss: 0.6000363171100617, Adversary Loss: 0.6922605872154236\n",
            "Epoch 30/100 - Main Loss: 0.599003154039383, Adversary Loss: 0.6918309509754181\n",
            "Epoch 31/100 - Main Loss: 0.5980765998363495, Adversary Loss: 0.6928859412670135\n",
            "Epoch 32/100 - Main Loss: 0.5971733927726746, Adversary Loss: 0.6925082921981811\n",
            "Epoch 33/100 - Main Loss: 0.5963522911071777, Adversary Loss: 0.697330504655838\n",
            "Epoch 34/100 - Main Loss: 0.5955072462558746, Adversary Loss: 0.6931016743183136\n",
            "Epoch 35/100 - Main Loss: 0.5946835458278656, Adversary Loss: 0.6933573007583618\n",
            "Epoch 36/100 - Main Loss: 0.5939862906932831, Adversary Loss: 0.6929355680942535\n",
            "Epoch 37/100 - Main Loss: 0.5932416915893555, Adversary Loss: 0.6921468853950501\n",
            "Epoch 38/100 - Main Loss: 0.5925518155097962, Adversary Loss: 0.6924195528030396\n",
            "Epoch 39/100 - Main Loss: 0.5919236540794373, Adversary Loss: 0.6923789262771607\n",
            "Epoch 40/100 - Main Loss: 0.5912724554538726, Adversary Loss: 0.6945493757724762\n",
            "Epoch 41/100 - Main Loss: 0.5906230092048645, Adversary Loss: 0.6924261271953582\n",
            "Epoch 42/100 - Main Loss: 0.590039873123169, Adversary Loss: 0.6922676026821136\n",
            "Epoch 43/100 - Main Loss: 0.5894559144973754, Adversary Loss: 0.6913451433181763\n",
            "Epoch 44/100 - Main Loss: 0.5888993680477143, Adversary Loss: 0.6924650132656097\n",
            "Epoch 45/100 - Main Loss: 0.5883212566375733, Adversary Loss: 0.694691801071167\n",
            "Epoch 46/100 - Main Loss: 0.5878325939178467, Adversary Loss: 0.6942621946334839\n",
            "Epoch 47/100 - Main Loss: 0.5873132526874543, Adversary Loss: 0.6930463552474976\n",
            "Epoch 48/100 - Main Loss: 0.5868153691291809, Adversary Loss: 0.6926617324352264\n",
            "Epoch 49/100 - Main Loss: 0.5863195180892944, Adversary Loss: 0.6928881585597992\n",
            "Epoch 50/100 - Main Loss: 0.5858514785766602, Adversary Loss: 0.6943385601043701\n",
            "Epoch 51/100 - Main Loss: 0.5854329466819763, Adversary Loss: 0.6927675783634186\n",
            "Epoch 52/100 - Main Loss: 0.5849951326847076, Adversary Loss: 0.6931861221790314\n",
            "Epoch 53/100 - Main Loss: 0.5846276462078095, Adversary Loss: 0.693103015422821\n",
            "Epoch 54/100 - Main Loss: 0.5841636955738068, Adversary Loss: 0.6932753086090088\n",
            "Epoch 55/100 - Main Loss: 0.5837576508522033, Adversary Loss: 0.693140584230423\n",
            "Epoch 56/100 - Main Loss: 0.5833994507789612, Adversary Loss: 0.6932729661464692\n",
            "Epoch 57/100 - Main Loss: 0.5830695271492005, Adversary Loss: 0.6927129983901977\n",
            "Epoch 58/100 - Main Loss: 0.5826823115348816, Adversary Loss: 0.6934880912303925\n",
            "Epoch 59/100 - Main Loss: 0.5823319673538208, Adversary Loss: 0.6931420683860778\n",
            "Epoch 60/100 - Main Loss: 0.5820000171661377, Adversary Loss: 0.6941171407699585\n",
            "Epoch 61/100 - Main Loss: 0.5816863358020783, Adversary Loss: 0.6930065274238586\n",
            "Epoch 62/100 - Main Loss: 0.5813071489334106, Adversary Loss: 0.6929026126861573\n",
            "Epoch 63/100 - Main Loss: 0.58099365234375, Adversary Loss: 0.6930260777473449\n",
            "Epoch 64/100 - Main Loss: 0.5807569086551666, Adversary Loss: 0.6932787299156189\n",
            "Epoch 65/100 - Main Loss: 0.5804150700569153, Adversary Loss: 0.6933677971363068\n",
            "Epoch 66/100 - Main Loss: 0.5801261305809021, Adversary Loss: 0.6932835817337036\n",
            "Epoch 67/100 - Main Loss: 0.5798683881759643, Adversary Loss: 0.6931449055671692\n",
            "Epoch 68/100 - Main Loss: 0.57958425283432, Adversary Loss: 0.6932124257087707\n",
            "Epoch 69/100 - Main Loss: 0.5792913317680359, Adversary Loss: 0.6931376755237579\n",
            "Epoch 70/100 - Main Loss: 0.5790281236171723, Adversary Loss: 0.6929598391056061\n",
            "Epoch 71/100 - Main Loss: 0.5787767231464386, Adversary Loss: 0.6931584358215332\n",
            "Epoch 72/100 - Main Loss: 0.5785178303718567, Adversary Loss: 0.6930773437023163\n",
            "Epoch 73/100 - Main Loss: 0.5782701790332794, Adversary Loss: 0.6933076798915863\n",
            "Epoch 74/100 - Main Loss: 0.5780264377593994, Adversary Loss: 0.6932080805301666\n",
            "Epoch 75/100 - Main Loss: 0.5778232753276825, Adversary Loss: 0.6930724561214447\n",
            "Epoch 76/100 - Main Loss: 0.5775634527206421, Adversary Loss: 0.6926272690296174\n",
            "Epoch 77/100 - Main Loss: 0.577352374792099, Adversary Loss: 0.6926708877086639\n",
            "Epoch 78/100 - Main Loss: 0.5771166563034058, Adversary Loss: 0.6927738606929779\n",
            "Epoch 79/100 - Main Loss: 0.5769321620464325, Adversary Loss: 0.6916457295417786\n",
            "Epoch 80/100 - Main Loss: 0.5766919493675232, Adversary Loss: 0.6917410671710968\n",
            "Epoch 81/100 - Main Loss: 0.5764679253101349, Adversary Loss: 0.6904060363769531\n",
            "Epoch 82/100 - Main Loss: 0.5762965500354766, Adversary Loss: 0.6929995596408844\n",
            "Epoch 83/100 - Main Loss: 0.5760757684707641, Adversary Loss: 0.695175975561142\n",
            "Epoch 84/100 - Main Loss: 0.5758975565433502, Adversary Loss: 0.6952390730381012\n",
            "Epoch 85/100 - Main Loss: 0.5756602168083191, Adversary Loss: 0.6930984735488892\n",
            "Epoch 86/100 - Main Loss: 0.5755007028579712, Adversary Loss: 0.6923352241516113\n",
            "Epoch 87/100 - Main Loss: 0.5753111422061921, Adversary Loss: 0.6932783365249634\n",
            "Epoch 88/100 - Main Loss: 0.5751156032085418, Adversary Loss: 0.6938095390796661\n",
            "Epoch 89/100 - Main Loss: 0.5749500572681427, Adversary Loss: 0.6931578755378723\n",
            "Epoch 90/100 - Main Loss: 0.5747457087039948, Adversary Loss: 0.692901611328125\n",
            "Epoch 91/100 - Main Loss: 0.5746331512928009, Adversary Loss: 0.6934659361839295\n",
            "Epoch 92/100 - Main Loss: 0.574395501613617, Adversary Loss: 0.6928887963294983\n",
            "Epoch 93/100 - Main Loss: 0.5742545604705811, Adversary Loss: 0.6935920536518096\n",
            "Epoch 94/100 - Main Loss: 0.5740348100662231, Adversary Loss: 0.693434190750122\n",
            "Epoch 95/100 - Main Loss: 0.573891019821167, Adversary Loss: 0.6931345760822296\n",
            "Epoch 96/100 - Main Loss: 0.5737040400505066, Adversary Loss: 0.6930902421474456\n",
            "Epoch 97/100 - Main Loss: 0.573555839061737, Adversary Loss: 0.6939018666744232\n",
            "Epoch 98/100 - Main Loss: 0.5734062671661377, Adversary Loss: 0.6933613717556\n",
            "Epoch 99/100 - Main Loss: 0.5731989443302155, Adversary Loss: 0.6932153344154358\n",
            "Test Accuracy: 0.8562\n",
            "Testing with lr_main=0.001, lr_adv=0.01\n",
            "Epoch 0/100 - Main Loss: 0.7196733355522156, Adversary Loss: 0.6953331351280212\n",
            "Epoch 1/100 - Main Loss: 0.7096736192703247, Adversary Loss: 0.7025177776813507\n",
            "Epoch 2/100 - Main Loss: 0.7005599200725555, Adversary Loss: 0.6943272888660431\n",
            "Epoch 3/100 - Main Loss: 0.692484837770462, Adversary Loss: 0.6937122285366059\n",
            "Epoch 4/100 - Main Loss: 0.6850639283657074, Adversary Loss: 0.6951919198036194\n",
            "Epoch 5/100 - Main Loss: 0.6784144639968872, Adversary Loss: 0.6937724709510803\n",
            "Epoch 6/100 - Main Loss: 0.6724074900150299, Adversary Loss: 0.6938507199287415\n",
            "Epoch 7/100 - Main Loss: 0.6668287634849548, Adversary Loss: 0.694561904668808\n",
            "Epoch 8/100 - Main Loss: 0.6615740597248078, Adversary Loss: 0.6949217200279236\n",
            "Epoch 9/100 - Main Loss: 0.6565013766288758, Adversary Loss: 0.6939537227153778\n",
            "Epoch 10/100 - Main Loss: 0.6515782415866852, Adversary Loss: 0.6947553515434265\n",
            "Epoch 11/100 - Main Loss: 0.6467214941978454, Adversary Loss: 0.6942926287651062\n",
            "Epoch 12/100 - Main Loss: 0.6422884285449981, Adversary Loss: 0.6939492106437684\n",
            "Epoch 13/100 - Main Loss: 0.6381508529186248, Adversary Loss: 0.6940823197364807\n",
            "Epoch 14/100 - Main Loss: 0.6340054154396058, Adversary Loss: 0.6943605542182922\n",
            "Epoch 15/100 - Main Loss: 0.6303365767002106, Adversary Loss: 0.6951591193675994\n",
            "Epoch 16/100 - Main Loss: 0.6268216073513031, Adversary Loss: 0.6943212807178497\n",
            "Epoch 17/100 - Main Loss: 0.62361860871315, Adversary Loss: 0.6953549385070801\n",
            "Epoch 18/100 - Main Loss: 0.6206446886062622, Adversary Loss: 0.6946453630924225\n",
            "Epoch 19/100 - Main Loss: 0.618096649646759, Adversary Loss: 0.6938425719738006\n",
            "Epoch 20/100 - Main Loss: 0.6156603991985321, Adversary Loss: 0.694394052028656\n",
            "Epoch 21/100 - Main Loss: 0.6133882880210877, Adversary Loss: 0.6943663537502289\n",
            "Epoch 22/100 - Main Loss: 0.6113540947437286, Adversary Loss: 0.693742823600769\n",
            "Epoch 23/100 - Main Loss: 0.609543913602829, Adversary Loss: 0.6934031963348388\n",
            "Epoch 24/100 - Main Loss: 0.6078192293643951, Adversary Loss: 0.6942093253135682\n",
            "Epoch 25/100 - Main Loss: 0.6062492489814758, Adversary Loss: 0.6943546175956726\n",
            "Epoch 26/100 - Main Loss: 0.6048520982265473, Adversary Loss: 0.6941593408584594\n",
            "Epoch 27/100 - Main Loss: 0.6034621357917785, Adversary Loss: 0.6944485187530518\n",
            "Epoch 28/100 - Main Loss: 0.602286434173584, Adversary Loss: 0.6948709785938263\n",
            "Epoch 29/100 - Main Loss: 0.6011448621749877, Adversary Loss: 0.6938399732112884\n",
            "Epoch 30/100 - Main Loss: 0.6000444531440735, Adversary Loss: 0.694022411108017\n",
            "Epoch 31/100 - Main Loss: 0.5990455448627472, Adversary Loss: 0.694436889886856\n",
            "Epoch 32/100 - Main Loss: 0.5980755865573884, Adversary Loss: 0.6945782363414764\n",
            "Epoch 33/100 - Main Loss: 0.5971854209899903, Adversary Loss: 0.6951997578144073\n",
            "Epoch 34/100 - Main Loss: 0.5963650643825531, Adversary Loss: 0.6931783139705658\n",
            "Epoch 35/100 - Main Loss: 0.5955314218997956, Adversary Loss: 0.6947163462638855\n",
            "Epoch 36/100 - Main Loss: 0.5948329627513885, Adversary Loss: 0.6953178644180298\n",
            "Epoch 37/100 - Main Loss: 0.5940416395664215, Adversary Loss: 0.6938705325126648\n",
            "Epoch 38/100 - Main Loss: 0.5933517813682556, Adversary Loss: 0.6939337968826294\n",
            "Epoch 39/100 - Main Loss: 0.592709070444107, Adversary Loss: 0.6938120305538178\n",
            "Epoch 40/100 - Main Loss: 0.5920589089393615, Adversary Loss: 0.6941118240356445\n",
            "Epoch 41/100 - Main Loss: 0.5914117813110351, Adversary Loss: 0.6942660748958588\n",
            "Epoch 42/100 - Main Loss: 0.5908514380455017, Adversary Loss: 0.6940148115158081\n",
            "Epoch 43/100 - Main Loss: 0.5902721762657166, Adversary Loss: 0.6938277125358582\n",
            "Epoch 44/100 - Main Loss: 0.589671391248703, Adversary Loss: 0.6934025108814239\n",
            "Epoch 45/100 - Main Loss: 0.5891277909278869, Adversary Loss: 0.693355530500412\n",
            "Epoch 46/100 - Main Loss: 0.5885894060134887, Adversary Loss: 0.6922926187515259\n",
            "Epoch 47/100 - Main Loss: 0.58812335729599, Adversary Loss: 0.6935944974422454\n",
            "Epoch 48/100 - Main Loss: 0.5876224100589752, Adversary Loss: 0.6962343275547027\n",
            "Epoch 49/100 - Main Loss: 0.5871242344379425, Adversary Loss: 0.6912920415401459\n",
            "Epoch 50/100 - Main Loss: 0.5867226123809814, Adversary Loss: 0.6948116183280945\n",
            "Epoch 51/100 - Main Loss: 0.5862110674381256, Adversary Loss: 0.6969580054283142\n",
            "Epoch 52/100 - Main Loss: 0.5857675433158874, Adversary Loss: 0.6940398216247559\n",
            "Epoch 53/100 - Main Loss: 0.5853838145732879, Adversary Loss: 0.6940656960010528\n",
            "Epoch 54/100 - Main Loss: 0.5849813759326935, Adversary Loss: 0.693729567527771\n",
            "Epoch 55/100 - Main Loss: 0.5845788717269897, Adversary Loss: 0.6934227228164673\n",
            "Epoch 56/100 - Main Loss: 0.5841820061206817, Adversary Loss: 0.692710793018341\n",
            "Epoch 57/100 - Main Loss: 0.5838204741477966, Adversary Loss: 0.6965409874916076\n",
            "Epoch 58/100 - Main Loss: 0.5834854125976563, Adversary Loss: 0.6935076892375946\n",
            "Epoch 59/100 - Main Loss: 0.5831118524074554, Adversary Loss: 0.6935056865215301\n",
            "Epoch 60/100 - Main Loss: 0.5828230142593384, Adversary Loss: 0.6925530433654785\n",
            "Epoch 61/100 - Main Loss: 0.5825212478637696, Adversary Loss: 0.694122177362442\n",
            "Epoch 62/100 - Main Loss: 0.5821807503700256, Adversary Loss: 0.6947926759719849\n",
            "Epoch 63/100 - Main Loss: 0.58187016248703, Adversary Loss: 0.6939935922622681\n",
            "Epoch 64/100 - Main Loss: 0.5815445184707642, Adversary Loss: 0.6940705120563507\n",
            "Epoch 65/100 - Main Loss: 0.5812755346298217, Adversary Loss: 0.6934660077095032\n",
            "Epoch 66/100 - Main Loss: 0.5809792935848236, Adversary Loss: 0.6928386926651001\n",
            "Epoch 67/100 - Main Loss: 0.580751633644104, Adversary Loss: 0.6947817981243134\n",
            "Epoch 68/100 - Main Loss: 0.5804560065269471, Adversary Loss: 0.6947175562381744\n",
            "Epoch 69/100 - Main Loss: 0.5801639914512634, Adversary Loss: 0.6940522909164428\n",
            "Epoch 70/100 - Main Loss: 0.5799878537654877, Adversary Loss: 0.693189126253128\n",
            "Epoch 71/100 - Main Loss: 0.5796893298625946, Adversary Loss: 0.6933466196060181\n",
            "Epoch 72/100 - Main Loss: 0.579442274570465, Adversary Loss: 0.6950627207756043\n",
            "Epoch 73/100 - Main Loss: 0.5792111992835999, Adversary Loss: 0.6932785749435425\n",
            "Epoch 74/100 - Main Loss: 0.5790404915809632, Adversary Loss: 0.6942720770835876\n",
            "Epoch 75/100 - Main Loss: 0.5787833094596863, Adversary Loss: 0.6933757543563843\n",
            "Epoch 76/100 - Main Loss: 0.5785746812820435, Adversary Loss: 0.694551420211792\n",
            "Epoch 77/100 - Main Loss: 0.5783578991889954, Adversary Loss: 0.6936223924160003\n",
            "Epoch 78/100 - Main Loss: 0.5781200706958771, Adversary Loss: 0.6933925807476043\n",
            "Epoch 79/100 - Main Loss: 0.5779556214809418, Adversary Loss: 0.6935343325138092\n",
            "Epoch 80/100 - Main Loss: 0.5777890145778656, Adversary Loss: 0.6941998958587646\n",
            "Epoch 81/100 - Main Loss: 0.5775680184364319, Adversary Loss: 0.6935124278068543\n",
            "Epoch 82/100 - Main Loss: 0.5773519247770309, Adversary Loss: 0.6935957252979279\n",
            "Epoch 83/100 - Main Loss: 0.577170342206955, Adversary Loss: 0.6937639772891998\n",
            "Epoch 84/100 - Main Loss: 0.5769638299942017, Adversary Loss: 0.6934870779514313\n",
            "Epoch 85/100 - Main Loss: 0.5768051862716674, Adversary Loss: 0.6934882044792176\n",
            "Epoch 86/100 - Main Loss: 0.5766206026077271, Adversary Loss: 0.6937953412532807\n",
            "Epoch 87/100 - Main Loss: 0.5764748871326446, Adversary Loss: 0.6938546299934387\n",
            "Epoch 88/100 - Main Loss: 0.576289814710617, Adversary Loss: 0.6938956260681153\n",
            "Epoch 89/100 - Main Loss: 0.5761675477027893, Adversary Loss: 0.6933750748634339\n",
            "Epoch 90/100 - Main Loss: 0.5759421288967133, Adversary Loss: 0.6931254804134369\n",
            "Epoch 91/100 - Main Loss: 0.5758029282093048, Adversary Loss: 0.6941173434257507\n",
            "Epoch 92/100 - Main Loss: 0.5756438374519348, Adversary Loss: 0.6934082269668579\n",
            "Epoch 93/100 - Main Loss: 0.575551176071167, Adversary Loss: 0.6937330484390258\n",
            "Epoch 94/100 - Main Loss: 0.5753289222717285, Adversary Loss: 0.6930505037307739\n",
            "Epoch 95/100 - Main Loss: 0.5752356827259064, Adversary Loss: 0.6926637291908264\n",
            "Epoch 96/100 - Main Loss: 0.5750510811805725, Adversary Loss: 0.6932424485683442\n",
            "Epoch 97/100 - Main Loss: 0.5749427676200867, Adversary Loss: 0.6934277713298798\n",
            "Epoch 98/100 - Main Loss: 0.5748067021369934, Adversary Loss: 0.6933644115924835\n",
            "Epoch 99/100 - Main Loss: 0.5746447026729584, Adversary Loss: 0.6935012340545654\n",
            "Test Accuracy: 0.8562\n",
            "Testing with lr_main=0.01, lr_adv=0.0001\n",
            "Epoch 0/100 - Main Loss: 0.6865127086639404, Adversary Loss: 0.6936353087425232\n",
            "Epoch 1/100 - Main Loss: 0.6309349536895752, Adversary Loss: 0.6940124928951263\n",
            "Epoch 2/100 - Main Loss: 0.606248527765274, Adversary Loss: 0.695681345462799\n",
            "Epoch 3/100 - Main Loss: 0.5957063257694244, Adversary Loss: 0.6950987040996551\n",
            "Epoch 4/100 - Main Loss: 0.5893253803253173, Adversary Loss: 0.6960180819034576\n",
            "Epoch 5/100 - Main Loss: 0.5846036672592163, Adversary Loss: 0.6972269952297211\n",
            "Epoch 6/100 - Main Loss: 0.5805022120475769, Adversary Loss: 0.6938814461231232\n",
            "Epoch 7/100 - Main Loss: 0.5786346018314361, Adversary Loss: 0.6915802121162414\n",
            "Epoch 8/100 - Main Loss: 0.5766196250915527, Adversary Loss: 0.6954907953739167\n",
            "Epoch 9/100 - Main Loss: 0.5749510169029236, Adversary Loss: 0.6912021398544311\n",
            "Epoch 10/100 - Main Loss: 0.573961877822876, Adversary Loss: 0.6939665377140045\n",
            "Epoch 11/100 - Main Loss: 0.5731263101100922, Adversary Loss: 0.6939029574394227\n",
            "Epoch 12/100 - Main Loss: 0.5726329505443573, Adversary Loss: 0.6950188994407653\n",
            "Epoch 13/100 - Main Loss: 0.5717725396156311, Adversary Loss: 0.6925813496112824\n",
            "Epoch 14/100 - Main Loss: 0.5709109246730805, Adversary Loss: 0.6946051120758057\n",
            "Epoch 15/100 - Main Loss: 0.5705792486667634, Adversary Loss: 0.6933975875377655\n",
            "Epoch 16/100 - Main Loss: 0.5697418630123139, Adversary Loss: 0.6944276213645935\n",
            "Epoch 17/100 - Main Loss: 0.5695904612541198, Adversary Loss: 0.6959132552146912\n",
            "Epoch 18/100 - Main Loss: 0.5688225507736206, Adversary Loss: 0.69254749417305\n",
            "Epoch 19/100 - Main Loss: 0.5688205480575561, Adversary Loss: 0.6933706700801849\n",
            "Epoch 20/100 - Main Loss: 0.5679067194461822, Adversary Loss: 0.6930419325828552\n",
            "Epoch 21/100 - Main Loss: 0.5679245948791504, Adversary Loss: 0.6930961608886719\n",
            "Epoch 22/100 - Main Loss: 0.5672937214374543, Adversary Loss: 0.6948638021945953\n",
            "Epoch 23/100 - Main Loss: 0.5670780122280121, Adversary Loss: 0.6926349818706512\n",
            "Epoch 24/100 - Main Loss: 0.5664906561374664, Adversary Loss: 0.6954452514648437\n",
            "Epoch 25/100 - Main Loss: 0.5663748860359192, Adversary Loss: 0.6967202663421631\n",
            "Epoch 26/100 - Main Loss: 0.5658413350582123, Adversary Loss: 0.6920808494091034\n",
            "Epoch 27/100 - Main Loss: 0.5654623687267304, Adversary Loss: 0.6935709357261658\n",
            "Epoch 28/100 - Main Loss: 0.5650219976902008, Adversary Loss: 0.6900272369384766\n",
            "Epoch 29/100 - Main Loss: 0.5651628196239471, Adversary Loss: 0.69527587890625\n",
            "Epoch 30/100 - Main Loss: 0.5651050746440888, Adversary Loss: 0.6920950770378113\n",
            "Epoch 31/100 - Main Loss: 0.564179515838623, Adversary Loss: 0.6924593329429627\n",
            "Epoch 32/100 - Main Loss: 0.5636441051959992, Adversary Loss: 0.694494217634201\n",
            "Epoch 33/100 - Main Loss: 0.5637999147176742, Adversary Loss: 0.6919270873069763\n",
            "Epoch 34/100 - Main Loss: 0.5629686713218689, Adversary Loss: 0.6957996547222137\n",
            "Epoch 35/100 - Main Loss: 0.5628996133804322, Adversary Loss: 0.6927217423915863\n",
            "Epoch 36/100 - Main Loss: 0.5624092578887939, Adversary Loss: 0.6904362320899964\n",
            "Epoch 37/100 - Main Loss: 0.5613372325897217, Adversary Loss: 0.6970704793930054\n",
            "Epoch 38/100 - Main Loss: 0.5616045594215393, Adversary Loss: 0.6930297553539276\n",
            "Epoch 39/100 - Main Loss: 0.561179929971695, Adversary Loss: 0.6970702886581421\n",
            "Epoch 40/100 - Main Loss: 0.560523945093155, Adversary Loss: 0.6944732010364533\n",
            "Epoch 41/100 - Main Loss: 0.5600568950176239, Adversary Loss: 0.6941464245319366\n",
            "Epoch 42/100 - Main Loss: 0.5599899590015411, Adversary Loss: 0.6942660331726074\n",
            "Epoch 43/100 - Main Loss: 0.5596701145172119, Adversary Loss: 0.6923529803752899\n",
            "Epoch 44/100 - Main Loss: 0.5592810332775116, Adversary Loss: 0.6951103687286377\n",
            "Epoch 45/100 - Main Loss: 0.5592586934566498, Adversary Loss: 0.6935996830463409\n",
            "Epoch 46/100 - Main Loss: 0.5590933620929718, Adversary Loss: 0.6941101789474488\n",
            "Epoch 47/100 - Main Loss: 0.558393532037735, Adversary Loss: 0.6925681233406067\n",
            "Epoch 48/100 - Main Loss: 0.5585874140262603, Adversary Loss: 0.6951209783554078\n",
            "Epoch 49/100 - Main Loss: 0.5580573976039886, Adversary Loss: 0.6931498110294342\n",
            "Epoch 50/100 - Main Loss: 0.5576169192790985, Adversary Loss: 0.6920887649059295\n",
            "Epoch 51/100 - Main Loss: 0.5573531508445739, Adversary Loss: 0.6927310764789582\n",
            "Epoch 52/100 - Main Loss: 0.5572093725204468, Adversary Loss: 0.6921656847000122\n",
            "Epoch 53/100 - Main Loss: 0.5568488895893097, Adversary Loss: 0.6930564045906067\n",
            "Epoch 54/100 - Main Loss: 0.5566141724586486, Adversary Loss: 0.6923734724521637\n",
            "Epoch 55/100 - Main Loss: 0.5568655908107758, Adversary Loss: 0.6949245631694794\n",
            "Epoch 56/100 - Main Loss: 0.5571126520633698, Adversary Loss: 0.6945660412311554\n",
            "Epoch 57/100 - Main Loss: 0.5562483966350555, Adversary Loss: 0.6921476483345032\n",
            "Epoch 58/100 - Main Loss: 0.5558152019977569, Adversary Loss: 0.6930549502372741\n",
            "Epoch 59/100 - Main Loss: 0.5558541536331176, Adversary Loss: 0.6928251326084137\n",
            "Epoch 60/100 - Main Loss: 0.55517857670784, Adversary Loss: 0.6933763504028321\n",
            "Epoch 61/100 - Main Loss: 0.5546950042247772, Adversary Loss: 0.6925070583820343\n",
            "Epoch 62/100 - Main Loss: 0.5546152830123902, Adversary Loss: 0.693468701839447\n",
            "Epoch 63/100 - Main Loss: 0.5545641183853149, Adversary Loss: 0.6941229104995728\n",
            "Epoch 64/100 - Main Loss: 0.554117226600647, Adversary Loss: 0.693087762594223\n",
            "Epoch 65/100 - Main Loss: 0.5542151868343353, Adversary Loss: 0.692875075340271\n",
            "Epoch 66/100 - Main Loss: 0.5540267527103424, Adversary Loss: 0.6939385056495666\n",
            "Epoch 67/100 - Main Loss: 0.5539877057075501, Adversary Loss: 0.6925803840160369\n",
            "Epoch 68/100 - Main Loss: 0.554040452837944, Adversary Loss: 0.6924619317054749\n",
            "Epoch 69/100 - Main Loss: 0.5533854752779007, Adversary Loss: 0.6933939695358277\n",
            "Epoch 70/100 - Main Loss: 0.5534706056118012, Adversary Loss: 0.6922527730464936\n",
            "Epoch 71/100 - Main Loss: 0.5536314070224762, Adversary Loss: 0.6928529500961303\n",
            "Epoch 72/100 - Main Loss: 0.5531382024288177, Adversary Loss: 0.6933499157428742\n",
            "Epoch 73/100 - Main Loss: 0.5532644987106323, Adversary Loss: 0.6937003076076508\n",
            "Epoch 74/100 - Main Loss: 0.5530620515346527, Adversary Loss: 0.6926157712936402\n",
            "Epoch 75/100 - Main Loss: 0.5528644263744354, Adversary Loss: 0.6930670201778412\n",
            "Epoch 76/100 - Main Loss: 0.5528482377529145, Adversary Loss: 0.6926087737083435\n",
            "Epoch 77/100 - Main Loss: 0.5527345478534699, Adversary Loss: 0.6930441915988922\n",
            "Epoch 78/100 - Main Loss: 0.5522175312042237, Adversary Loss: 0.6928858757019043\n",
            "Epoch 79/100 - Main Loss: 0.5522912442684174, Adversary Loss: 0.6929599702358246\n",
            "Epoch 80/100 - Main Loss: 0.5520873725414276, Adversary Loss: 0.6922512352466583\n",
            "Epoch 81/100 - Main Loss: 0.5512849986553192, Adversary Loss: 0.6928658306598663\n",
            "Epoch 82/100 - Main Loss: 0.5512442708015441, Adversary Loss: 0.693030321598053\n",
            "Epoch 83/100 - Main Loss: 0.5510601699352264, Adversary Loss: 0.6934380888938904\n",
            "Epoch 84/100 - Main Loss: 0.5505911588668824, Adversary Loss: 0.6927902638912201\n",
            "Epoch 85/100 - Main Loss: 0.5499863445758819, Adversary Loss: 0.6924678206443786\n",
            "Epoch 86/100 - Main Loss: 0.5494860112667084, Adversary Loss: 0.6937976717948914\n",
            "Epoch 87/100 - Main Loss: 0.5493590414524079, Adversary Loss: 0.6938029587268829\n",
            "Epoch 88/100 - Main Loss: 0.54829181432724, Adversary Loss: 0.6925090968608856\n",
            "Epoch 89/100 - Main Loss: 0.5485614359378814, Adversary Loss: 0.6933683633804322\n",
            "Epoch 90/100 - Main Loss: 0.5494572132825851, Adversary Loss: 0.6925532162189484\n",
            "Epoch 91/100 - Main Loss: 0.548947525024414, Adversary Loss: 0.6929523944854736\n",
            "Epoch 92/100 - Main Loss: 0.5499521732330322, Adversary Loss: 0.6929283022880555\n",
            "Epoch 93/100 - Main Loss: 0.5485147327184677, Adversary Loss: 0.6931155562400818\n",
            "Epoch 94/100 - Main Loss: 0.547567355632782, Adversary Loss: 0.6927277863025665\n",
            "Epoch 95/100 - Main Loss: 0.5479792952537537, Adversary Loss: 0.693929922580719\n",
            "Epoch 96/100 - Main Loss: 0.5480078876018524, Adversary Loss: 0.6936391234397888\n",
            "Epoch 97/100 - Main Loss: 0.5472498536109924, Adversary Loss: 0.6926935017108917\n",
            "Epoch 98/100 - Main Loss: 0.5469424009323121, Adversary Loss: 0.6928339600563049\n",
            "Epoch 99/100 - Main Loss: 0.5463884115219116, Adversary Loss: 0.6928817212581635\n",
            "Test Accuracy: 0.8625\n",
            "Testing with lr_main=0.01, lr_adv=0.001\n",
            "Epoch 0/100 - Main Loss: 0.6902963161468506, Adversary Loss: 0.6940303802490234\n",
            "Epoch 1/100 - Main Loss: 0.6379612684249878, Adversary Loss: 0.6938627123832702\n",
            "Epoch 2/100 - Main Loss: 0.6107638359069825, Adversary Loss: 0.692874938249588\n",
            "Epoch 3/100 - Main Loss: 0.597396731376648, Adversary Loss: 0.6927162766456604\n",
            "Epoch 4/100 - Main Loss: 0.5915330410003662, Adversary Loss: 0.6934196710586548\n",
            "Epoch 5/100 - Main Loss: 0.5869262754917145, Adversary Loss: 0.6927446961402893\n",
            "Epoch 6/100 - Main Loss: 0.5826702117919922, Adversary Loss: 0.6922130465507508\n",
            "Epoch 7/100 - Main Loss: 0.5796541690826416, Adversary Loss: 0.6937975883483887\n",
            "Epoch 8/100 - Main Loss: 0.5781440496444702, Adversary Loss: 0.6929442822933197\n",
            "Epoch 9/100 - Main Loss: 0.5763360917568207, Adversary Loss: 0.6926305532455445\n",
            "Epoch 10/100 - Main Loss: 0.5744059205055236, Adversary Loss: 0.6927313506603241\n",
            "Epoch 11/100 - Main Loss: 0.5739148139953614, Adversary Loss: 0.6921906232833862\n",
            "Epoch 12/100 - Main Loss: 0.5724129140377044, Adversary Loss: 0.6937318265438079\n",
            "Epoch 13/100 - Main Loss: 0.5715031027793884, Adversary Loss: 0.692329341173172\n",
            "Epoch 14/100 - Main Loss: 0.570526522397995, Adversary Loss: 0.6928826332092285\n",
            "Epoch 15/100 - Main Loss: 0.5695222437381744, Adversary Loss: 0.6940441489219665\n",
            "Epoch 16/100 - Main Loss: 0.5700939953327179, Adversary Loss: 0.6919453978538513\n",
            "Epoch 17/100 - Main Loss: 0.5693661391735076, Adversary Loss: 0.692957729101181\n",
            "Epoch 18/100 - Main Loss: 0.5677660465240478, Adversary Loss: 0.6930600345134735\n",
            "Epoch 19/100 - Main Loss: 0.5670507788658142, Adversary Loss: 0.6932805299758911\n",
            "Epoch 20/100 - Main Loss: 0.5667239964008332, Adversary Loss: 0.6928192257881165\n",
            "Epoch 21/100 - Main Loss: 0.5660260915756226, Adversary Loss: 0.6932351410388946\n",
            "Epoch 22/100 - Main Loss: 0.5651374161243439, Adversary Loss: 0.6921772181987762\n",
            "Epoch 23/100 - Main Loss: 0.5646255135536193, Adversary Loss: 0.693988835811615\n",
            "Epoch 24/100 - Main Loss: 0.564197450876236, Adversary Loss: 0.6938389539718628\n",
            "Epoch 25/100 - Main Loss: 0.5638538300991058, Adversary Loss: 0.6932359457015991\n",
            "Epoch 26/100 - Main Loss: 0.5635591208934784, Adversary Loss: 0.6934851229190826\n",
            "Epoch 27/100 - Main Loss: 0.5629787385463715, Adversary Loss: 0.6930667459964752\n",
            "Epoch 28/100 - Main Loss: 0.5626420259475708, Adversary Loss: 0.6932411253452301\n",
            "Epoch 29/100 - Main Loss: 0.5625060856342315, Adversary Loss: 0.6928942441940308\n",
            "Epoch 30/100 - Main Loss: 0.561867567896843, Adversary Loss: 0.6935192465782165\n",
            "Epoch 31/100 - Main Loss: 0.5617944538593292, Adversary Loss: 0.6931673109531402\n",
            "Epoch 32/100 - Main Loss: 0.5618341624736786, Adversary Loss: 0.6930176556110382\n",
            "Epoch 33/100 - Main Loss: 0.5607304811477661, Adversary Loss: 0.693050491809845\n",
            "Epoch 34/100 - Main Loss: 0.5610826075077057, Adversary Loss: 0.6930225729942322\n",
            "Epoch 35/100 - Main Loss: 0.5607181847095489, Adversary Loss: 0.6931131660938263\n",
            "Epoch 36/100 - Main Loss: 0.5603491842746735, Adversary Loss: 0.693231874704361\n",
            "Epoch 37/100 - Main Loss: 0.5605462908744812, Adversary Loss: 0.6931635081768036\n",
            "Epoch 38/100 - Main Loss: 0.560469776391983, Adversary Loss: 0.6932506084442138\n",
            "Epoch 39/100 - Main Loss: 0.5597970187664032, Adversary Loss: 0.6930426001548767\n",
            "Epoch 40/100 - Main Loss: 0.5593501806259156, Adversary Loss: 0.693136203289032\n",
            "Epoch 41/100 - Main Loss: 0.5588933944702148, Adversary Loss: 0.693133395910263\n",
            "Epoch 42/100 - Main Loss: 0.5597274959087372, Adversary Loss: 0.693085926771164\n",
            "Epoch 43/100 - Main Loss: 0.5608473718166351, Adversary Loss: 0.6931362986564636\n",
            "Epoch 44/100 - Main Loss: 0.5597884893417359, Adversary Loss: 0.6922671139240265\n",
            "Epoch 45/100 - Main Loss: 0.5580965518951416, Adversary Loss: 0.6932750105857849\n",
            "Epoch 46/100 - Main Loss: 0.5583918809890747, Adversary Loss: 0.693436861038208\n",
            "Epoch 47/100 - Main Loss: 0.5575316429138184, Adversary Loss: 0.6928590118885041\n",
            "Epoch 48/100 - Main Loss: 0.5574443012475967, Adversary Loss: 0.6937599658966065\n",
            "Epoch 49/100 - Main Loss: 0.5578214466571808, Adversary Loss: 0.693037623167038\n",
            "Epoch 50/100 - Main Loss: 0.556908306479454, Adversary Loss: 0.6928674399852752\n",
            "Epoch 51/100 - Main Loss: 0.5566239565610885, Adversary Loss: 0.6931606829166412\n",
            "Epoch 52/100 - Main Loss: 0.5566769242286682, Adversary Loss: 0.6938491344451905\n",
            "Epoch 53/100 - Main Loss: 0.5566446781158447, Adversary Loss: 0.693077689409256\n",
            "Epoch 54/100 - Main Loss: 0.5566716432571411, Adversary Loss: 0.6930896401405334\n",
            "Epoch 55/100 - Main Loss: 0.5562298059463501, Adversary Loss: 0.6931043565273285\n",
            "Epoch 56/100 - Main Loss: 0.5559193551540375, Adversary Loss: 0.6929664134979248\n",
            "Epoch 57/100 - Main Loss: 0.5557518720626831, Adversary Loss: 0.6930196821689606\n",
            "Epoch 58/100 - Main Loss: 0.5556157469749451, Adversary Loss: 0.6923741519451141\n",
            "Epoch 59/100 - Main Loss: 0.5560509562492371, Adversary Loss: 0.6940890073776245\n",
            "Epoch 60/100 - Main Loss: 0.5556144297122956, Adversary Loss: 0.6935781478881836\n",
            "Epoch 61/100 - Main Loss: 0.5546409904956817, Adversary Loss: 0.6929090082645416\n",
            "Epoch 62/100 - Main Loss: 0.5545583009719849, Adversary Loss: 0.6930398941040039\n",
            "Epoch 63/100 - Main Loss: 0.5540731012821197, Adversary Loss: 0.6931042790412902\n",
            "Epoch 64/100 - Main Loss: 0.5536958336830139, Adversary Loss: 0.6929978728294373\n",
            "Epoch 65/100 - Main Loss: 0.5538956880569458, Adversary Loss: 0.6929857015609742\n",
            "Epoch 66/100 - Main Loss: 0.5541215538978577, Adversary Loss: 0.692993438243866\n",
            "Epoch 67/100 - Main Loss: 0.5530143439769745, Adversary Loss: 0.6931070685386658\n",
            "Epoch 68/100 - Main Loss: 0.5531198441982269, Adversary Loss: 0.692875725030899\n",
            "Epoch 69/100 - Main Loss: 0.5530859351158142, Adversary Loss: 0.6932024836540223\n",
            "Epoch 70/100 - Main Loss: 0.5531789839267731, Adversary Loss: 0.6933480083942414\n",
            "Epoch 71/100 - Main Loss: 0.5523031234741211, Adversary Loss: 0.693124282360077\n",
            "Epoch 72/100 - Main Loss: 0.552695220708847, Adversary Loss: 0.6929280042648316\n",
            "Epoch 73/100 - Main Loss: 0.552076929807663, Adversary Loss: 0.6930986285209656\n",
            "Epoch 74/100 - Main Loss: 0.5518197000026703, Adversary Loss: 0.6930312037467956\n",
            "Epoch 75/100 - Main Loss: 0.5514451563358307, Adversary Loss: 0.6927102446556092\n",
            "Epoch 76/100 - Main Loss: 0.551083093881607, Adversary Loss: 0.6935250699520111\n",
            "Epoch 77/100 - Main Loss: 0.5509065747261047, Adversary Loss: 0.6934697568416596\n",
            "Epoch 78/100 - Main Loss: 0.5505690187215805, Adversary Loss: 0.6930084586143493\n",
            "Epoch 79/100 - Main Loss: 0.5505951344966888, Adversary Loss: 0.6932362258434296\n",
            "Epoch 80/100 - Main Loss: 0.5504849433898926, Adversary Loss: 0.6929663240909576\n",
            "Epoch 81/100 - Main Loss: 0.5504580140113831, Adversary Loss: 0.692909836769104\n",
            "Epoch 82/100 - Main Loss: 0.5504627466201782, Adversary Loss: 0.6931253969669342\n",
            "Epoch 83/100 - Main Loss: 0.5500044047832489, Adversary Loss: 0.692779004573822\n",
            "Epoch 84/100 - Main Loss: 0.5498916864395141, Adversary Loss: 0.6924357950687409\n",
            "Epoch 85/100 - Main Loss: 0.5498620092868804, Adversary Loss: 0.6935636401176453\n",
            "Epoch 86/100 - Main Loss: 0.5496253907680512, Adversary Loss: 0.6936406016349792\n",
            "Epoch 87/100 - Main Loss: 0.5497431218624115, Adversary Loss: 0.6930444121360779\n",
            "Epoch 88/100 - Main Loss: 0.5495980978012085, Adversary Loss: 0.693114024400711\n",
            "Epoch 89/100 - Main Loss: 0.5493792295455933, Adversary Loss: 0.6930768251419067\n",
            "Epoch 90/100 - Main Loss: 0.5493591129779816, Adversary Loss: 0.6928087532520294\n",
            "Epoch 91/100 - Main Loss: 0.5499683916568756, Adversary Loss: 0.6930073380470276\n",
            "Epoch 92/100 - Main Loss: 0.5493006825447082, Adversary Loss: 0.693864506483078\n",
            "Epoch 93/100 - Main Loss: 0.549393630027771, Adversary Loss: 0.6933987438678741\n",
            "Epoch 94/100 - Main Loss: 0.5491962134838104, Adversary Loss: 0.6931367158889771\n",
            "Epoch 95/100 - Main Loss: 0.549269026517868, Adversary Loss: 0.6933252334594726\n",
            "Epoch 96/100 - Main Loss: 0.5485367178916931, Adversary Loss: 0.6923573613166809\n",
            "Epoch 97/100 - Main Loss: 0.5487690806388855, Adversary Loss: 0.6938518285751343\n",
            "Epoch 98/100 - Main Loss: 0.5486490547657012, Adversary Loss: 0.6930627703666687\n",
            "Epoch 99/100 - Main Loss: 0.5485490202903748, Adversary Loss: 0.6926018834114075\n",
            "Test Accuracy: 0.8688\n",
            "Testing with lr_main=0.01, lr_adv=0.01\n",
            "Epoch 0/100 - Main Loss: 0.6844529747962952, Adversary Loss: 0.7018688678741455\n",
            "Epoch 1/100 - Main Loss: 0.6344807088375092, Adversary Loss: 0.6975637435913086\n",
            "Epoch 2/100 - Main Loss: 0.6086726129055023, Adversary Loss: 0.6971298456192017\n",
            "Epoch 3/100 - Main Loss: 0.5973073184490204, Adversary Loss: 0.6930937111377716\n",
            "Epoch 4/100 - Main Loss: 0.5907765626907349, Adversary Loss: 0.6949422359466553\n",
            "Epoch 5/100 - Main Loss: 0.5855690598487854, Adversary Loss: 0.6956086397171021\n",
            "Epoch 6/100 - Main Loss: 0.5812211871147156, Adversary Loss: 0.6941003322601318\n",
            "Epoch 7/100 - Main Loss: 0.5783386707305909, Adversary Loss: 0.6956721842288971\n",
            "Epoch 8/100 - Main Loss: 0.5763395369052887, Adversary Loss: 0.6954844474792481\n",
            "Epoch 9/100 - Main Loss: 0.5749503314495087, Adversary Loss: 0.6942561030387878\n",
            "Epoch 10/100 - Main Loss: 0.5740162670612335, Adversary Loss: 0.6937471508979798\n",
            "Epoch 11/100 - Main Loss: 0.5725319921970368, Adversary Loss: 0.6946837484836579\n",
            "Epoch 12/100 - Main Loss: 0.5720352292060852, Adversary Loss: 0.69333536028862\n",
            "Epoch 13/100 - Main Loss: 0.570625102519989, Adversary Loss: 0.6940518379211426\n",
            "Epoch 14/100 - Main Loss: 0.5700445771217346, Adversary Loss: 0.6949648201465607\n",
            "Epoch 15/100 - Main Loss: 0.5694066822528839, Adversary Loss: 0.6939181923866272\n",
            "Epoch 16/100 - Main Loss: 0.5690693020820617, Adversary Loss: 0.6963452219963073\n",
            "Epoch 17/100 - Main Loss: 0.5680742025375366, Adversary Loss: 0.6937083423137664\n",
            "Epoch 18/100 - Main Loss: 0.5676544904708862, Adversary Loss: 0.6945626080036164\n",
            "Epoch 19/100 - Main Loss: 0.5671052932739258, Adversary Loss: 0.6941418886184693\n",
            "Epoch 20/100 - Main Loss: 0.5664403319358826, Adversary Loss: 0.6943287134170533\n",
            "Epoch 21/100 - Main Loss: 0.5658142149448395, Adversary Loss: 0.6936264514923096\n",
            "Epoch 22/100 - Main Loss: 0.5656832873821258, Adversary Loss: 0.6947489976882935\n",
            "Epoch 23/100 - Main Loss: 0.5645670533180237, Adversary Loss: 0.6940548181533813\n",
            "Epoch 24/100 - Main Loss: 0.5646058917045593, Adversary Loss: 0.694408792257309\n",
            "Epoch 25/100 - Main Loss: 0.5642281591892242, Adversary Loss: 0.6940706253051758\n",
            "Epoch 26/100 - Main Loss: 0.5637348651885986, Adversary Loss: 0.6939473032951355\n",
            "Epoch 27/100 - Main Loss: 0.5635560572147369, Adversary Loss: 0.6940047025680542\n",
            "Epoch 28/100 - Main Loss: 0.5634142994880676, Adversary Loss: 0.6939398288726807\n",
            "Epoch 29/100 - Main Loss: 0.562002032995224, Adversary Loss: 0.6932919383049011\n",
            "Epoch 30/100 - Main Loss: 0.5613353729248047, Adversary Loss: 0.6946997225284577\n",
            "Epoch 31/100 - Main Loss: 0.5605715274810791, Adversary Loss: 0.6936487793922425\n",
            "Epoch 32/100 - Main Loss: 0.5605966031551362, Adversary Loss: 0.6945638477802276\n",
            "Epoch 33/100 - Main Loss: 0.5598820209503174, Adversary Loss: 0.6944438755512238\n",
            "Epoch 34/100 - Main Loss: 0.5588910102844238, Adversary Loss: 0.6937508940696716\n",
            "Epoch 35/100 - Main Loss: 0.5584986865520477, Adversary Loss: 0.6933119833469391\n",
            "Epoch 36/100 - Main Loss: 0.5584603071212768, Adversary Loss: 0.6933735251426697\n",
            "Epoch 37/100 - Main Loss: 0.5575845867395401, Adversary Loss: 0.694742751121521\n",
            "Epoch 38/100 - Main Loss: 0.5571311295032502, Adversary Loss: 0.694341641664505\n",
            "Epoch 39/100 - Main Loss: 0.5567409455776214, Adversary Loss: 0.6942971467971801\n",
            "Epoch 40/100 - Main Loss: 0.5565751671791077, Adversary Loss: 0.6924921870231628\n",
            "Epoch 41/100 - Main Loss: 0.5558045923709869, Adversary Loss: 0.6956193149089813\n",
            "Epoch 42/100 - Main Loss: 0.5559823930263519, Adversary Loss: 0.6945262670516967\n",
            "Epoch 43/100 - Main Loss: 0.5554559379816055, Adversary Loss: 0.6935505211353302\n",
            "Epoch 44/100 - Main Loss: 0.554899275302887, Adversary Loss: 0.6942604362964631\n",
            "Epoch 45/100 - Main Loss: 0.5544096529483795, Adversary Loss: 0.6937742471694947\n",
            "Epoch 46/100 - Main Loss: 0.5538577616214753, Adversary Loss: 0.6928666949272155\n",
            "Epoch 47/100 - Main Loss: 0.5541862428188324, Adversary Loss: 0.6929274499416351\n",
            "Epoch 48/100 - Main Loss: 0.5543999910354614, Adversary Loss: 0.690757942199707\n",
            "Epoch 49/100 - Main Loss: 0.553692352771759, Adversary Loss: 0.6965301334857941\n",
            "Epoch 50/100 - Main Loss: 0.5531113564968109, Adversary Loss: 0.6941773772239686\n",
            "Epoch 51/100 - Main Loss: 0.5520997047424316, Adversary Loss: 0.6933835566043853\n",
            "Epoch 52/100 - Main Loss: 0.5511141896247864, Adversary Loss: 0.6935898125171661\n",
            "Epoch 53/100 - Main Loss: 0.5514276444911956, Adversary Loss: 0.6945448040962219\n",
            "Epoch 54/100 - Main Loss: 0.5510461926460266, Adversary Loss: 0.6936109781265258\n",
            "Epoch 55/100 - Main Loss: 0.5506358861923217, Adversary Loss: 0.6928474843502045\n",
            "Epoch 56/100 - Main Loss: 0.5500313878059387, Adversary Loss: 0.693336009979248\n",
            "Epoch 57/100 - Main Loss: 0.5499303936958313, Adversary Loss: 0.6951948046684265\n",
            "Epoch 58/100 - Main Loss: 0.5502602636814118, Adversary Loss: 0.6938125848770141\n",
            "Epoch 59/100 - Main Loss: 0.5500152766704559, Adversary Loss: 0.6937860429286957\n",
            "Epoch 60/100 - Main Loss: 0.5493049383163452, Adversary Loss: 0.6933990955352783\n",
            "Epoch 61/100 - Main Loss: 0.550698208808899, Adversary Loss: 0.6932795822620392\n",
            "Epoch 62/100 - Main Loss: 0.5501855492591858, Adversary Loss: 0.6931683897972107\n",
            "Epoch 63/100 - Main Loss: 0.5485631942749023, Adversary Loss: 0.6947216033935547\n",
            "Epoch 64/100 - Main Loss: 0.5487472295761109, Adversary Loss: 0.6921888947486877\n",
            "Epoch 65/100 - Main Loss: 0.5480997383594512, Adversary Loss: 0.6942676365375519\n",
            "Epoch 66/100 - Main Loss: 0.5482934355735779, Adversary Loss: 0.6927176654338837\n",
            "Epoch 67/100 - Main Loss: 0.5478204488754272, Adversary Loss: 0.6934486567974091\n",
            "Epoch 68/100 - Main Loss: 0.5478347063064575, Adversary Loss: 0.6937878906726838\n",
            "Epoch 69/100 - Main Loss: 0.5478432357311249, Adversary Loss: 0.6927042663097381\n",
            "Epoch 70/100 - Main Loss: 0.5474630177021027, Adversary Loss: 0.6927128314971924\n",
            "Epoch 71/100 - Main Loss: 0.5470221698284149, Adversary Loss: 0.6955735146999359\n",
            "Epoch 72/100 - Main Loss: 0.5463786900043488, Adversary Loss: 0.6938370823860168\n",
            "Epoch 73/100 - Main Loss: 0.5471813142299652, Adversary Loss: 0.6936280906200409\n",
            "Epoch 74/100 - Main Loss: 0.546403956413269, Adversary Loss: 0.6935001134872436\n",
            "Epoch 75/100 - Main Loss: 0.5460593640804291, Adversary Loss: 0.6934582352638244\n",
            "Epoch 76/100 - Main Loss: 0.5456352889537811, Adversary Loss: 0.6935918867588043\n",
            "Epoch 77/100 - Main Loss: 0.545598155260086, Adversary Loss: 0.6938495516777039\n",
            "Epoch 78/100 - Main Loss: 0.5452603936195374, Adversary Loss: 0.6933394968509674\n",
            "Epoch 79/100 - Main Loss: 0.5454186916351318, Adversary Loss: 0.693602693080902\n",
            "Epoch 80/100 - Main Loss: 0.5447863042354584, Adversary Loss: 0.6940143167972564\n",
            "Epoch 81/100 - Main Loss: 0.5453435778617859, Adversary Loss: 0.6934034585952759\n",
            "Epoch 82/100 - Main Loss: 0.544394725561142, Adversary Loss: 0.694232600927353\n",
            "Epoch 83/100 - Main Loss: 0.5448991775512695, Adversary Loss: 0.6945180058479309\n",
            "Epoch 84/100 - Main Loss: 0.5447166204452515, Adversary Loss: 0.6922857224941253\n",
            "Epoch 85/100 - Main Loss: 0.5439929604530335, Adversary Loss: 0.6943789780139923\n",
            "Epoch 86/100 - Main Loss: 0.5441372931003571, Adversary Loss: 0.6937363624572754\n",
            "Epoch 87/100 - Main Loss: 0.543821108341217, Adversary Loss: 0.6933321118354797\n",
            "Epoch 88/100 - Main Loss: 0.5438277125358582, Adversary Loss: 0.6937926530838012\n",
            "Epoch 89/100 - Main Loss: 0.5436444938182831, Adversary Loss: 0.6936502754688263\n",
            "Epoch 90/100 - Main Loss: 0.5436865329742432, Adversary Loss: 0.6929386496543884\n",
            "Epoch 91/100 - Main Loss: 0.5435181379318237, Adversary Loss: 0.6936180174350739\n",
            "Epoch 92/100 - Main Loss: 0.5436221987009049, Adversary Loss: 0.6941470086574555\n",
            "Epoch 93/100 - Main Loss: 0.5438792407512665, Adversary Loss: 0.6935710549354553\n",
            "Epoch 94/100 - Main Loss: 0.5433969259262085, Adversary Loss: 0.6933839559555054\n",
            "Epoch 95/100 - Main Loss: 0.5432006061077118, Adversary Loss: 0.6933394730091095\n",
            "Epoch 96/100 - Main Loss: 0.5434429347515106, Adversary Loss: 0.6937750160694123\n",
            "Epoch 97/100 - Main Loss: 0.5432804822921753, Adversary Loss: 0.6926045477390289\n",
            "Epoch 98/100 - Main Loss: 0.5435434341430664, Adversary Loss: 0.6931616485118866\n",
            "Epoch 99/100 - Main Loss: 0.5431548714637756, Adversary Loss: 0.6942134737968445\n",
            "Test Accuracy: 0.9000\n",
            "Best Test Accuracy: 0.9000 with lr_main=0.01, lr_adv=0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define the main model and adversary\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, output_dim=1):\n",
        "        super(MainModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "class Adversary(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, output_dim=1):\n",
        "        super(Adversary, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# 1. Data Generation\n",
        "# Adjusting the number of informative, redundant, and repeated features to avoid the error\n",
        "X, y = make_classification(n_samples=800, n_features=10, n_informative=5, n_redundant=0, n_repeated=0, n_classes=2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)  # Reshaping for binary classification\n",
        "protected_attribute_train = torch.tensor(np.random.randint(0, 2, size=(800, 1)), dtype=torch.float32)\n",
        "\n",
        "# 2. Training and Adversarial Debiasing\n",
        "def adversarial_debiasing(X_train, y_train, protected_attribute_train, epochs, lr_main, lr_adv, lam):\n",
        "    main_model = MainModel(input_dim=X_train.shape[1], hidden_dim=64, output_dim=1)\n",
        "    adversary = Adversary(input_dim=X_train.shape[1], hidden_dim=64, output_dim=1)\n",
        "\n",
        "    criterion_main = nn.BCEWithLogitsLoss()\n",
        "    criterion_adv = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    optimizer_main = optim.Adam(main_model.parameters(), lr=lr_main)\n",
        "    optimizer_adv = optim.Adam(adversary.parameters(), lr=lr_adv)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        main_model.train()\n",
        "        adversary.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        adversary_loss = 0\n",
        "\n",
        "        for i in range(0, len(X_train), 64):  # Assuming batch size of 64\n",
        "            # Create the batch\n",
        "            X_batch = X_train[i:i+64]\n",
        "            y_batch = y_train[i:i+64]\n",
        "            protected_batch = protected_attribute_train[i:i+64]\n",
        "\n",
        "            # Make sure both tensors are of the same size\n",
        "            assert X_batch.size(0) == y_batch.size(0)\n",
        "\n",
        "            # Forward pass through the main model\n",
        "            main_output = main_model(X_batch)\n",
        "\n",
        "            # Compute loss for the main model\n",
        "            main_loss = criterion_main(main_output, y_batch)\n",
        "\n",
        "            # Forward pass through the adversary\n",
        "            adversary_output = adversary(X_batch)\n",
        "\n",
        "            # Compute loss for the adversary\n",
        "            adversary_loss = criterion_adv(adversary_output, protected_batch)\n",
        "\n",
        "            # Backpropagation for the main model\n",
        "            optimizer_main.zero_grad()\n",
        "            main_loss.backward(retain_graph=True)\n",
        "            optimizer_main.step()\n",
        "\n",
        "            # Backpropagation for the adversary\n",
        "            optimizer_adv.zero_grad()\n",
        "            adversary_loss.backward()\n",
        "            optimizer_adv.step()\n",
        "\n",
        "            total_loss += main_loss.item()\n",
        "            adversary_loss += adversary_loss.item()\n",
        "\n",
        "        # Print the losses at each epoch\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Main Loss: {total_loss}, Adversary Loss: {adversary_loss}\")\n",
        "\n",
        "    return main_model, adversary\n",
        "\n",
        "# Running the test with a small learning rate\n",
        "lr_main = 0.0001\n",
        "lr_adv = 0.0001\n",
        "\n",
        "main_model, adversary = adversarial_debiasing(X_train_tensor, y_train_tensor, protected_attribute_train, epochs=100,\n",
        "                                             lr_main=lr_main, lr_adv=lr_adv, lam=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uqtgJc_ntmq",
        "outputId": "fee24ccd-6531-46fd-d186-932794ed09a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Main Loss: 9.161132574081421, Adversary Loss: 1.4039194583892822\n",
            "Epoch 2/100, Main Loss: 9.11779499053955, Adversary Loss: 1.4031028747558594\n",
            "Epoch 3/100, Main Loss: 9.075917840003967, Adversary Loss: 1.402252197265625\n",
            "Epoch 4/100, Main Loss: 9.034689903259277, Adversary Loss: 1.4014239311218262\n",
            "Epoch 5/100, Main Loss: 8.994082748889923, Adversary Loss: 1.4006328582763672\n",
            "Epoch 6/100, Main Loss: 8.953988552093506, Adversary Loss: 1.3998767137527466\n",
            "Epoch 7/100, Main Loss: 8.914318919181824, Adversary Loss: 1.3991550207138062\n",
            "Epoch 8/100, Main Loss: 8.875040590763092, Adversary Loss: 1.398463249206543\n",
            "Epoch 9/100, Main Loss: 8.83612984418869, Adversary Loss: 1.3977956771850586\n",
            "Epoch 10/100, Main Loss: 8.797556936740875, Adversary Loss: 1.397143840789795\n",
            "Epoch 11/100, Main Loss: 8.759333968162537, Adversary Loss: 1.396510362625122\n",
            "Epoch 12/100, Main Loss: 8.72147274017334, Adversary Loss: 1.3958982229232788\n",
            "Epoch 13/100, Main Loss: 8.68386834859848, Adversary Loss: 1.3953050374984741\n",
            "Epoch 14/100, Main Loss: 8.646487653255463, Adversary Loss: 1.3947278261184692\n",
            "Epoch 15/100, Main Loss: 8.609289646148682, Adversary Loss: 1.3941655158996582\n",
            "Epoch 16/100, Main Loss: 8.572319567203522, Adversary Loss: 1.3936166763305664\n",
            "Epoch 17/100, Main Loss: 8.535587072372437, Adversary Loss: 1.3930789232254028\n",
            "Epoch 18/100, Main Loss: 8.499033272266388, Adversary Loss: 1.3925538063049316\n",
            "Epoch 19/100, Main Loss: 8.462610483169556, Adversary Loss: 1.3920425176620483\n",
            "Epoch 20/100, Main Loss: 8.426333785057068, Adversary Loss: 1.391535997390747\n",
            "Epoch 21/100, Main Loss: 8.390235543251038, Adversary Loss: 1.3910398483276367\n",
            "Epoch 22/100, Main Loss: 8.354275941848755, Adversary Loss: 1.3905460834503174\n",
            "Epoch 23/100, Main Loss: 8.318458557128906, Adversary Loss: 1.390066146850586\n",
            "Epoch 24/100, Main Loss: 8.2828129529953, Adversary Loss: 1.3895951509475708\n",
            "Epoch 25/100, Main Loss: 8.247332096099854, Adversary Loss: 1.3891327381134033\n",
            "Epoch 26/100, Main Loss: 8.212028443813324, Adversary Loss: 1.3886775970458984\n",
            "Epoch 27/100, Main Loss: 8.176944613456726, Adversary Loss: 1.388231635093689\n",
            "Epoch 28/100, Main Loss: 8.142121255397797, Adversary Loss: 1.3877947330474854\n",
            "Epoch 29/100, Main Loss: 8.107501685619354, Adversary Loss: 1.387366771697998\n",
            "Epoch 30/100, Main Loss: 8.073069393634796, Adversary Loss: 1.3869397640228271\n",
            "Epoch 31/100, Main Loss: 8.038801491260529, Adversary Loss: 1.3865221738815308\n",
            "Epoch 32/100, Main Loss: 8.00470232963562, Adversary Loss: 1.3861048221588135\n",
            "Epoch 33/100, Main Loss: 7.970782995223999, Adversary Loss: 1.3856844902038574\n",
            "Epoch 34/100, Main Loss: 7.937044084072113, Adversary Loss: 1.3852773904800415\n",
            "Epoch 35/100, Main Loss: 7.903474986553192, Adversary Loss: 1.384880781173706\n",
            "Epoch 36/100, Main Loss: 7.8700292110443115, Adversary Loss: 1.3844842910766602\n",
            "Epoch 37/100, Main Loss: 7.8367326855659485, Adversary Loss: 1.384095311164856\n",
            "Epoch 38/100, Main Loss: 7.803585648536682, Adversary Loss: 1.3837140798568726\n",
            "Epoch 39/100, Main Loss: 7.770620822906494, Adversary Loss: 1.3833420276641846\n",
            "Epoch 40/100, Main Loss: 7.737836480140686, Adversary Loss: 1.3829774856567383\n",
            "Epoch 41/100, Main Loss: 7.7052003145217896, Adversary Loss: 1.3826203346252441\n",
            "Epoch 42/100, Main Loss: 7.672718346118927, Adversary Loss: 1.382269024848938\n",
            "Epoch 43/100, Main Loss: 7.640463471412659, Adversary Loss: 1.3819196224212646\n",
            "Epoch 44/100, Main Loss: 7.608423292636871, Adversary Loss: 1.3815727233886719\n",
            "Epoch 45/100, Main Loss: 7.576593279838562, Adversary Loss: 1.3812330961227417\n",
            "Epoch 46/100, Main Loss: 7.544940769672394, Adversary Loss: 1.380897879600525\n",
            "Epoch 47/100, Main Loss: 7.513430893421173, Adversary Loss: 1.380568027496338\n",
            "Epoch 48/100, Main Loss: 7.482108414173126, Adversary Loss: 1.3802411556243896\n",
            "Epoch 49/100, Main Loss: 7.451017796993256, Adversary Loss: 1.379920482635498\n",
            "Epoch 50/100, Main Loss: 7.4201608300209045, Adversary Loss: 1.3796014785766602\n",
            "Epoch 51/100, Main Loss: 7.389525830745697, Adversary Loss: 1.3792883157730103\n",
            "Epoch 52/100, Main Loss: 7.359118700027466, Adversary Loss: 1.3789807558059692\n",
            "Epoch 53/100, Main Loss: 7.328955173492432, Adversary Loss: 1.3786754608154297\n",
            "Epoch 54/100, Main Loss: 7.299070656299591, Adversary Loss: 1.3783763647079468\n",
            "Epoch 55/100, Main Loss: 7.269463360309601, Adversary Loss: 1.3780760765075684\n",
            "Epoch 56/100, Main Loss: 7.240130305290222, Adversary Loss: 1.3777801990509033\n",
            "Epoch 57/100, Main Loss: 7.211071968078613, Adversary Loss: 1.3774868249893188\n",
            "Epoch 58/100, Main Loss: 7.1823020577430725, Adversary Loss: 1.3771939277648926\n",
            "Epoch 59/100, Main Loss: 7.1538087129592896, Adversary Loss: 1.37690269947052\n",
            "Epoch 60/100, Main Loss: 7.125578045845032, Adversary Loss: 1.3766121864318848\n",
            "Epoch 61/100, Main Loss: 7.097628474235535, Adversary Loss: 1.3763209581375122\n",
            "Epoch 62/100, Main Loss: 7.069940090179443, Adversary Loss: 1.3760297298431396\n",
            "Epoch 63/100, Main Loss: 7.042543709278107, Adversary Loss: 1.3757380247116089\n",
            "Epoch 64/100, Main Loss: 7.015440106391907, Adversary Loss: 1.3754463195800781\n",
            "Epoch 65/100, Main Loss: 6.988592565059662, Adversary Loss: 1.3751577138900757\n",
            "Epoch 66/100, Main Loss: 6.961995422840118, Adversary Loss: 1.3748661279678345\n",
            "Epoch 67/100, Main Loss: 6.9356569945812225, Adversary Loss: 1.3745677471160889\n",
            "Epoch 68/100, Main Loss: 6.909606218338013, Adversary Loss: 1.374269723892212\n",
            "Epoch 69/100, Main Loss: 6.883860766887665, Adversary Loss: 1.3739728927612305\n",
            "Epoch 70/100, Main Loss: 6.85843899846077, Adversary Loss: 1.3736709356307983\n",
            "Epoch 71/100, Main Loss: 6.833343833684921, Adversary Loss: 1.3733677864074707\n",
            "Epoch 72/100, Main Loss: 6.8085620403289795, Adversary Loss: 1.3730647563934326\n",
            "Epoch 73/100, Main Loss: 6.78406634926796, Adversary Loss: 1.37276291847229\n",
            "Epoch 74/100, Main Loss: 6.759863346815109, Adversary Loss: 1.3724617958068848\n",
            "Epoch 75/100, Main Loss: 6.73594930768013, Adversary Loss: 1.3721582889556885\n",
            "Epoch 76/100, Main Loss: 6.7123019099235535, Adversary Loss: 1.3718570470809937\n",
            "Epoch 77/100, Main Loss: 6.688930690288544, Adversary Loss: 1.3715567588806152\n",
            "Epoch 78/100, Main Loss: 6.665854603052139, Adversary Loss: 1.371256709098816\n",
            "Epoch 79/100, Main Loss: 6.643085956573486, Adversary Loss: 1.3709609508514404\n",
            "Epoch 80/100, Main Loss: 6.62063392996788, Adversary Loss: 1.3706767559051514\n",
            "Epoch 81/100, Main Loss: 6.5984949469566345, Adversary Loss: 1.370389699935913\n",
            "Epoch 82/100, Main Loss: 6.576655179262161, Adversary Loss: 1.370103359222412\n",
            "Epoch 83/100, Main Loss: 6.555103987455368, Adversary Loss: 1.3698184490203857\n",
            "Epoch 84/100, Main Loss: 6.533845663070679, Adversary Loss: 1.3695335388183594\n",
            "Epoch 85/100, Main Loss: 6.512896955013275, Adversary Loss: 1.3692481517791748\n",
            "Epoch 86/100, Main Loss: 6.492267072200775, Adversary Loss: 1.368962287902832\n",
            "Epoch 87/100, Main Loss: 6.4719522297382355, Adversary Loss: 1.3686753511428833\n",
            "Epoch 88/100, Main Loss: 6.451963365077972, Adversary Loss: 1.3683902025222778\n",
            "Epoch 89/100, Main Loss: 6.432279318571091, Adversary Loss: 1.3681021928787231\n",
            "Epoch 90/100, Main Loss: 6.412881463766098, Adversary Loss: 1.3678151369094849\n",
            "Epoch 91/100, Main Loss: 6.393746465444565, Adversary Loss: 1.3675287961959839\n",
            "Epoch 92/100, Main Loss: 6.3748608231544495, Adversary Loss: 1.3672428131103516\n",
            "Epoch 93/100, Main Loss: 6.356233358383179, Adversary Loss: 1.3669592142105103\n",
            "Epoch 94/100, Main Loss: 6.337867319583893, Adversary Loss: 1.3666751384735107\n",
            "Epoch 95/100, Main Loss: 6.319765686988831, Adversary Loss: 1.3663884401321411\n",
            "Epoch 96/100, Main Loss: 6.301905304193497, Adversary Loss: 1.366104006767273\n",
            "Epoch 97/100, Main Loss: 6.284282922744751, Adversary Loss: 1.3658198118209839\n",
            "Epoch 98/100, Main Loss: 6.2668987810611725, Adversary Loss: 1.3655331134796143\n",
            "Epoch 99/100, Main Loss: 6.249756425619125, Adversary Loss: 1.3652472496032715\n",
            "Epoch 100/100, Main Loss: 6.2328736782073975, Adversary Loss: 1.3649665117263794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Data Generation (Adjusted n_features and n_informative, n_redundant, n_repeated)\n",
        "X, y = make_classification(n_samples=800, n_features=8, n_informative=5, n_redundant=2, n_repeated=0, n_classes=2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Main Model\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, output_dim=1):\n",
        "        super(MainModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return torch.sigmoid(self.fc2(x))\n",
        "\n",
        "# Adversary Model\n",
        "class Adversary(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=32, output_dim=1):\n",
        "        super(Adversary, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return torch.sigmoid(self.fc2(x))\n",
        "\n",
        "# Training loop with learning rate scheduler\n",
        "def adversarial_debiasing(X_train_tensor, y_train_tensor, protected_attribute_train, epochs=100, lr_main=0.001, lr_adv=0.001, lam=0.1, batch_size=64):\n",
        "    main_model = MainModel(input_dim=X_train_tensor.shape[1])\n",
        "    adversary = Adversary(input_dim=X_train_tensor.shape[1])\n",
        "\n",
        "    # Loss functions\n",
        "    criterion_main = nn.BCELoss()\n",
        "    criterion_adv = nn.BCELoss()\n",
        "\n",
        "    # Optimizers\n",
        "    optimizer_main = optim.Adam(main_model.parameters(), lr=lr_main)\n",
        "    optimizer_adv = optim.Adam(adversary.parameters(), lr=lr_adv)\n",
        "\n",
        "    # Learning Rate Scheduler (for main model)\n",
        "    scheduler_main = optim.lr_scheduler.StepLR(optimizer_main, step_size=10, gamma=0.1)\n",
        "\n",
        "    losses = {'main': [], 'adversary': []}\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        main_model.train()\n",
        "        adversary.train()\n",
        "\n",
        "        # Shuffle training data\n",
        "        permutation = torch.randperm(X_train_tensor.size(0))\n",
        "\n",
        "        # Batch training loop\n",
        "        for i in range(0, len(X_train_tensor), batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            batch_X = X_train_tensor[indices]\n",
        "            batch_y = y_train_tensor[indices]\n",
        "            batch_protected = protected_attribute_train[indices]\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer_main.zero_grad()\n",
        "            optimizer_adv.zero_grad()\n",
        "\n",
        "            # Main model forward pass\n",
        "            main_output = main_model(batch_X)\n",
        "\n",
        "            # Adversary model forward pass\n",
        "            adversary_output = adversary(batch_X)\n",
        "\n",
        "            # Compute losses\n",
        "            main_loss = criterion_main(main_output, batch_y)\n",
        "            adversary_loss = criterion_adv(adversary_output, batch_protected)\n",
        "\n",
        "            # Fairness penalty\n",
        "            fairness_penalty = lam * adversary_loss\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = main_loss + fairness_penalty\n",
        "            total_loss.backward()\n",
        "\n",
        "            # Optimizer step\n",
        "            optimizer_main.step()\n",
        "            optimizer_adv.step()\n",
        "\n",
        "            # Record losses\n",
        "            losses['main'].append(main_loss.item())\n",
        "            losses['adversary'].append(adversary_loss.item())\n",
        "\n",
        "        # Update the learning rate using the scheduler\n",
        "        scheduler_main.step()\n",
        "\n",
        "        # Print training progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Main Loss: {main_loss.item():.4f}, Adversary Loss: {adversary_loss.item():.4f}\")\n",
        "\n",
        "    return main_model, adversary, losses\n",
        "\n",
        "# Run the training\n",
        "main_model, adversary, losses = adversarial_debiasing(X_train_tensor, y_train_tensor, torch.tensor(np.random.randint(0, 2, size=(800, 1)), dtype=torch.float32), epochs=100,\n",
        "                                                      lr_main=0.001, lr_adv=0.001, lam=0.1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "def evaluate_model(main_model, adversary, X_test, y_test, protected_attribute_test):\n",
        "    main_model.eval()\n",
        "    adversary.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        main_output = main_model(X_test)\n",
        "        adversary_output = adversary(X_test)\n",
        "\n",
        "        main_output = (main_output > 0.5).float()\n",
        "        accuracy = (main_output == y_test).float().mean().item()\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate on test data\n",
        "test_accuracy = evaluate_model(main_model, adversary, X_test_tensor, y_test_tensor, torch.tensor(np.random.randint(0, 2, size=(160, 1)), dtype=torch.float32))\n",
        "\n",
        "# Save the output in a text file\n",
        "with open('/content/adversarial_debiasing_output.txt', 'w') as f:\n",
        "    f.write(\"Training and Evaluation Results:\\n\")\n",
        "    f.write(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
        "    f.write(\"\\nLosses over epochs:\\n\")\n",
        "    f.write(f\"Main Losses: {losses['main']}\\n\")\n",
        "    f.write(f\"Adversary Losses: {losses['adversary']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe806khMpKcR",
        "outputId": "2d2a358e-28a0-4dc3-c953-d021d37477a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Main Loss: 0.7063, Adversary Loss: 0.7025\n",
            "Epoch 2/100, Main Loss: 0.6679, Adversary Loss: 0.6869\n",
            "Epoch 3/100, Main Loss: 0.6262, Adversary Loss: 0.6904\n",
            "Epoch 4/100, Main Loss: 0.5928, Adversary Loss: 0.6947\n",
            "Epoch 5/100, Main Loss: 0.6091, Adversary Loss: 0.6928\n",
            "Epoch 6/100, Main Loss: 0.6076, Adversary Loss: 0.6866\n",
            "Epoch 7/100, Main Loss: 0.5146, Adversary Loss: 0.6757\n",
            "Epoch 8/100, Main Loss: 0.5264, Adversary Loss: 0.6717\n",
            "Epoch 9/100, Main Loss: 0.4545, Adversary Loss: 0.6839\n",
            "Epoch 10/100, Main Loss: 0.5152, Adversary Loss: 0.6692\n",
            "Epoch 11/100, Main Loss: 0.5656, Adversary Loss: 0.6868\n",
            "Epoch 12/100, Main Loss: 0.4740, Adversary Loss: 0.6919\n",
            "Epoch 13/100, Main Loss: 0.5221, Adversary Loss: 0.6795\n",
            "Epoch 14/100, Main Loss: 0.5919, Adversary Loss: 0.6827\n",
            "Epoch 15/100, Main Loss: 0.4612, Adversary Loss: 0.6999\n",
            "Epoch 16/100, Main Loss: 0.4813, Adversary Loss: 0.6890\n",
            "Epoch 17/100, Main Loss: 0.5007, Adversary Loss: 0.6755\n",
            "Epoch 18/100, Main Loss: 0.4765, Adversary Loss: 0.6907\n",
            "Epoch 19/100, Main Loss: 0.4864, Adversary Loss: 0.6805\n",
            "Epoch 20/100, Main Loss: 0.4877, Adversary Loss: 0.6521\n",
            "Epoch 21/100, Main Loss: 0.4867, Adversary Loss: 0.6773\n",
            "Epoch 22/100, Main Loss: 0.5166, Adversary Loss: 0.7048\n",
            "Epoch 23/100, Main Loss: 0.4845, Adversary Loss: 0.6797\n",
            "Epoch 24/100, Main Loss: 0.5218, Adversary Loss: 0.6827\n",
            "Epoch 25/100, Main Loss: 0.4970, Adversary Loss: 0.6734\n",
            "Epoch 26/100, Main Loss: 0.4282, Adversary Loss: 0.6872\n",
            "Epoch 27/100, Main Loss: 0.5542, Adversary Loss: 0.6903\n",
            "Epoch 28/100, Main Loss: 0.4885, Adversary Loss: 0.6815\n",
            "Epoch 29/100, Main Loss: 0.5740, Adversary Loss: 0.6878\n",
            "Epoch 30/100, Main Loss: 0.4837, Adversary Loss: 0.6881\n",
            "Epoch 31/100, Main Loss: 0.4390, Adversary Loss: 0.6709\n",
            "Epoch 32/100, Main Loss: 0.4442, Adversary Loss: 0.6891\n",
            "Epoch 33/100, Main Loss: 0.5413, Adversary Loss: 0.7110\n",
            "Epoch 34/100, Main Loss: 0.5402, Adversary Loss: 0.6732\n",
            "Epoch 35/100, Main Loss: 0.4647, Adversary Loss: 0.6896\n",
            "Epoch 36/100, Main Loss: 0.5500, Adversary Loss: 0.6785\n",
            "Epoch 37/100, Main Loss: 0.4955, Adversary Loss: 0.6828\n",
            "Epoch 38/100, Main Loss: 0.4410, Adversary Loss: 0.6688\n",
            "Epoch 39/100, Main Loss: 0.4887, Adversary Loss: 0.7047\n",
            "Epoch 40/100, Main Loss: 0.4649, Adversary Loss: 0.6849\n",
            "Epoch 41/100, Main Loss: 0.5049, Adversary Loss: 0.6745\n",
            "Epoch 42/100, Main Loss: 0.5223, Adversary Loss: 0.6820\n",
            "Epoch 43/100, Main Loss: 0.4872, Adversary Loss: 0.6509\n",
            "Epoch 44/100, Main Loss: 0.5089, Adversary Loss: 0.6817\n",
            "Epoch 45/100, Main Loss: 0.4741, Adversary Loss: 0.6805\n",
            "Epoch 46/100, Main Loss: 0.5055, Adversary Loss: 0.6864\n",
            "Epoch 47/100, Main Loss: 0.4664, Adversary Loss: 0.6869\n",
            "Epoch 48/100, Main Loss: 0.5016, Adversary Loss: 0.6763\n",
            "Epoch 49/100, Main Loss: 0.4606, Adversary Loss: 0.7159\n",
            "Epoch 50/100, Main Loss: 0.5003, Adversary Loss: 0.6769\n",
            "Epoch 51/100, Main Loss: 0.5690, Adversary Loss: 0.6471\n",
            "Epoch 52/100, Main Loss: 0.4969, Adversary Loss: 0.6908\n",
            "Epoch 53/100, Main Loss: 0.5585, Adversary Loss: 0.6827\n",
            "Epoch 54/100, Main Loss: 0.5564, Adversary Loss: 0.6704\n",
            "Epoch 55/100, Main Loss: 0.4671, Adversary Loss: 0.6753\n",
            "Epoch 56/100, Main Loss: 0.4809, Adversary Loss: 0.6483\n",
            "Epoch 57/100, Main Loss: 0.5390, Adversary Loss: 0.6757\n",
            "Epoch 58/100, Main Loss: 0.4904, Adversary Loss: 0.6743\n",
            "Epoch 59/100, Main Loss: 0.4451, Adversary Loss: 0.6923\n",
            "Epoch 60/100, Main Loss: 0.5553, Adversary Loss: 0.6824\n",
            "Epoch 61/100, Main Loss: 0.4726, Adversary Loss: 0.6914\n",
            "Epoch 62/100, Main Loss: 0.4702, Adversary Loss: 0.6952\n",
            "Epoch 63/100, Main Loss: 0.4741, Adversary Loss: 0.6951\n",
            "Epoch 64/100, Main Loss: 0.4666, Adversary Loss: 0.6772\n",
            "Epoch 65/100, Main Loss: 0.4472, Adversary Loss: 0.6836\n",
            "Epoch 66/100, Main Loss: 0.4850, Adversary Loss: 0.6780\n",
            "Epoch 67/100, Main Loss: 0.5348, Adversary Loss: 0.6422\n",
            "Epoch 68/100, Main Loss: 0.5785, Adversary Loss: 0.6778\n",
            "Epoch 69/100, Main Loss: 0.4468, Adversary Loss: 0.7063\n",
            "Epoch 70/100, Main Loss: 0.5665, Adversary Loss: 0.6639\n",
            "Epoch 71/100, Main Loss: 0.4253, Adversary Loss: 0.6791\n",
            "Epoch 72/100, Main Loss: 0.4846, Adversary Loss: 0.6692\n",
            "Epoch 73/100, Main Loss: 0.5204, Adversary Loss: 0.6808\n",
            "Epoch 74/100, Main Loss: 0.4854, Adversary Loss: 0.6598\n",
            "Epoch 75/100, Main Loss: 0.5044, Adversary Loss: 0.6714\n",
            "Epoch 76/100, Main Loss: 0.5029, Adversary Loss: 0.6682\n",
            "Epoch 77/100, Main Loss: 0.4750, Adversary Loss: 0.6801\n",
            "Epoch 78/100, Main Loss: 0.5294, Adversary Loss: 0.6549\n",
            "Epoch 79/100, Main Loss: 0.4653, Adversary Loss: 0.6678\n",
            "Epoch 80/100, Main Loss: 0.5125, Adversary Loss: 0.6883\n",
            "Epoch 81/100, Main Loss: 0.4493, Adversary Loss: 0.6583\n",
            "Epoch 82/100, Main Loss: 0.4225, Adversary Loss: 0.6633\n",
            "Epoch 83/100, Main Loss: 0.5130, Adversary Loss: 0.6668\n",
            "Epoch 84/100, Main Loss: 0.5244, Adversary Loss: 0.6488\n",
            "Epoch 85/100, Main Loss: 0.5767, Adversary Loss: 0.7011\n",
            "Epoch 86/100, Main Loss: 0.4378, Adversary Loss: 0.6508\n",
            "Epoch 87/100, Main Loss: 0.6310, Adversary Loss: 0.6545\n",
            "Epoch 88/100, Main Loss: 0.4738, Adversary Loss: 0.6766\n",
            "Epoch 89/100, Main Loss: 0.5059, Adversary Loss: 0.6505\n",
            "Epoch 90/100, Main Loss: 0.4634, Adversary Loss: 0.6570\n",
            "Epoch 91/100, Main Loss: 0.5847, Adversary Loss: 0.6406\n",
            "Epoch 92/100, Main Loss: 0.5195, Adversary Loss: 0.6719\n",
            "Epoch 93/100, Main Loss: 0.6288, Adversary Loss: 0.6407\n",
            "Epoch 94/100, Main Loss: 0.5566, Adversary Loss: 0.6102\n",
            "Epoch 95/100, Main Loss: 0.5552, Adversary Loss: 0.6640\n",
            "Epoch 96/100, Main Loss: 0.4570, Adversary Loss: 0.6811\n",
            "Epoch 97/100, Main Loss: 0.4740, Adversary Loss: 0.6922\n",
            "Epoch 98/100, Main Loss: 0.5544, Adversary Loss: 0.6724\n",
            "Epoch 99/100, Main Loss: 0.4232, Adversary Loss: 0.6576\n",
            "Epoch 100/100, Main Loss: 0.5363, Adversary Loss: 0.6509\n",
            "Test Accuracy: 0.7812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MainModel, self).__init__()\n",
        "        # Ensure the number of input features matches the size of the first layer\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))  # Apply ReLU activation to first layer\n",
        "        x = F.relu(self.fc2(x))  # Apply ReLU activation to second layer\n",
        "        x = self.fc3(x)          # No activation here for the output layer\n",
        "        return x\n",
        "\n",
        "# Example to initialize the model\n",
        "input_dim = 8  # Number of features in the input data\n",
        "hidden_dim = 128  # Size of the hidden layer\n",
        "output_dim = 1  # Output dimension (binary classification in this case)\n",
        "\n",
        "# Initialize the main model\n",
        "main_model = MainModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
        "\n",
        "\n",
        "class WassersteinAdversary(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(WassersteinAdversary, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Hyperparameters for further testing\n",
        "learning_rates_main = [0.001, 0.0001, 0.00001]\n",
        "learning_rates_adv = [0.001, 0.0001, 0.00001]\n",
        "optimizers = ['adam', 'sgd']\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "\n",
        "# Function to test different hyperparameters and optimizers\n",
        "def test_hyperparameters(X_train, y_train, X_test, y_test, protected_attribute_train):\n",
        "    best_test_acc = 0\n",
        "    best_params = {}\n",
        "\n",
        "    for lr_main in learning_rates_main:\n",
        "        for lr_adv in learning_rates_adv:\n",
        "            for optimizer in optimizers:\n",
        "                print(f\"Testing with lr_main={lr_main}, lr_adv={lr_adv}, optimizer={optimizer}\")\n",
        "                main_model, adversary, losses = adversarial_debiasing(\n",
        "                    X_train, y_train, protected_attribute_train,\n",
        "                    epochs=epochs, lr_main=lr_main, lr_adv=lr_adv,\n",
        "                    optimizer=optimizer, batch_size=batch_size\n",
        "                )\n",
        "                test_acc = evaluate_model(main_model, adversary, X_test, y_test)\n",
        "                if test_acc > best_test_acc:\n",
        "                    best_test_acc = test_acc\n",
        "                    best_params = {'lr_main': lr_main, 'lr_adv': lr_adv, 'optimizer': optimizer}\n",
        "\n",
        "                print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "    print(f\"Best Test Accuracy: {best_test_acc} with parameters: {best_params}\")\n",
        "    return best_params, best_test_acc\n",
        "\n",
        "# Data Generation (Adjusted to fix the error)\n",
        "X, y = make_classification(n_samples=800, n_features=8, n_informative=5, n_redundant=2, n_repeated=0, n_classes=2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Generate random protected attribute for testing\n",
        "protected_attribute_train = torch.tensor(np.random.randint(0, 2, size=(X_train.shape[0], 1)), dtype=torch.float32)\n",
        "protected_attribute_test = torch.tensor(np.random.randint(0, 2, size=(X_test.shape[0], 1)), dtype=torch.float32)\n",
        "\n",
        "# 2. Data Augmentation using SMOTE (if needed)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "def augment_data(X_train, y_train):\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_res, y_res = smote.fit_resample(X_train, y_train)\n",
        "    return X_res, y_res\n",
        "\n",
        "X_train_augmented, y_train_augmented = augment_data(X_train, y_train)\n",
        "X_train_augmented = scaler.fit_transform(X_train_augmented)\n",
        "\n",
        "# 3. Model definition\n",
        "def adversarial_debiasing(X_train, y_train, protected_attribute_train, epochs, lr_main, lr_adv, optimizer, batch_size=64):\n",
        "    main_model = MainModel(input_dim=X_train.shape[1], hidden_dim=128, output_dim=1)\n",
        "\n",
        "    # Add hidden_dim and output_dim to WassersteinAdversary initialization\n",
        "    adversary = WassersteinAdversary(input_dim=X_train.shape[1], hidden_dim=128, output_dim=1)\n",
        "\n",
        "    # Choose optimizer\n",
        "    if optimizer == 'adam':\n",
        "        optimizer_main = torch.optim.Adam(main_model.parameters(), lr=lr_main)\n",
        "        optimizer_adv = torch.optim.Adam(adversary.parameters(), lr=lr_adv)\n",
        "    elif optimizer == 'sgd':\n",
        "        optimizer_main = torch.optim.SGD(main_model.parameters(), lr=lr_main)\n",
        "        optimizer_adv = torch.optim.SGD(adversary.parameters(), lr=lr_adv)\n",
        "\n",
        "    # Loss functions\n",
        "    criterion_main = nn.BCEWithLogitsLoss()\n",
        "    criterion_adv = nn.MSELoss()  # For Wasserstein, use MSE loss\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        main_model.train()\n",
        "        adversary.train()\n",
        "\n",
        "        optimizer_main.zero_grad()\n",
        "        optimizer_adv.zero_grad()\n",
        "\n",
        "        main_output = main_model(X_train)\n",
        "        adversary_output = adversary(X_train)\n",
        "\n",
        "        # Main loss\n",
        "        main_loss = criterion_main(main_output, y_train)\n",
        "        main_loss.backward(retain_graph=True)\n",
        "\n",
        "        # Adversary loss (Wasserstein)\n",
        "        adversary_loss = criterion_adv(adversary_output, protected_attribute_train)\n",
        "        adversary_loss.backward()\n",
        "\n",
        "        optimizer_main.step()\n",
        "        optimizer_adv.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}/{epochs}, Main Loss: {main_loss.item()}, Adversary Loss: {adversary_loss.item()}\")\n",
        "\n",
        "    return main_model, adversary, {'main_loss': main_loss.item(), 'adversary_loss': adversary_loss.item()}\n",
        "\n",
        "# 4. Evaluate Model\n",
        "def evaluate_model(main_model, adversary, X_test, y_test):\n",
        "    main_model.eval()\n",
        "    with torch.no_grad():\n",
        "        main_output = main_model(X_test)\n",
        "        test_accuracy = (torch.sigmoid(main_output).round() == y_test).float().mean()\n",
        "        return test_accuracy.item()\n",
        "\n",
        "# 5. Run testing with different hyperparameters and optimizers\n",
        "best_params, best_test_acc = test_hyperparameters(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, protected_attribute_train)\n",
        "print(f\"Best Test Accuracy: {best_test_acc} with parameters: {best_params}\")\n",
        "\n",
        "# 6. Fairness Metrics Calculation\n",
        "def calculate_demographic_parity(model, X_test, y_test, protected_attributes):\n",
        "    predictions = model(X_test)\n",
        "    dp_diff = torch.mean(predictions[protected_attributes == 1]) - torch.mean(predictions[protected_attributes == 0])\n",
        "    return dp_diff.item()\n",
        "\n",
        "def calculate_equal_opportunity(model, X_test, y_test, protected_attributes):\n",
        "    predictions = model(X_test)\n",
        "\n",
        "    # Convert boolean to float for mean calculation\n",
        "    tp_rate = torch.mean(((predictions == 1) & (y_test == 1)).float())\n",
        "    tp_rate_group_1 = torch.mean(((predictions[protected_attributes == 1] == 1) &\n",
        "                                  (y_test[protected_attributes == 1] == 1)).float())\n",
        "    tp_rate_group_0 = torch.mean(((predictions[protected_attributes == 0] == 1) &\n",
        "                                  (y_test[protected_attributes == 0] == 1)).float())\n",
        "\n",
        "    return tp_rate_group_1 - tp_rate_group_0\n",
        "\n",
        "dp = calculate_demographic_parity(main_model, X_test_tensor, y_test_tensor, protected_attribute_test)\n",
        "eo = calculate_equal_opportunity(main_model, X_test_tensor, y_test_tensor, protected_attribute_test)\n",
        "print(f\"Demographic Parity: {dp}, Equal Opportunity Difference: {eo}\")\n",
        "\n",
        "# 7. Saving the Model\n",
        "torch.save(main_model.state_dict(), \"/content/main_model.pth\")\n",
        "torch.save(adversary.state_dict(), \"/content/adversary_model.pth\")\n",
        "\n",
        "def load_models():\n",
        "    input_dim = X_train.shape[1]\n",
        "    hidden_dim = 128  # Example value, adjust as needed\n",
        "    output_dim = 1  # Example value for binary classification\n",
        "\n",
        "    # Initialize models with the necessary arguments\n",
        "    main_model = MainModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
        "    adversary = WassersteinAdversary(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
        "\n",
        "    # Try loading the state_dict and handle errors\n",
        "    try:\n",
        "        main_model.load_state_dict(torch.load('main_model.pth'))\n",
        "        adversary.load_state_dict(torch.load('adversary.pth'))\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading models: {e}\")\n",
        "\n",
        "    return main_model, adversary\n",
        "\n",
        "main_model, adversary = load_models()\n",
        "\n",
        "# 9. Making Predictions with the Trained Model\n",
        "def make_predictions(X_input):\n",
        "    X_input_tensor = torch.tensor(X_input, dtype=torch.float32)\n",
        "    main_model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = main_model(X_input_tensor)\n",
        "        predictions = torch.sigmoid(output).round()\n",
        "        return predictions\n",
        "\n",
        "# Example prediction\n",
        "new_data = X_test[:5]  # First 5 samples from the test set\n",
        "predictions = make_predictions(new_data)\n",
        "print(f\"Predictions for new data: {predictions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4LwNq7HqmOS",
        "outputId": "ee0cfe13-167c-4ea8-c3a6-b935c56610a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with lr_main=0.001, lr_adv=0.001, optimizer=adam\n",
            "Epoch 0/100, Main Loss: 0.687650740146637, Adversary Loss: 0.3750351369380951\n",
            "Epoch 10/100, Main Loss: 0.5579169988632202, Adversary Loss: 0.28453516960144043\n",
            "Epoch 20/100, Main Loss: 0.4623716473579407, Adversary Loss: 0.26981037855148315\n",
            "Epoch 30/100, Main Loss: 0.4102172255516052, Adversary Loss: 0.2621779143810272\n",
            "Epoch 40/100, Main Loss: 0.370026558637619, Adversary Loss: 0.25557741522789\n",
            "Epoch 50/100, Main Loss: 0.34132421016693115, Adversary Loss: 0.2496689110994339\n",
            "Epoch 60/100, Main Loss: 0.3176529109477997, Adversary Loss: 0.24457593262195587\n",
            "Epoch 70/100, Main Loss: 0.29843056201934814, Adversary Loss: 0.24006609618663788\n",
            "Epoch 80/100, Main Loss: 0.282288134098053, Adversary Loss: 0.2360507994890213\n",
            "Epoch 90/100, Main Loss: 0.267956018447876, Adversary Loss: 0.232503741979599\n",
            "Test Accuracy: 0.84375\n",
            "Testing with lr_main=0.001, lr_adv=0.001, optimizer=sgd\n",
            "Epoch 0/100, Main Loss: 0.7019885182380676, Adversary Loss: 0.5039302110671997\n",
            "Epoch 10/100, Main Loss: 0.701333224773407, Adversary Loss: 0.44684988260269165\n",
            "Epoch 20/100, Main Loss: 0.7006822824478149, Adversary Loss: 0.40710073709487915\n",
            "Epoch 30/100, Main Loss: 0.7000355124473572, Adversary Loss: 0.3791452944278717\n",
            "Epoch 40/100, Main Loss: 0.6993929147720337, Adversary Loss: 0.35923874378204346\n",
            "Epoch 50/100, Main Loss: 0.6987540125846863, Adversary Loss: 0.34484589099884033\n",
            "Epoch 60/100, Main Loss: 0.6981188058853149, Adversary Loss: 0.3342478275299072\n",
            "Epoch 70/100, Main Loss: 0.6974873542785645, Adversary Loss: 0.3262784779071808\n",
            "Epoch 80/100, Main Loss: 0.6968598961830139, Adversary Loss: 0.3201439082622528\n",
            "Epoch 90/100, Main Loss: 0.6962357759475708, Adversary Loss: 0.31530293822288513\n",
            "Test Accuracy: 0.46875\n",
            "Testing with lr_main=0.001, lr_adv=0.0001, optimizer=adam\n",
            "Epoch 0/100, Main Loss: 0.6987366676330566, Adversary Loss: 0.5849164724349976\n",
            "Epoch 10/100, Main Loss: 0.5748022198677063, Adversary Loss: 0.5406289100646973\n",
            "Epoch 20/100, Main Loss: 0.4787634313106537, Adversary Loss: 0.5006709694862366\n",
            "Epoch 30/100, Main Loss: 0.4235709607601166, Adversary Loss: 0.4652740955352783\n",
            "Epoch 40/100, Main Loss: 0.37776118516921997, Adversary Loss: 0.43438464403152466\n",
            "Epoch 50/100, Main Loss: 0.3457600474357605, Adversary Loss: 0.4077320992946625\n",
            "Epoch 60/100, Main Loss: 0.32068756222724915, Adversary Loss: 0.38494643568992615\n",
            "Epoch 70/100, Main Loss: 0.3008441925048828, Adversary Loss: 0.36561375856399536\n",
            "Epoch 80/100, Main Loss: 0.2835220694541931, Adversary Loss: 0.34932392835617065\n",
            "Epoch 90/100, Main Loss: 0.2677876353263855, Adversary Loss: 0.3356669545173645\n",
            "Test Accuracy: 0.84375\n",
            "Testing with lr_main=0.001, lr_adv=0.0001, optimizer=sgd\n",
            "Epoch 0/100, Main Loss: 0.7070611715316772, Adversary Loss: 0.6949875950813293\n",
            "Epoch 10/100, Main Loss: 0.7064184546470642, Adversary Loss: 0.6792351007461548\n",
            "Epoch 20/100, Main Loss: 0.7057796716690063, Adversary Loss: 0.6641021966934204\n",
            "Epoch 30/100, Main Loss: 0.7051447629928589, Adversary Loss: 0.6495643854141235\n",
            "Epoch 40/100, Main Loss: 0.704513669013977, Adversary Loss: 0.6355980634689331\n",
            "Epoch 50/100, Main Loss: 0.7038866281509399, Adversary Loss: 0.6221805214881897\n",
            "Epoch 60/100, Main Loss: 0.7032634019851685, Adversary Loss: 0.609289824962616\n",
            "Epoch 70/100, Main Loss: 0.7026435136795044, Adversary Loss: 0.5969053506851196\n",
            "Epoch 80/100, Main Loss: 0.7020276188850403, Adversary Loss: 0.585006833076477\n",
            "Epoch 90/100, Main Loss: 0.7014153003692627, Adversary Loss: 0.5735749006271362\n",
            "Test Accuracy: 0.4937500059604645\n",
            "Testing with lr_main=0.001, lr_adv=1e-05, optimizer=adam\n",
            "Epoch 0/100, Main Loss: 0.698349118232727, Adversary Loss: 0.34744468331336975\n",
            "Epoch 10/100, Main Loss: 0.5713375210762024, Adversary Loss: 0.34525617957115173\n",
            "Epoch 20/100, Main Loss: 0.4825236201286316, Adversary Loss: 0.34311026334762573\n",
            "Epoch 30/100, Main Loss: 0.4258708357810974, Adversary Loss: 0.3410111665725708\n",
            "Epoch 40/100, Main Loss: 0.3801661431789398, Adversary Loss: 0.3389614522457123\n",
            "Epoch 50/100, Main Loss: 0.34895139932632446, Adversary Loss: 0.3369620740413666\n",
            "Epoch 60/100, Main Loss: 0.3240063190460205, Adversary Loss: 0.33501309156417847\n",
            "Epoch 70/100, Main Loss: 0.30345308780670166, Adversary Loss: 0.3331141173839569\n",
            "Epoch 80/100, Main Loss: 0.2856065332889557, Adversary Loss: 0.33126410841941833\n",
            "Epoch 90/100, Main Loss: 0.2693832814693451, Adversary Loss: 0.3294621407985687\n",
            "Test Accuracy: 0.856249988079071\n",
            "Testing with lr_main=0.001, lr_adv=1e-05, optimizer=sgd\n",
            "Epoch 0/100, Main Loss: 0.696304202079773, Adversary Loss: 0.7403333783149719\n",
            "Epoch 10/100, Main Loss: 0.695732593536377, Adversary Loss: 0.7382570505142212\n",
            "Epoch 20/100, Main Loss: 0.6951627731323242, Adversary Loss: 0.7361900210380554\n",
            "Epoch 30/100, Main Loss: 0.6945946216583252, Adversary Loss: 0.7341324090957642\n",
            "Epoch 40/100, Main Loss: 0.6940281987190247, Adversary Loss: 0.7320841550827026\n",
            "Epoch 50/100, Main Loss: 0.6934636831283569, Adversary Loss: 0.7300451993942261\n",
            "Epoch 60/100, Main Loss: 0.6929009556770325, Adversary Loss: 0.7280154228210449\n",
            "Epoch 70/100, Main Loss: 0.692339301109314, Adversary Loss: 0.7259949445724487\n",
            "Epoch 80/100, Main Loss: 0.6917790770530701, Adversary Loss: 0.7239834666252136\n",
            "Epoch 90/100, Main Loss: 0.6912201642990112, Adversary Loss: 0.7219813466072083\n",
            "Test Accuracy: 0.6000000238418579\n",
            "Testing with lr_main=0.0001, lr_adv=0.001, optimizer=adam\n",
            "Epoch 0/100, Main Loss: 0.6927956342697144, Adversary Loss: 0.41717249155044556\n",
            "Epoch 10/100, Main Loss: 0.6768142580986023, Adversary Loss: 0.3025857210159302\n",
            "Epoch 20/100, Main Loss: 0.6617026925086975, Adversary Loss: 0.2887556850910187\n",
            "Epoch 30/100, Main Loss: 0.647055447101593, Adversary Loss: 0.2760886549949646\n",
            "Epoch 40/100, Main Loss: 0.632635772228241, Adversary Loss: 0.2666086256504059\n",
            "Epoch 50/100, Main Loss: 0.618270993232727, Adversary Loss: 0.25975674390792847\n",
            "Epoch 60/100, Main Loss: 0.603801429271698, Adversary Loss: 0.25390809774398804\n",
            "Epoch 70/100, Main Loss: 0.589279055595398, Adversary Loss: 0.24879980087280273\n",
            "Epoch 80/100, Main Loss: 0.5747416615486145, Adversary Loss: 0.24433867633342743\n",
            "Epoch 90/100, Main Loss: 0.5601752400398254, Adversary Loss: 0.2402791678905487\n",
            "Test Accuracy: 0.8062499761581421\n",
            "Testing with lr_main=0.0001, lr_adv=0.001, optimizer=sgd\n",
            "Epoch 0/100, Main Loss: 0.6995925903320312, Adversary Loss: 0.4806711673736572\n",
            "Epoch 10/100, Main Loss: 0.6995443105697632, Adversary Loss: 0.4162519872188568\n",
            "Epoch 20/100, Main Loss: 0.6994960904121399, Adversary Loss: 0.3742990493774414\n",
            "Epoch 30/100, Main Loss: 0.6994478106498718, Adversary Loss: 0.3468378484249115\n",
            "Epoch 40/100, Main Loss: 0.6993995904922485, Adversary Loss: 0.3287356197834015\n",
            "Epoch 50/100, Main Loss: 0.69935142993927, Adversary Loss: 0.3166863024234772\n",
            "Epoch 60/100, Main Loss: 0.699303150177002, Adversary Loss: 0.3085591197013855\n",
            "Epoch 70/100, Main Loss: 0.6992549896240234, Adversary Loss: 0.3029797673225403\n",
            "Epoch 80/100, Main Loss: 0.6992068290710449, Adversary Loss: 0.2990613877773285\n",
            "Epoch 90/100, Main Loss: 0.6991586685180664, Adversary Loss: 0.2962307035923004\n",
            "Test Accuracy: 0.4000000059604645\n",
            "Testing with lr_main=0.0001, lr_adv=0.0001, optimizer=adam\n",
            "Epoch 0/100, Main Loss: 0.6870474815368652, Adversary Loss: 0.44237273931503296\n",
            "Epoch 10/100, Main Loss: 0.6701377034187317, Adversary Loss: 0.41471487283706665\n",
            "Epoch 20/100, Main Loss: 0.6544528603553772, Adversary Loss: 0.39058977365493774\n",
            "Epoch 30/100, Main Loss: 0.6394746899604797, Adversary Loss: 0.3701266348361969\n",
            "Epoch 40/100, Main Loss: 0.6248527765274048, Adversary Loss: 0.35316890478134155\n",
            "Epoch 50/100, Main Loss: 0.6104544401168823, Adversary Loss: 0.3393736481666565\n",
            "Epoch 60/100, Main Loss: 0.5961761474609375, Adversary Loss: 0.3283081650733948\n",
            "Epoch 70/100, Main Loss: 0.582075834274292, Adversary Loss: 0.3195202648639679\n",
            "Epoch 80/100, Main Loss: 0.5681742429733276, Adversary Loss: 0.312572181224823\n",
            "Epoch 90/100, Main Loss: 0.5545857548713684, Adversary Loss: 0.30707019567489624\n",
            "Test Accuracy: 0.768750011920929\n",
            "Testing with lr_main=0.0001, lr_adv=0.0001, optimizer=sgd\n",
            "Epoch 0/100, Main Loss: 0.6940047740936279, Adversary Loss: 0.5535709857940674\n",
            "Epoch 10/100, Main Loss: 0.6939499974250793, Adversary Loss: 0.5419694781303406\n",
            "Epoch 20/100, Main Loss: 0.6938952207565308, Adversary Loss: 0.5308874249458313\n",
            "Epoch 30/100, Main Loss: 0.693840503692627, Adversary Loss: 0.520301103591919\n",
            "Epoch 40/100, Main Loss: 0.6937857866287231, Adversary Loss: 0.5101879835128784\n",
            "Epoch 50/100, Main Loss: 0.6937311291694641, Adversary Loss: 0.500526487827301\n",
            "Epoch 60/100, Main Loss: 0.6936764717102051, Adversary Loss: 0.49129611253738403\n",
            "Epoch 70/100, Main Loss: 0.693621814250946, Adversary Loss: 0.4824771285057068\n",
            "Epoch 80/100, Main Loss: 0.693567156791687, Adversary Loss: 0.4740508496761322\n",
            "Epoch 90/100, Main Loss: 0.6935127377510071, Adversary Loss: 0.46599945425987244\n",
            "Test Accuracy: 0.5062500238418579\n",
            "Testing with lr_main=0.0001, lr_adv=1e-05, optimizer=adam\n",
            "Epoch 0/100, Main Loss: 0.6951843500137329, Adversary Loss: 0.41789427399635315\n",
            "Epoch 10/100, Main Loss: 0.6798458099365234, Adversary Loss: 0.4153798222541809\n",
            "Epoch 20/100, Main Loss: 0.6651269197463989, Adversary Loss: 0.4129011631011963\n",
            "Epoch 30/100, Main Loss: 0.6507424712181091, Adversary Loss: 0.4104619026184082\n",
            "Epoch 40/100, Main Loss: 0.6364226937294006, Adversary Loss: 0.4080637991428375\n",
            "Epoch 50/100, Main Loss: 0.6219339966773987, Adversary Loss: 0.40570729970932007\n",
            "Epoch 60/100, Main Loss: 0.6072355508804321, Adversary Loss: 0.4033927023410797\n",
            "Epoch 70/100, Main Loss: 0.5923786163330078, Adversary Loss: 0.40111979842185974\n",
            "Epoch 80/100, Main Loss: 0.5774468183517456, Adversary Loss: 0.3988880515098572\n",
            "Epoch 90/100, Main Loss: 0.5625468492507935, Adversary Loss: 0.3966968357563019\n",
            "Test Accuracy: 0.762499988079071\n",
            "Testing with lr_main=0.0001, lr_adv=1e-05, optimizer=sgd\n",
            "Epoch 0/100, Main Loss: 0.6935781240463257, Adversary Loss: 0.7611919641494751\n",
            "Epoch 10/100, Main Loss: 0.6935038566589355, Adversary Loss: 0.7591400146484375\n",
            "Epoch 20/100, Main Loss: 0.6934295892715454, Adversary Loss: 0.757097601890564\n",
            "Epoch 30/100, Main Loss: 0.6933554410934448, Adversary Loss: 0.7550643682479858\n",
            "Epoch 40/100, Main Loss: 0.6932812333106995, Adversary Loss: 0.7530404925346375\n",
            "Epoch 50/100, Main Loss: 0.6932070851325989, Adversary Loss: 0.7510259747505188\n",
            "Epoch 60/100, Main Loss: 0.6931329965591431, Adversary Loss: 0.749020516872406\n",
            "Epoch 70/100, Main Loss: 0.693058967590332, Adversary Loss: 0.7470243573188782\n",
            "Epoch 80/100, Main Loss: 0.692984938621521, Adversary Loss: 0.745037317276001\n",
            "Epoch 90/100, Main Loss: 0.6929109692573547, Adversary Loss: 0.7430594563484192\n",
            "Test Accuracy: 0.4437499940395355\n",
            "Testing with lr_main=1e-05, lr_adv=0.001, optimizer=adam\n",
            "Epoch 0/100, Main Loss: 0.6802364587783813, Adversary Loss: 0.4812466502189636\n",
            "Epoch 10/100, Main Loss: 0.678552508354187, Adversary Loss: 0.32064422965049744\n",
            "Epoch 20/100, Main Loss: 0.6768754124641418, Adversary Loss: 0.29283151030540466\n",
            "Epoch 30/100, Main Loss: 0.6752035617828369, Adversary Loss: 0.277570903301239\n",
            "Epoch 40/100, Main Loss: 0.6735385060310364, Adversary Loss: 0.2690357565879822\n",
            "Epoch 50/100, Main Loss: 0.671876072883606, Adversary Loss: 0.2614600658416748\n",
            "Epoch 60/100, Main Loss: 0.6702145934104919, Adversary Loss: 0.25479304790496826\n",
            "Epoch 70/100, Main Loss: 0.6685563325881958, Adversary Loss: 0.24925506114959717\n",
            "Epoch 80/100, Main Loss: 0.6668989658355713, Adversary Loss: 0.24448004364967346\n",
            "Epoch 90/100, Main Loss: 0.6652458906173706, Adversary Loss: 0.24031193554401398\n",
            "Test Accuracy: 0.71875\n",
            "Testing with lr_main=1e-05, lr_adv=0.001, optimizer=sgd\n",
            "Epoch 0/100, Main Loss: 0.6989531517028809, Adversary Loss: 0.4199534058570862\n",
            "Epoch 10/100, Main Loss: 0.6989470720291138, Adversary Loss: 0.3769771158695221\n",
            "Epoch 20/100, Main Loss: 0.6989408731460571, Adversary Loss: 0.347848504781723\n",
            "Epoch 30/100, Main Loss: 0.6989349126815796, Adversary Loss: 0.32802218198776245\n",
            "Epoch 40/100, Main Loss: 0.6989288330078125, Adversary Loss: 0.31444913148880005\n",
            "Epoch 50/100, Main Loss: 0.6989226937294006, Adversary Loss: 0.30508360266685486\n",
            "Epoch 60/100, Main Loss: 0.6989165544509888, Adversary Loss: 0.29855257272720337\n",
            "Epoch 70/100, Main Loss: 0.6989104151725769, Adversary Loss: 0.29393449425697327\n",
            "Epoch 80/100, Main Loss: 0.6989043951034546, Adversary Loss: 0.29061055183410645\n",
            "Epoch 90/100, Main Loss: 0.6988982558250427, Adversary Loss: 0.2881649136543274\n",
            "Test Accuracy: 0.48750001192092896\n",
            "Testing with lr_main=1e-05, lr_adv=0.0001, optimizer=adam\n",
            "Epoch 0/100, Main Loss: 0.6951810121536255, Adversary Loss: 0.4405623972415924\n",
            "Epoch 10/100, Main Loss: 0.6936737298965454, Adversary Loss: 0.413571834564209\n",
            "Epoch 20/100, Main Loss: 0.6921752095222473, Adversary Loss: 0.38997209072113037\n",
            "Epoch 30/100, Main Loss: 0.6906836628913879, Adversary Loss: 0.3698599338531494\n",
            "Epoch 40/100, Main Loss: 0.6891998052597046, Adversary Loss: 0.3530542254447937\n",
            "Epoch 50/100, Main Loss: 0.6877241730690002, Adversary Loss: 0.33919796347618103\n",
            "Epoch 60/100, Main Loss: 0.6862584352493286, Adversary Loss: 0.3278697729110718\n",
            "Epoch 70/100, Main Loss: 0.684801459312439, Adversary Loss: 0.3186355233192444\n",
            "Epoch 80/100, Main Loss: 0.6833518743515015, Adversary Loss: 0.31112104654312134\n",
            "Epoch 90/100, Main Loss: 0.6819078326225281, Adversary Loss: 0.305003821849823\n",
            "Test Accuracy: 0.65625\n",
            "Testing with lr_main=1e-05, lr_adv=0.0001, optimizer=sgd\n",
            "Epoch 0/100, Main Loss: 0.6851573586463928, Adversary Loss: 0.6112732887268066\n",
            "Epoch 10/100, Main Loss: 0.6851513385772705, Adversary Loss: 0.5973001718521118\n",
            "Epoch 20/100, Main Loss: 0.6851452589035034, Adversary Loss: 0.5839336514472961\n",
            "Epoch 30/100, Main Loss: 0.6851392984390259, Adversary Loss: 0.5711466670036316\n",
            "Epoch 40/100, Main Loss: 0.6851332783699036, Adversary Loss: 0.5589132905006409\n",
            "Epoch 50/100, Main Loss: 0.6851272583007812, Adversary Loss: 0.5472087264060974\n",
            "Epoch 60/100, Main Loss: 0.6851212978363037, Adversary Loss: 0.5360094308853149\n",
            "Epoch 70/100, Main Loss: 0.6851152181625366, Adversary Loss: 0.5252929329872131\n",
            "Epoch 80/100, Main Loss: 0.6851092576980591, Adversary Loss: 0.5150375366210938\n",
            "Epoch 90/100, Main Loss: 0.6851032972335815, Adversary Loss: 0.5052227973937988\n",
            "Test Accuracy: 0.5874999761581421\n",
            "Testing with lr_main=1e-05, lr_adv=1e-05, optimizer=adam\n",
            "Epoch 0/100, Main Loss: 0.6876078844070435, Adversary Loss: 0.830437958240509\n",
            "Epoch 10/100, Main Loss: 0.6859062910079956, Adversary Loss: 0.8243004083633423\n",
            "Epoch 20/100, Main Loss: 0.6842149496078491, Adversary Loss: 0.8182042837142944\n",
            "Epoch 30/100, Main Loss: 0.6825360059738159, Adversary Loss: 0.8121539950370789\n",
            "Epoch 40/100, Main Loss: 0.6808651685714722, Adversary Loss: 0.80615234375\n",
            "Epoch 50/100, Main Loss: 0.6792027354240417, Adversary Loss: 0.8002005815505981\n",
            "Epoch 60/100, Main Loss: 0.6775497198104858, Adversary Loss: 0.7942992448806763\n",
            "Epoch 70/100, Main Loss: 0.675902783870697, Adversary Loss: 0.788448691368103\n",
            "Epoch 80/100, Main Loss: 0.6742598414421082, Adversary Loss: 0.7826486825942993\n",
            "Epoch 90/100, Main Loss: 0.6726226210594177, Adversary Loss: 0.7768988609313965\n",
            "Test Accuracy: 0.6625000238418579\n",
            "Testing with lr_main=1e-05, lr_adv=1e-05, optimizer=sgd\n",
            "Epoch 0/100, Main Loss: 0.6942766308784485, Adversary Loss: 0.5543404817581177\n",
            "Epoch 10/100, Main Loss: 0.6942695379257202, Adversary Loss: 0.5531977415084839\n",
            "Epoch 20/100, Main Loss: 0.6942626237869263, Adversary Loss: 0.552060067653656\n",
            "Epoch 30/100, Main Loss: 0.6942556500434875, Adversary Loss: 0.550927460193634\n",
            "Epoch 40/100, Main Loss: 0.6942485570907593, Adversary Loss: 0.5497997999191284\n",
            "Epoch 50/100, Main Loss: 0.6942415237426758, Adversary Loss: 0.5486769676208496\n",
            "Epoch 60/100, Main Loss: 0.6942345499992371, Adversary Loss: 0.5475593209266663\n",
            "Epoch 70/100, Main Loss: 0.6942275762557983, Adversary Loss: 0.5464465022087097\n",
            "Epoch 80/100, Main Loss: 0.6942206621170044, Adversary Loss: 0.54533851146698\n",
            "Epoch 90/100, Main Loss: 0.6942135691642761, Adversary Loss: 0.5442355871200562\n",
            "Test Accuracy: 0.48750001192092896\n",
            "Best Test Accuracy: 0.856249988079071 with parameters: {'lr_main': 0.001, 'lr_adv': 1e-05, 'optimizer': 'adam'}\n",
            "Best Test Accuracy: 0.856249988079071 with parameters: {'lr_main': 0.001, 'lr_adv': 1e-05, 'optimizer': 'adam'}\n",
            "Demographic Parity: 0.012512575834989548, Equal Opportunity Difference: 0.0\n",
            "Error loading models: [Errno 2] No such file or directory: 'adversary.pth'\n",
            "Predictions for new data: tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load the model checkpoint\n",
        "main_model_checkpoint = torch.load('main_model.pth')\n",
        "adversary_model_checkpoint = torch.load('adversary_model.pth')\n",
        "\n",
        "# Check the state_dict keys of the loaded models\n",
        "print(\"Main model state_dict keys:\")\n",
        "for key in main_model_checkpoint.keys():\n",
        "    print(key)\n",
        "\n",
        "print(\"\\nAdversary model state_dict keys:\")\n",
        "for key in adversary_model_checkpoint.keys():\n",
        "    print(key)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMneTFwsrbKb",
        "outputId": "743abe42-ed6e-412c-b12b-a7e368239323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main model state_dict keys:\n",
            "fc1.weight\n",
            "fc1.bias\n",
            "fc2.weight\n",
            "fc2.bias\n",
            "\n",
            "Adversary model state_dict keys:\n",
            "fc1.weight\n",
            "fc1.bias\n",
            "fc2.weight\n",
            "fc2.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the Main Model\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MainModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x.view(-1, 1)  # Reshaping the output to [batch_size, 1]\n",
        "\n",
        "# Define the Wasserstein Adversary Model\n",
        "class WassersteinAdversary(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(WassersteinAdversary, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x.view(-1, 1)  # Reshaping the output to [batch_size, 1]\n",
        "\n",
        "# Function to test hyperparameters\n",
        "def test_hyperparameters(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, protected_attribute_train):\n",
        "    best_params = None\n",
        "    best_test_acc = 0\n",
        "\n",
        "    # Testing with different hyperparameters\n",
        "    for lr_main in [0.001, 0.0001]:\n",
        "        for lr_adv in [0.001, 0.0001]:\n",
        "            for optimizer_type in ['adam', 'sgd']:\n",
        "                print(f\"Testing with lr_main={lr_main}, lr_adv={lr_adv}, optimizer={optimizer_type}\")\n",
        "\n",
        "                # Initialize the models\n",
        "                main_model = MainModel(input_dim=X_train_tensor.shape[1], hidden_dim=128, output_dim=1)\n",
        "                adversary = WassersteinAdversary(input_dim=X_train_tensor.shape[1], hidden_dim=128, output_dim=1)\n",
        "\n",
        "                # Choose optimizer\n",
        "                if optimizer_type == 'adam':\n",
        "                    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=lr_main)\n",
        "                    optimizer_adv = torch.optim.Adam(adversary.parameters(), lr=lr_adv)\n",
        "                else:\n",
        "                    optimizer_main = torch.optim.SGD(main_model.parameters(), lr=lr_main)\n",
        "                    optimizer_adv = torch.optim.SGD(adversary.parameters(), lr=lr_adv)\n",
        "\n",
        "                # Training loop\n",
        "                for epoch in range(100):\n",
        "                    main_model.train()\n",
        "                    adversary.train()\n",
        "\n",
        "                    # Forward pass through the model\n",
        "                    output = main_model(X_train_tensor)\n",
        "                    print(f\"Epoch {epoch}, Output shape: {output.shape}, Target shape: {y_train_tensor.shape}\")\n",
        "\n",
        "                    # Check the output and target shape for potential mismatches\n",
        "                    if output.shape != y_train_tensor.shape:\n",
        "                        print(f\"Warning: Output shape {output.shape} differs from target shape {y_train_tensor.shape}\")\n",
        "\n",
        "                    # Loss computation\n",
        "                    loss = F.binary_cross_entropy_with_logits(output, y_train_tensor)\n",
        "\n",
        "                    # Backpropagation and optimizer step\n",
        "                    optimizer_main.zero_grad()\n",
        "                    optimizer_adv.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer_main.step()\n",
        "                    optimizer_adv.step()\n",
        "\n",
        "                # Evaluation on the test set\n",
        "                main_model.eval()\n",
        "                with torch.no_grad():\n",
        "                    test_output = main_model(X_test_tensor)\n",
        "                    test_preds = torch.sigmoid(test_output).round()\n",
        "                    test_acc = (test_preds == y_test_tensor).float().mean()\n",
        "\n",
        "                # Track best test accuracy\n",
        "                if test_acc > best_test_acc:\n",
        "                    best_test_acc = test_acc\n",
        "                    best_params = {'lr_main': lr_main, 'lr_adv': lr_adv, 'optimizer': optimizer_type}\n",
        "\n",
        "    print(f\"Best Test Accuracy: {best_test_acc} with parameters: {best_params}\")\n",
        "    return best_params, best_test_acc\n",
        "\n",
        "# Save the model after training\n",
        "def save_models(main_model, adversary):\n",
        "    torch.save(main_model.state_dict(), 'main_model.pth')\n",
        "    torch.save(adversary.state_dict(), 'adversary.pth')\n",
        "    print(\"Models saved successfully!\")\n",
        "\n",
        "# Example usage:\n",
        "# Assuming X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, and protected_attribute_train are defined\n",
        "best_params, best_test_acc = test_hyperparameters(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, protected_attribute_train)\n",
        "print(f\"Best Test Accuracy: {best_test_acc} with parameters: {best_params}\")\n",
        "\n",
        "# After training is complete, save the models\n",
        "save_models(main_model, adversary)\n",
        "\n",
        "# Load models for prediction or further use\n",
        "def load_models():\n",
        "    main_model = MainModel(input_dim=X_train_tensor.shape[1], hidden_dim=128, output_dim=1)\n",
        "    adversary = WassersteinAdversary(input_dim=X_train_tensor.shape[1], hidden_dim=128, output_dim=1)\n",
        "\n",
        "    try:\n",
        "        # Try loading model weights\n",
        "        main_model.load_state_dict(torch.load('main_model.pth'))\n",
        "        adversary.load_state_dict(torch.load('adversary.pth'))\n",
        "        print(\"Models loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading models: {e}\")\n",
        "\n",
        "    return main_model, adversary\n",
        "\n",
        "main_model, adversary = load_models()\n",
        "\n",
        "# Now use the models for prediction\n",
        "def make_predictions(model, X_input_tensor):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(X_input_tensor)\n",
        "        predictions = torch.sigmoid(output).round()\n",
        "        return predictions\n",
        "\n",
        "# Example prediction\n",
        "new_data = X_test_tensor[:5]  # First 5 samples from the test set\n",
        "predictions = make_predictions(main_model, new_data)\n",
        "print(f\"Predictions for new data: {predictions}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKZ9vAGpsNKY",
        "outputId": "b0250809-2736-4c92-9097-f79d1704acc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with lr_main=0.001, lr_adv=0.001, optimizer=adam\n",
            "Epoch 0, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 1, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 2, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 3, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 4, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 5, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 6, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 7, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 8, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 9, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 10, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 11, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 12, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 13, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 14, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 15, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 16, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 17, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 18, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 19, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 20, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 21, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 22, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 23, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 24, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 25, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 26, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 27, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 28, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 29, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 30, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 31, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 32, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 33, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 34, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 35, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 36, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 37, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 38, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 39, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 40, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 41, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 42, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 43, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 44, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 45, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 46, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 47, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 48, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 49, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 50, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 51, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 52, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 53, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 54, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 55, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 56, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 57, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 58, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 59, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 60, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 61, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 62, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 63, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 64, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 65, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 66, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 67, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 68, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 69, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 70, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 71, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 72, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 73, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 74, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 75, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 76, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 77, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 78, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 79, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 80, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 81, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 82, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 83, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 84, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 85, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 86, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 87, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 88, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 89, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 90, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 91, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 92, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 93, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 94, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 95, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 96, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 97, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 98, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 99, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Testing with lr_main=0.001, lr_adv=0.001, optimizer=sgd\n",
            "Epoch 0, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 1, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 2, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 3, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 4, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 5, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 6, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 7, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 8, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 9, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 10, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 11, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 12, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 13, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 14, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 15, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 16, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 17, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 18, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 19, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 20, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 21, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 22, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 23, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 24, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 25, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 26, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 27, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 28, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 29, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 30, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 31, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 32, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 33, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 34, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 35, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 36, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 37, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 38, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 39, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 40, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 41, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 42, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 43, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 44, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 45, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 46, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 47, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 48, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 49, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 50, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 51, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 52, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 53, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 54, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 55, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 56, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 57, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 58, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 59, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 60, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 61, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 62, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 63, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 64, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 65, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 66, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 67, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 68, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 69, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 70, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 71, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 72, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 73, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 74, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 75, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 76, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 77, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 78, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 79, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 80, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 81, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 82, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 83, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 84, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 85, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 86, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 87, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 88, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 89, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 90, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 91, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 92, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 93, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 94, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 95, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 96, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 97, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 98, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 99, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Testing with lr_main=0.001, lr_adv=0.0001, optimizer=adam\n",
            "Epoch 0, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 1, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 2, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 3, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 4, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 5, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 6, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 7, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 8, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 9, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 10, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 11, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 12, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 13, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 14, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 15, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 16, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 17, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 18, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 19, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 20, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 21, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 22, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 23, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 24, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 25, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 26, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 27, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 28, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 29, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 30, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 31, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 32, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 33, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 34, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 35, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 36, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 37, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 38, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 39, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 40, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 41, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 42, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 43, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 44, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 45, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 46, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 47, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 48, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 49, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 50, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 51, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 52, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 53, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 54, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 55, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 56, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 57, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 58, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 59, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 60, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 61, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 62, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 63, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 64, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 65, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 66, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 67, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 68, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 69, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 70, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 71, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 72, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 73, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 74, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 75, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 76, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 77, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 78, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 79, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 80, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 81, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 82, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 83, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 84, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 85, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 86, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 87, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 88, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 89, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 90, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 91, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 92, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 93, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 94, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 95, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 96, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 97, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 98, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 99, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Testing with lr_main=0.001, lr_adv=0.0001, optimizer=sgd\n",
            "Epoch 0, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 1, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 2, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 3, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 4, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 5, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 6, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 7, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 8, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 9, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 10, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 11, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 12, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 13, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 14, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 15, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 16, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 17, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 18, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 19, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 20, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 21, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 22, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 23, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 24, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 25, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 26, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 27, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 28, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 29, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 30, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 31, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 32, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 33, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 34, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 35, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 36, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 37, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 38, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 39, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 40, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 41, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 42, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 43, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 44, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 45, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 46, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 47, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 48, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 49, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 50, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 51, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 52, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 53, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 54, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 55, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 56, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 57, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 58, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 59, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 60, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 61, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 62, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 63, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 64, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 65, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 66, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 67, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 68, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 69, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 70, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 71, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 72, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 73, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 74, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 75, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 76, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 77, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 78, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 79, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 80, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 81, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 82, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 83, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 84, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 85, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 86, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 87, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 88, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 89, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 90, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 91, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 92, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 93, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 94, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 95, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 96, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 97, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 98, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 99, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Testing with lr_main=0.0001, lr_adv=0.001, optimizer=adam\n",
            "Epoch 0, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 1, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 2, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 3, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 4, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 5, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 6, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 7, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 8, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 9, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 10, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 11, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 12, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 13, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 14, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 15, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 16, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 17, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 18, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 19, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 20, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 21, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 22, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 23, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 24, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 25, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 26, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 27, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 28, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 29, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 30, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 31, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 32, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 33, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 34, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 35, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 36, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 37, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 38, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 39, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 40, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 41, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 42, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 43, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 44, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 45, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 46, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 47, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 48, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 49, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 50, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 51, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 52, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 53, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 54, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 55, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 56, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 57, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 58, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 59, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 60, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 61, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 62, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 63, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 64, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 65, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 66, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 67, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 68, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 69, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 70, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 71, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 72, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 73, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 74, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 75, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 76, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 77, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 78, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 79, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 80, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 81, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 82, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 83, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 84, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 85, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 86, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 87, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 88, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 89, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 90, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 91, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 92, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 93, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 94, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 95, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 96, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 97, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 98, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 99, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Testing with lr_main=0.0001, lr_adv=0.001, optimizer=sgd\n",
            "Epoch 0, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 1, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 2, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 3, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 4, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 5, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 6, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 7, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 8, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 9, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 10, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 11, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 12, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 13, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 14, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 15, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 16, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 17, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 18, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 19, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 20, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 21, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 22, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 23, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 24, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 25, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 26, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 27, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 28, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 29, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 30, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 31, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 32, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 33, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 34, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 35, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 36, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 37, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 38, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 39, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 40, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 41, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 42, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 43, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 44, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 45, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 46, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 47, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 48, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 49, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 50, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 51, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 52, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 53, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 54, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 55, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 56, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 57, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 58, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 59, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 60, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 61, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 62, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 63, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 64, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 65, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 66, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 67, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 68, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 69, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 70, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 71, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 72, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 73, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 74, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 75, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 76, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 77, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 78, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 79, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 80, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 81, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 82, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 83, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 84, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 85, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 86, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 87, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 88, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 89, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 90, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 91, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 92, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 93, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 94, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 95, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 96, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 97, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 98, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 99, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Testing with lr_main=0.0001, lr_adv=0.0001, optimizer=adam\n",
            "Epoch 0, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 1, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 2, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 3, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 4, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 5, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 6, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 7, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 8, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 9, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 10, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 11, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 12, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 13, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 14, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 15, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 16, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 17, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 18, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 19, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 20, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 21, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 22, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 23, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 24, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 25, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 26, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 27, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 28, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 29, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 30, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 31, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 32, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 33, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 34, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 35, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 36, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 37, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 38, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 39, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 40, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 41, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 42, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 43, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 44, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 45, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 46, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 47, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 48, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 49, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 50, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 51, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 52, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 53, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 54, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 55, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 56, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 57, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 58, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 59, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 60, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 61, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 62, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 63, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 64, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 65, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 66, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 67, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 68, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 69, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 70, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 71, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 72, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 73, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 74, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 75, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 76, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 77, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 78, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 79, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 80, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 81, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 82, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 83, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 84, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 85, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 86, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 87, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 88, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 89, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 90, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 91, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 92, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 93, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 94, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 95, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 96, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 97, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 98, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 99, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Testing with lr_main=0.0001, lr_adv=0.0001, optimizer=sgd\n",
            "Epoch 0, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 1, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 2, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 3, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 4, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 5, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 6, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 7, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 8, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 9, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 10, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 11, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 12, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 13, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 14, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 15, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 16, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 17, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 18, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 19, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 20, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 21, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 22, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 23, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 24, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 25, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 26, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 27, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 28, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 29, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 30, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 31, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 32, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 33, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 34, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 35, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 36, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 37, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 38, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 39, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 40, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 41, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 42, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 43, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 44, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 45, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 46, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 47, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 48, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 49, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 50, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 51, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 52, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 53, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 54, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 55, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 56, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 57, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 58, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 59, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 60, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 61, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 62, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 63, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 64, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 65, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 66, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 67, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 68, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 69, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 70, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 71, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 72, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 73, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 74, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 75, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 76, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 77, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 78, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 79, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 80, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 81, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 82, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 83, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 84, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 85, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 86, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 87, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 88, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 89, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 90, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 91, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 92, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 93, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 94, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 95, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 96, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 97, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 98, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Epoch 99, Output shape: torch.Size([640, 1]), Target shape: torch.Size([640, 1])\n",
            "Best Test Accuracy: 0.8500000238418579 with parameters: {'lr_main': 0.001, 'lr_adv': 0.0001, 'optimizer': 'adam'}\n",
            "Best Test Accuracy: 0.8500000238418579 with parameters: {'lr_main': 0.001, 'lr_adv': 0.0001, 'optimizer': 'adam'}\n",
            "Models saved successfully!\n",
            "Models loaded successfully!\n",
            "Predictions for new data: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "# Define the Main Model\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MainModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Define the Adversary Model\n",
        "class AdversaryModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(AdversaryModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Training function for both models\n",
        "def train_model(model, adversary, X_train, y_train, X_adv, y_adv, lr_main, lr_adv, optimizer_type='adam', num_epochs=100):\n",
        "    # Create optimizer for both models\n",
        "    if optimizer_type == 'adam':\n",
        "        optimizer_main = optim.Adam(model.parameters(), lr=lr_main)\n",
        "        optimizer_adv = optim.Adam(adversary.parameters(), lr=lr_adv)\n",
        "    elif optimizer_type == 'sgd':\n",
        "        optimizer_main = optim.SGD(model.parameters(), lr=lr_main)\n",
        "        optimizer_adv = optim.SGD(adversary.parameters(), lr=lr_adv)\n",
        "\n",
        "    # Define loss function\n",
        "    loss_fn = nn.BCELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Forward pass for main model\n",
        "        optimizer_main.zero_grad()\n",
        "        output_main = model(X_train)\n",
        "        loss_main = loss_fn(output_main, y_train)\n",
        "\n",
        "        # Forward pass for adversary model\n",
        "        optimizer_adv.zero_grad()\n",
        "        output_adv = adversary(X_adv)\n",
        "        loss_adv = loss_fn(output_adv, y_adv)\n",
        "\n",
        "        # Backward pass and optimize both models\n",
        "        loss_main.backward()\n",
        "        optimizer_main.step()\n",
        "\n",
        "        loss_adv.backward()\n",
        "        optimizer_adv.step()\n",
        "\n",
        "        # Log training progress\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}/{num_epochs}, Main Loss: {loss_main.item():.4f}, Adversary Loss: {loss_adv.item():.4f}\")\n",
        "\n",
        "    return model, adversary\n",
        "\n",
        "# Prediction function\n",
        "def make_predictions(model, new_data):\n",
        "    with torch.no_grad():\n",
        "        output = model(new_data)\n",
        "        predictions = torch.sigmoid(output).round()\n",
        "        return predictions\n",
        "\n",
        "# Hyperparameter testing function\n",
        "def test_hyperparameters(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, protected_attribute_train):\n",
        "    best_acc = 0\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    # Example hyperparameters to test\n",
        "    learning_rates = [0.001, 0.0001]\n",
        "    adversary_lrs = [0.001, 0.0001]\n",
        "    optimizers = ['adam', 'sgd']\n",
        "\n",
        "    for lr_main in learning_rates:\n",
        "        for lr_adv in adversary_lrs:\n",
        "            for optimizer_type in optimizers:\n",
        "                # Initialize models\n",
        "                model = MainModel(X_train_tensor.shape[1], 64, 1)\n",
        "                adversary = AdversaryModel(X_train_tensor.shape[1], 64, 1)\n",
        "\n",
        "                print(f\"Testing with lr_main={lr_main}, lr_adv={lr_adv}, optimizer={optimizer_type}\")\n",
        "\n",
        "                # Train models\n",
        "                model, adversary = train_model(model, adversary, X_train_tensor, y_train_tensor, X_train_tensor, protected_attribute_train, lr_main, lr_adv, optimizer_type)\n",
        "\n",
        "                # Make predictions on the test set\n",
        "                predictions = make_predictions(model, X_test_tensor)\n",
        "\n",
        "                # Evaluate performance\n",
        "                accuracy = accuracy_score(y_test_tensor.numpy(), predictions.numpy())\n",
        "                if accuracy > best_acc:\n",
        "                    best_acc = accuracy\n",
        "                    best_params = {'lr_main': lr_main, 'lr_adv': lr_adv, 'optimizer': optimizer_type}\n",
        "                    best_model = model  # Store the best model\n",
        "\n",
        "    return best_params, best_acc, best_model\n",
        "\n",
        "# Confusion Matrix Plotting\n",
        "def plot_confusion_matrix(model, X_test, y_test):\n",
        "    y_pred = make_predictions(model, X_test)\n",
        "    cm = confusion_matrix(y_test.numpy(), y_pred.numpy())\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.show()\n",
        "\n",
        "# Example of usage\n",
        "# X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, protected_attribute_train should be defined beforehand\n",
        "best_params, best_test_acc, best_model = test_hyperparameters(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, protected_attribute_train)\n",
        "print(f\"Best Test Accuracy: {best_test_acc} with parameters: {best_params}\")\n",
        "\n",
        "# Plot confusion matrix for the best model\n",
        "plot_confusion_matrix(best_model, X_test_tensor, y_test_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3uO3P_58w3Cf",
        "outputId": "48d6f0ca-d378-43a0-83fc-67fbfc674c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with lr_main=0.001, lr_adv=0.001, optimizer=adam\n",
            "Epoch 0/100, Main Loss: 0.7335, Adversary Loss: 0.7041\n",
            "Epoch 10/100, Main Loss: 0.6888, Adversary Loss: 0.6923\n",
            "Epoch 20/100, Main Loss: 0.6505, Adversary Loss: 0.6881\n",
            "Epoch 30/100, Main Loss: 0.6172, Adversary Loss: 0.6851\n",
            "Epoch 40/100, Main Loss: 0.5877, Adversary Loss: 0.6823\n",
            "Epoch 50/100, Main Loss: 0.5613, Adversary Loss: 0.6797\n",
            "Epoch 60/100, Main Loss: 0.5379, Adversary Loss: 0.6773\n",
            "Epoch 70/100, Main Loss: 0.5173, Adversary Loss: 0.6748\n",
            "Epoch 80/100, Main Loss: 0.4995, Adversary Loss: 0.6722\n",
            "Epoch 90/100, Main Loss: 0.4841, Adversary Loss: 0.6697\n",
            "Testing with lr_main=0.001, lr_adv=0.001, optimizer=sgd\n",
            "Epoch 0/100, Main Loss: 0.6977, Adversary Loss: 0.7007\n",
            "Epoch 10/100, Main Loss: 0.6960, Adversary Loss: 0.7005\n",
            "Epoch 20/100, Main Loss: 0.6944, Adversary Loss: 0.7002\n",
            "Epoch 30/100, Main Loss: 0.6927, Adversary Loss: 0.7000\n",
            "Epoch 40/100, Main Loss: 0.6911, Adversary Loss: 0.6998\n",
            "Epoch 50/100, Main Loss: 0.6895, Adversary Loss: 0.6995\n",
            "Epoch 60/100, Main Loss: 0.6879, Adversary Loss: 0.6993\n",
            "Epoch 70/100, Main Loss: 0.6864, Adversary Loss: 0.6991\n",
            "Epoch 80/100, Main Loss: 0.6848, Adversary Loss: 0.6989\n",
            "Epoch 90/100, Main Loss: 0.6833, Adversary Loss: 0.6987\n",
            "Testing with lr_main=0.001, lr_adv=0.0001, optimizer=adam\n",
            "Epoch 0/100, Main Loss: 0.7207, Adversary Loss: 0.7023\n",
            "Epoch 10/100, Main Loss: 0.6775, Adversary Loss: 0.7004\n",
            "Epoch 20/100, Main Loss: 0.6416, Adversary Loss: 0.6986\n",
            "Epoch 30/100, Main Loss: 0.6118, Adversary Loss: 0.6971\n",
            "Epoch 40/100, Main Loss: 0.5863, Adversary Loss: 0.6957\n",
            "Epoch 50/100, Main Loss: 0.5641, Adversary Loss: 0.6945\n",
            "Epoch 60/100, Main Loss: 0.5444, Adversary Loss: 0.6935\n",
            "Epoch 70/100, Main Loss: 0.5271, Adversary Loss: 0.6925\n",
            "Epoch 80/100, Main Loss: 0.5116, Adversary Loss: 0.6918\n",
            "Epoch 90/100, Main Loss: 0.4978, Adversary Loss: 0.6911\n",
            "Testing with lr_main=0.001, lr_adv=0.0001, optimizer=sgd\n",
            "Epoch 0/100, Main Loss: 0.6985, Adversary Loss: 0.6999\n",
            "Epoch 10/100, Main Loss: 0.6970, Adversary Loss: 0.6999\n",
            "Epoch 20/100, Main Loss: 0.6955, Adversary Loss: 0.6999\n",
            "Epoch 30/100, Main Loss: 0.6940, Adversary Loss: 0.6999\n",
            "Epoch 40/100, Main Loss: 0.6926, Adversary Loss: 0.6999\n",
            "Epoch 50/100, Main Loss: 0.6911, Adversary Loss: 0.6999\n",
            "Epoch 60/100, Main Loss: 0.6897, Adversary Loss: 0.6999\n",
            "Epoch 70/100, Main Loss: 0.6883, Adversary Loss: 0.6999\n",
            "Epoch 80/100, Main Loss: 0.6869, Adversary Loss: 0.6998\n",
            "Epoch 90/100, Main Loss: 0.6855, Adversary Loss: 0.6998\n",
            "Testing with lr_main=0.0001, lr_adv=0.001, optimizer=adam\n",
            "Epoch 0/100, Main Loss: 0.7137, Adversary Loss: 0.7102\n",
            "Epoch 10/100, Main Loss: 0.7086, Adversary Loss: 0.6976\n",
            "Epoch 20/100, Main Loss: 0.7036, Adversary Loss: 0.6930\n",
            "Epoch 30/100, Main Loss: 0.6986, Adversary Loss: 0.6900\n",
            "Epoch 40/100, Main Loss: 0.6938, Adversary Loss: 0.6874\n",
            "Epoch 50/100, Main Loss: 0.6891, Adversary Loss: 0.6850\n",
            "Epoch 60/100, Main Loss: 0.6845, Adversary Loss: 0.6826\n",
            "Epoch 70/100, Main Loss: 0.6800, Adversary Loss: 0.6802\n",
            "Epoch 80/100, Main Loss: 0.6756, Adversary Loss: 0.6777\n",
            "Epoch 90/100, Main Loss: 0.6713, Adversary Loss: 0.6752\n",
            "Testing with lr_main=0.0001, lr_adv=0.001, optimizer=sgd\n",
            "Epoch 0/100, Main Loss: 0.7108, Adversary Loss: 0.6980\n",
            "Epoch 10/100, Main Loss: 0.7106, Adversary Loss: 0.6979\n",
            "Epoch 20/100, Main Loss: 0.7104, Adversary Loss: 0.6977\n",
            "Epoch 30/100, Main Loss: 0.7103, Adversary Loss: 0.6976\n",
            "Epoch 40/100, Main Loss: 0.7101, Adversary Loss: 0.6974\n",
            "Epoch 50/100, Main Loss: 0.7099, Adversary Loss: 0.6973\n",
            "Epoch 60/100, Main Loss: 0.7097, Adversary Loss: 0.6972\n",
            "Epoch 70/100, Main Loss: 0.7095, Adversary Loss: 0.6971\n",
            "Epoch 80/100, Main Loss: 0.7093, Adversary Loss: 0.6969\n",
            "Epoch 90/100, Main Loss: 0.7092, Adversary Loss: 0.6968\n",
            "Testing with lr_main=0.0001, lr_adv=0.0001, optimizer=adam\n",
            "Epoch 0/100, Main Loss: 0.6783, Adversary Loss: 0.7076\n",
            "Epoch 10/100, Main Loss: 0.6740, Adversary Loss: 0.7061\n",
            "Epoch 20/100, Main Loss: 0.6697, Adversary Loss: 0.7046\n",
            "Epoch 30/100, Main Loss: 0.6656, Adversary Loss: 0.7033\n",
            "Epoch 40/100, Main Loss: 0.6616, Adversary Loss: 0.7021\n",
            "Epoch 50/100, Main Loss: 0.6576, Adversary Loss: 0.7010\n",
            "Epoch 60/100, Main Loss: 0.6538, Adversary Loss: 0.7000\n",
            "Epoch 70/100, Main Loss: 0.6500, Adversary Loss: 0.6990\n",
            "Epoch 80/100, Main Loss: 0.6463, Adversary Loss: 0.6981\n",
            "Epoch 90/100, Main Loss: 0.6427, Adversary Loss: 0.6972\n",
            "Testing with lr_main=0.0001, lr_adv=0.0001, optimizer=sgd\n",
            "Epoch 0/100, Main Loss: 0.7403, Adversary Loss: 0.6932\n",
            "Epoch 10/100, Main Loss: 0.7401, Adversary Loss: 0.6932\n",
            "Epoch 20/100, Main Loss: 0.7399, Adversary Loss: 0.6932\n",
            "Epoch 30/100, Main Loss: 0.7397, Adversary Loss: 0.6932\n",
            "Epoch 40/100, Main Loss: 0.7395, Adversary Loss: 0.6932\n",
            "Epoch 50/100, Main Loss: 0.7393, Adversary Loss: 0.6932\n",
            "Epoch 60/100, Main Loss: 0.7392, Adversary Loss: 0.6932\n",
            "Epoch 70/100, Main Loss: 0.7390, Adversary Loss: 0.6932\n",
            "Epoch 80/100, Main Loss: 0.7388, Adversary Loss: 0.6932\n",
            "Epoch 90/100, Main Loss: 0.7386, Adversary Loss: 0.6932\n",
            "Best Test Accuracy: 0.50625 with parameters: {'lr_main': 0.001, 'lr_adv': 0.001, 'optimizer': 'adam'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Example model definitions\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MainModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(8, 512)  # Adjust input features if needed\n",
        "        self.layer2 = nn.Linear(512, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = self.sigmoid(self.layer2(x))\n",
        "        return x\n",
        "\n",
        "class AdversaryModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdversaryModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(8, 512)\n",
        "        self.layer2 = nn.Linear(512, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = self.sigmoid(self.layer2(x))\n",
        "        return x\n",
        "\n",
        "# Optimizer setup\n",
        "def get_optimizer(model, optimizer_type='adam', lr=0.001):\n",
        "    if optimizer_type == 'adam':\n",
        "        return optim.Adam(model.parameters(), lr=lr)\n",
        "    elif optimizer_type == 'sgd':\n",
        "        return optim.SGD(model.parameters(), lr=lr)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported optimizer type\")\n",
        "\n",
        "# Training function\n",
        "def train_model(main_model, adversary_model, train_loader, num_epochs=10, lr_main=0.001, lr_adv=0.001, optimizer_type='adam'):\n",
        "    main_optimizer = get_optimizer(main_model, optimizer_type, lr_main)\n",
        "    adv_optimizer = get_optimizer(adversary_model, optimizer_type, lr_adv)\n",
        "\n",
        "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "\n",
        "    main_model.train()\n",
        "    adversary_model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        main_loss_total = 0.0\n",
        "        adversary_loss_total = 0.0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.float(), target.float()\n",
        "\n",
        "            # Zero the gradients for each batch\n",
        "            main_optimizer.zero_grad()\n",
        "            adv_optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass through the models\n",
        "            main_output = main_model(data)\n",
        "            adversary_output = adversary_model(data)\n",
        "\n",
        "            # Calculate loss\n",
        "            main_loss = criterion(main_output, target)\n",
        "            adversary_loss = criterion(adversary_output, target)\n",
        "\n",
        "            # Backpropagate\n",
        "            main_loss.backward()\n",
        "            adversary_loss.backward()\n",
        "\n",
        "            # Update weights\n",
        "            main_optimizer.step()\n",
        "            adv_optimizer.step()\n",
        "\n",
        "            # Accumulate losses\n",
        "            main_loss_total += main_loss.item()\n",
        "            adversary_loss_total += adversary_loss.item()\n",
        "\n",
        "        # Print losses for each epoch\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Main Loss: {main_loss_total / len(train_loader)}')\n",
        "        print(f'Adversary Loss: {adversary_loss_total / len(train_loader)}')\n",
        "\n",
        "# Example Data (Replace with your actual data)\n",
        "# Assuming X_train has 8 features, and y_train is binary (0 or 1)\n",
        "X_train = torch.randn(1000, 8)  # 1000 samples, 8 features\n",
        "y_train = torch.randint(0, 2, (1000, 1)).float()  # 1000 binary labels (0 or 1)\n",
        "\n",
        "# Create DataLoader\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize models\n",
        "main_model = MainModel()\n",
        "adversary_model = AdversaryModel()\n",
        "\n",
        "# Train models\n",
        "train_model(main_model, adversary_model, train_loader, num_epochs=10, lr_main=0.001, lr_adv=0.001, optimizer_type='adam')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyk3Ky6w2cAt",
        "outputId": "c4c9c9ed-ed96-4987-c426-bf10c71b4a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Main Loss: 0.6996959671378136\n",
            "Adversary Loss: 0.6998715344816446\n",
            "Epoch 2/10\n",
            "Main Loss: 0.6949649713933468\n",
            "Adversary Loss: 0.6933724638074636\n",
            "Epoch 3/10\n",
            "Main Loss: 0.6852658372372389\n",
            "Adversary Loss: 0.6840245500206947\n",
            "Epoch 4/10\n",
            "Main Loss: 0.6813098639249802\n",
            "Adversary Loss: 0.6801000498235226\n",
            "Epoch 5/10\n",
            "Main Loss: 0.6814685203135014\n",
            "Adversary Loss: 0.6812159009277821\n",
            "Epoch 6/10\n",
            "Main Loss: 0.6759241335093975\n",
            "Adversary Loss: 0.6750838663429022\n",
            "Epoch 7/10\n",
            "Main Loss: 0.6718279700726271\n",
            "Adversary Loss: 0.6716123782098293\n",
            "Epoch 8/10\n",
            "Main Loss: 0.6789219491183758\n",
            "Adversary Loss: 0.6783938761800528\n",
            "Epoch 9/10\n",
            "Main Loss: 0.6710894778370857\n",
            "Adversary Loss: 0.6708820331841707\n",
            "Epoch 10/10\n",
            "Main Loss: 0.6709917355328798\n",
            "Adversary Loss: 0.6705822758376598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Define the main model with more layers and Dropout\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MainModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 512)  # Input layer, assume input size 784 (e.g., 28x28 images)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 1)\n",
        "        self.dropout = nn.Dropout(0.5)  # Adding dropout to regularize\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)  # Flatten the input\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "# Define the adversary model (simple linear)\n",
        "class AdversaryModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdversaryModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "# Adjust training function to include learning rate scheduler, L2 regularization, and dropout\n",
        "def train_model(main_model, adversary_model, train_loader, num_epochs=100, lr_main=0.001, lr_adv=0.001, optimizer_type='adam'):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    main_model.to(device)\n",
        "    adversary_model.to(device)\n",
        "\n",
        "    # Choose optimizer (Adam or SGD)\n",
        "    if optimizer_type == 'adam':\n",
        "        main_optimizer = optim.Adam(main_model.parameters(), lr=lr_main, weight_decay=1e-4)  # L2 regularization via weight_decay\n",
        "        adv_optimizer = optim.Adam(adversary_model.parameters(), lr=lr_adv, weight_decay=1e-4)\n",
        "    elif optimizer_type == 'sgd':\n",
        "        main_optimizer = optim.SGD(main_model.parameters(), lr=lr_main, momentum=0.9, weight_decay=1e-4)\n",
        "        adv_optimizer = optim.SGD(adversary_model.parameters(), lr=lr_adv, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    # Learning Rate Scheduler\n",
        "    main_scheduler = StepLR(main_optimizer, step_size=30, gamma=0.1)\n",
        "    adv_scheduler = StepLR(adv_optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "    # Loss function (using BCEWithLogitsLoss for binary classification with class imbalance handling)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([2.0]).to(device))  # Adjusting pos_weight for class imbalance\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        main_model.train()\n",
        "        adversary_model.train()\n",
        "        running_loss_main = 0.0\n",
        "        running_loss_adv = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass through main model\n",
        "            main_optimizer.zero_grad()\n",
        "            outputs_main = main_model(inputs)\n",
        "            loss_main = criterion(outputs_main.squeeze(), labels.float())\n",
        "            loss_main.backward()\n",
        "            main_optimizer.step()\n",
        "\n",
        "            # Forward pass through adversary model\n",
        "            adv_optimizer.zero_grad()\n",
        "            outputs_adv = adversary_model(inputs)\n",
        "            loss_adv = criterion(outputs_adv.squeeze(), labels.float())\n",
        "            loss_adv.backward()\n",
        "            adv_optimizer.step()\n",
        "\n",
        "            running_loss_main += loss_main.item()\n",
        "            running_loss_adv += loss_adv.item()\n",
        "\n",
        "        # Print loss every epoch\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Main Loss: {running_loss_main/len(train_loader)}\")\n",
        "        print(f\"Adversary Loss: {running_loss_adv/len(train_loader)}\")\n",
        "\n",
        "        # Step the scheduler\n",
        "        main_scheduler.step()\n",
        "        adv_scheduler.step()\n",
        "\n",
        "# Dataset preparation (dummy data, replace with actual data)\n",
        "# For testing purposes\n",
        "from torch.utils.data import TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "X_train = np.random.randn(1000, 784).astype(np.float32)  # 1000 samples, 784 features (e.g., 28x28 images)\n",
        "y_train = np.random.randint(0, 2, 1000).astype(np.float32)  # 1000 labels, binary classification\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize models\n",
        "main_model = MainModel()\n",
        "adversary_model = AdversaryModel()\n",
        "\n",
        "# Train models with improvements\n",
        "train_model(main_model, adversary_model, train_loader, num_epochs=10, lr_main=0.001, lr_adv=0.001, optimizer_type='adam')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2XCeo3z4Yjy",
        "outputId": "aa694de0-20ff-4862-8df7-70d761bd0293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Main Loss: 0.9616876598447561\n",
            "Adversary Loss: 0.9605867024511099\n",
            "Epoch 2/10\n",
            "Main Loss: 0.9482017960399389\n",
            "Adversary Loss: 0.8265467435121536\n",
            "Epoch 3/10\n",
            "Main Loss: 0.8550773784518242\n",
            "Adversary Loss: 0.7120877783745527\n",
            "Epoch 4/10\n",
            "Main Loss: 0.7434312906116247\n",
            "Adversary Loss: 0.7096046321094036\n",
            "Epoch 5/10\n",
            "Main Loss: 0.750073155388236\n",
            "Adversary Loss: 0.7064590696245432\n",
            "Epoch 6/10\n",
            "Main Loss: 0.7534256596118212\n",
            "Adversary Loss: 0.7004389967769384\n",
            "Epoch 7/10\n",
            "Main Loss: 0.7367540877312422\n",
            "Adversary Loss: 0.7095443960279226\n",
            "Epoch 8/10\n",
            "Main Loss: 0.7319745775312185\n",
            "Adversary Loss: 0.710007457062602\n",
            "Epoch 9/10\n",
            "Main Loss: 0.7320457268506289\n",
            "Adversary Loss: 0.70590665563941\n",
            "Epoch 10/10\n",
            "Main Loss: 0.7280416339635849\n",
            "Adversary Loss: 0.6974890194833279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the model architectures\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MainModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 512)  # Assuming input features are 784 (like MNIST images)\n",
        "        self.fc2 = nn.Linear(512, 1)  # Output size 1 for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "class AdversaryModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdversaryModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 512)  # Assuming input features are 784\n",
        "        self.fc2 = nn.Linear(512, 1)  # Output size 1 for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(main_model, adversary_model, train_loader, num_epochs, lr_main, lr_adv, optimizer_type='adam'):\n",
        "    # Move models to device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    main_model = main_model.to(device)\n",
        "    adversary_model = adversary_model.to(device)\n",
        "\n",
        "    # Set the optimizer\n",
        "    if optimizer_type == 'adam':\n",
        "        optimizer_main = optim.Adam(main_model.parameters(), lr=lr_main)\n",
        "        optimizer_adv = optim.Adam(adversary_model.parameters(), lr=lr_adv)\n",
        "    elif optimizer_type == 'sgd':\n",
        "        optimizer_main = optim.SGD(main_model.parameters(), lr=lr_main)\n",
        "        optimizer_adv = optim.SGD(adversary_model.parameters(), lr=lr_adv)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer type: {optimizer_type}\")\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        main_model.train()\n",
        "        adversary_model.train()\n",
        "        running_loss_main = 0.0\n",
        "        running_loss_adv = 0.0\n",
        "\n",
        "        for data in train_loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Ensure the labels have the correct shape (32, 1) and correct data type (float32)\n",
        "            labels = labels.view(-1, 1).float()  # Reshape labels to be of shape (32, 1) and convert to float\n",
        "\n",
        "            # Forward pass through main model\n",
        "            optimizer_main.zero_grad()\n",
        "            outputs_main = main_model(inputs)\n",
        "            loss_main = criterion(outputs_main, labels)\n",
        "            loss_main.backward()\n",
        "            optimizer_main.step()\n",
        "\n",
        "            # Forward pass through adversary model\n",
        "            optimizer_adv.zero_grad()\n",
        "            outputs_adv = adversary_model(inputs)\n",
        "            loss_adv = criterion(outputs_adv, labels)\n",
        "            loss_adv.backward()\n",
        "            optimizer_adv.step()\n",
        "\n",
        "            running_loss_main += loss_main.item()\n",
        "            running_loss_adv += loss_adv.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Main Loss: {running_loss_main/len(train_loader)}, Adversary Loss: {running_loss_adv/len(train_loader)}\")\n",
        "\n",
        "# Function to run learning rate experiment\n",
        "def run_learning_rate_experiment(main_model, adversary_model, train_loader, num_epochs=10):\n",
        "    learning_rates = [0.001, 0.0001, 0.00001]  # List of learning rates to test\n",
        "    for lr in learning_rates:\n",
        "        print(f\"\\nTesting with lr_main={lr}, lr_adv={lr}\")\n",
        "        train_model(main_model, adversary_model, train_loader, num_epochs=num_epochs, lr_main=lr, lr_adv=lr, optimizer_type='adam')\n",
        "\n",
        "# Example data (replace with your actual dataset)\n",
        "X_train = torch.randn(1000, 784)  # 1000 samples with 784 features (e.g., 28x28 images flattened)\n",
        "y_train = torch.randint(0, 2, (1000,))  # Binary labels (0 or 1)\n",
        "\n",
        "# Create DataLoader\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize models\n",
        "main_model = MainModel()\n",
        "adversary_model = AdversaryModel()\n",
        "\n",
        "# Run learning rate experiment\n",
        "run_learning_rate_experiment(main_model, adversary_model, train_loader, num_epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm69UejJKKSC",
        "outputId": "73007d1a-edf3-4f45-fc37-8d40df112b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing with lr_main=0.001, lr_adv=0.001\n",
            "Epoch 1/10, Main Loss: 0.7294054143130779, Adversary Loss: 0.7266864106059074\n",
            "Epoch 2/10, Main Loss: 0.34954395424574614, Adversary Loss: 0.34782551508396864\n",
            "Epoch 3/10, Main Loss: 0.1485012632329017, Adversary Loss: 0.14701094687916338\n",
            "Epoch 4/10, Main Loss: 0.051853902521543205, Adversary Loss: 0.05008336156606674\n",
            "Epoch 5/10, Main Loss: 0.021264682232867926, Adversary Loss: 0.020779706770554185\n",
            "Epoch 6/10, Main Loss: 0.011535355035448447, Adversary Loss: 0.011363425146555528\n",
            "Epoch 7/10, Main Loss: 0.007579914992675185, Adversary Loss: 0.007494582765502855\n",
            "Epoch 8/10, Main Loss: 0.005525691318325698, Adversary Loss: 0.005467819471959956\n",
            "Epoch 9/10, Main Loss: 0.004279882115952205, Adversary Loss: 0.004235947235429194\n",
            "Epoch 10/10, Main Loss: 0.0034306457091588527, Adversary Loss: 0.003393252794921864\n",
            "\n",
            "Testing with lr_main=0.0001, lr_adv=0.0001\n",
            "Epoch 1/10, Main Loss: 0.0028273021162021905, Adversary Loss: 0.002797112647385802\n",
            "Epoch 2/10, Main Loss: 0.0017859140862128697, Adversary Loss: 0.0017672799294814467\n",
            "Epoch 3/10, Main Loss: 0.0012955964157299604, Adversary Loss: 0.0012826534366467968\n",
            "Epoch 4/10, Main Loss: 0.0009917045554175274, Adversary Loss: 0.0009837077432166552\n",
            "Epoch 5/10, Main Loss: 0.0007839951158530312, Adversary Loss: 0.000776387685618829\n",
            "Epoch 6/10, Main Loss: 0.0006307529420155333, Adversary Loss: 0.0006241506616788683\n",
            "Epoch 7/10, Main Loss: 0.0005115969424878131, Adversary Loss: 0.0005073770544186118\n",
            "Epoch 8/10, Main Loss: 0.0004288028371774999, Adversary Loss: 0.00042495296929701\n",
            "Epoch 9/10, Main Loss: 0.00037143299414310604, Adversary Loss: 0.0003673968030852848\n",
            "Epoch 10/10, Main Loss: 0.00031706155596111785, Adversary Loss: 0.0003140078379146871\n",
            "\n",
            "Testing with lr_main=1e-05, lr_adv=1e-05\n",
            "Epoch 1/10, Main Loss: 0.0002857429981304449, Adversary Loss: 0.0002835539207808324\n",
            "Epoch 2/10, Main Loss: 0.0002706065793063317, Adversary Loss: 0.00026725622001322336\n",
            "Epoch 3/10, Main Loss: 0.00025681343277028645, Adversary Loss: 0.0002544542071518663\n",
            "Epoch 4/10, Main Loss: 0.00024551696242269827, Adversary Loss: 0.00024324107835127506\n",
            "Epoch 5/10, Main Loss: 0.0002356418958697759, Adversary Loss: 0.00023361399235000135\n",
            "Epoch 6/10, Main Loss: 0.00022443529132942786, Adversary Loss: 0.00022242684826778714\n",
            "Epoch 7/10, Main Loss: 0.0002139594957952795, Adversary Loss: 0.00021200267474341672\n",
            "Epoch 8/10, Main Loss: 0.00020828436436204356, Adversary Loss: 0.0002064621853605786\n",
            "Epoch 9/10, Main Loss: 0.00019937701836170163, Adversary Loss: 0.00019769807249758742\n",
            "Epoch 10/10, Main Loss: 0.0001921156217576936, Adversary Loss: 0.00019043910288019106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmQzLIwDawYW",
        "outputId": "6a8c46e5-bb4c-408d-af7f-0e548d70704d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.2 colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Define your model (for example purposes)\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Example data\n",
        "input_size = 8  # Features\n",
        "num_classes = 10  # For multi-class classification\n",
        "\n",
        "# Dummy data (16 samples, each with 8 features)\n",
        "X_train = torch.randn(16, input_size)\n",
        "y_train = torch.randint(0, num_classes, (16,))  # Target labels (class indices)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Model\n",
        "main_model = SimpleModel(input_size=input_size, num_classes=num_classes)\n",
        "adversary_model = SimpleModel(input_size=input_size, num_classes=num_classes)\n",
        "\n",
        "# Define the loss function for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (Adam)\n",
        "optimizer = optim.Adam(main_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(main_model, adversary_model, train_loader, num_epochs=10, lr_main=0.001, lr_adv=0.001, optimizer_type='adam'):\n",
        "    for epoch in range(num_epochs):\n",
        "        main_model.train()\n",
        "        adversary_model.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass through main model\n",
        "            output = main_model(data)\n",
        "            main_loss = criterion(output, target)\n",
        "\n",
        "            # Forward pass through adversary model\n",
        "            adversary_output = adversary_model(data)\n",
        "            adversary_loss = criterion(adversary_output, target)\n",
        "\n",
        "            # Combine losses\n",
        "            total_loss = main_loss + adversary_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Main Loss: {main_loss.item()}, Adversary Loss: {adversary_loss.item()}\")\n",
        "\n",
        "# Run the training loop\n",
        "train_model(main_model, adversary_model, train_loader, num_epochs=10, lr_main=0.001, lr_adv=0.001, optimizer_type='adam')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0475WPUUVCgO",
        "outputId": "c4dfc8f3-da30-4804-a81c-54f469a4998e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Main Loss: 2.6833317279815674, Adversary Loss: 2.4014956951141357\n",
            "Epoch 2/10, Main Loss: 2.676924228668213, Adversary Loss: 2.4014956951141357\n",
            "Epoch 3/10, Main Loss: 2.6705286502838135, Adversary Loss: 2.401495933532715\n",
            "Epoch 4/10, Main Loss: 2.6641452312469482, Adversary Loss: 2.4014956951141357\n",
            "Epoch 5/10, Main Loss: 2.657775402069092, Adversary Loss: 2.4014956951141357\n",
            "Epoch 6/10, Main Loss: 2.6514179706573486, Adversary Loss: 2.401495933532715\n",
            "Epoch 7/10, Main Loss: 2.6450746059417725, Adversary Loss: 2.401495933532715\n",
            "Epoch 8/10, Main Loss: 2.638744354248047, Adversary Loss: 2.4014956951141357\n",
            "Epoch 9/10, Main Loss: 2.632427453994751, Adversary Loss: 2.4014956951141357\n",
            "Epoch 10/10, Main Loss: 2.6261258125305176, Adversary Loss: 2.4014956951141357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Define your model (for example purposes)\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Example data\n",
        "input_size = 8  # Features\n",
        "num_classes = 10  # For multi-class classification\n",
        "\n",
        "# Dummy data (16 samples, each with 8 features)\n",
        "X_train = torch.randn(16, input_size)\n",
        "y_train = torch.randint(0, num_classes, (16,))  # Target labels (class indices)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Model\n",
        "main_model = SimpleModel(input_size=input_size, num_classes=num_classes)\n",
        "adversary_model = SimpleModel(input_size=input_size, num_classes=num_classes)\n",
        "\n",
        "# Define the loss function for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (Adam)\n",
        "optimizer = optim.Adam(list(main_model.parameters()) + list(adversary_model.parameters()), lr=0.001)\n",
        "\n",
        "# Training loop with gradient check\n",
        "def train_model(main_model, adversary_model, train_loader, num_epochs=10, lr_main=0.001, lr_adv=0.001, optimizer_type='adam'):\n",
        "    for epoch in range(num_epochs):\n",
        "        main_model.train()\n",
        "        adversary_model.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass through main model\n",
        "            output = main_model(data)\n",
        "            main_loss = criterion(output, target)\n",
        "\n",
        "            # Forward pass through adversary model\n",
        "            adversary_output = adversary_model(data)\n",
        "            adversary_loss = criterion(adversary_output, target)\n",
        "\n",
        "            # Combine losses\n",
        "            total_loss = main_loss + adversary_loss\n",
        "\n",
        "            # Backward pass\n",
        "            total_loss.backward()\n",
        "\n",
        "            # Check gradients\n",
        "            for name, param in adversary_model.named_parameters():\n",
        "                if param.grad is not None:\n",
        "                    print(f\"Gradients for {name}: {param.grad.mean()}\")\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print loss every 10 batches\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Main Loss: {main_loss.item()}, Adversary Loss: {adversary_loss.item()}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Main Loss: {main_loss.item()}, Adversary Loss: {adversary_loss.item()}\")\n",
        "\n",
        "# Run the training loop with gradient checking\n",
        "train_model(main_model, adversary_model, train_loader, num_epochs=10, lr_main=0.001, lr_adv=0.001, optimizer_type='adam')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXu21xtzaql5",
        "outputId": "828decfd-6e49-407d-c1b6-047ea22c768c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients for fc.weight: 1.6298144778215118e-10\n",
            "Gradients for fc.bias: 7.450580707946131e-10\n",
            "Epoch 1/10, Main Loss: 2.4936845302581787, Adversary Loss: 2.515117883682251\n",
            "Epoch 1/10, Main Loss: 2.4936845302581787, Adversary Loss: 2.515117883682251\n",
            "Gradients for fc.weight: 1.7462298274040222e-10\n",
            "Gradients for fc.bias: -1.4901161415892261e-09\n",
            "Epoch 2/10, Main Loss: 2.48751163482666, Adversary Loss: 2.5089244842529297\n",
            "Epoch 2/10, Main Loss: 2.48751163482666, Adversary Loss: 2.5089244842529297\n",
            "Gradients for fc.weight: -1.6298144778215118e-10\n",
            "Gradients for fc.bias: 1.4901161415892261e-09\n",
            "Epoch 3/10, Main Loss: 2.4813544750213623, Adversary Loss: 2.502747058868408\n",
            "Epoch 3/10, Main Loss: 2.4813544750213623, Adversary Loss: 2.502747058868408\n",
            "Gradients for fc.weight: -1.0477378686868377e-10\n",
            "Gradients for fc.bias: 0.0\n",
            "Epoch 4/10, Main Loss: 2.4752132892608643, Adversary Loss: 2.4965856075286865\n",
            "Epoch 4/10, Main Loss: 2.4752132892608643, Adversary Loss: 2.4965856075286865\n",
            "Gradients for fc.weight: 2.9103830456733704e-10\n",
            "Gradients for fc.bias: 2.2351742678949904e-09\n",
            "Epoch 5/10, Main Loss: 2.4690890312194824, Adversary Loss: 2.4904401302337646\n",
            "Epoch 5/10, Main Loss: 2.4690890312194824, Adversary Loss: 2.4904401302337646\n",
            "Gradients for fc.weight: -1.6996637430821693e-09\n",
            "Gradients for fc.bias: -1.4901161415892261e-09\n",
            "Epoch 6/10, Main Loss: 2.4629814624786377, Adversary Loss: 2.484311580657959\n",
            "Epoch 6/10, Main Loss: 2.4629814624786377, Adversary Loss: 2.484311580657959\n",
            "Gradients for fc.weight: -9.196810535350153e-10\n",
            "Gradients for fc.bias: 1.4901161415892261e-09\n",
            "Epoch 7/10, Main Loss: 2.4568912982940674, Adversary Loss: 2.4781992435455322\n",
            "Epoch 7/10, Main Loss: 2.4568912982940674, Adversary Loss: 2.4781992435455322\n",
            "Gradients for fc.weight: -8.847564347824743e-10\n",
            "Gradients for fc.bias: 1.4901161415892261e-09\n",
            "Epoch 8/10, Main Loss: 2.4508185386657715, Adversary Loss: 2.472104072570801\n",
            "Epoch 8/10, Main Loss: 2.4508185386657715, Adversary Loss: 2.472104072570801\n",
            "Gradients for fc.weight: 1.1757947948609626e-09\n",
            "Gradients for fc.bias: -1.4901161415892261e-09\n",
            "Epoch 9/10, Main Loss: 2.4447638988494873, Adversary Loss: 2.4660260677337646\n",
            "Epoch 9/10, Main Loss: 2.4447638988494873, Adversary Loss: 2.4660260677337646\n",
            "Gradients for fc.weight: 1.4668330994282996e-09\n",
            "Gradients for fc.bias: 4.470348535789981e-09\n",
            "Epoch 10/10, Main Loss: 2.4387266635894775, Adversary Loss: 2.459965229034424\n",
            "Epoch 10/10, Main Loss: 2.4387266635894775, Adversary Loss: 2.459965229034424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create a text file to save all the outputs (again, appending this time)\n",
        "log_file = open(\"portfolio_optimization_results.txt\", \"a\")\n",
        "\n",
        "# Function to log output to the text file\n",
        "def log_output(output):\n",
        "    log_file.write(output + \"\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# Portfolio Optimization with Simple Gradient Descent\n",
        "# -------------------------------\n",
        "\n",
        "def optimize_weights(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, lr=1e-2, epochs=500):\n",
        "    \"\"\"\n",
        "    Simple gradient descent for portfolio optimization\n",
        "    - Sigma: Covariance matrix\n",
        "    - mu: Expected returns\n",
        "    - lam: Risk-return tradeoff parameter\n",
        "    - gamma: Sparsity regularization parameter\n",
        "    - eta: Magnitude regularization parameter\n",
        "    - lr: Learning rate\n",
        "    - epochs: Number of iterations\n",
        "    \"\"\"\n",
        "    w = np.ones(Sigma.shape[0]) / Sigma.shape[0]  # Equal distribution of weights\n",
        "\n",
        "    log_output(f\"Initial weights: {w}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        grad = grad_objective(w, Sigma, mu, lam, eta)\n",
        "        w -= lr * grad\n",
        "\n",
        "        # Apply Proximal Operator for L0 (sparsity)\n",
        "        w[np.abs(w) < gamma] = 0\n",
        "\n",
        "        # Ensure non-negative weights and sum to 1\n",
        "        w = np.maximum(w, 0)\n",
        "        w /= np.sum(w + 1e-8)\n",
        "\n",
        "        if epoch % 50 == 0:  # Log every 50th iteration\n",
        "            log_output(f\"Epoch {epoch}: Weights: {w}\")\n",
        "\n",
        "    return w\n",
        "\n",
        "# -------------------------------\n",
        "# Gradient of the Objective Function\n",
        "# -------------------------------\n",
        "\n",
        "def grad_objective(w, Sigma, mu, lam, eta):\n",
        "    \"\"\"\n",
        "    Gradient of the risk-return objective function with L0 and L1 penalties\n",
        "    \"\"\"\n",
        "    grad_risk = 2 * Sigma @ w  # Risk (variance)\n",
        "    grad_ret = lam * mu  # Return (expected return)\n",
        "    grad_l1 = eta * np.sign(w)  # L1 penalty for magnitude regularization\n",
        "\n",
        "    log_output(f\"Gradient risk: {grad_risk}, Gradient return: {grad_ret}, Gradient L1: {grad_l1}\")\n",
        "\n",
        "    return grad_risk - grad_ret + grad_l1\n",
        "\n",
        "# -------------------------------\n",
        "# Stress Testing\n",
        "# -------------------------------\n",
        "\n",
        "def extreme_market_scenarios(Sigma, mu, scenarios):\n",
        "    \"\"\"\n",
        "    Perform stress tests by modifying Sigma and mu to simulate market scenarios.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for scenario in scenarios:\n",
        "        stressed_Sigma = Sigma + scenario['sigma_change']\n",
        "        stressed_mu = mu + scenario['mu_change']\n",
        "        optimized_weights = optimize_weights(stressed_Sigma, stressed_mu)\n",
        "        results[scenario['name']] = optimized_weights\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------------------\n",
        "# Simulate Fat-Tailed Returns (Student's T-distribution)\n",
        "# -------------------------------\n",
        "\n",
        "def simulate_fat_tailed_returns(Sigma, mu, dof=4, n_samples=1000):\n",
        "    \"\"\"\n",
        "    Simulate returns from a fat-tailed distribution (Student's t-distribution).\n",
        "    \"\"\"\n",
        "    t_returns = np.random.standard_t(dof, size=(n_samples, len(mu)))\n",
        "    fat_tailed_returns = np.dot(t_returns, Sigma) + mu\n",
        "    return fat_tailed_returns\n",
        "\n",
        "# -------------------------------\n",
        "# Training function with AdamW optimizer and class weights\n",
        "# -------------------------------\n",
        "\n",
        "def train_model(main_model, adversary_model, train_loader, num_epochs=10, lr_main=0.01, lr_adv=0.01, optimizer_type='adamw'):\n",
        "    main_optimizer = optim.AdamW(main_model.parameters(), lr=lr_main)\n",
        "    adversary_optimizer = optim.AdamW(adversary_model.parameters(), lr=lr_adv)\n",
        "\n",
        "    # Loss function (binary cross entropy with class weights)\n",
        "    class_weights = torch.tensor([1.0, 2.0]).to(device)  # Adjust class weights\n",
        "    criterion = nn.BCEWithLogitsLoss(weight=class_weights)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        main_model.train()\n",
        "        adversary_model.train()\n",
        "\n",
        "        main_loss_total = 0\n",
        "        adversary_loss_total = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Ensure target is the correct shape for binary classification\n",
        "            if target.dim() == 1:  # If target has shape (batch_size,)\n",
        "                target = target.unsqueeze(1)  # Convert it to shape (batch_size, 1)\n",
        "\n",
        "            target = target.float()  # Ensure target is of float type for BCEWithLogitsLoss\n",
        "\n",
        "            print(f\"Batch {batch_idx+1}/{len(train_loader)} - Data shape: {data.shape}, Target shape: {target.shape}\")\n",
        "\n",
        "            # Main Model Training\n",
        "            main_optimizer.zero_grad()\n",
        "            output_main = main_model(data)\n",
        "            print(f\"Main model output shape: {output_main.shape}\")\n",
        "\n",
        "            print(f\"Before Loss Calculation: Output shape: {output_main.squeeze().shape}, Target shape: {target.squeeze().shape}\")\n",
        "\n",
        "            main_loss = criterion(output_main.squeeze(), target.squeeze())\n",
        "            print(f\"Main loss: {main_loss.item()}\")\n",
        "            main_loss.backward()\n",
        "            main_optimizer.step()\n",
        "            main_loss_total += main_loss.item()\n",
        "\n",
        "            # Adversary Model Training\n",
        "            adversary_optimizer.zero_grad()\n",
        "            output_adv = adversary_model(data)\n",
        "            print(f\"Adversary model output shape: {output_adv.shape}\")\n",
        "\n",
        "            adversary_loss = criterion(output_adv.squeeze(), target.squeeze())\n",
        "            adversary_loss.backward()\n",
        "            adversary_optimizer.step()\n",
        "            adversary_loss_total += adversary_loss.item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Main Loss: {main_loss.item()}, Adversary Loss: {adversary_loss.item()}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Main Loss: {main_loss_total/len(train_loader)}, Adversary Loss: {adversary_loss_total/len(train_loader)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Running the Optimization and Stress Tests\n",
        "# -------------------------------\n",
        "\n",
        "# Example Data (Covariance matrix and expected returns)\n",
        "Sigma = np.array([[0.02, 0.015, -0.005], [0.015, 0.03, -0.008], [-0.005, -0.008, 0.01]])\n",
        "mu = np.array([0.01, 0.005, -0.002])  # Expected returns for 3 assets\n",
        "\n",
        "# Run the Portfolio Optimization\n",
        "optimized_weights = optimize_weights(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2)\n",
        "log_output(f\"Optimized Weights: {optimized_weights}\")\n",
        "\n",
        "# Stress Test Scenarios (market conditions)\n",
        "extreme_scenarios = [\n",
        "    {'name': 'Black Swan', 'sigma_change': 0.1 * np.identity(3), 'mu_change': -0.05 * np.ones(3)},\n",
        "    {'name': 'Liquidity Crisis', 'sigma_change': 0.2 * np.identity(3), 'mu_change': 0.0 * np.ones(3)},\n",
        "    {'name': 'Interest Rate Shock', 'sigma_change': 0.05 * np.identity(3), 'mu_change': 0.02 * np.ones(3)}\n",
        "]\n",
        "\n",
        "# Perform Stress Tests\n",
        "stress_test_results = extreme_market_scenarios(Sigma, mu, extreme_scenarios)\n",
        "log_output(f\"Stress Test Results: {stress_test_results}\")\n",
        "\n",
        "# Simulate Fat-Tailed Returns\n",
        "fat_tailed_returns = simulate_fat_tailed_returns(Sigma, mu)\n",
        "log_output(f\"Fat-Tailed Returns Simulation (First 5 Samples): {fat_tailed_returns[:5]}\")\n",
        "\n",
        "# Close the log file after writing all results\n",
        "log_file.close()\n"
      ],
      "metadata": {
        "id": "TpDe9um4bX0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsYtvsuLmwbt",
        "outputId": "efea5748-d0a9-44cd-aebe-3e5a566faee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deap\n",
            "  Downloading deap-1.4.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deap) (2.0.2)\n",
            "Downloading deap-1.4.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deap\n",
            "Successfully installed deap-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from deap import base, creator, tools, algorithms\n",
        "import random\n",
        "\n",
        "# Create a text file to save all the outputs (appending this time)\n",
        "log_file = open(\"portfolio_optimization_results.txt\", \"a\")\n",
        "\n",
        "# Function to log output to the text file\n",
        "def log_output(output):\n",
        "    log_file.write(output + \"\\n\")\n",
        "    print(output)  # Also print to the console for real-time monitoring\n",
        "\n",
        "# -------------------------------\n",
        "# Portfolio Optimization with Simple Gradient Descent\n",
        "# -------------------------------\n",
        "\n",
        "def optimize_weights(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, lr=1e-2, epochs=500):\n",
        "    \"\"\"\n",
        "    Simple gradient descent for portfolio optimization\n",
        "    - Sigma: Covariance matrix\n",
        "    - mu: Expected returns\n",
        "    - lam: Risk-return tradeoff parameter\n",
        "    - gamma: Sparsity regularization parameter\n",
        "    - eta: Magnitude regularization parameter\n",
        "    - lr: Learning rate\n",
        "    - epochs: Number of iterations\n",
        "    \"\"\"\n",
        "    w = np.ones(Sigma.shape[0]) / Sigma.shape[0]  # Equal distribution of weights\n",
        "\n",
        "    log_output(f\"Initial weights: {w}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        grad = grad_objective(w, Sigma, mu, lam, eta)\n",
        "        w -= lr * grad\n",
        "\n",
        "        # Apply Proximal Operator for L0 (sparsity)\n",
        "        w[np.abs(w) < gamma] = 0\n",
        "\n",
        "        # Ensure non-negative weights and sum to 1\n",
        "        w = np.maximum(w, 0)\n",
        "        w /= np.sum(w + 1e-8)\n",
        "\n",
        "        if epoch % 50 == 0:  # Log every 50th iteration\n",
        "            log_output(f\"Epoch {epoch}: Weights: {w}\")\n",
        "\n",
        "    return w\n",
        "\n",
        "# -------------------------------\n",
        "# Gradient of the Objective Function\n",
        "# -------------------------------\n",
        "\n",
        "def grad_objective(w, Sigma, mu, lam, eta):\n",
        "    \"\"\"\n",
        "    Gradient of the risk-return objective function with L0 and L1 penalties\n",
        "    \"\"\"\n",
        "    grad_risk = 2 * Sigma @ w  # Risk (variance)\n",
        "    grad_ret = lam * mu  # Return (expected return)\n",
        "    grad_l1 = eta * np.sign(w)  # L1 penalty for magnitude regularization\n",
        "\n",
        "    log_output(f\"Gradient risk: {grad_risk}, Gradient return: {grad_ret}, Gradient L1: {grad_l1}\")\n",
        "\n",
        "    return grad_risk - grad_ret + grad_l1\n",
        "\n",
        "# -------------------------------\n",
        "# Stress Testing\n",
        "# -------------------------------\n",
        "\n",
        "def extreme_market_scenarios(Sigma, mu, scenarios):\n",
        "    \"\"\"\n",
        "    Perform stress tests by modifying Sigma and mu to simulate market scenarios.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for scenario in scenarios:\n",
        "        stressed_Sigma = Sigma + scenario['sigma_change']\n",
        "        stressed_mu = mu + scenario['mu_change']\n",
        "        optimized_weights = optimize_weights(stressed_Sigma, stressed_mu)\n",
        "        results[scenario['name']] = optimized_weights\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------------------\n",
        "# Simulate Fat-Tailed Returns (Student's T-distribution)\n",
        "# -------------------------------\n",
        "\n",
        "def simulate_fat_tailed_returns(Sigma, mu, dof=4, n_samples=1000):\n",
        "    \"\"\"\n",
        "    Simulate returns from a fat-tailed distribution (Student's t-distribution).\n",
        "    \"\"\"\n",
        "    t_returns = np.random.standard_t(dof, size=(n_samples, len(mu)))\n",
        "    fat_tailed_returns = np.dot(t_returns, Sigma) + mu\n",
        "    return fat_tailed_returns\n",
        "\n",
        "# -------------------------------\n",
        "# Hyperparameter Tuning (Grid Search)\n",
        "# -------------------------------\n",
        "\n",
        "def hyperparameter_tuning():\n",
        "    param_grid = {\n",
        "        'gamma': [1e-4, 1e-3, 1e-2],\n",
        "        'eta': [1e-4, 1e-3, 1e-2],\n",
        "    }\n",
        "    grid_search = GridSearchCV(estimator=PortfolioOptimizer(), param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    log_output(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Genetic Algorithm for Optimization\n",
        "# -------------------------------\n",
        "\n",
        "def genetic_algorithm_optimizer(Sigma, mu):\n",
        "    # Genetic algorithm code here\n",
        "    # Create the individual structure and evaluate fitness\n",
        "\n",
        "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "    toolbox = base.Toolbox()\n",
        "    toolbox.register(\"attr_float\", random.random)\n",
        "    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=len(Sigma))\n",
        "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "    toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
        "    toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
        "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "    toolbox.register(\"evaluate\", lambda w: objective(w, Sigma, mu, 0.1, 1e-2, 1e-2))\n",
        "\n",
        "    population = toolbox.population(n=10)\n",
        "    # Run the genetic algorithm steps\n",
        "    # Apply crossover, mutation, and selection\n",
        "\n",
        "# -------------------------------\n",
        "# Monte Carlo Simulation for Large Portfolio (with Adam optimizer)\n",
        "# -------------------------------\n",
        "\n",
        "def monte_carlo_testing_large(Sigma, mu, n_samples=1000, n_assets=1000, lr=1e-3, epochs=500):\n",
        "    w_init = np.ones(n_assets) / n_assets\n",
        "    for sample in range(n_samples):\n",
        "        # Perform Monte Carlo simulation and optimization for large portfolio\n",
        "        pass  # Implement Monte Carlo logic\n",
        "\n",
        "    return w_init\n",
        "\n",
        "# -------------------------------\n",
        "# Running the Optimization and Stress Tests\n",
        "# -------------------------------\n",
        "\n",
        "# Example Data (Covariance matrix and expected returns)\n",
        "Sigma = np.array([[0.02, 0.015, -0.005], [0.015, 0.03, -0.008], [-0.005, -0.008, 0.01]])\n",
        "mu = np.array([0.01, 0.005, -0.002])  # Expected returns for 3 assets\n",
        "\n",
        "# Run the Portfolio Optimization\n",
        "optimized_weights = optimize_weights(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2)\n",
        "\n",
        "log_output(f\"Optimized Weights: {optimized_weights}\")\n",
        "\n",
        "# Stress Test Scenarios (market conditions)\n",
        "extreme_scenarios = [\n",
        "    {'name': 'Black Swan', 'sigma_change': 0.3 * np.identity(3), 'mu_change': -0.1 * np.ones(3)},\n",
        "    {'name': 'Liquidity Crisis', 'sigma_change': 0.2 * np.identity(3), 'mu_change': 0.0 * np.ones(3)},\n",
        "    {'name': 'Interest Rate Shock', 'sigma_change': 0.05 * np.identity(3), 'mu_change': 0.02 * np.ones(3)}\n",
        "]\n",
        "\n",
        "# Perform Stress Tests\n",
        "stress_test_results = extreme_market_scenarios(Sigma, mu, extreme_scenarios)\n",
        "\n",
        "log_output(f\"Stress Test Results: {stress_test_results}\")\n",
        "\n",
        "# Simulate Fat-Tailed Returns\n",
        "fat_tailed_returns = simulate_fat_tailed_returns(Sigma, mu)\n",
        "\n",
        "log_output(f\"Fat-Tailed Returns Simulation (First 5 Samples): {fat_tailed_returns[:5]}\")\n",
        "\n",
        "# Close the log file after writing all results\n",
        "log_file.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpEWCrWIb9AO",
        "outputId": "ffacab55-4e4a-4cac-c4a6-ed3731f1165a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Gradient risk: [ 0.02        0.02466667 -0.002     ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 0: Weights: [0.33328118 0.33322947 0.33348932]\n",
            "Gradient risk: [ 0.01999324  0.02465637 -0.0019947 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01998648  0.02464608 -0.00198939], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01997971  0.02463579 -0.00198409], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01997295  0.02462549 -0.00197878], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01996618  0.0246152  -0.00197348], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01995942  0.0246049  -0.00196817], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01995265  0.0245946  -0.00196286], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01994588  0.0245843  -0.00195756], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01993912  0.024574   -0.00195225], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01993235  0.0245637  -0.00194694], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01992558  0.02455339 -0.00194163], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01991881  0.02454309 -0.00193632], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01991203  0.02453278 -0.00193101], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01990526  0.02452248 -0.0019257 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01989849  0.02451217 -0.00192039], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01989171  0.02450186 -0.00191507], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01988494  0.02449155 -0.00190976], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01987816  0.02448124 -0.00190445], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01987139  0.02447092 -0.00189913], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01986461  0.02446061 -0.00189382], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01985783  0.0244503  -0.0018885 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01985105  0.02443998 -0.00188319], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01984427  0.02442966 -0.00187787], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01983749  0.02441934 -0.00187255], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01983071  0.02440903 -0.00186723], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01982393  0.0243987  -0.00186192], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01981714  0.02438838 -0.0018566 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01981036  0.02437806 -0.00185128], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01980358  0.02436774 -0.00184596], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01979679  0.02435741 -0.00184063], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01979     0.02434708 -0.00183531], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01978322  0.02433676 -0.00182999], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01977643  0.02432643 -0.00182467], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01976964  0.0243161  -0.00181934], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01976285  0.02430577 -0.00181402], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01975606  0.02429544 -0.0018087 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01974927  0.0242851  -0.00180337], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01974248  0.02427477 -0.00179804], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01973568  0.02426443 -0.00179272], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01972889  0.0242541  -0.00178739], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0197221   0.02424376 -0.00178206], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0197153   0.02423342 -0.00177674], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0197085   0.02422308 -0.00177141], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01970171  0.02421274 -0.00176608], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01969491  0.0242024  -0.00176075], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01968811  0.02419206 -0.00175542], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01968131  0.02418171 -0.00175009], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01967451  0.02417137 -0.00174475], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01966771  0.02416102 -0.00173942], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01966091  0.02415068 -0.00173409], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 50: Weights: [0.33066279 0.32802419 0.34131299]\n",
            "Gradient risk: [ 0.01965411  0.02414033 -0.00172876], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0196473   0.02412998 -0.00172342], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0196405   0.02411963 -0.00171809], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0196337   0.02410928 -0.00171275], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01962689  0.02409892 -0.00170742], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01962008  0.02408857 -0.00170208], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01961328  0.02407822 -0.00169674], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01960647  0.02406786 -0.00169141], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01959966  0.0240575  -0.00168607], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01959285  0.02404715 -0.00168073], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01958604  0.02403679 -0.00167539], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01957923  0.02402643 -0.00167005], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01957242  0.02401607 -0.00166471], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01956561  0.0240057  -0.00165937], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01955879  0.02399534 -0.00165403], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01955198  0.02398498 -0.00164869], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01954517  0.02397461 -0.00164334], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01953835  0.02396425 -0.001638  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01953153  0.02395388 -0.00163266], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01952472  0.02394351 -0.00162731], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0195179   0.02393314 -0.00162197], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01951108  0.02392277 -0.00161662], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01950426  0.0239124  -0.00161128], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01949744  0.02390203 -0.00160593], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01949062  0.02389165 -0.00160058], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0194838   0.02388128 -0.00159523], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01947698  0.0238709  -0.00158989], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01947016  0.02386053 -0.00158454], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01946333  0.02385015 -0.00157919], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01945651  0.02383977 -0.00157384], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01944968  0.02382939 -0.00156849], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01944286  0.02381901 -0.00156314], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01943603  0.02380863 -0.00155779], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0194292   0.02379825 -0.00155244], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01942238  0.02378786 -0.00154708], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01941555  0.02377748 -0.00154173], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01940872  0.0237671  -0.00153638], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01940189  0.02375671 -0.00153102], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01939506  0.02374632 -0.00152567], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01938823  0.02373593 -0.00152031], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01938139  0.02372554 -0.00151496], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01937456  0.02371515 -0.0015096 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01936773  0.02370476 -0.00150424], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01936089  0.02369437 -0.00149889], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01935406  0.02368398 -0.00149353], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01934722  0.02367358 -0.00148817], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01934039  0.02366319 -0.00148281], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01933355  0.02365279 -0.00147745], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01932671  0.02364239 -0.00147209], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01931987  0.023632   -0.00146673], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 100: Weights: [0.32802376 0.3227961  0.34918011]\n",
            "Gradient risk: [ 0.01931303  0.0236216  -0.00146137], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01930619  0.0236112  -0.00145601], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01929935  0.0236008  -0.00145065], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01929251  0.02359039 -0.00144529], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01928567  0.02357999 -0.00143992], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01927883  0.02356959 -0.00143456], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01927198  0.02355918 -0.0014292 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01926514  0.02354878 -0.00142383], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01925829  0.02353837 -0.00141847], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01925145  0.02352796 -0.0014131 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0192446   0.02351756 -0.00140774], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01923775  0.02350715 -0.00140237], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01923091  0.02349674 -0.001397  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01922406  0.02348632 -0.00139164], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01921721  0.02347591 -0.00138627], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01921036  0.0234655  -0.0013809 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01920351  0.02345509 -0.00137553], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01919666  0.02344467 -0.00137016], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01918981  0.02343425 -0.00136479], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01918295  0.02342384 -0.00135942], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0191761   0.02341342 -0.00135405], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01916925  0.023403   -0.00134868], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01916239  0.02339258 -0.00134331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01915554  0.02338216 -0.00133794], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01914868  0.02337174 -0.00133256], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01914182  0.02336132 -0.00132719], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01913497  0.0233509  -0.00132182], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01912811  0.02334047 -0.00131644], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01912125  0.02333005 -0.00131107], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01911439  0.02331962 -0.00130569], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01910753  0.02330919 -0.00130032], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01910067  0.02329877 -0.00129494], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01909381  0.02328834 -0.00128956], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01908695  0.02327791 -0.00128419], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01908009  0.02326748 -0.00127881], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01907322  0.02325705 -0.00127343], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01906636  0.02324662 -0.00126805], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0190595   0.02323618 -0.00126267], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01905263  0.02322575 -0.00125729], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01904577  0.02321532 -0.00125191], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0190389   0.02320488 -0.00124653], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01903203  0.02319444 -0.00124115], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01902516  0.02318401 -0.00123577], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0190183   0.02317357 -0.00123039], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01901143  0.02316313 -0.00122501], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01900456  0.02315269 -0.00121962], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01899769  0.02314225 -0.00121424], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01899082  0.02313181 -0.00120886], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01898395  0.02312137 -0.00120347], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01897707  0.02311092 -0.00119809], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 150: Weights: [0.32536536 0.31754834 0.35708627]\n",
            "Gradient risk: [ 0.0189702   0.02310048 -0.0011927 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01896333  0.02309004 -0.00118732], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01895645  0.02307959 -0.00118193], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01894958  0.02306914 -0.00117654], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01894271  0.0230587  -0.00117116], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01893583  0.02304825 -0.00116577], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01892895  0.0230378  -0.00116038], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01892208  0.02302735 -0.00115499], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0189152  0.0230169 -0.0011496], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01890832  0.02300645 -0.00114422], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01890144  0.022996   -0.00113883], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01889456  0.02298554 -0.00113344], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01888768  0.02297509 -0.00112805], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0188808   0.02296464 -0.00112265], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01887392  0.02295418 -0.00111726], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01886704  0.02294373 -0.00111187], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01886016  0.02293327 -0.00110648], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01885327  0.02292281 -0.00110109], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01884639  0.02291235 -0.00109569], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01883951  0.02290189 -0.0010903 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01883262  0.02289143 -0.00108491], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01882574  0.02288097 -0.00107951], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01881885  0.02287051 -0.00107412], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01881196  0.02286005 -0.00106872], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01880508  0.02284959 -0.00106333], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01879819  0.02283912 -0.00105793], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0187913   0.02282866 -0.00105253], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01878441  0.02281819 -0.00104714], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01877752  0.02280773 -0.00104174], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01877063  0.02279726 -0.00103634], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01876374  0.02278679 -0.00103094], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01875685  0.02277632 -0.00102555], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01874996  0.02276585 -0.00102015], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01874307  0.02275538 -0.00101475], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01873617  0.02274491 -0.00100935], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01872928  0.02273444 -0.00100395], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01872239  0.02272397 -0.00099855], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01871549  0.0227135  -0.00099315], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0187086   0.02270302 -0.00098774], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0187017   0.02269255 -0.00098234], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0186948   0.02268207 -0.00097694], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01868791  0.0226716  -0.00097154], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01868101  0.02266112 -0.00096613], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01867411  0.02265064 -0.00096073], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01866721  0.02264016 -0.00095533], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01866031  0.02262969 -0.00094992], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01865341  0.02261921 -0.00094452], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01864651  0.02260873 -0.00093911], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01863961  0.02259824 -0.00093371], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01863271  0.02258776 -0.0009283 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 200: Weights: [0.32268892 0.31228408 0.36502697]\n",
            "Gradient risk: [ 0.01862581  0.02257728 -0.00092289], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01861891  0.0225668  -0.00091749], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.018612    0.02255631 -0.00091208], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0186051   0.02254583 -0.00090667], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0185982   0.02253534 -0.00090127], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01859129  0.02252486 -0.00089586], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01858439  0.02251437 -0.00089045], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01857748  0.02250388 -0.00088504], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01857057  0.0224934  -0.00087963], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01856367  0.02248291 -0.00087422], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01855676  0.02247242 -0.00086881], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01854985  0.02246193 -0.0008634 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01854294  0.02245144 -0.00085799], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01853604  0.02244095 -0.00085258], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01852913  0.02243045 -0.00084717], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01852222  0.02241996 -0.00084175], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01851531  0.02240947 -0.00083634], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01850839  0.02239897 -0.00083093], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01850148  0.02238848 -0.00082552], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01849457  0.02237798 -0.0008201 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01848766  0.02236749 -0.00081469], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01848075  0.02235699 -0.00080928], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01847383  0.02234649 -0.00080386], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01846692  0.02233599 -0.00079845], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01846     0.02232549 -0.00079303], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01845309  0.022315   -0.00078761], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01844617  0.0223045  -0.0007822 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01843926  0.02229399 -0.00077678], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01843234  0.02228349 -0.00077137], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01842542  0.02227299 -0.00076595], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01841851  0.02226249 -0.00076053], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01841159  0.02225198 -0.00075511], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01840467  0.02224148 -0.0007497 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01839775  0.02223098 -0.00074428], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01839083  0.02222047 -0.00073886], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01838391  0.02220997 -0.00073344], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01837699  0.02219946 -0.00072802], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01837007  0.02218895 -0.0007226 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01836315  0.02217844 -0.00071718], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01835623  0.02216794 -0.00071176], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0183493   0.02215743 -0.00070634], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01834238  0.02214692 -0.00070092], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01833546  0.02213641 -0.00069549], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01832853  0.0221259  -0.00069007], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01832161  0.02211539 -0.00068465], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01831468  0.02210487 -0.00067923], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01830776  0.02209436 -0.00067381], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01830083  0.02208385 -0.00066838], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01829391  0.02207333 -0.00066296], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01828698  0.02206282 -0.00065753], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 250: Weights: [0.31999578 0.30700655 0.37299764]\n",
            "Gradient risk: [ 0.01828005  0.0220523  -0.00065211], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01827312  0.02204179 -0.00064669], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0182662   0.02203127 -0.00064126], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01825927  0.02202076 -0.00063584], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01825234  0.02201024 -0.00063041], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01824541  0.02199972 -0.00062498], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01823848  0.0219892  -0.00061956], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01823155  0.02197868 -0.00061413], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01822462  0.02196816 -0.0006087 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01821769  0.02195764 -0.00060328], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01821075  0.02194712 -0.00059785], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01820382  0.0219366  -0.00059242], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01819689  0.02192608 -0.00058699], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01818996  0.02191556 -0.00058157], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01818302  0.02190504 -0.00057614], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01817609  0.02189451 -0.00057071], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01816915  0.02188399 -0.00056528], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01816222  0.02187346 -0.00055985], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01815528  0.02186294 -0.00055442], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01814835  0.02185241 -0.00054899], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01814141  0.02184189 -0.00054356], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01813447  0.02183136 -0.00053813], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01812754  0.02182083 -0.0005327 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0181206   0.0218103  -0.00052726], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01811366  0.02179978 -0.00052183], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01810672  0.02178925 -0.0005164 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01809978  0.02177872 -0.00051097], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01809284  0.02176819 -0.00050554], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01808591  0.02175766 -0.0005001 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01807896  0.02174713 -0.00049467], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01807202  0.0217366  -0.00048924], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01806508  0.02172606 -0.0004838 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01805814  0.02171553 -0.00047837], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0180512   0.021705   -0.00047293], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01804426  0.02169447 -0.0004675 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01803732  0.02168393 -0.00046206], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01803037  0.0216734  -0.00045663], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01802343  0.02166286 -0.00045119], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01801649  0.02165233 -0.00044576], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01800954  0.02164179 -0.00044032], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0180026   0.02163125 -0.00043488], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01799565  0.02162072 -0.00042945], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01798871  0.02161018 -0.00042401], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01798176  0.02159964 -0.00041857], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01797481  0.0215891  -0.00041314], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01796787  0.02157856 -0.0004077 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01796092  0.02156803 -0.00040226], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01795397  0.02155749 -0.00039682], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01794702  0.02154695 -0.00039138], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01794008  0.02153641 -0.00038594], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 300: Weights: [0.31728733 0.30171903 0.38099361]\n",
            "Gradient risk: [ 0.01793313  0.02152586 -0.00038051], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01792618  0.02151532 -0.00037507], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01791923  0.02150478 -0.00036963], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01791228  0.02149424 -0.00036419], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01790533  0.0214837  -0.00035875], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01789838  0.02147315 -0.00035331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01789143  0.02146261 -0.00034787], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01788448  0.02145206 -0.00034242], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01787753  0.02144152 -0.00033698], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01787057  0.02143097 -0.00033154], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01786362  0.02142043 -0.0003261 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01785667  0.02140988 -0.00032066], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01784972  0.02139934 -0.00031522], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01784276  0.02138879 -0.00030977], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01783581  0.02137824 -0.00030433], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01782885  0.02136769 -0.00029889], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0178219   0.02135715 -0.00029345], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01781494  0.0213466  -0.000288  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01780799  0.02133605 -0.00028256], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01780103  0.0213255  -0.00027711], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01779408  0.02131495 -0.00027167], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01778712  0.0213044  -0.00026623], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01778016  0.02129385 -0.00026078], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01777321  0.0212833  -0.00025534], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01776625  0.02127275 -0.00024989], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01775929  0.02126219 -0.00024445], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01775233  0.02125164 -0.000239  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01774538  0.02124109 -0.00023356], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01773842  0.02123054 -0.00022811], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01773146  0.02121998 -0.00022266], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0177245   0.02120943 -0.00021722], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01771754  0.02119887 -0.00021177], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01771058  0.02118832 -0.00020632], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01770362  0.02117776 -0.00020088], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01769666  0.02116721 -0.00019543], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0176897   0.02115665 -0.00018998], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01768274  0.0211461  -0.00018453], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01767577  0.02113554 -0.00017909], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01766881  0.02112498 -0.00017364], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01766185  0.02111443 -0.00016819], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01765489  0.02110387 -0.00016274], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01764792  0.02109331 -0.00015729], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01764096  0.02108275 -0.00015184], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.017634    0.0210722  -0.00014639], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01762703  0.02106164 -0.00014094], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01762007  0.02105108 -0.00013549], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0176131   0.02104052 -0.00013004], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01760614  0.02102996 -0.00012459], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01759917  0.0210194  -0.00011914], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01759221  0.02100884 -0.00011369], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 350: Weights: [0.31456498 0.29642481 0.38901018]\n",
            "Gradient risk: [ 0.01758524  0.02099828 -0.00010824], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01757828  0.02098771 -0.00010279], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75713092e-02  2.09771524e-02 -9.73413823e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75643425e-02  2.09665905e-02 -9.18901440e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75573754e-02  2.09560282e-02 -8.64387114e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75504081e-02  2.09454656e-02 -8.09870858e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75434404e-02  2.09349027e-02 -7.55352685e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75364725e-02  2.09243394e-02 -7.00832608e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75295043e-02  2.09137759e-02 -6.46310640e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75225357e-02  2.09032120e-02 -5.91786795e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75155669e-02  2.08926478e-02 -5.37261085e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75085978e-02  2.08820834e-02 -4.82733524e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75016285e-02  2.08715186e-02 -4.28204125e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.74946588e-02  2.08609535e-02 -3.73672900e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.74876889e-02  2.08503881e-02 -3.19139863e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.74807186e-02  2.08398224e-02 -2.64605027e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.74737481e-02  2.08292565e-02 -2.10068404e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.74667773e-02  2.08186902e-02 -1.55530009e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.74598063e-02  2.08081236e-02 -1.00989855e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.74528350e-02  2.07975568e-02 -4.64479531e-06], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.74458634e-02 2.07869897e-02 8.09568209e-07], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.74388915e-02 2.07764223e-02 6.26410377e-06], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.74319193e-02 2.07658546e-02 1.17188101e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.74249469e-02 2.07552866e-02 1.71736858e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.74179743e-02 2.07447184e-02 2.26287297e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.74110013e-02 2.07341499e-02 2.80839403e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.74040281e-02 2.07235811e-02 3.35393165e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73970547e-02 2.07130121e-02 3.89948568e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73900809e-02 2.07024428e-02 4.44505600e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73831070e-02 2.06918733e-02 4.99064248e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73761327e-02 2.06813034e-02 5.53624499e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73691583e-02 2.06707334e-02 6.08186338e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73621835e-02 2.06601631e-02 6.62749754e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73552085e-02 2.06495925e-02 7.17314733e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73482333e-02 2.06390217e-02 7.71881262e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73412578e-02 2.06284506e-02 8.26449328e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73342821e-02 2.06178793e-02 8.81018918e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73273061e-02 2.06073078e-02 9.35590019e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73203299e-02 2.05967360e-02 9.90162617e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01731335 0.02058616 0.00010447], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01730638 0.02057559 0.00010993], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0172994  0.02056502 0.00011539], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01729242 0.02055445 0.00012085], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01728545 0.02054387 0.0001263 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01727847 0.0205333  0.00013176], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01727149 0.02052273 0.00013722], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01726451 0.02051215 0.00014268], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01725753 0.02050158 0.00014814], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01725055 0.02049101 0.0001536 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01724358 0.02048043 0.00015906], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 400: Weights: [0.31183014 0.29112724 0.39704259]\n",
            "Gradient risk: [0.0172366  0.02046986 0.00016451], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01722962 0.02045928 0.00016997], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01722264 0.02044871 0.00017543], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01721566 0.02043813 0.00018089], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01720868 0.02042756 0.00018635], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0172017  0.02041698 0.00019181], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01719472 0.02040641 0.00019727], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01718774 0.02039583 0.00020273], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01718076 0.02038525 0.00020819], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01717378 0.02037468 0.00021365], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0171668  0.0203641  0.00021911], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01715981 0.02035353 0.00022457], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01715283 0.02034295 0.00023003], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01714585 0.02033237 0.00023549], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01713887 0.02032179 0.00024095], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01713189 0.02031122 0.00024641], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0171249  0.02030064 0.00025188], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01711792 0.02029006 0.00025734], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01711094 0.02027949 0.0002628 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01710396 0.02026891 0.00026826], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01709697 0.02025833 0.00027372], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01708999 0.02024775 0.00027918], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01708301 0.02023717 0.00028464], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01707602 0.02022659 0.0002901 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01706904 0.02021602 0.00029557], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01706205 0.02020544 0.00030103], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01705507 0.02019486 0.00030649], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01704809 0.02018428 0.00031195], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0170411  0.0201737  0.00031741], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01703412 0.02016312 0.00032288], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01702713 0.02015254 0.00032834], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01702015 0.02014196 0.0003338 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01701316 0.02013138 0.00033926], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01700618 0.0201208  0.00034473], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01699919 0.02011022 0.00035019], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0169922  0.02009964 0.00035565], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01698522 0.02008906 0.00036112], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01697823 0.02007848 0.00036658], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01697125 0.0200679  0.00037204], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01696426 0.02005732 0.0003775 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01695727 0.02004674 0.00038297], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01695029 0.02003616 0.00038843], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0169433  0.02002558 0.00039389], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01693631 0.020015   0.00039936], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01692933 0.02000442 0.00040482], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01692234 0.01999384 0.00041028], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01691535 0.01998326 0.00041575], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01690836 0.01997267 0.00042121], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01690138 0.01996209 0.00042668], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01689439 0.01995151 0.00043214], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 450: Weights: [0.30908427 0.28582964 0.40508605]\n",
            "Gradient risk: [0.0168874  0.01994093 0.0004376 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01688041 0.01993035 0.00044307], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01687342 0.01991977 0.00044853], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01686643 0.01990919 0.000454  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01685945 0.0198986  0.00045946], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01685246 0.01988802 0.00046492], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01684547 0.01987744 0.00047039], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01683848 0.01986686 0.00047585], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01683149 0.01985628 0.00048132], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0168245  0.01984569 0.00048678], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01681751 0.01983511 0.00049225], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01681052 0.01982453 0.00049771], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01680353 0.01981395 0.00050318], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01679654 0.01980337 0.00050864], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01678955 0.01979278 0.00051411], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01678256 0.0197822  0.00051957], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01677557 0.01977162 0.00052504], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01676858 0.01976104 0.0005305 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01676159 0.01975045 0.00053597], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0167546  0.01973987 0.00054143], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01674761 0.01972929 0.0005469 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01674062 0.01971871 0.00055236], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01673363 0.01970812 0.00055783], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01672664 0.01969754 0.00056329], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01671965 0.01968696 0.00056876], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01671266 0.01967638 0.00057422], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01670567 0.01966579 0.00057969], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01669868 0.01965521 0.00058515], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01669169 0.01964463 0.00059062], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01668469 0.01963405 0.00059608], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0166777  0.01962346 0.00060155], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01667071 0.01961288 0.00060701], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01666372 0.0196023  0.00061248], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01665673 0.01959172 0.00061794], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01664974 0.01958113 0.00062341], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01664274 0.01957055 0.00062888], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01663575 0.01955997 0.00063434], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01662876 0.01954939 0.00063981], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01662177 0.0195388  0.00064527], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01661478 0.01952822 0.00065074], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01660778 0.01951764 0.0006562 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01660079 0.01950706 0.00066167], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0165938  0.01949647 0.00066714], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01658681 0.01948589 0.0006726 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01657981 0.01947531 0.00067807], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01657282 0.01946473 0.00068353], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01656583 0.01945414 0.000689  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01655884 0.01944356 0.00069446], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01655184 0.01943298 0.00069993], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Optimized Weights: [0.30638402 0.28064122 0.41297473]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Gradient risk: [0.22       0.22466667 0.198     ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 0: Weights: [0.33328084 0.33322881 0.33349031]\n",
            "Gradient risk: [0.2199617  0.2245936  0.19809952], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21992339 0.2245205  0.1981991 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21988506 0.22444738 0.19829871], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21984672 0.22437423 0.19839837], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21980836 0.22430105 0.19849807], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21976998 0.22422784 0.19859781], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21973158 0.2241546  0.1986976 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21969316 0.22408133 0.19879742], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21965473 0.22400803 0.19889729], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21961628 0.2239347  0.19899721], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21957781 0.22386134 0.19909716], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21953932 0.22378795 0.19919716], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21950081 0.22371453 0.19929719], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21946229 0.22364108 0.19939727], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21942375 0.22356761 0.1994974 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21938519 0.2234941  0.19959756], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21934662 0.22342057 0.19969777], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21930802 0.223347   0.19979802], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21926941 0.22327341 0.19989831], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21923078 0.22319978 0.19999864], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21919214 0.22312613 0.20009902], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21915347 0.22305245 0.20019944], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21911479 0.22297873 0.2002999 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21907609 0.22290499 0.2004004 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21903737 0.22283122 0.20050095], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21899863 0.22275742 0.20060153], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21895988 0.22268359 0.20070216], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21892111 0.22260973 0.20080284], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21888232 0.22253584 0.20090355], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21884351 0.22246192 0.2010043 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21880469 0.22238798 0.2011051 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21876585 0.222314   0.20120594], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21872699 0.22223999 0.20130682], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21868811 0.22216596 0.20140775], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21864921 0.22209189 0.20150872], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2186103  0.2220178  0.20160972], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21857137 0.22194367 0.20171078], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21853242 0.22186952 0.20181187], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21849346 0.22179534 0.201913  ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21845447 0.22172113 0.20201418], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21841547 0.22164688 0.2021154 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21837645 0.22157261 0.20221666], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21833741 0.22149831 0.20231796], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21829836 0.22142399 0.20241931], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21825929 0.22134963 0.2025207 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2182202  0.22127524 0.20262213], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21818109 0.22120082 0.2027236 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21814196 0.22112638 0.20282511], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21810282 0.2210519  0.20292667], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21806366 0.2209774  0.20302826], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 50: Weights: [0.33062536 0.32794985 0.34142477]\n",
            "Gradient risk: [0.21802448 0.22090286 0.2031299 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21798528 0.2208283  0.20323159], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21794607 0.22075371 0.20333331], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21790683 0.22067909 0.20343507], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21786758 0.22060444 0.20353688], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21782832 0.22052976 0.20363873], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21778903 0.22045505 0.20374062], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21774973 0.22038031 0.20384256], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21771041 0.22030554 0.20394453], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21767107 0.22023074 0.20404655], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21763171 0.22015592 0.20414861], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21759234 0.22008106 0.20425071], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21755295 0.22000618 0.20435285], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21751354 0.21993126 0.20445504], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21747411 0.21985632 0.20455727], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21743466 0.21978135 0.20465953], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2173952  0.21970635 0.20476185], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21735572 0.21963132 0.2048642 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21731622 0.21955626 0.20496659], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21727671 0.21948117 0.20506903], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21723717 0.21940606 0.20517151], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21719762 0.21933091 0.20527403], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21715805 0.21925573 0.20537659], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21711847 0.21918053 0.20547919], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21707886 0.2191053  0.20558184], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21703924 0.21903003 0.20568453], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2169996  0.21895474 0.20578726], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21695994 0.21887942 0.20589003], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21692027 0.21880407 0.20599284], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21688058 0.21872869 0.2060957 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21684086 0.21865329 0.20619859], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21680114 0.21857785 0.20630153], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21676139 0.21850238 0.20640451], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21672163 0.21842689 0.20650753], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21668185 0.21835136 0.2066106 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21664205 0.21827581 0.2067137 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21660223 0.21820023 0.20681685], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2165624  0.21812462 0.20692004], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21652254 0.21804898 0.20702327], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21648267 0.21797331 0.20712654], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21644279 0.21789761 0.20722985], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21640288 0.21782189 0.20733321], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21636296 0.21774613 0.20743661], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21632302 0.21767035 0.20754005], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21628306 0.21759453 0.20764353], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21624308 0.21751869 0.20774705], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21620309 0.21744282 0.20785061], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21616308 0.21736692 0.20795422], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21612305 0.21729099 0.20805787], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.216083   0.21721504 0.20816155], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 100: Weights: [0.32790809 0.32256698 0.3495249 ]\n",
            "Gradient risk: [0.21604294 0.21713905 0.20826528], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21600286 0.21706303 0.20836906], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21596276 0.21698699 0.20847287], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21592264 0.21691092 0.20857673], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21588251 0.21683482 0.20868062], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21584235 0.21675869 0.20878456], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21580218 0.21668253 0.20888854], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.215762   0.21660634 0.20899256], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21572179 0.21653012 0.20909663], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21568157 0.21645388 0.20920073], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21564133 0.2163776  0.20930488], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21560107 0.2163013  0.20940906], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21556079 0.21622497 0.20951329], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2155205  0.21614861 0.20961756], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21548019 0.21607222 0.20972188], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21543986 0.2159958  0.20982623], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21539951 0.21591935 0.20993062], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21535915 0.21584288 0.21003506], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21531877 0.21576637 0.21013954], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21527837 0.21568984 0.21024406], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21523795 0.21561328 0.21034862], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21519751 0.21553669 0.21045322], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21515706 0.21546007 0.21055787], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21511659 0.21538342 0.21066255], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2150761  0.21530674 0.21076728], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2150356  0.21523004 0.21087205], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21499507 0.21515331 0.21097685], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21495453 0.21507654 0.2110817 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21491398 0.21499975 0.2111866 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2148734  0.21492293 0.21129153], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21483281 0.21484609 0.2113965 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2147922  0.21476921 0.21150152], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21475157 0.2146923  0.21160658], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21471092 0.21461537 0.21171168], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21467026 0.21453841 0.21181682], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21462957 0.21446142 0.211922  ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21458888 0.2143844  0.21202722], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21454816 0.21430735 0.21213248], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21450742 0.21423027 0.21223779], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21446667 0.21415317 0.21234314], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2144259  0.21407604 0.21244852], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21438512 0.21399888 0.21255395], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21434431 0.21392168 0.21265942], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21430349 0.21384447 0.21276493], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21426265 0.21376722 0.21287048], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21422179 0.21368994 0.21297608], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21418092 0.21361264 0.21308171], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21414002 0.21353531 0.21318739], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21409911 0.21345795 0.21329311], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21405818 0.21338056 0.21339886], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 150: Weights: [0.32512919 0.31708165 0.35778913]\n",
            "Gradient risk: [0.21401724 0.21330314 0.21350466], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21397628 0.21322569 0.2136105 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21393529 0.21314822 0.21371638], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2138943  0.21307072 0.21382231], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21385328 0.21299319 0.21392827], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21381225 0.21291563 0.21403428], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2137712  0.21283804 0.21414032], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21373013 0.21276042 0.21424641], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21368904 0.21268278 0.21435254], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21364794 0.21260511 0.2144587 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21360682 0.2125274  0.21456491], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21356568 0.21244967 0.21467117], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21352452 0.21237192 0.21477746], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21348335 0.21229413 0.21488379], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21344216 0.21221632 0.21499016], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21340095 0.21213848 0.21509658], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21335972 0.2120606  0.21520303], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21331848 0.21198271 0.21530953], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21327721 0.21190478 0.21541607], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21323594 0.21182682 0.21552265], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21319464 0.21174884 0.21562927], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21315332 0.21167083 0.21573593], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21311199 0.21159279 0.21584263], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21307064 0.21151472 0.21594937], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21302928 0.21143662 0.21605615], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21298789 0.2113585  0.21616298], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21294649 0.21128035 0.21626984], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21290507 0.21120217 0.21637675], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21286363 0.21112396 0.21648369], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21282218 0.21104572 0.21659068], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21278071 0.21096746 0.21669771], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21273922 0.21088916 0.21680478], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21269771 0.21081084 0.21691189], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21265619 0.21073249 0.21701904], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21261464 0.21065412 0.21712623], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21257308 0.21057571 0.21723346], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21253151 0.21049728 0.21734073], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21248991 0.21041882 0.21744805], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2124483  0.21034033 0.2175554 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21240667 0.21026181 0.2176628 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21236502 0.21018327 0.21777023], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21232336 0.21010469 0.21787771], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21228168 0.21002609 0.21798522], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21223998 0.20994746 0.21809278], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21219826 0.20986881 0.21820038], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21215653 0.20979012 0.21830802], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21211478 0.20971141 0.2184157 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21207301 0.20963267 0.21852342], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21203122 0.2095539  0.21863118], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21198942 0.2094751  0.21873898], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 200: Weights: [0.32228888 0.31149555 0.36621554]\n",
            "Gradient risk: [0.21194759 0.20939628 0.21884682], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21190575 0.20931743 0.2189547 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2118639  0.20923855 0.21906262], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21182202 0.20915964 0.21917059], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21178013 0.2090807  0.21927859], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21173822 0.20900174 0.21938663], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2116963  0.20892275 0.21949472], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21165435 0.20884373 0.21960284], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21161239 0.20876468 0.21971101], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21157041 0.20868561 0.21981921], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21152842 0.20860651 0.21992746], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2114864  0.20852738 0.22003575], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21144437 0.20844822 0.22014407], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21140232 0.20836903 0.22025244], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21136026 0.20828982 0.22036085], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21131817 0.20821058 0.2204693 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21127607 0.20813131 0.22057779], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21123396 0.20805201 0.22068632], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21119182 0.20797269 0.22079488], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21114967 0.20789334 0.22090349], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2111075  0.20781396 0.22101214], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21106531 0.20773455 0.22112083], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2110231  0.20765512 0.22122956], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21098088 0.20757565 0.22133834], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21093864 0.20749616 0.22144715], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21089638 0.20741665 0.221556  ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21085411 0.2073371  0.22166489], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21081182 0.20725753 0.22177382], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21076951 0.20717793 0.22188279], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21072718 0.2070983  0.2219918 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21068484 0.20701865 0.22210086], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21064248 0.20693896 0.22220995], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2106001  0.20685925 0.22231908], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2105577  0.20677952 0.22242825], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21051529 0.20669975 0.22253747], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21047286 0.20661996 0.22264672], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21043041 0.20654014 0.22275601], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21038794 0.20646029 0.22286535], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21034546 0.20638041 0.22297472], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21030296 0.20630051 0.22308413], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21026044 0.20622058 0.22319359], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21021791 0.20614063 0.22330308], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21017535 0.20606064 0.22341261], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21013278 0.20598063 0.22352218], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2100902  0.20590059 0.2236318 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21004759 0.20582052 0.22374145], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21000497 0.20574043 0.22385114], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20996233 0.20566031 0.22396088], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20991967 0.20558016 0.22407065], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.209877   0.20549998 0.22418046], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 250: Weights: [0.31938752 0.30581058 0.37480187]\n",
            "Gradient risk: [0.20983431 0.20541978 0.22429032], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2097916  0.20533955 0.22440021], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20974888 0.20525929 0.22451014], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20970613 0.20517901 0.22462011], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20966337 0.2050987  0.22473012], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20962059 0.20501836 0.22484018], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2095778  0.20493799 0.22495027], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20953499 0.2048576  0.2250604 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20949216 0.20477717 0.22517057], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20944931 0.20469673 0.22528078], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20940645 0.20461625 0.22539103], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20936357 0.20453575 0.22550132], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20932067 0.20445522 0.22561165], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20927775 0.20437466 0.22572202], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20923482 0.20429408 0.22583243], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20919187 0.20421347 0.22594288], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2091489  0.20413283 0.22605337], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20910592 0.20405216 0.2261639 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20906292 0.20397147 0.22627447], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2090199  0.20389075 0.22638508], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20897686 0.20381001 0.22649572], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20893381 0.20372923 0.22660641], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20889073 0.20364843 0.22671714], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20884765 0.20356761 0.22682791], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20880454 0.20348675 0.22693871], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20876142 0.20340587 0.22704956], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20871828 0.20332496 0.22716044], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20867512 0.20324403 0.22727137], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20863195 0.20316307 0.22738233], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20858876 0.20308208 0.22749333], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20854555 0.20300106 0.22760438], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20850232 0.20292002 0.22771546], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20845908 0.20283895 0.22782658], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20841582 0.20275785 0.22793774], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20837254 0.20267673 0.22804894], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20832925 0.20259558 0.22816018], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20828593 0.2025144  0.22827146], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20824261 0.2024332  0.22838278], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20819926 0.20235197 0.22849414], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2081559  0.20227071 0.22860553], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20811252 0.20218943 0.22871697], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20806912 0.20210812 0.22882845], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2080257  0.20202678 0.22893996], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20798227 0.20194542 0.22905151], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20793882 0.20186403 0.22916311], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20789536 0.20178261 0.22927474], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20785187 0.20170116 0.22938641], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20780837 0.20161969 0.22949812], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20776486 0.2015382  0.22960987], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20772132 0.20145667 0.22972166], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 300: Weights: [0.31642556 0.30002891 0.3835455 ]\n",
            "Gradient risk: [0.20767777 0.20137512 0.22983349], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2076342  0.20129354 0.22994536], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20759061 0.20121194 0.23005727], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20754701 0.20113031 0.23016921], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20750339 0.20104865 0.2302812 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20745975 0.20096697 0.23039322], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2074161  0.20088526 0.23050528], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20737243 0.20080352 0.23061739], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20732874 0.20072176 0.23072953], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20728503 0.20063997 0.23084171], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20724131 0.20055815 0.23095393], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20719757 0.20047631 0.23106619], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20715381 0.20039444 0.23117848], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20711004 0.20031254 0.23129082], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20706625 0.20023062 0.23140319], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20702244 0.20014867 0.23151561], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20697861 0.2000667  0.23162806], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20693477 0.1999847  0.23174055], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20689091 0.19990267 0.23185308], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20684704 0.19982062 0.23196565], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20680314 0.19973854 0.23207826], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20675923 0.19965643 0.23219091], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2067153  0.1995743  0.23230359], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20667136 0.19949214 0.23241632], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2066274  0.19940995 0.23252908], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20658342 0.19932774 0.23264188], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20653942 0.1992455  0.23275472], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20649541 0.19916324 0.2328676 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20645138 0.19908095 0.23298052], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20640734 0.19899863 0.23309347], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20636327 0.19891629 0.23320647], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20631919 0.19883392 0.2333195 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20627509 0.19875152 0.23343258], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20623098 0.1986691  0.23354569], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20618685 0.19858665 0.23365884], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2061427  0.19850418 0.23377203], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20609853 0.19842168 0.23388525], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20605435 0.19833915 0.23399852], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20601015 0.1982566  0.23411182], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20596594 0.19817402 0.23422516], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2059217  0.19809142 0.23433854], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20587745 0.19800879 0.23445196], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20583319 0.19792613 0.23456542], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2057889  0.19784345 0.23467892], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2057446  0.19776074 0.23479245], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20570028 0.19767801 0.23490602], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20565595 0.19759525 0.23501964], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2056116  0.19751246 0.23513328], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20556723 0.19742965 0.23524697], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20552284 0.19734681 0.2353607 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 350: Weights: [0.31340357 0.29415294 0.39244346]\n",
            "Gradient risk: [0.20547844 0.19726395 0.23547446], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20543402 0.19718106 0.23558827], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20538958 0.19709815 0.23570211], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20534513 0.1970152  0.23581599], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20530066 0.19693224 0.2359299 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20525617 0.19684924 0.23604386], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20521167 0.19676623 0.23615785], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20516715 0.19668318 0.23627189], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20512261 0.19660011 0.23638596], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20507806 0.19651701 0.23650006], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20503348 0.19643389 0.23661421], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2049889  0.19635075 0.2367284 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20494429 0.19626757 0.23684262], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20489967 0.19618437 0.23695688], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20485503 0.19610115 0.23707118], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20481038 0.1960179  0.23718551], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2047657  0.19593462 0.23729989], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20472101 0.19585132 0.2374143 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20467631 0.19576799 0.23752875], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20463159 0.19568464 0.23764324], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20458685 0.19560126 0.23775777], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20454209 0.19551786 0.23787233], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20449732 0.19543443 0.23798694], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20445253 0.19535097 0.23810158], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20440772 0.19526749 0.23821625], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2043629  0.19518399 0.23833097], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20431806 0.19510045 0.23844572], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2042732  0.1950169  0.23856052], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20422833 0.19493331 0.23867534], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20418344 0.19484971 0.23879021], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20413853 0.19476607 0.23890512], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2040936  0.19468241 0.23902006], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20404866 0.19459873 0.23913504], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20400371 0.19451502 0.23925006], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20395873 0.19443129 0.23936511], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20391374 0.19434753 0.23948021], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20386873 0.19426374 0.23959534], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20382371 0.19417993 0.23971051], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20377867 0.19409609 0.23982571], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20373361 0.19401223 0.23994096], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20368853 0.19392834 0.24005624], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20364344 0.19384443 0.24017156], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20359834 0.19376049 0.24028691], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20355321 0.19367653 0.24040231], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20350807 0.19359254 0.24051774], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20346291 0.19350853 0.24063321], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20341774 0.19342449 0.24074871], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20337255 0.19334043 0.24086426], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20332734 0.19325634 0.24097984], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20328211 0.19317223 0.24109546], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 400: Weights: [0.31032224 0.2881853  0.40149242]\n",
            "Gradient risk: [0.20323687 0.19308809 0.24121111], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20319161 0.19300393 0.24132681], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20314634 0.19291974 0.24144254], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20310105 0.19283552 0.24155831], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20305574 0.19275128 0.24167411], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20301042 0.19266702 0.24178995], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20296507 0.19258273 0.24190583], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20291972 0.19249842 0.24202175], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20287434 0.19241408 0.2421377 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20282895 0.19232972 0.24225369], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20278354 0.19224533 0.24236972], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20273812 0.19216091 0.24248579], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20269268 0.19207647 0.24260189], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20264722 0.19199201 0.24271803], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20260175 0.19190752 0.24283421], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20255626 0.19182301 0.24295042], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20251075 0.19173847 0.24306667], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20246523 0.19165391 0.24318296], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20241969 0.19156932 0.24329929], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20237413 0.19148471 0.24341565], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20232856 0.19140007 0.24353205], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20228297 0.19131541 0.24364848], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20223736 0.19123072 0.24376495], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20219174 0.19114601 0.24388146], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2021461  0.19106128 0.24399801], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20210044 0.19097652 0.24411459], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20205477 0.19089173 0.24423121], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20200908 0.19080692 0.24434787], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20196338 0.19072209 0.24446457], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20191766 0.19063723 0.2445813 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20187192 0.19055234 0.24469806], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20182616 0.19046743 0.24481487], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20178039 0.1903825  0.24493171], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20173461 0.19029754 0.24504859], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2016888  0.19021256 0.2451655 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20164298 0.19012756 0.24528245], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20159714 0.19004252 0.24539944], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20155129 0.18995747 0.24551646], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20150542 0.18987239 0.24563352], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20145953 0.18978729 0.24575062], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20141363 0.18970216 0.24586775], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20136771 0.189617   0.24598492], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20132178 0.18953183 0.24610213], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20127583 0.18944662 0.24621938], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20122986 0.1893614  0.24633665], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20118387 0.18927615 0.24645397], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20113787 0.18919087 0.24657132], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20109185 0.18910557 0.24668871], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20104582 0.18902025 0.24680614], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20099977 0.1889349  0.2469236 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 450: Weights: [0.30718238 0.2821289  0.41068868]\n",
            "Gradient risk: [0.2009537  0.18884953 0.2470411 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20090762 0.18876413 0.24715863], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20086152 0.18867871 0.2472762 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20081541 0.18859327 0.24739381], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20076927 0.1885078  0.24751145], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20072313 0.18842231 0.24762913], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20067696 0.18833679 0.24774685], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20063078 0.18825125 0.2478646 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20058458 0.18816568 0.24798239], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20053837 0.18808009 0.24810021], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20049214 0.18799448 0.24821807], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20044589 0.18790884 0.24833597], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20039963 0.18782318 0.2484539 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20035335 0.1877375  0.24857187], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20030706 0.18765179 0.24868987], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20026075 0.18756605 0.24880791], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20021442 0.1874803  0.24892599], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20016808 0.18739452 0.2490441 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20012172 0.18730871 0.24916225], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20007534 0.18722288 0.24928044], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20002895 0.18713703 0.24939866], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19998254 0.18705115 0.24951691], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19993611 0.18696525 0.2496352 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19988967 0.18687933 0.24975353], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19984322 0.18679338 0.2498719 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19979674 0.18670741 0.24999029], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19975025 0.18662141 0.25010873], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19970375 0.18653539 0.2502272 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19965723 0.18644935 0.25034571], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19961069 0.18636328 0.25046425], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19956413 0.18627719 0.25058283], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19951756 0.18619108 0.25070144], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19947098 0.18610494 0.25082009], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19942437 0.18601878 0.25093877], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19937775 0.18593259 0.2510575 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19933112 0.18584638 0.25117625], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19928447 0.18576015 0.25129504], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1992378  0.1856739  0.25141387], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19919112 0.18558762 0.25153273], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19914442 0.18550131 0.25165163], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1990977  0.18541499 0.25177056], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19905097 0.18532864 0.25188953], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19900422 0.18524226 0.25200854], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19895746 0.18515587 0.25212758], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19891068 0.18506945 0.25224665], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19886388 0.184983   0.25236576], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19881707 0.18489654 0.25248491], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19877024 0.18481004 0.25260409], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1987234  0.18472353 0.2527233 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Gradient risk: [0.15333333 0.158      0.13133333], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 0: Weights: [0.33328097 0.33322905 0.33348995]\n",
            "Gradient risk: [0.1533056  0.15794795 0.1314013 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15327786 0.15789591 0.13146929], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15325012 0.15784386 0.13153728], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15322238 0.1577918  0.13160528], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15319463 0.15773974 0.13167329], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15316687 0.15768767 0.13174131], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15313911 0.1576356  0.13180933], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15311135 0.15758352 0.13187736], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15308358 0.15753144 0.13194541], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15305581 0.15747935 0.13201346], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15302804 0.15742725 0.13208151], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15300026 0.15737516 0.13214958], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15297247 0.15732305 0.13221766], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15294469 0.15727094 0.13228574], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15291689 0.15721883 0.13235383], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1528891  0.15716671 0.13242193], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15286129 0.15711458 0.13249003], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15283349 0.15706245 0.13255815], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15280568 0.15701032 0.13262627], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15277786 0.15695818 0.1326944 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15275005 0.15690603 0.13276254], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15272222 0.15685388 0.13283069], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1526944  0.15680172 0.13289885], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15266656 0.15674956 0.13296701], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15263873 0.1566974  0.13303518], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15261089 0.15664522 0.13310336], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15258304 0.15659305 0.13317155], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1525552  0.15654087 0.13323974], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15252734 0.15648868 0.13330795], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15249949 0.15643649 0.13337616], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15247162 0.15638429 0.13344438], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15244376 0.15633209 0.1335126 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15241589 0.15627988 0.13358084], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15238802 0.15622767 0.13364908], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15236014 0.15617546 0.13371733], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15233226 0.15612323 0.13378559], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15230437 0.15607101 0.13385386], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15227648 0.15601878 0.13392213], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15224858 0.15596654 0.13399041], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15222068 0.1559143  0.1340587 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15219278 0.15586205 0.134127  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15216487 0.1558098  0.13419531], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15213696 0.15575755 0.13426362], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15210905 0.15570529 0.13433194], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15208113 0.15565302 0.13440027], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1520532  0.15560075 0.1344686 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15202527 0.15554848 0.13453695], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15199734 0.1554962  0.1346053 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15196941 0.15544391 0.13467366], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15194147 0.15539162 0.13474202], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 50: Weights: [0.33065202 0.3280028  0.34134515]\n",
            "Gradient risk: [0.15191352 0.15533933 0.1348104 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15188557 0.15528703 0.13487878], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15185762 0.15523472 0.13494717], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15182966 0.15518242 0.13501557], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1518017  0.1551301  0.13508397], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15177374 0.15507779 0.13515238], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15174577 0.15502546 0.1352208 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15171779 0.15497314 0.13528923], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15168982 0.1549208  0.13535766], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15166183 0.15486847 0.1354261 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15163385 0.15481613 0.13549455], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15160586 0.15476378 0.13556301], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15157787 0.15471143 0.13563147], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15154987 0.15465907 0.13569995], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15152187 0.15460671 0.13576843], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15149386 0.15455435 0.13583691], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15146585 0.15450198 0.13590541], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15143784 0.15444961 0.13597391], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15140982 0.15439723 0.13604241], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1513818  0.15434485 0.13611093], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15135377 0.15429246 0.13617945], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15132574 0.15424007 0.13624798], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15129771 0.15418767 0.13631652], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15126967 0.15413527 0.13638507], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15124163 0.15408287 0.13645362], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15121358 0.15403046 0.13652218], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15118553 0.15397804 0.13659074], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15115748 0.15392562 0.13665932], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15112942 0.1538732  0.1367279 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15110136 0.15382077 0.13679649], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15107329 0.15376834 0.13686508], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15104522 0.15371591 0.13693368], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15101715 0.15366347 0.13700229], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15098907 0.15361102 0.13707091], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15096099 0.15355857 0.13713953], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15093291 0.15350612 0.13720816], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15090482 0.15345366 0.1372768 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15087672 0.1534012  0.13734544], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15084863 0.15334873 0.1374141 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15082052 0.15329626 0.13748275], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15079242 0.15324379 0.13755142], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15076431 0.15319131 0.13762009], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1507362  0.15313883 0.13768877], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15070808 0.15308634 0.13775746], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15067996 0.15303385 0.13782615], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15065184 0.15298135 0.13789485], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15062371 0.15292885 0.13796356], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15059558 0.15287635 0.13803227], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15056744 0.15282384 0.13810099], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1505393  0.15277132 0.13816972], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 100: Weights: [0.32800226 0.32275358 0.34924414]\n",
            "Gradient risk: [0.15051116 0.15271881 0.13823846], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15048301 0.15266629 0.1383072 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15045486 0.15261376 0.13837595], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15042671 0.15256123 0.1384447 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15039855 0.1525087  0.13851346], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15037038 0.15245616 0.13858223], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15034222 0.15240362 0.13865101], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15031405 0.15235107 0.13871979], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15028587 0.15229852 0.13878858], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1502577  0.15224597 0.13885737], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15022951 0.15219341 0.13892618], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15020133 0.15214085 0.13899498], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15017314 0.15208829 0.1390638 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15014495 0.15203572 0.13913262], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15011675 0.15198314 0.13920145], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15008855 0.15193057 0.13927028], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15006035 0.15187798 0.13933913], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15003214 0.1518254  0.13940797], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15000393 0.15177281 0.13947683], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14997571 0.15172022 0.13954569], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14994749 0.15166762 0.13961456], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14991927 0.15161502 0.13968343], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14989104 0.15156241 0.13975231], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14986281 0.1515098  0.1398212 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14983458 0.15145719 0.13989009], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14980634 0.15140458 0.13995899], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1497781  0.15135195 0.1400279 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14974986 0.15129933 0.14009681], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14972161 0.1512467  0.14016573], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14969336 0.15119407 0.14023466], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1496651  0.15114144 0.14030359], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14963684 0.1510888  0.14037253], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14960858 0.15103615 0.14044147], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14958031 0.15098351 0.14051043], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14955204 0.15093086 0.14057938], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14952377 0.1508782  0.14064835], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14949549 0.15082555 0.14071732], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14946721 0.15077288 0.14078629], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14943892 0.15072022 0.14085528], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14941064 0.15066755 0.14092426], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14938234 0.15061488 0.14099326], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14935405 0.1505622  0.14106226], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14932575 0.15050952 0.14113127], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14929745 0.15045684 0.14120028], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14926914 0.15040415 0.1412693 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14924083 0.15035146 0.14133833], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14921252 0.15029877 0.14140736], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1491842  0.15024607 0.14147639], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14915588 0.15019337 0.14154544], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14912755 0.15014067 0.14161449], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 150: Weights: [0.32533299 0.31748454 0.35718244]\n",
            "Gradient risk: [0.14909923 0.15008796 0.14168354], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14907089 0.15003525 0.14175261], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14904256 0.14998253 0.14182167], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14901422 0.14992981 0.14189075], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14898588 0.14987709 0.14195983], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14895753 0.14982437 0.14202891], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14892918 0.14977164 0.142098  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14890083 0.14971891 0.1421671 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14887248 0.14966617 0.14223621], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14884412 0.14961343 0.14230532], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14881575 0.14956069 0.14237443], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14878739 0.14950794 0.14244355], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14875902 0.1494552  0.14251268], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14873064 0.14940244 0.14258181], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14870227 0.14934969 0.14265095], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14867389 0.14929693 0.1427201 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1486455  0.14924417 0.14278925], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14861712 0.1491914  0.1428584 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14858873 0.14913863 0.14292756], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14856033 0.14908586 0.14299673], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14853193 0.14903308 0.1430659 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14850353 0.14898031 0.14313508], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14847513 0.14892752 0.14320427], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14844672 0.14887474 0.14327346], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14841831 0.14882195 0.14334265], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1483899  0.14876916 0.14341186], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14836148 0.14871637 0.14348106], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14833306 0.14866357 0.14355028], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14830463 0.14861077 0.14361949], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1482762  0.14855796 0.14368872], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14824777 0.14850516 0.14375795], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14821934 0.14845235 0.14382718], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1481909  0.14839953 0.14389642], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14816246 0.14834672 0.14396567], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14813401 0.1482939  0.14403492], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14810557 0.14824107 0.14410418], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14807712 0.14818825 0.14417344], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14804866 0.14813542 0.14424271], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1480202  0.14808259 0.14431198], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14799174 0.14802975 0.14438126], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14796328 0.14797692 0.14445054], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14793481 0.14792408 0.14451983], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14790634 0.14787123 0.14458913], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14787786 0.14781839 0.14465843], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14784939 0.14776554 0.14472773], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1478209  0.14771268 0.14479704], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14779242 0.14765983 0.14486636], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14776393 0.14760697 0.14493568], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14773544 0.14755411 0.14500501], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14770695 0.14750125 0.14507434], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 200: Weights: [0.32264554 0.31219892 0.36515551]\n",
            "Gradient risk: [0.14767845 0.14744838 0.14514368], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14764995 0.14739551 0.14521302], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14762145 0.14734264 0.14528237], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14759294 0.14728976 0.14535172], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14756443 0.14723688 0.14542108], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14753591 0.147184   0.14549044], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1475074  0.14713112 0.14555981], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14747888 0.14707823 0.14562918], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14745036 0.14702534 0.14569856], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14742183 0.14697245 0.14576795], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1473933  0.14691956 0.14583733], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14736477 0.14686666 0.14590673], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14733623 0.14681376 0.14597613], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14730769 0.14676086 0.14604553], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14727915 0.14670795 0.14611494], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1472506  0.14665504 0.14618435], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14722206 0.14660213 0.14625377], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1471935  0.14654922 0.14632319], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14716495 0.1464963  0.14639262], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14713639 0.14644338 0.14646206], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14710783 0.14639046 0.1465315 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14707927 0.14633754 0.14660094], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1470507  0.14628461 0.14667039], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14702213 0.14623168 0.14673984], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14699356 0.14617875 0.1468093 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14696498 0.14612582 0.14687876], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1469364  0.14607288 0.14694823], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14690782 0.14601994 0.1470177 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14687923 0.145967   0.14708718], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14685064 0.14591406 0.14715666], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14682205 0.14586111 0.14722615], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14679346 0.14580816 0.14729564], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14676486 0.14575521 0.14736513], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14673626 0.14570226 0.14743463], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14670765 0.1456493  0.14750414], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14667905 0.14559634 0.14757365], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14665044 0.14554338 0.14764316], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14662182 0.14549042 0.14771268], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14659321 0.14543745 0.14778221], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14656459 0.14538448 0.14785173], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14653596 0.14533151 0.14792127], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14650734 0.14527854 0.14799081], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14647871 0.14522557 0.14806035], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14645008 0.14517259 0.14812989], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14642145 0.14511961 0.14819945], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14639281 0.14506663 0.148269  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14636417 0.14501364 0.14833856], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14633553 0.14496066 0.14840813], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14630688 0.14490767 0.14847769], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14627823 0.14485468 0.14854727], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 250: Weights: [0.31994129 0.30689997 0.37315871]\n",
            "Gradient risk: [0.14624958 0.14480168 0.14861685], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14622092 0.14474869 0.14868643], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14619227 0.14469569 0.14875602], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14616361 0.14464269 0.14882561], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14613494 0.14458969 0.1488952 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14610628 0.14453669 0.1489648 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14607761 0.14448368 0.14903441], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14604894 0.14443067 0.14910401], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14602026 0.14437766 0.14917363], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14599158 0.14432465 0.14924324], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1459629  0.14427164 0.14931286], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14593422 0.14421862 0.14938249], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14590553 0.1441656  0.14945212], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14587684 0.14411258 0.14952175], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14584815 0.14405956 0.14959139], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14581945 0.14400653 0.14966103], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14579076 0.14395351 0.14973068], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14576205 0.14390048 0.14980033], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14573335 0.14384745 0.14986998], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14570464 0.14379442 0.14993964], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14567594 0.14374138 0.15000931], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14564722 0.14368835 0.15007897], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14561851 0.14363531 0.15014864], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14558979 0.14358227 0.15021832], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14556107 0.14352923 0.150288  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14553235 0.14347618 0.15035768], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14550362 0.14342314 0.15042737], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14547489 0.14337009 0.15049706], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14544616 0.14331704 0.15056675], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14541743 0.14326399 0.15063645], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14538869 0.14321094 0.15070615], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14535995 0.14315788 0.15077586], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14533121 0.14310482 0.15084557], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14530246 0.14305177 0.15091529], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14527371 0.14299871 0.150985  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14524496 0.14294564 0.15105473], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14521621 0.14289258 0.15112445], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14518745 0.14283952 0.15119418], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14515869 0.14278645 0.15126391], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14512993 0.14273338 0.15133365], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14510117 0.14268031 0.15140339], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1450724  0.14262724 0.15147314], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14504363 0.14257416 0.15154288], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14501486 0.14252109 0.15161264], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14498608 0.14246801 0.15168239], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1449573  0.14241493 0.15175215], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14492852 0.14236185 0.15182192], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14489974 0.14230877 0.15189168], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14487096 0.14225569 0.15196145], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14484217 0.1422026  0.15203123], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 300: Weights: [0.31722163 0.30159101 0.38118732]\n",
            "Gradient risk: [0.14481338 0.14214952 0.152101  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14478458 0.14209643 0.15217078], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14475579 0.14204334 0.15224057], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14472699 0.14199025 0.15231036], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14469819 0.14193716 0.15238015], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14466938 0.14188406 0.15244994], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14464058 0.14183097 0.15251974], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14461177 0.14177787 0.15258955], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14458295 0.14172477 0.15265935], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14455414 0.14167167 0.15272916], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14452532 0.14161857 0.15279897], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1444965  0.14156547 0.15286879], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14446768 0.14151237 0.15293861], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14443886 0.14145926 0.15300843], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14441003 0.14140616 0.15307826], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1443812  0.14135305 0.15314808], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14435237 0.14129994 0.15321792], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14432353 0.14124683 0.15328775], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1442947  0.14119372 0.15335759], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14426586 0.1411406  0.15342743], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14423701 0.14108749 0.15349728], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14420817 0.14103437 0.15356713], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14417932 0.14098126 0.15363698], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14415047 0.14092814 0.15370684], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14412162 0.14087502 0.1537767 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14409277 0.1408219  0.15384656], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14406391 0.14076878 0.15391642], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14403505 0.14071565 0.15398629], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14400619 0.14066253 0.15405616], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14397732 0.14060941 0.15412604], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14394845 0.14055628 0.15419591], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14391959 0.14050315 0.15426579], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14389071 0.14045002 0.15433568], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14386184 0.14039689 0.15440556], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14383296 0.14034376 0.15447545], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14380408 0.14029063 0.15454534], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1437752  0.1402375  0.15461524], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14374632 0.14018436 0.15468514], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14371743 0.14013123 0.15475504], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14368854 0.14007809 0.15482494], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14365965 0.14002496 0.15489485], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14363076 0.13997182 0.15496476], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14360186 0.13991868 0.15503467], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14357296 0.13986554 0.15510459], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14354406 0.1398124  0.15517451], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14351516 0.13975926 0.15524443], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14348626 0.13970611 0.15531435], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14345735 0.13965297 0.15538428], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14342844 0.13959983 0.15545421], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14339953 0.13954668 0.15552414], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 350: Weights: [0.31448799 0.29627539 0.38923659]\n",
            "Gradient risk: [0.14337061 0.13949353 0.15559408], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1433417  0.13944039 0.15566402], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14331278 0.13938724 0.15573396], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14328386 0.13933409 0.1558039 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14325493 0.13928094 0.15587385], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14322601 0.13922779 0.1559438 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14319708 0.13917464 0.15601375], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14316815 0.13912149 0.1560837 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14313921 0.13906833 0.15615366], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14311028 0.13901518 0.15622362], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14308134 0.13896203 0.15629358], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1430524  0.13890887 0.15636355], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14302346 0.13885572 0.15643351], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14299452 0.13880256 0.15650348], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14296557 0.1387494  0.15657346], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14293662 0.13869624 0.15664343], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14290767 0.13864309 0.15671341], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14287872 0.13858993 0.15678339], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14284976 0.13853677 0.15685337], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14282081 0.13848361 0.15692335], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14279185 0.13843045 0.15699334], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14276288 0.13837729 0.15706333], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14273392 0.13832412 0.15713332], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14270495 0.13827096 0.15720332], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14267599 0.1382178  0.15727331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14264702 0.13816463 0.15734331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14261804 0.13811147 0.15741331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14258907 0.1380583  0.15748331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14256009 0.13800514 0.15755332], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14253111 0.13795197 0.15762333], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14250213 0.13789881 0.15769334], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14247315 0.13784564 0.15776335], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14244416 0.13779247 0.15783337], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14241518 0.13773931 0.15790338], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14238619 0.13768614 0.1579734 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14235719 0.13763297 0.15804342], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1423282  0.1375798  0.15811345], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14229921 0.13752663 0.15818347], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14227021 0.13747346 0.1582535 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14224121 0.13742029 0.15832353], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14221221 0.13736712 0.15839356], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1421832  0.13731395 0.15846359], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1421542  0.13726078 0.15853363], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14212519 0.13720761 0.15860367], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14209618 0.13715444 0.15867371], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14206717 0.13710127 0.15874375], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14203815 0.1370481  0.15881379], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14200913 0.13699492 0.15888384], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14198012 0.13694175 0.15895388], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1419511  0.13688858 0.15902393], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 400: Weights: [0.31174181 0.29095648 0.39730168]\n",
            "Gradient risk: [0.14192207 0.13683541 0.15909399], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14189305 0.13678223 0.15916404], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14186402 0.13672906 0.15923409], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.141835   0.13667589 0.15930415], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14180597 0.13662271 0.15937421], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14177693 0.13656954 0.15944427], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1417479  0.13651637 0.15951433], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14171886 0.13646319 0.1595844 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14168983 0.13641002 0.15965446], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14166079 0.13635684 0.15972453], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14163174 0.13630367 0.1597946 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1416027  0.1362505  0.15986467], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14157365 0.13619732 0.15993475], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14154461 0.13614415 0.16000482], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14151556 0.13609097 0.1600749 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14148651 0.1360378  0.16014497], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14145745 0.13598463 0.16021505], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1414284  0.13593145 0.16028513], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14139934 0.13587828 0.16035522], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14137028 0.1358251  0.1604253 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14134122 0.13577193 0.16049539], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14131216 0.13571876 0.16056548], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14128309 0.13566558 0.16063556], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14125403 0.13561241 0.16070566], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14122496 0.13555924 0.16077575], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14119589 0.13550606 0.16084584], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14116682 0.13545289 0.16091594], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14113774 0.13539972 0.16098603], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14110867 0.13534654 0.16105613], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14107959 0.13529337 0.16112623], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14105051 0.1352402  0.16119633], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14102143 0.13518703 0.16126643], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14099235 0.13513385 0.16133654], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14096326 0.13508068 0.16140664], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14093417 0.13502751 0.16147675], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14090509 0.13497434 0.16154685], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.140876   0.13492117 0.16161696], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1408469  0.134868   0.16168707], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14081781 0.13481483 0.16175718], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14078872 0.13476166 0.1618273 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14075962 0.13470849 0.16189741], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14073052 0.13465532 0.16196753], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14070142 0.13460215 0.16203764], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14067232 0.13454898 0.16210776], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14064321 0.13449581 0.16217788], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14061411 0.13444264 0.162248  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.140585   0.13438948 0.16231812], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14055589 0.13433631 0.16238824], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14052678 0.13428314 0.16245836], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14049767 0.13422998 0.16252849], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 450: Weights: [0.30898455 0.28563765 0.40537776]\n",
            "Gradient risk: [0.14046856 0.13417681 0.16259861], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14043944 0.13412365 0.16266874], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14041032 0.13407048 0.16273887], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1403812  0.13401732 0.16280899], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14035208 0.13396416 0.16287912], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14032296 0.13391099 0.16294925], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14029384 0.13385783 0.16301939], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14026471 0.13380467 0.16308952], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14023558 0.13375151 0.16315965], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14020645 0.13369835 0.16322978], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14017732 0.13364519 0.16329992], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14014819 0.13359203 0.16337005], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14011906 0.13353887 0.16344019], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14008992 0.13348571 0.16351033], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14006079 0.13343255 0.16358047], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14003165 0.1333794  0.16365061], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14000251 0.13332624 0.16372075], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13997337 0.13327308 0.16379089], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13994422 0.13321993 0.16386103], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13991508 0.13316678 0.16393117], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13988593 0.13311362 0.16400131], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13985678 0.13306047 0.16407146], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13982764 0.13300732 0.1641416 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13979848 0.13295417 0.16421174], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13976933 0.13290102 0.16428189], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13974018 0.13284787 0.16435204], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13971102 0.13279472 0.16442218], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13968187 0.13274157 0.16449233], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13965271 0.13268843 0.16456248], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13962355 0.13263528 0.16463263], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13959439 0.13258213 0.16470278], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13956522 0.13252899 0.16477293], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13953606 0.13247585 0.16484308], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13950689 0.1324227  0.16491323], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13947773 0.13236956 0.16498338], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13944856 0.13231642 0.16505353], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13941939 0.13226328 0.16512368], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13939022 0.13221014 0.16519383], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13936104 0.13215701 0.16526399], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13933187 0.13210387 0.16533414], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13930269 0.13205074 0.16540429], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13927351 0.1319976  0.16547445], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13924434 0.13194447 0.1655446 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13921516 0.13189133 0.16561476], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13918597 0.1318382  0.16568491], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13915679 0.13178507 0.16575507], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13912761 0.13173194 0.16582522], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13909842 0.13167882 0.16589538], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13906923 0.13162569 0.16596553], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Gradient risk: [0.05333333 0.058      0.03133333], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 0: Weights: [0.33328113 0.33322937 0.33348947]\n",
            "Gradient risk: [0.05332134 0.0579793  0.03135426], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05330936 0.0579586  0.03137518], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05329737 0.05793791 0.0313961 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05328538 0.05791721 0.03141703], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05327338 0.05789651 0.03143796], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05326139 0.05787581 0.03145889], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0532494  0.0578551  0.03147982], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0532374  0.0578344  0.03150075], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05322541 0.0578137  0.03152168], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05321341 0.05779299 0.03154262], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05320141 0.05777229 0.03156355], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05318941 0.05775158 0.03158449], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05317741 0.05773087 0.03160543], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05316541 0.05771016 0.03162637], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05315341 0.05768945 0.03164731], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05314141 0.05766874 0.03166825], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05312941 0.05764803 0.03168919], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0531174  0.05762732 0.03171014], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0531054  0.05760661 0.03173108], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05309339 0.05758589 0.03175203], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05308138 0.05756518 0.03177298], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05306937 0.05754446 0.03179393], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05305737 0.05752374 0.03181488], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05304536 0.05750303 0.03183583], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05303334 0.05748231 0.03185678], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05302133 0.05746159 0.03187773], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05300932 0.05744087 0.03189869], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05299731 0.05742015 0.03191964], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05298529 0.05739943 0.0319406 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05297327 0.0573787  0.03196156], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05296126 0.05735798 0.03198252], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05294924 0.05733726 0.03200348], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05293722 0.05731653 0.03202444], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0529252  0.0572958  0.03204541], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05291318 0.05727508 0.03206637], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05290116 0.05725435 0.03208733], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05288914 0.05723362 0.0321083 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05287711 0.05721289 0.03212927], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05286509 0.05719216 0.03215024], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05285306 0.05717143 0.03217121], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05284104 0.0571507  0.03219218], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05282901 0.05712997 0.03221315], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05281698 0.05710923 0.03223412], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05280496 0.0570885  0.0322551 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05279293 0.05706776 0.03227607], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0527809  0.05704703 0.03229705], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05276886 0.05702629 0.03231803], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05275683 0.05700555 0.032339  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0527448  0.05698482 0.03235998], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05273276 0.05696408 0.03238096], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 50: Weights: [0.33066429 0.32802716 0.34130852]\n",
            "Gradient risk: [0.05272073 0.05694334 0.03240195], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05270869 0.0569226  0.03242293], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05269666 0.05690186 0.03244391], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05268462 0.05688112 0.0324649 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05267258 0.05686037 0.03248588], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05266054 0.05683963 0.03250687], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0526485  0.05681889 0.03252786], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05263646 0.05679814 0.03254885], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05262442 0.0567774  0.03256984], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05261238 0.05675665 0.03259083], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05260033 0.0567359  0.03261182], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05258829 0.05671516 0.03263281], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05257624 0.05669441 0.03265381], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0525642  0.05667366 0.0326748 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05255215 0.05665291 0.0326958 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0525401  0.05663216 0.03271679], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05252805 0.05661141 0.03273779], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.052516   0.05659066 0.03275879], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05250395 0.05656991 0.03277979], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0524919  0.05654915 0.03280079], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05247985 0.0565284  0.03282179], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0524678  0.05650765 0.03284279], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05245574 0.05648689 0.0328638 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05244369 0.05646614 0.0328848 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05243163 0.05644538 0.03290581], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05241958 0.05642463 0.03292681], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05240752 0.05640387 0.03294782], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05239546 0.05638311 0.03296883], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0523834  0.05636235 0.03298984], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05237134 0.05634159 0.03301085], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05235928 0.05632084 0.03303186], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05234722 0.05630008 0.03305287], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05233516 0.05627931 0.03307388], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0523231  0.05625855 0.0330949 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05231104 0.05623779 0.03311591], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05229897 0.05621703 0.03313693], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05228691 0.05619627 0.03315794], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05227484 0.0561755  0.03317896], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05226277 0.05615474 0.03319998], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05225071 0.05613397 0.033221  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05223864 0.05611321 0.03324202], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05222657 0.05609244 0.03326304], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0522145  0.05607168 0.03328406], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05220243 0.05605091 0.03330508], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05219036 0.05603014 0.03332611], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05217829 0.05600938 0.03334713], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05216622 0.05598861 0.03336816], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05215414 0.05596784 0.03338918], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05214207 0.05594707 0.03341021], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05212999 0.0559263  0.03343124], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 100: Weights: [0.3280347  0.32281781 0.34914746]\n",
            "Gradient risk: [0.05211792 0.05590553 0.03345226], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05210584 0.05588476 0.03347329], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05209376 0.05586399 0.03349432], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05208169 0.05584322 0.03351535], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05206961 0.05582245 0.03353638], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05205753 0.05580167 0.03355742], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05204545 0.0557809  0.03357845], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05203337 0.05576013 0.03359948], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05202129 0.05573935 0.03362052], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0520092  0.05571858 0.03364155], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05199712 0.0556978  0.03366259], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05198504 0.05567703 0.03368363], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05197295 0.05565625 0.03370466], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05196087 0.05563548 0.0337257 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05194878 0.0556147  0.03374674], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05193669 0.05559392 0.03376778], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05192461 0.05557315 0.03378882], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05191252 0.05555237 0.03380986], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05190043 0.05553159 0.0338309 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05188834 0.05551081 0.03385195], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05187625 0.05549003 0.03387299], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05186416 0.05546925 0.03389403], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05185207 0.05544847 0.03391508], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05183998 0.05542769 0.03393612], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05182788 0.05540691 0.03395717], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05181579 0.05538613 0.03397822], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0518037  0.05536535 0.03399926], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0517916  0.05534457 0.03402031], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05177951 0.05532379 0.03404136], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05176741 0.055303   0.03406241], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05175531 0.05528222 0.03408346], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05174322 0.05526144 0.03410451], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05173112 0.05524066 0.03412556], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05171902 0.05521987 0.03414662], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05170692 0.05519909 0.03416767], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05169482 0.0551783  0.03418872], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05168272 0.05515752 0.03420978], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05167062 0.05513673 0.03423083], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05165852 0.05511595 0.03425189], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05164641 0.05509516 0.03427294], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05163431 0.05507438 0.034294  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05162221 0.05505359 0.03431506], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0516101  0.05503281 0.03433611], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.051598   0.05501202 0.03435717], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05158589 0.05499123 0.03437823], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05157378 0.05497044 0.03439929], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05156168 0.05494966 0.03442035], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05154957 0.05492887 0.03444141], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05153746 0.05490808 0.03446247], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05152535 0.05488729 0.03448354], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 150: Weights: [0.32539373 0.31760451 0.35700173]\n",
            "Gradient risk: [0.05151324 0.05486651 0.0345046 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05150113 0.05484572 0.03452566], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05148902 0.05482493 0.03454672], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05147691 0.05480414 0.03456779], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0514648  0.05478335 0.03458885], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05145268 0.05476256 0.03460992], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05144057 0.05474177 0.03463098], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05142846 0.05472098 0.03465205], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05141634 0.05470019 0.03467312], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05140423 0.0546794  0.03469418], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05139211 0.05465861 0.03471525], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05138    0.05463782 0.03473632], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05136788 0.05461703 0.03475739], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05135576 0.05459624 0.03477846], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05134364 0.05457545 0.03479953], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05133152 0.05455465 0.0348206 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05131941 0.05453386 0.03484167], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05130729 0.05451307 0.03486274], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05129517 0.05449228 0.03488381], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05128304 0.05447149 0.03490489], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05127092 0.05445069 0.03492596], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0512588  0.0544299  0.03494703], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05124668 0.05440911 0.03496811], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05123456 0.05438832 0.03498918], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05122243 0.05436752 0.03501025], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05121031 0.05434673 0.03503133], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05119818 0.05432594 0.0350524 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05118606 0.05430515 0.03507348], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05117393 0.05428435 0.03509456], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05116181 0.05426356 0.03511563], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05114968 0.05424277 0.03513671], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05113755 0.05422197 0.03515779], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05112542 0.05420118 0.03517887], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0511133  0.05418039 0.03519994], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05110117 0.05415959 0.03522102], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05108904 0.0541388  0.0352421 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05107691 0.054118   0.03526318], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05106478 0.05409721 0.03528426], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05105265 0.05407642 0.03530534], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05104052 0.05405562 0.03532642], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05102838 0.05403483 0.0353475 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05101625 0.05401404 0.03536858], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05100412 0.05399324 0.03538967], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05099198 0.05397245 0.03541075], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05097985 0.05395165 0.03543183], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05096772 0.05393086 0.03545291], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05095558 0.05391007 0.035474  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05094345 0.05388927 0.03549508], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05093131 0.05386848 0.03551616], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05091917 0.05384768 0.03553725], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 200: Weights: [0.32274279 0.31239046 0.36486673]\n",
            "Gradient risk: [0.05090704 0.05382689 0.03555833], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0508949  0.05380609 0.03557942], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05088276 0.0537853  0.0356005 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05087062 0.05376451 0.03562159], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05085848 0.05374371 0.03564267], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05084635 0.05372292 0.03566376], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05083421 0.05370213 0.03568484], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05082207 0.05368133 0.03570593], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05080992 0.05366054 0.03572702], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05079778 0.05363974 0.03574811], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05078564 0.05361895 0.03576919], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0507735  0.05359816 0.03579028], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05076136 0.05357736 0.03581137], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05074922 0.05355657 0.03583246], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05073707 0.05353578 0.03585354], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05072493 0.05351498 0.03587463], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05071278 0.05349419 0.03589572], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05070064 0.0534734  0.03591681], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05068849 0.0534526  0.0359379 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05067635 0.05343181 0.03595899], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0506642  0.05341102 0.03598008], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05065206 0.05339023 0.03600117], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05063991 0.05336943 0.03602226], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05062776 0.05334864 0.03604335], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05061562 0.05332785 0.03606444], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05060347 0.05330706 0.03608553], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05059132 0.05328626 0.03610662], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05057917 0.05326547 0.03612771], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05056702 0.05324468 0.0361488 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05055487 0.05322389 0.03616989], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05054272 0.0532031  0.03619099], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05053057 0.05318231 0.03621208], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05051842 0.05316152 0.03623317], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05050627 0.05314072 0.03625426], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05049412 0.05311993 0.03627535], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05048197 0.05309914 0.03629645], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05046981 0.05307835 0.03631754], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05045766 0.05305756 0.03633863], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05044551 0.05303677 0.03635972], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05043336 0.05301598 0.03638082], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0504212  0.05299519 0.03640191], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05040905 0.0529744  0.036423  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05039689 0.05295362 0.0364441 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05038474 0.05293283 0.03646519], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05037258 0.05291204 0.03648628], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05036043 0.05289125 0.03650738], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05034827 0.05287046 0.03652847], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05033612 0.05284967 0.03654956], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05032396 0.05282889 0.03657066], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0503118  0.0528081  0.03659175], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 250: Weights: [0.32008326 0.30717887 0.37273784]\n",
            "Gradient risk: [0.05029964 0.05278731 0.03661285], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05028749 0.05276653 0.03663394], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05027533 0.05274574 0.03665503], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05026317 0.05272495 0.03667613], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05025101 0.05270417 0.03669722], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05023885 0.05268338 0.03671832], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05022669 0.0526626  0.03673941], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05021453 0.05264181 0.0367605 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05020237 0.05262103 0.0367816 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05019021 0.05260024 0.03680269], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05017805 0.05257946 0.03682379], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05016589 0.05255867 0.03684488], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05015373 0.05253789 0.03686598], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05014157 0.05251711 0.03688707], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05012941 0.05249632 0.03690817], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05011725 0.05247554 0.03692926], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05010508 0.05245476 0.03695035], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05009292 0.05243398 0.03697145], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05008076 0.0524132  0.03699254], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05006859 0.05239241 0.03701364], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05005643 0.05237163 0.03703473], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05004427 0.05235085 0.03705582], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0500321  0.05233007 0.03707692], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05001994 0.05230929 0.03709801], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05000777 0.05228851 0.03711911], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04999561 0.05226774 0.0371402 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04998344 0.05224696 0.0371613 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04997128 0.05222618 0.03718239], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04995911 0.0522054  0.03720348], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04994695 0.05218462 0.03722458], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04993478 0.05216385 0.03724567], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04992261 0.05214307 0.03726676], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04991045 0.05212229 0.03728786], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04989828 0.05210152 0.03730895], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04988611 0.05208074 0.03733004], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04987394 0.05205997 0.03735114], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04986178 0.0520392  0.03737223], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04984961 0.05201842 0.03739332], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04983744 0.05199765 0.03741442], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04982527 0.05197688 0.03743551], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0498131 0.0519561 0.0374566], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04980093 0.05193533 0.03747769], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04978877 0.05191456 0.03749879], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0497766  0.05189379 0.03751988], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04976443 0.05187302 0.03754097], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04975226 0.05185225 0.03756206], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04974009 0.05183148 0.03758315], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04972792 0.05181071 0.03760424], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04971575 0.05178994 0.03762534], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04970358 0.05176917 0.03764643], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 300: Weights: [0.31741657 0.30197297 0.38061043]\n",
            "Gradient risk: [0.0496914  0.05174841 0.03766752], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04967923 0.05172764 0.03768861], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04966706 0.05170687 0.0377097 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04965489 0.05168611 0.03773079], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04964272 0.05166534 0.03775188], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04963055 0.05164458 0.03777297], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04961837 0.05162381 0.03779406], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0496062  0.05160305 0.03781515], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04959403 0.05158228 0.03783624], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04958186 0.05156152 0.03785733], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04956968 0.05154076 0.03787842], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04955751 0.05152    0.03789951], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04954534 0.05149924 0.03792059], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04953317 0.05147848 0.03794168], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04952099 0.05145772 0.03796277], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04950882 0.05143696 0.03798386], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04949664 0.0514162  0.03800495], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04948447 0.05139544 0.03802603], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0494723  0.05137468 0.03804712], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04946012 0.05135393 0.03806821], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04944795 0.05133317 0.03808929], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04943577 0.05131242 0.03811038], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0494236  0.05129166 0.03813146], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04941142 0.05127091 0.03815255], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04939925 0.05125015 0.03817364], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04938707 0.0512294  0.03819472], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0493749  0.05120865 0.0382158 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04936272 0.0511879  0.03823689], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04935055 0.05116715 0.03825797], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04933837 0.0511464  0.03827906], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0493262  0.05112565 0.03830014], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04931402 0.0511049  0.03832122], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04930184 0.05108415 0.0383423 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04928967 0.0510634  0.03836339], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04927749 0.05104265 0.03838447], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04926532 0.05102191 0.03840555], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04925314 0.05100116 0.03842663], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04924096 0.05098042 0.03844771], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04922879 0.05095967 0.03846879], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04921661 0.05093893 0.03848987], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04920443 0.05091819 0.03851095], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04919226 0.05089745 0.03853203], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04918008 0.0508767  0.03855311], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0491679  0.05085596 0.03857419], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04915572 0.05083522 0.03859527], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04914355 0.05081448 0.03861635], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04913137 0.05079375 0.03863742], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04911919 0.05077301 0.0386585 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04910701 0.05075227 0.03867958], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04909484 0.05073154 0.03870065], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 350: Weights: [0.31474413 0.29677596 0.38847987]\n",
            "Gradient risk: [0.04908266 0.0507108  0.03872173], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04907048 0.05069007 0.0387428 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0490583  0.05066933 0.03876388], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04904613 0.0506486  0.03878495], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04903395 0.05062787 0.03880603], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04902177 0.05060714 0.0388271 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04900959 0.0505864  0.03884817], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04899741 0.05056568 0.03886925], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04898524 0.05054495 0.03889032], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04897306 0.05052422 0.03891139], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04896088 0.05050349 0.03893246], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0489487  0.05048276 0.03895353], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04893652 0.05046204 0.0389746 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04892434 0.05044131 0.03899567], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04891217 0.05042059 0.03901674], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04889999 0.05039987 0.03903781], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04888781 0.05037914 0.03905888], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04887563 0.05035842 0.03907994], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04886345 0.0503377  0.03910101], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04885127 0.05031698 0.03912208], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0488391  0.05029626 0.03914314], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04882692 0.05027555 0.03916421], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04881474 0.05025483 0.03918527], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04880256 0.05023411 0.03920634], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04879038 0.0502134  0.0392274 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0487782  0.05019268 0.03924847], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04876602 0.05017197 0.03926953], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04875385 0.05015126 0.03929059], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04874167 0.05013054 0.03931165], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04872949 0.05010983 0.03933271], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04871731 0.05008912 0.03935377], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04870513 0.05006841 0.03937483], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04869295 0.05004771 0.03939589], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04868078 0.050027   0.03941695], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0486686  0.05000629 0.03943801], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04865642 0.04998559 0.03945907], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04864424 0.04996488 0.03948012], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04863206 0.04994418 0.03950118], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04861988 0.04992348 0.03952224], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04860771 0.04990278 0.03954329], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04859553 0.04988208 0.03956435], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04858335 0.04986138 0.0395854 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04857117 0.04984068 0.03960645], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04855899 0.04981998 0.0396275 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04854682 0.04979928 0.03964856], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04853464 0.04977859 0.03966961], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04852246 0.04975789 0.03969066], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04851028 0.0497372  0.03971171], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0484981  0.04971651 0.03973276], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04848593 0.04969581 0.03975381], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 400: Weights: [0.31206738 0.29159105 0.39634154]\n",
            "Gradient risk: [0.04847375 0.04967512 0.03977485], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04846157 0.04965443 0.0397959 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04844939 0.04963375 0.03981695], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04843722 0.04961306 0.03983799], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04842504 0.04959237 0.03985904], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04841286 0.04957169 0.03988008], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04840069 0.049551   0.03990113], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04838851 0.04953032 0.03992217], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04837633 0.04950964 0.03994321], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04836415 0.04948896 0.03996425], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04835198 0.04946828 0.0399853 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0483398  0.0494476  0.04000634], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04832762 0.04942692 0.04002737], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04831545 0.04940624 0.04004841], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04830327 0.04938557 0.04006945], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0482911  0.04936489 0.04009049], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04827892 0.04934422 0.04011153], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04826674 0.04932355 0.04013256], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04825457 0.04930287 0.0401536 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04824239 0.0492822  0.04017463], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04823022 0.04926153 0.04019566], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04821804 0.04924087 0.0402167 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04820587 0.0492202  0.04023773], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04819369 0.04919953 0.04025876], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04818151 0.04917887 0.04027979], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04816934 0.04915821 0.04030082], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04815717 0.04913754 0.04032185], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04814499 0.04911688 0.04034287], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04813282 0.04909622 0.0403639 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04812064 0.04907556 0.04038493], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04810847 0.0490549  0.04040595], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04809629 0.04903425 0.04042698], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04808412 0.04901359 0.040448  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04807195 0.04899294 0.04046902], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04805977 0.04897229 0.04049005], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0480476  0.04895163 0.04051107], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04803542 0.04893098 0.04053209], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04802325 0.04891033 0.04055311], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04801108 0.04888969 0.04057413], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04799891 0.04886904 0.04059514], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04798673 0.04884839 0.04061616], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04797456 0.04882775 0.04063718], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04796239 0.0488071  0.04065819], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04795022 0.04878646 0.04067921], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04793804 0.04876582 0.04070022], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04792587 0.04874518 0.04072123], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0479137  0.04872454 0.04074224], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04790153 0.04870391 0.04076325], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04788936 0.04868327 0.04078426], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04787719 0.04866264 0.04080527], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 450: Weights: [0.30938774 0.2864214  0.40419083]\n",
            "Gradient risk: [0.04786502 0.048642   0.04082628], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04785285 0.04862137 0.04084729], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04784068 0.04860074 0.04086829], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04782851 0.04858011 0.0408893 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04781634 0.04855948 0.0409103 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04780417 0.04853885 0.04093131], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.047792   0.04851823 0.04095231], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04777983 0.0484976  0.04097331], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04776766 0.04847698 0.04099431], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04775549 0.04845636 0.04101531], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04774332 0.04843574 0.04103631], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04773115 0.04841512 0.0410573 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04771899 0.0483945  0.0410783 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04770682 0.04837389 0.0410993 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04769465 0.04835327 0.04112029], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04768248 0.04833266 0.04114128], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04767032 0.04831204 0.04116228], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04765815 0.04829143 0.04118327], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04764598 0.04827082 0.04120426], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04763382 0.04825022 0.04122525], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04762165 0.04822961 0.04124624], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04760948 0.048209   0.04126722], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04759732 0.0481884  0.04128821], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04758515 0.0481678  0.0413092 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04757299 0.04814719 0.04133018], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04756082 0.04812659 0.04135116], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04754866 0.04810599 0.04137215], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0475365  0.0480854  0.04139313], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04752433 0.0480648  0.04141411], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04751217 0.04804421 0.04143509], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0475     0.04802361 0.04145606], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04748784 0.04800302 0.04147704], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04747568 0.04798243 0.04149802], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04746352 0.04796184 0.04151899], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04745135 0.04794126 0.04153997], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04743919 0.04792067 0.04156094], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04742703 0.04790008 0.04158191], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04741487 0.0478795  0.04160288], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04740271 0.04785892 0.04162385], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04739055 0.04783834 0.04164482], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04737839 0.04781776 0.04166578], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04736623 0.04779718 0.04168675], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04735407 0.04777661 0.04170771], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04734191 0.04775603 0.04172868], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04732975 0.04773546 0.04174964], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04731759 0.04771489 0.0417706 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04730543 0.04769432 0.04179156], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04729327 0.04767375 0.04181252], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04728112 0.04765318 0.04183348], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Stress Test Results: {'Black Swan': array([0.30404941, 0.27611053, 0.41984003]), 'Liquidity Crisis': array([0.30627312, 0.28042857, 0.41329828]), 'Interest Rate Shock': array([0.30676026, 0.28137299, 0.41186672])}\n",
            "Fat-Tailed Returns Simulation (First 5 Samples): [[ 0.0443474   0.04927412 -0.03397333]\n",
            " [ 0.04065603  0.00568087 -0.01220843]\n",
            " [ 0.02260453  0.03754705 -0.00629874]\n",
            " [ 0.00967396  0.00759283 -0.00295339]\n",
            " [ 0.05874762  0.04475414 -0.01583817]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from deap import base, creator, tools, algorithms\n",
        "import random\n",
        "\n",
        "# Create a text file to save all the outputs (appending this time)\n",
        "log_file = open(\"portfolio_optimization_results.txt\", \"a\")\n",
        "\n",
        "# Function to log output to the text file\n",
        "def log_output(output):\n",
        "    log_file.write(output + \"\\n\")\n",
        "    print(output)  # Also print to the console for real-time monitoring\n",
        "\n",
        "# -------------------------------\n",
        "# Portfolio Optimization with Simple Gradient Descent\n",
        "# -------------------------------\n",
        "\n",
        "def optimize_weights(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, lr=1e-2, epochs=500):\n",
        "    \"\"\"\n",
        "    Simple gradient descent for portfolio optimization\n",
        "    - Sigma: Covariance matrix\n",
        "    - mu: Expected returns\n",
        "    - lam: Risk-return tradeoff parameter\n",
        "    - gamma: Sparsity regularization parameter\n",
        "    - eta: Magnitude regularization parameter\n",
        "    - lr: Learning rate\n",
        "    - epochs: Number of iterations\n",
        "    \"\"\"\n",
        "    w = np.ones(Sigma.shape[0]) / Sigma.shape[0]  # Equal distribution of weights\n",
        "\n",
        "    log_output(f\"Initial weights: {w}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        grad = grad_objective(w, Sigma, mu, lam, eta)\n",
        "        w -= lr * grad\n",
        "\n",
        "        # Apply Proximal Operator for L0 (sparsity)\n",
        "        w[np.abs(w) < gamma] = 0\n",
        "\n",
        "        # Ensure non-negative weights and sum to 1\n",
        "        w = np.maximum(w, 0)\n",
        "        w /= np.sum(w + 1e-8)\n",
        "\n",
        "        if epoch % 50 == 0:  # Log every 50th iteration\n",
        "            log_output(f\"Epoch {epoch}: Weights: {w}\")\n",
        "\n",
        "    return w\n",
        "\n",
        "# -------------------------------\n",
        "# Gradient of the Objective Function\n",
        "# -------------------------------\n",
        "\n",
        "def grad_objective(w, Sigma, mu, lam, eta):\n",
        "    \"\"\"\n",
        "    Gradient of the risk-return objective function with L0 and L1 penalties\n",
        "    \"\"\"\n",
        "    grad_risk = 2 * Sigma @ w  # Risk (variance)\n",
        "    grad_ret = lam * mu  # Return (expected return)\n",
        "    grad_l1 = eta * np.sign(w)  # L1 penalty for magnitude regularization\n",
        "\n",
        "    log_output(f\"Gradient risk: {grad_risk}, Gradient return: {grad_ret}, Gradient L1: {grad_l1}\")\n",
        "\n",
        "    return grad_risk - grad_ret + grad_l1\n",
        "\n",
        "# -------------------------------\n",
        "# Stress Testing\n",
        "# -------------------------------\n",
        "\n",
        "def extreme_market_scenarios(Sigma, mu, scenarios):\n",
        "    \"\"\"\n",
        "    Perform stress tests by modifying Sigma and mu to simulate market scenarios.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for scenario in scenarios:\n",
        "        stressed_Sigma = Sigma + scenario['sigma_change']\n",
        "        stressed_mu = mu + scenario['mu_change']\n",
        "        optimized_weights = optimize_weights(stressed_Sigma, stressed_mu)\n",
        "        results[scenario['name']] = optimized_weights\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------------------\n",
        "# Simulate Fat-Tailed Returns (Student's T-distribution)\n",
        "# -------------------------------\n",
        "\n",
        "def simulate_fat_tailed_returns(Sigma, mu, dof=4, n_samples=1000):\n",
        "    \"\"\"\n",
        "    Simulate returns from a fat-tailed distribution (Student's t-distribution).\n",
        "    \"\"\"\n",
        "    t_returns = np.random.standard_t(dof, size=(n_samples, len(mu)))\n",
        "    fat_tailed_returns = np.dot(t_returns, Sigma) + mu\n",
        "    return fat_tailed_returns\n",
        "\n",
        "# -------------------------------\n",
        "# Hyperparameter Tuning (Grid Search)\n",
        "# -------------------------------\n",
        "\n",
        "def hyperparameter_tuning():\n",
        "    param_grid = {\n",
        "        'gamma': [1e-4, 1e-3, 1e-2],\n",
        "        'eta': [1e-4, 1e-3, 1e-2],\n",
        "    }\n",
        "    grid_search = GridSearchCV(estimator=PortfolioOptimizer(), param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    log_output(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Genetic Algorithm for Optimization\n",
        "# -------------------------------\n",
        "\n",
        "def genetic_algorithm_optimizer(Sigma, mu):\n",
        "    # Genetic algorithm code here\n",
        "    # Create the individual structure and evaluate fitness\n",
        "\n",
        "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "    toolbox = base.Toolbox()\n",
        "    toolbox.register(\"attr_float\", random.random)\n",
        "    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=len(Sigma))\n",
        "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "    toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
        "    toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
        "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "    toolbox.register(\"evaluate\", lambda w: objective(w, Sigma, mu, 0.1, 1e-2, 1e-2))\n",
        "\n",
        "    population = toolbox.population(n=10)\n",
        "    # Run the genetic algorithm steps\n",
        "    # Apply crossover, mutation, and selection\n",
        "\n",
        "# -------------------------------\n",
        "# Monte Carlo Simulation for Large Portfolio (with Adam optimizer)\n",
        "# -------------------------------\n",
        "\n",
        "def monte_carlo_testing_large(Sigma, mu, n_samples=1000, n_assets=1000, lr=1e-3, epochs=500):\n",
        "    w_init = np.ones(n_assets) / n_assets\n",
        "    for sample in range(n_samples):\n",
        "        # Perform Monte Carlo simulation and optimization for large portfolio\n",
        "        pass  # Implement Monte Carlo logic\n",
        "\n",
        "    return w_init\n",
        "\n",
        "# -------------------------------\n",
        "# Running the Optimization and Stress Tests\n",
        "# -------------------------------\n",
        "\n",
        "# Example Data (Covariance matrix and expected returns)\n",
        "Sigma = np.array([[0.02, 0.015, -0.005], [0.015, 0.03, -0.008], [-0.005, -0.008, 0.01]])\n",
        "mu = np.array([0.01, 0.005, -0.002])  # Expected returns for 3 assets\n",
        "\n",
        "# Run the Portfolio Optimization\n",
        "optimized_weights = optimize_weights(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2)\n",
        "\n",
        "log_output(f\"Optimized Weights: {optimized_weights}\")\n",
        "\n",
        "# Stress Test Scenarios (market conditions)\n",
        "extreme_scenarios = [\n",
        "    {'name': 'Black Swan', 'sigma_change': 0.3 * np.identity(3), 'mu_change': -0.1 * np.ones(3)},\n",
        "    {'name': 'Liquidity Crisis', 'sigma_change': 0.2 * np.identity(3), 'mu_change': 0.0 * np.ones(3)},\n",
        "    {'name': 'Interest Rate Shock', 'sigma_change': 0.05 * np.identity(3), 'mu_change': 0.02 * np.ones(3)}\n",
        "]\n",
        "\n",
        "# Perform Stress Tests\n",
        "stress_test_results = extreme_market_scenarios(Sigma, mu, extreme_scenarios)\n",
        "\n",
        "log_output(f\"Stress Test Results: {stress_test_results}\")\n",
        "\n",
        "# Simulate Fat-Tailed Returns\n",
        "fat_tailed_returns = simulate_fat_tailed_returns(Sigma, mu)\n",
        "\n",
        "log_output(f\"Fat-Tailed Returns Simulation (First 5 Samples): {fat_tailed_returns[:5]}\")\n",
        "\n",
        "# Close the log file after writing all results\n",
        "log_file.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NderJYSXmuvY",
        "outputId": "330e9159-5352-4833-e770-3042b467b8be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Gradient risk: [ 0.02        0.02466667 -0.002     ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 0: Weights: [0.33328118 0.33322947 0.33348932]\n",
            "Gradient risk: [ 0.01999324  0.02465637 -0.0019947 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01998648  0.02464608 -0.00198939], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01997971  0.02463579 -0.00198409], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01997295  0.02462549 -0.00197878], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01996618  0.0246152  -0.00197348], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01995942  0.0246049  -0.00196817], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01995265  0.0245946  -0.00196286], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01994588  0.0245843  -0.00195756], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01993912  0.024574   -0.00195225], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01993235  0.0245637  -0.00194694], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01992558  0.02455339 -0.00194163], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01991881  0.02454309 -0.00193632], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01991203  0.02453278 -0.00193101], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01990526  0.02452248 -0.0019257 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01989849  0.02451217 -0.00192039], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01989171  0.02450186 -0.00191507], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01988494  0.02449155 -0.00190976], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01987816  0.02448124 -0.00190445], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01987139  0.02447092 -0.00189913], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01986461  0.02446061 -0.00189382], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01985783  0.0244503  -0.0018885 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01985105  0.02443998 -0.00188319], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01984427  0.02442966 -0.00187787], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01983749  0.02441934 -0.00187255], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01983071  0.02440903 -0.00186723], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01982393  0.0243987  -0.00186192], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01981714  0.02438838 -0.0018566 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01981036  0.02437806 -0.00185128], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01980358  0.02436774 -0.00184596], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01979679  0.02435741 -0.00184063], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01979     0.02434708 -0.00183531], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01978322  0.02433676 -0.00182999], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01977643  0.02432643 -0.00182467], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01976964  0.0243161  -0.00181934], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01976285  0.02430577 -0.00181402], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01975606  0.02429544 -0.0018087 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01974927  0.0242851  -0.00180337], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01974248  0.02427477 -0.00179804], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01973568  0.02426443 -0.00179272], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01972889  0.0242541  -0.00178739], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0197221   0.02424376 -0.00178206], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0197153   0.02423342 -0.00177674], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0197085   0.02422308 -0.00177141], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01970171  0.02421274 -0.00176608], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01969491  0.0242024  -0.00176075], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01968811  0.02419206 -0.00175542], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01968131  0.02418171 -0.00175009], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01967451  0.02417137 -0.00174475], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01966771  0.02416102 -0.00173942], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01966091  0.02415068 -0.00173409], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 50: Weights: [0.33066279 0.32802419 0.34131299]\n",
            "Gradient risk: [ 0.01965411  0.02414033 -0.00172876], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0196473   0.02412998 -0.00172342], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0196405   0.02411963 -0.00171809], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0196337   0.02410928 -0.00171275], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01962689  0.02409892 -0.00170742], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01962008  0.02408857 -0.00170208], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01961328  0.02407822 -0.00169674], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01960647  0.02406786 -0.00169141], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01959966  0.0240575  -0.00168607], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01959285  0.02404715 -0.00168073], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01958604  0.02403679 -0.00167539], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01957923  0.02402643 -0.00167005], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01957242  0.02401607 -0.00166471], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01956561  0.0240057  -0.00165937], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01955879  0.02399534 -0.00165403], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01955198  0.02398498 -0.00164869], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01954517  0.02397461 -0.00164334], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01953835  0.02396425 -0.001638  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01953153  0.02395388 -0.00163266], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01952472  0.02394351 -0.00162731], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0195179   0.02393314 -0.00162197], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01951108  0.02392277 -0.00161662], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01950426  0.0239124  -0.00161128], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01949744  0.02390203 -0.00160593], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01949062  0.02389165 -0.00160058], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0194838   0.02388128 -0.00159523], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01947698  0.0238709  -0.00158989], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01947016  0.02386053 -0.00158454], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01946333  0.02385015 -0.00157919], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01945651  0.02383977 -0.00157384], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01944968  0.02382939 -0.00156849], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01944286  0.02381901 -0.00156314], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01943603  0.02380863 -0.00155779], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0194292   0.02379825 -0.00155244], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01942238  0.02378786 -0.00154708], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01941555  0.02377748 -0.00154173], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01940872  0.0237671  -0.00153638], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01940189  0.02375671 -0.00153102], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01939506  0.02374632 -0.00152567], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01938823  0.02373593 -0.00152031], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01938139  0.02372554 -0.00151496], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01937456  0.02371515 -0.0015096 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01936773  0.02370476 -0.00150424], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01936089  0.02369437 -0.00149889], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01935406  0.02368398 -0.00149353], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01934722  0.02367358 -0.00148817], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01934039  0.02366319 -0.00148281], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01933355  0.02365279 -0.00147745], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01932671  0.02364239 -0.00147209], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01931987  0.023632   -0.00146673], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 100: Weights: [0.32802376 0.3227961  0.34918011]\n",
            "Gradient risk: [ 0.01931303  0.0236216  -0.00146137], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01930619  0.0236112  -0.00145601], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01929935  0.0236008  -0.00145065], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01929251  0.02359039 -0.00144529], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01928567  0.02357999 -0.00143992], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01927883  0.02356959 -0.00143456], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01927198  0.02355918 -0.0014292 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01926514  0.02354878 -0.00142383], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01925829  0.02353837 -0.00141847], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01925145  0.02352796 -0.0014131 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0192446   0.02351756 -0.00140774], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01923775  0.02350715 -0.00140237], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01923091  0.02349674 -0.001397  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01922406  0.02348632 -0.00139164], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01921721  0.02347591 -0.00138627], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01921036  0.0234655  -0.0013809 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01920351  0.02345509 -0.00137553], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01919666  0.02344467 -0.00137016], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01918981  0.02343425 -0.00136479], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01918295  0.02342384 -0.00135942], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0191761   0.02341342 -0.00135405], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01916925  0.023403   -0.00134868], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01916239  0.02339258 -0.00134331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01915554  0.02338216 -0.00133794], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01914868  0.02337174 -0.00133256], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01914182  0.02336132 -0.00132719], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01913497  0.0233509  -0.00132182], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01912811  0.02334047 -0.00131644], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01912125  0.02333005 -0.00131107], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01911439  0.02331962 -0.00130569], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01910753  0.02330919 -0.00130032], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01910067  0.02329877 -0.00129494], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01909381  0.02328834 -0.00128956], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01908695  0.02327791 -0.00128419], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01908009  0.02326748 -0.00127881], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01907322  0.02325705 -0.00127343], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01906636  0.02324662 -0.00126805], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0190595   0.02323618 -0.00126267], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01905263  0.02322575 -0.00125729], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01904577  0.02321532 -0.00125191], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0190389   0.02320488 -0.00124653], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01903203  0.02319444 -0.00124115], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01902516  0.02318401 -0.00123577], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0190183   0.02317357 -0.00123039], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01901143  0.02316313 -0.00122501], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01900456  0.02315269 -0.00121962], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01899769  0.02314225 -0.00121424], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01899082  0.02313181 -0.00120886], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01898395  0.02312137 -0.00120347], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01897707  0.02311092 -0.00119809], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 150: Weights: [0.32536536 0.31754834 0.35708627]\n",
            "Gradient risk: [ 0.0189702   0.02310048 -0.0011927 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01896333  0.02309004 -0.00118732], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01895645  0.02307959 -0.00118193], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01894958  0.02306914 -0.00117654], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01894271  0.0230587  -0.00117116], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01893583  0.02304825 -0.00116577], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01892895  0.0230378  -0.00116038], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01892208  0.02302735 -0.00115499], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0189152  0.0230169 -0.0011496], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01890832  0.02300645 -0.00114422], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01890144  0.022996   -0.00113883], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01889456  0.02298554 -0.00113344], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01888768  0.02297509 -0.00112805], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0188808   0.02296464 -0.00112265], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01887392  0.02295418 -0.00111726], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01886704  0.02294373 -0.00111187], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01886016  0.02293327 -0.00110648], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01885327  0.02292281 -0.00110109], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01884639  0.02291235 -0.00109569], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01883951  0.02290189 -0.0010903 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01883262  0.02289143 -0.00108491], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01882574  0.02288097 -0.00107951], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01881885  0.02287051 -0.00107412], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01881196  0.02286005 -0.00106872], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01880508  0.02284959 -0.00106333], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01879819  0.02283912 -0.00105793], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0187913   0.02282866 -0.00105253], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01878441  0.02281819 -0.00104714], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01877752  0.02280773 -0.00104174], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01877063  0.02279726 -0.00103634], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01876374  0.02278679 -0.00103094], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01875685  0.02277632 -0.00102555], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01874996  0.02276585 -0.00102015], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01874307  0.02275538 -0.00101475], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01873617  0.02274491 -0.00100935], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01872928  0.02273444 -0.00100395], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01872239  0.02272397 -0.00099855], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01871549  0.0227135  -0.00099315], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0187086   0.02270302 -0.00098774], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0187017   0.02269255 -0.00098234], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0186948   0.02268207 -0.00097694], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01868791  0.0226716  -0.00097154], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01868101  0.02266112 -0.00096613], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01867411  0.02265064 -0.00096073], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01866721  0.02264016 -0.00095533], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01866031  0.02262969 -0.00094992], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01865341  0.02261921 -0.00094452], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01864651  0.02260873 -0.00093911], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01863961  0.02259824 -0.00093371], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01863271  0.02258776 -0.0009283 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 200: Weights: [0.32268892 0.31228408 0.36502697]\n",
            "Gradient risk: [ 0.01862581  0.02257728 -0.00092289], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01861891  0.0225668  -0.00091749], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.018612    0.02255631 -0.00091208], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0186051   0.02254583 -0.00090667], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0185982   0.02253534 -0.00090127], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01859129  0.02252486 -0.00089586], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01858439  0.02251437 -0.00089045], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01857748  0.02250388 -0.00088504], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01857057  0.0224934  -0.00087963], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01856367  0.02248291 -0.00087422], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01855676  0.02247242 -0.00086881], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01854985  0.02246193 -0.0008634 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01854294  0.02245144 -0.00085799], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01853604  0.02244095 -0.00085258], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01852913  0.02243045 -0.00084717], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01852222  0.02241996 -0.00084175], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01851531  0.02240947 -0.00083634], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01850839  0.02239897 -0.00083093], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01850148  0.02238848 -0.00082552], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01849457  0.02237798 -0.0008201 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01848766  0.02236749 -0.00081469], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01848075  0.02235699 -0.00080928], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01847383  0.02234649 -0.00080386], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01846692  0.02233599 -0.00079845], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01846     0.02232549 -0.00079303], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01845309  0.022315   -0.00078761], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01844617  0.0223045  -0.0007822 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01843926  0.02229399 -0.00077678], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01843234  0.02228349 -0.00077137], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01842542  0.02227299 -0.00076595], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01841851  0.02226249 -0.00076053], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01841159  0.02225198 -0.00075511], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01840467  0.02224148 -0.0007497 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01839775  0.02223098 -0.00074428], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01839083  0.02222047 -0.00073886], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01838391  0.02220997 -0.00073344], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01837699  0.02219946 -0.00072802], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01837007  0.02218895 -0.0007226 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01836315  0.02217844 -0.00071718], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01835623  0.02216794 -0.00071176], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0183493   0.02215743 -0.00070634], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01834238  0.02214692 -0.00070092], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01833546  0.02213641 -0.00069549], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01832853  0.0221259  -0.00069007], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01832161  0.02211539 -0.00068465], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01831468  0.02210487 -0.00067923], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01830776  0.02209436 -0.00067381], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01830083  0.02208385 -0.00066838], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01829391  0.02207333 -0.00066296], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01828698  0.02206282 -0.00065753], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 250: Weights: [0.31999578 0.30700655 0.37299764]\n",
            "Gradient risk: [ 0.01828005  0.0220523  -0.00065211], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01827312  0.02204179 -0.00064669], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0182662   0.02203127 -0.00064126], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01825927  0.02202076 -0.00063584], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01825234  0.02201024 -0.00063041], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01824541  0.02199972 -0.00062498], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01823848  0.0219892  -0.00061956], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01823155  0.02197868 -0.00061413], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01822462  0.02196816 -0.0006087 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01821769  0.02195764 -0.00060328], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01821075  0.02194712 -0.00059785], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01820382  0.0219366  -0.00059242], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01819689  0.02192608 -0.00058699], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01818996  0.02191556 -0.00058157], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01818302  0.02190504 -0.00057614], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01817609  0.02189451 -0.00057071], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01816915  0.02188399 -0.00056528], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01816222  0.02187346 -0.00055985], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01815528  0.02186294 -0.00055442], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01814835  0.02185241 -0.00054899], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01814141  0.02184189 -0.00054356], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01813447  0.02183136 -0.00053813], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01812754  0.02182083 -0.0005327 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0181206   0.0218103  -0.00052726], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01811366  0.02179978 -0.00052183], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01810672  0.02178925 -0.0005164 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01809978  0.02177872 -0.00051097], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01809284  0.02176819 -0.00050554], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01808591  0.02175766 -0.0005001 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01807896  0.02174713 -0.00049467], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01807202  0.0217366  -0.00048924], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01806508  0.02172606 -0.0004838 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01805814  0.02171553 -0.00047837], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0180512   0.021705   -0.00047293], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01804426  0.02169447 -0.0004675 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01803732  0.02168393 -0.00046206], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01803037  0.0216734  -0.00045663], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01802343  0.02166286 -0.00045119], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01801649  0.02165233 -0.00044576], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01800954  0.02164179 -0.00044032], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0180026   0.02163125 -0.00043488], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01799565  0.02162072 -0.00042945], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01798871  0.02161018 -0.00042401], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01798176  0.02159964 -0.00041857], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01797481  0.0215891  -0.00041314], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01796787  0.02157856 -0.0004077 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01796092  0.02156803 -0.00040226], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01795397  0.02155749 -0.00039682], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01794702  0.02154695 -0.00039138], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01794008  0.02153641 -0.00038594], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 300: Weights: [0.31728733 0.30171903 0.38099361]\n",
            "Gradient risk: [ 0.01793313  0.02152586 -0.00038051], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01792618  0.02151532 -0.00037507], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01791923  0.02150478 -0.00036963], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01791228  0.02149424 -0.00036419], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01790533  0.0214837  -0.00035875], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01789838  0.02147315 -0.00035331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01789143  0.02146261 -0.00034787], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01788448  0.02145206 -0.00034242], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01787753  0.02144152 -0.00033698], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01787057  0.02143097 -0.00033154], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01786362  0.02142043 -0.0003261 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01785667  0.02140988 -0.00032066], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01784972  0.02139934 -0.00031522], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01784276  0.02138879 -0.00030977], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01783581  0.02137824 -0.00030433], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01782885  0.02136769 -0.00029889], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0178219   0.02135715 -0.00029345], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01781494  0.0213466  -0.000288  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01780799  0.02133605 -0.00028256], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01780103  0.0213255  -0.00027711], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01779408  0.02131495 -0.00027167], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01778712  0.0213044  -0.00026623], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01778016  0.02129385 -0.00026078], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01777321  0.0212833  -0.00025534], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01776625  0.02127275 -0.00024989], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01775929  0.02126219 -0.00024445], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01775233  0.02125164 -0.000239  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01774538  0.02124109 -0.00023356], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01773842  0.02123054 -0.00022811], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01773146  0.02121998 -0.00022266], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0177245   0.02120943 -0.00021722], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01771754  0.02119887 -0.00021177], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01771058  0.02118832 -0.00020632], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01770362  0.02117776 -0.00020088], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01769666  0.02116721 -0.00019543], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0176897   0.02115665 -0.00018998], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01768274  0.0211461  -0.00018453], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01767577  0.02113554 -0.00017909], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01766881  0.02112498 -0.00017364], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01766185  0.02111443 -0.00016819], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01765489  0.02110387 -0.00016274], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01764792  0.02109331 -0.00015729], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01764096  0.02108275 -0.00015184], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.017634    0.0210722  -0.00014639], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01762703  0.02106164 -0.00014094], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01762007  0.02105108 -0.00013549], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.0176131   0.02104052 -0.00013004], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01760614  0.02102996 -0.00012459], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01759917  0.0210194  -0.00011914], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01759221  0.02100884 -0.00011369], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 350: Weights: [0.31456498 0.29642481 0.38901018]\n",
            "Gradient risk: [ 0.01758524  0.02099828 -0.00010824], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 0.01757828  0.02098771 -0.00010279], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75713092e-02  2.09771524e-02 -9.73413823e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75643425e-02  2.09665905e-02 -9.18901440e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75573754e-02  2.09560282e-02 -8.64387114e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75504081e-02  2.09454656e-02 -8.09870858e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75434404e-02  2.09349027e-02 -7.55352685e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75364725e-02  2.09243394e-02 -7.00832608e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75295043e-02  2.09137759e-02 -6.46310640e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75225357e-02  2.09032120e-02 -5.91786795e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75155669e-02  2.08926478e-02 -5.37261085e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75085978e-02  2.08820834e-02 -4.82733524e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.75016285e-02  2.08715186e-02 -4.28204125e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.74946588e-02  2.08609535e-02 -3.73672900e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.74876889e-02  2.08503881e-02 -3.19139863e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.74807186e-02  2.08398224e-02 -2.64605027e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.74737481e-02  2.08292565e-02 -2.10068404e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.74667773e-02  2.08186902e-02 -1.55530009e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.74598063e-02  2.08081236e-02 -1.00989855e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [ 1.74528350e-02  2.07975568e-02 -4.64479531e-06], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.74458634e-02 2.07869897e-02 8.09568209e-07], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.74388915e-02 2.07764223e-02 6.26410377e-06], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.74319193e-02 2.07658546e-02 1.17188101e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.74249469e-02 2.07552866e-02 1.71736858e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.74179743e-02 2.07447184e-02 2.26287297e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.74110013e-02 2.07341499e-02 2.80839403e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.74040281e-02 2.07235811e-02 3.35393165e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73970547e-02 2.07130121e-02 3.89948568e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73900809e-02 2.07024428e-02 4.44505600e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73831070e-02 2.06918733e-02 4.99064248e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73761327e-02 2.06813034e-02 5.53624499e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73691583e-02 2.06707334e-02 6.08186338e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73621835e-02 2.06601631e-02 6.62749754e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73552085e-02 2.06495925e-02 7.17314733e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73482333e-02 2.06390217e-02 7.71881262e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73412578e-02 2.06284506e-02 8.26449328e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73342821e-02 2.06178793e-02 8.81018918e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73273061e-02 2.06073078e-02 9.35590019e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [1.73203299e-02 2.05967360e-02 9.90162617e-05], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01731335 0.02058616 0.00010447], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01730638 0.02057559 0.00010993], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0172994  0.02056502 0.00011539], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01729242 0.02055445 0.00012085], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01728545 0.02054387 0.0001263 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01727847 0.0205333  0.00013176], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01727149 0.02052273 0.00013722], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01726451 0.02051215 0.00014268], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01725753 0.02050158 0.00014814], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01725055 0.02049101 0.0001536 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01724358 0.02048043 0.00015906], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 400: Weights: [0.31183014 0.29112724 0.39704259]\n",
            "Gradient risk: [0.0172366  0.02046986 0.00016451], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01722962 0.02045928 0.00016997], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01722264 0.02044871 0.00017543], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01721566 0.02043813 0.00018089], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01720868 0.02042756 0.00018635], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0172017  0.02041698 0.00019181], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01719472 0.02040641 0.00019727], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01718774 0.02039583 0.00020273], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01718076 0.02038525 0.00020819], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01717378 0.02037468 0.00021365], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0171668  0.0203641  0.00021911], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01715981 0.02035353 0.00022457], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01715283 0.02034295 0.00023003], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01714585 0.02033237 0.00023549], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01713887 0.02032179 0.00024095], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01713189 0.02031122 0.00024641], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0171249  0.02030064 0.00025188], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01711792 0.02029006 0.00025734], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01711094 0.02027949 0.0002628 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01710396 0.02026891 0.00026826], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01709697 0.02025833 0.00027372], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01708999 0.02024775 0.00027918], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01708301 0.02023717 0.00028464], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01707602 0.02022659 0.0002901 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01706904 0.02021602 0.00029557], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01706205 0.02020544 0.00030103], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01705507 0.02019486 0.00030649], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01704809 0.02018428 0.00031195], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0170411  0.0201737  0.00031741], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01703412 0.02016312 0.00032288], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01702713 0.02015254 0.00032834], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01702015 0.02014196 0.0003338 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01701316 0.02013138 0.00033926], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01700618 0.0201208  0.00034473], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01699919 0.02011022 0.00035019], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0169922  0.02009964 0.00035565], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01698522 0.02008906 0.00036112], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01697823 0.02007848 0.00036658], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01697125 0.0200679  0.00037204], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01696426 0.02005732 0.0003775 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01695727 0.02004674 0.00038297], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01695029 0.02003616 0.00038843], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0169433  0.02002558 0.00039389], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01693631 0.020015   0.00039936], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01692933 0.02000442 0.00040482], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01692234 0.01999384 0.00041028], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01691535 0.01998326 0.00041575], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01690836 0.01997267 0.00042121], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01690138 0.01996209 0.00042668], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01689439 0.01995151 0.00043214], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 450: Weights: [0.30908427 0.28582964 0.40508605]\n",
            "Gradient risk: [0.0168874  0.01994093 0.0004376 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01688041 0.01993035 0.00044307], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01687342 0.01991977 0.00044853], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01686643 0.01990919 0.000454  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01685945 0.0198986  0.00045946], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01685246 0.01988802 0.00046492], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01684547 0.01987744 0.00047039], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01683848 0.01986686 0.00047585], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01683149 0.01985628 0.00048132], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0168245  0.01984569 0.00048678], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01681751 0.01983511 0.00049225], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01681052 0.01982453 0.00049771], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01680353 0.01981395 0.00050318], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01679654 0.01980337 0.00050864], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01678955 0.01979278 0.00051411], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01678256 0.0197822  0.00051957], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01677557 0.01977162 0.00052504], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01676858 0.01976104 0.0005305 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01676159 0.01975045 0.00053597], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0167546  0.01973987 0.00054143], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01674761 0.01972929 0.0005469 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01674062 0.01971871 0.00055236], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01673363 0.01970812 0.00055783], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01672664 0.01969754 0.00056329], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01671965 0.01968696 0.00056876], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01671266 0.01967638 0.00057422], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01670567 0.01966579 0.00057969], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01669868 0.01965521 0.00058515], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01669169 0.01964463 0.00059062], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01668469 0.01963405 0.00059608], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0166777  0.01962346 0.00060155], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01667071 0.01961288 0.00060701], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01666372 0.0196023  0.00061248], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01665673 0.01959172 0.00061794], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01664974 0.01958113 0.00062341], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01664274 0.01957055 0.00062888], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01663575 0.01955997 0.00063434], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01662876 0.01954939 0.00063981], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01662177 0.0195388  0.00064527], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01661478 0.01952822 0.00065074], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01660778 0.01951764 0.0006562 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01660079 0.01950706 0.00066167], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0165938  0.01949647 0.00066714], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01658681 0.01948589 0.0006726 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01657981 0.01947531 0.00067807], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01657282 0.01946473 0.00068353], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01656583 0.01945414 0.000689  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01655884 0.01944356 0.00069446], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.01655184 0.01943298 0.00069993], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Optimized Weights: [0.30638402 0.28064122 0.41297473]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Gradient risk: [0.22       0.22466667 0.198     ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 0: Weights: [0.33328084 0.33322881 0.33349031]\n",
            "Gradient risk: [0.2199617  0.2245936  0.19809952], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21992339 0.2245205  0.1981991 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21988506 0.22444738 0.19829871], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21984672 0.22437423 0.19839837], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21980836 0.22430105 0.19849807], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21976998 0.22422784 0.19859781], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21973158 0.2241546  0.1986976 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21969316 0.22408133 0.19879742], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21965473 0.22400803 0.19889729], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21961628 0.2239347  0.19899721], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21957781 0.22386134 0.19909716], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21953932 0.22378795 0.19919716], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21950081 0.22371453 0.19929719], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21946229 0.22364108 0.19939727], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21942375 0.22356761 0.1994974 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21938519 0.2234941  0.19959756], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21934662 0.22342057 0.19969777], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21930802 0.223347   0.19979802], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21926941 0.22327341 0.19989831], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21923078 0.22319978 0.19999864], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21919214 0.22312613 0.20009902], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21915347 0.22305245 0.20019944], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21911479 0.22297873 0.2002999 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21907609 0.22290499 0.2004004 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21903737 0.22283122 0.20050095], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21899863 0.22275742 0.20060153], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21895988 0.22268359 0.20070216], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21892111 0.22260973 0.20080284], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21888232 0.22253584 0.20090355], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21884351 0.22246192 0.2010043 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21880469 0.22238798 0.2011051 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21876585 0.222314   0.20120594], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21872699 0.22223999 0.20130682], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21868811 0.22216596 0.20140775], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21864921 0.22209189 0.20150872], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2186103  0.2220178  0.20160972], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21857137 0.22194367 0.20171078], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21853242 0.22186952 0.20181187], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21849346 0.22179534 0.201913  ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21845447 0.22172113 0.20201418], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21841547 0.22164688 0.2021154 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21837645 0.22157261 0.20221666], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21833741 0.22149831 0.20231796], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21829836 0.22142399 0.20241931], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21825929 0.22134963 0.2025207 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2182202  0.22127524 0.20262213], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21818109 0.22120082 0.2027236 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21814196 0.22112638 0.20282511], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21810282 0.2210519  0.20292667], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21806366 0.2209774  0.20302826], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 50: Weights: [0.33062536 0.32794985 0.34142477]\n",
            "Gradient risk: [0.21802448 0.22090286 0.2031299 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21798528 0.2208283  0.20323159], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21794607 0.22075371 0.20333331], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21790683 0.22067909 0.20343507], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21786758 0.22060444 0.20353688], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21782832 0.22052976 0.20363873], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21778903 0.22045505 0.20374062], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21774973 0.22038031 0.20384256], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21771041 0.22030554 0.20394453], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21767107 0.22023074 0.20404655], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21763171 0.22015592 0.20414861], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21759234 0.22008106 0.20425071], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21755295 0.22000618 0.20435285], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21751354 0.21993126 0.20445504], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21747411 0.21985632 0.20455727], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21743466 0.21978135 0.20465953], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2173952  0.21970635 0.20476185], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21735572 0.21963132 0.2048642 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21731622 0.21955626 0.20496659], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21727671 0.21948117 0.20506903], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21723717 0.21940606 0.20517151], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21719762 0.21933091 0.20527403], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21715805 0.21925573 0.20537659], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21711847 0.21918053 0.20547919], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21707886 0.2191053  0.20558184], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21703924 0.21903003 0.20568453], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2169996  0.21895474 0.20578726], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21695994 0.21887942 0.20589003], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21692027 0.21880407 0.20599284], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21688058 0.21872869 0.2060957 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21684086 0.21865329 0.20619859], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21680114 0.21857785 0.20630153], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21676139 0.21850238 0.20640451], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21672163 0.21842689 0.20650753], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21668185 0.21835136 0.2066106 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21664205 0.21827581 0.2067137 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21660223 0.21820023 0.20681685], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2165624  0.21812462 0.20692004], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21652254 0.21804898 0.20702327], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21648267 0.21797331 0.20712654], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21644279 0.21789761 0.20722985], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21640288 0.21782189 0.20733321], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21636296 0.21774613 0.20743661], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21632302 0.21767035 0.20754005], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21628306 0.21759453 0.20764353], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21624308 0.21751869 0.20774705], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21620309 0.21744282 0.20785061], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21616308 0.21736692 0.20795422], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21612305 0.21729099 0.20805787], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.216083   0.21721504 0.20816155], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 100: Weights: [0.32790809 0.32256698 0.3495249 ]\n",
            "Gradient risk: [0.21604294 0.21713905 0.20826528], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21600286 0.21706303 0.20836906], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21596276 0.21698699 0.20847287], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21592264 0.21691092 0.20857673], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21588251 0.21683482 0.20868062], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21584235 0.21675869 0.20878456], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21580218 0.21668253 0.20888854], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.215762   0.21660634 0.20899256], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21572179 0.21653012 0.20909663], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21568157 0.21645388 0.20920073], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21564133 0.2163776  0.20930488], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21560107 0.2163013  0.20940906], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21556079 0.21622497 0.20951329], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2155205  0.21614861 0.20961756], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21548019 0.21607222 0.20972188], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21543986 0.2159958  0.20982623], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21539951 0.21591935 0.20993062], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21535915 0.21584288 0.21003506], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21531877 0.21576637 0.21013954], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21527837 0.21568984 0.21024406], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21523795 0.21561328 0.21034862], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21519751 0.21553669 0.21045322], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21515706 0.21546007 0.21055787], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21511659 0.21538342 0.21066255], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2150761  0.21530674 0.21076728], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2150356  0.21523004 0.21087205], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21499507 0.21515331 0.21097685], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21495453 0.21507654 0.2110817 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21491398 0.21499975 0.2111866 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2148734  0.21492293 0.21129153], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21483281 0.21484609 0.2113965 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2147922  0.21476921 0.21150152], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21475157 0.2146923  0.21160658], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21471092 0.21461537 0.21171168], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21467026 0.21453841 0.21181682], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21462957 0.21446142 0.211922  ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21458888 0.2143844  0.21202722], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21454816 0.21430735 0.21213248], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21450742 0.21423027 0.21223779], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21446667 0.21415317 0.21234314], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2144259  0.21407604 0.21244852], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21438512 0.21399888 0.21255395], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21434431 0.21392168 0.21265942], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21430349 0.21384447 0.21276493], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21426265 0.21376722 0.21287048], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21422179 0.21368994 0.21297608], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21418092 0.21361264 0.21308171], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21414002 0.21353531 0.21318739], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21409911 0.21345795 0.21329311], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21405818 0.21338056 0.21339886], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 150: Weights: [0.32512919 0.31708165 0.35778913]\n",
            "Gradient risk: [0.21401724 0.21330314 0.21350466], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21397628 0.21322569 0.2136105 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21393529 0.21314822 0.21371638], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2138943  0.21307072 0.21382231], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21385328 0.21299319 0.21392827], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21381225 0.21291563 0.21403428], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2137712  0.21283804 0.21414032], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21373013 0.21276042 0.21424641], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21368904 0.21268278 0.21435254], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21364794 0.21260511 0.2144587 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21360682 0.2125274  0.21456491], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21356568 0.21244967 0.21467117], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21352452 0.21237192 0.21477746], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21348335 0.21229413 0.21488379], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21344216 0.21221632 0.21499016], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21340095 0.21213848 0.21509658], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21335972 0.2120606  0.21520303], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21331848 0.21198271 0.21530953], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21327721 0.21190478 0.21541607], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21323594 0.21182682 0.21552265], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21319464 0.21174884 0.21562927], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21315332 0.21167083 0.21573593], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21311199 0.21159279 0.21584263], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21307064 0.21151472 0.21594937], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21302928 0.21143662 0.21605615], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21298789 0.2113585  0.21616298], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21294649 0.21128035 0.21626984], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21290507 0.21120217 0.21637675], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21286363 0.21112396 0.21648369], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21282218 0.21104572 0.21659068], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21278071 0.21096746 0.21669771], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21273922 0.21088916 0.21680478], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21269771 0.21081084 0.21691189], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21265619 0.21073249 0.21701904], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21261464 0.21065412 0.21712623], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21257308 0.21057571 0.21723346], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21253151 0.21049728 0.21734073], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21248991 0.21041882 0.21744805], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2124483  0.21034033 0.2175554 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21240667 0.21026181 0.2176628 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21236502 0.21018327 0.21777023], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21232336 0.21010469 0.21787771], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21228168 0.21002609 0.21798522], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21223998 0.20994746 0.21809278], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21219826 0.20986881 0.21820038], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21215653 0.20979012 0.21830802], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21211478 0.20971141 0.2184157 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21207301 0.20963267 0.21852342], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21203122 0.2095539  0.21863118], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21198942 0.2094751  0.21873898], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 200: Weights: [0.32228888 0.31149555 0.36621554]\n",
            "Gradient risk: [0.21194759 0.20939628 0.21884682], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21190575 0.20931743 0.2189547 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2118639  0.20923855 0.21906262], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21182202 0.20915964 0.21917059], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21178013 0.2090807  0.21927859], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21173822 0.20900174 0.21938663], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2116963  0.20892275 0.21949472], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21165435 0.20884373 0.21960284], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21161239 0.20876468 0.21971101], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21157041 0.20868561 0.21981921], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21152842 0.20860651 0.21992746], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2114864  0.20852738 0.22003575], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21144437 0.20844822 0.22014407], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21140232 0.20836903 0.22025244], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21136026 0.20828982 0.22036085], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21131817 0.20821058 0.2204693 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21127607 0.20813131 0.22057779], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21123396 0.20805201 0.22068632], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21119182 0.20797269 0.22079488], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21114967 0.20789334 0.22090349], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2111075  0.20781396 0.22101214], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21106531 0.20773455 0.22112083], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2110231  0.20765512 0.22122956], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21098088 0.20757565 0.22133834], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21093864 0.20749616 0.22144715], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21089638 0.20741665 0.221556  ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21085411 0.2073371  0.22166489], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21081182 0.20725753 0.22177382], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21076951 0.20717793 0.22188279], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21072718 0.2070983  0.2219918 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21068484 0.20701865 0.22210086], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21064248 0.20693896 0.22220995], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2106001  0.20685925 0.22231908], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2105577  0.20677952 0.22242825], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21051529 0.20669975 0.22253747], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21047286 0.20661996 0.22264672], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21043041 0.20654014 0.22275601], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21038794 0.20646029 0.22286535], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21034546 0.20638041 0.22297472], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21030296 0.20630051 0.22308413], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21026044 0.20622058 0.22319359], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21021791 0.20614063 0.22330308], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21017535 0.20606064 0.22341261], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21013278 0.20598063 0.22352218], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2100902  0.20590059 0.2236318 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21004759 0.20582052 0.22374145], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.21000497 0.20574043 0.22385114], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20996233 0.20566031 0.22396088], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20991967 0.20558016 0.22407065], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.209877   0.20549998 0.22418046], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 250: Weights: [0.31938752 0.30581058 0.37480187]\n",
            "Gradient risk: [0.20983431 0.20541978 0.22429032], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2097916  0.20533955 0.22440021], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20974888 0.20525929 0.22451014], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20970613 0.20517901 0.22462011], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20966337 0.2050987  0.22473012], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20962059 0.20501836 0.22484018], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2095778  0.20493799 0.22495027], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20953499 0.2048576  0.2250604 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20949216 0.20477717 0.22517057], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20944931 0.20469673 0.22528078], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20940645 0.20461625 0.22539103], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20936357 0.20453575 0.22550132], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20932067 0.20445522 0.22561165], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20927775 0.20437466 0.22572202], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20923482 0.20429408 0.22583243], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20919187 0.20421347 0.22594288], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2091489  0.20413283 0.22605337], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20910592 0.20405216 0.2261639 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20906292 0.20397147 0.22627447], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2090199  0.20389075 0.22638508], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20897686 0.20381001 0.22649572], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20893381 0.20372923 0.22660641], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20889073 0.20364843 0.22671714], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20884765 0.20356761 0.22682791], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20880454 0.20348675 0.22693871], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20876142 0.20340587 0.22704956], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20871828 0.20332496 0.22716044], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20867512 0.20324403 0.22727137], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20863195 0.20316307 0.22738233], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20858876 0.20308208 0.22749333], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20854555 0.20300106 0.22760438], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20850232 0.20292002 0.22771546], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20845908 0.20283895 0.22782658], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20841582 0.20275785 0.22793774], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20837254 0.20267673 0.22804894], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20832925 0.20259558 0.22816018], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20828593 0.2025144  0.22827146], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20824261 0.2024332  0.22838278], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20819926 0.20235197 0.22849414], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2081559  0.20227071 0.22860553], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20811252 0.20218943 0.22871697], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20806912 0.20210812 0.22882845], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2080257  0.20202678 0.22893996], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20798227 0.20194542 0.22905151], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20793882 0.20186403 0.22916311], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20789536 0.20178261 0.22927474], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20785187 0.20170116 0.22938641], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20780837 0.20161969 0.22949812], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20776486 0.2015382  0.22960987], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20772132 0.20145667 0.22972166], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 300: Weights: [0.31642556 0.30002891 0.3835455 ]\n",
            "Gradient risk: [0.20767777 0.20137512 0.22983349], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2076342  0.20129354 0.22994536], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20759061 0.20121194 0.23005727], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20754701 0.20113031 0.23016921], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20750339 0.20104865 0.2302812 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20745975 0.20096697 0.23039322], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2074161  0.20088526 0.23050528], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20737243 0.20080352 0.23061739], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20732874 0.20072176 0.23072953], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20728503 0.20063997 0.23084171], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20724131 0.20055815 0.23095393], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20719757 0.20047631 0.23106619], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20715381 0.20039444 0.23117848], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20711004 0.20031254 0.23129082], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20706625 0.20023062 0.23140319], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20702244 0.20014867 0.23151561], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20697861 0.2000667  0.23162806], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20693477 0.1999847  0.23174055], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20689091 0.19990267 0.23185308], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20684704 0.19982062 0.23196565], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20680314 0.19973854 0.23207826], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20675923 0.19965643 0.23219091], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2067153  0.1995743  0.23230359], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20667136 0.19949214 0.23241632], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2066274  0.19940995 0.23252908], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20658342 0.19932774 0.23264188], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20653942 0.1992455  0.23275472], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20649541 0.19916324 0.2328676 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20645138 0.19908095 0.23298052], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20640734 0.19899863 0.23309347], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20636327 0.19891629 0.23320647], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20631919 0.19883392 0.2333195 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20627509 0.19875152 0.23343258], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20623098 0.1986691  0.23354569], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20618685 0.19858665 0.23365884], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2061427  0.19850418 0.23377203], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20609853 0.19842168 0.23388525], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20605435 0.19833915 0.23399852], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20601015 0.1982566  0.23411182], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20596594 0.19817402 0.23422516], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2059217  0.19809142 0.23433854], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20587745 0.19800879 0.23445196], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20583319 0.19792613 0.23456542], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2057889  0.19784345 0.23467892], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2057446  0.19776074 0.23479245], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20570028 0.19767801 0.23490602], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20565595 0.19759525 0.23501964], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2056116  0.19751246 0.23513328], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20556723 0.19742965 0.23524697], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20552284 0.19734681 0.2353607 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 350: Weights: [0.31340357 0.29415294 0.39244346]\n",
            "Gradient risk: [0.20547844 0.19726395 0.23547446], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20543402 0.19718106 0.23558827], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20538958 0.19709815 0.23570211], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20534513 0.1970152  0.23581599], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20530066 0.19693224 0.2359299 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20525617 0.19684924 0.23604386], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20521167 0.19676623 0.23615785], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20516715 0.19668318 0.23627189], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20512261 0.19660011 0.23638596], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20507806 0.19651701 0.23650006], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20503348 0.19643389 0.23661421], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2049889  0.19635075 0.2367284 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20494429 0.19626757 0.23684262], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20489967 0.19618437 0.23695688], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20485503 0.19610115 0.23707118], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20481038 0.1960179  0.23718551], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2047657  0.19593462 0.23729989], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20472101 0.19585132 0.2374143 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20467631 0.19576799 0.23752875], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20463159 0.19568464 0.23764324], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20458685 0.19560126 0.23775777], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20454209 0.19551786 0.23787233], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20449732 0.19543443 0.23798694], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20445253 0.19535097 0.23810158], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20440772 0.19526749 0.23821625], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2043629  0.19518399 0.23833097], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20431806 0.19510045 0.23844572], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2042732  0.1950169  0.23856052], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20422833 0.19493331 0.23867534], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20418344 0.19484971 0.23879021], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20413853 0.19476607 0.23890512], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2040936  0.19468241 0.23902006], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20404866 0.19459873 0.23913504], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20400371 0.19451502 0.23925006], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20395873 0.19443129 0.23936511], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20391374 0.19434753 0.23948021], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20386873 0.19426374 0.23959534], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20382371 0.19417993 0.23971051], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20377867 0.19409609 0.23982571], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20373361 0.19401223 0.23994096], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20368853 0.19392834 0.24005624], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20364344 0.19384443 0.24017156], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20359834 0.19376049 0.24028691], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20355321 0.19367653 0.24040231], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20350807 0.19359254 0.24051774], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20346291 0.19350853 0.24063321], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20341774 0.19342449 0.24074871], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20337255 0.19334043 0.24086426], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20332734 0.19325634 0.24097984], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20328211 0.19317223 0.24109546], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 400: Weights: [0.31032224 0.2881853  0.40149242]\n",
            "Gradient risk: [0.20323687 0.19308809 0.24121111], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20319161 0.19300393 0.24132681], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20314634 0.19291974 0.24144254], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20310105 0.19283552 0.24155831], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20305574 0.19275128 0.24167411], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20301042 0.19266702 0.24178995], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20296507 0.19258273 0.24190583], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20291972 0.19249842 0.24202175], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20287434 0.19241408 0.2421377 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20282895 0.19232972 0.24225369], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20278354 0.19224533 0.24236972], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20273812 0.19216091 0.24248579], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20269268 0.19207647 0.24260189], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20264722 0.19199201 0.24271803], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20260175 0.19190752 0.24283421], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20255626 0.19182301 0.24295042], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20251075 0.19173847 0.24306667], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20246523 0.19165391 0.24318296], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20241969 0.19156932 0.24329929], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20237413 0.19148471 0.24341565], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20232856 0.19140007 0.24353205], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20228297 0.19131541 0.24364848], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20223736 0.19123072 0.24376495], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20219174 0.19114601 0.24388146], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2021461  0.19106128 0.24399801], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20210044 0.19097652 0.24411459], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20205477 0.19089173 0.24423121], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20200908 0.19080692 0.24434787], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20196338 0.19072209 0.24446457], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20191766 0.19063723 0.2445813 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20187192 0.19055234 0.24469806], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20182616 0.19046743 0.24481487], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20178039 0.1903825  0.24493171], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20173461 0.19029754 0.24504859], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.2016888  0.19021256 0.2451655 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20164298 0.19012756 0.24528245], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20159714 0.19004252 0.24539944], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20155129 0.18995747 0.24551646], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20150542 0.18987239 0.24563352], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20145953 0.18978729 0.24575062], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20141363 0.18970216 0.24586775], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20136771 0.189617   0.24598492], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20132178 0.18953183 0.24610213], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20127583 0.18944662 0.24621938], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20122986 0.1893614  0.24633665], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20118387 0.18927615 0.24645397], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20113787 0.18919087 0.24657132], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20109185 0.18910557 0.24668871], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20104582 0.18902025 0.24680614], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20099977 0.1889349  0.2469236 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 450: Weights: [0.30718238 0.2821289  0.41068868]\n",
            "Gradient risk: [0.2009537  0.18884953 0.2470411 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20090762 0.18876413 0.24715863], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20086152 0.18867871 0.2472762 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20081541 0.18859327 0.24739381], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20076927 0.1885078  0.24751145], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20072313 0.18842231 0.24762913], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20067696 0.18833679 0.24774685], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20063078 0.18825125 0.2478646 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20058458 0.18816568 0.24798239], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20053837 0.18808009 0.24810021], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20049214 0.18799448 0.24821807], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20044589 0.18790884 0.24833597], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20039963 0.18782318 0.2484539 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20035335 0.1877375  0.24857187], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20030706 0.18765179 0.24868987], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20026075 0.18756605 0.24880791], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20021442 0.1874803  0.24892599], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20016808 0.18739452 0.2490441 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20012172 0.18730871 0.24916225], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20007534 0.18722288 0.24928044], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.20002895 0.18713703 0.24939866], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19998254 0.18705115 0.24951691], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19993611 0.18696525 0.2496352 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19988967 0.18687933 0.24975353], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19984322 0.18679338 0.2498719 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19979674 0.18670741 0.24999029], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19975025 0.18662141 0.25010873], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19970375 0.18653539 0.2502272 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19965723 0.18644935 0.25034571], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19961069 0.18636328 0.25046425], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19956413 0.18627719 0.25058283], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19951756 0.18619108 0.25070144], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19947098 0.18610494 0.25082009], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19942437 0.18601878 0.25093877], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19937775 0.18593259 0.2510575 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19933112 0.18584638 0.25117625], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19928447 0.18576015 0.25129504], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1992378  0.1856739  0.25141387], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19919112 0.18558762 0.25153273], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19914442 0.18550131 0.25165163], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1990977  0.18541499 0.25177056], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19905097 0.18532864 0.25188953], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19900422 0.18524226 0.25200854], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19895746 0.18515587 0.25212758], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19891068 0.18506945 0.25224665], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19886388 0.184983   0.25236576], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19881707 0.18489654 0.25248491], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.19877024 0.18481004 0.25260409], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1987234  0.18472353 0.2527233 ], Gradient return: [-0.009  -0.0095 -0.0102], Gradient L1: [0.01 0.01 0.01]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Gradient risk: [0.15333333 0.158      0.13133333], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 0: Weights: [0.33328097 0.33322905 0.33348995]\n",
            "Gradient risk: [0.1533056  0.15794795 0.1314013 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15327786 0.15789591 0.13146929], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15325012 0.15784386 0.13153728], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15322238 0.1577918  0.13160528], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15319463 0.15773974 0.13167329], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15316687 0.15768767 0.13174131], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15313911 0.1576356  0.13180933], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15311135 0.15758352 0.13187736], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15308358 0.15753144 0.13194541], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15305581 0.15747935 0.13201346], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15302804 0.15742725 0.13208151], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15300026 0.15737516 0.13214958], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15297247 0.15732305 0.13221766], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15294469 0.15727094 0.13228574], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15291689 0.15721883 0.13235383], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1528891  0.15716671 0.13242193], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15286129 0.15711458 0.13249003], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15283349 0.15706245 0.13255815], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15280568 0.15701032 0.13262627], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15277786 0.15695818 0.1326944 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15275005 0.15690603 0.13276254], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15272222 0.15685388 0.13283069], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1526944  0.15680172 0.13289885], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15266656 0.15674956 0.13296701], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15263873 0.1566974  0.13303518], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15261089 0.15664522 0.13310336], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15258304 0.15659305 0.13317155], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1525552  0.15654087 0.13323974], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15252734 0.15648868 0.13330795], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15249949 0.15643649 0.13337616], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15247162 0.15638429 0.13344438], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15244376 0.15633209 0.1335126 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15241589 0.15627988 0.13358084], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15238802 0.15622767 0.13364908], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15236014 0.15617546 0.13371733], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15233226 0.15612323 0.13378559], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15230437 0.15607101 0.13385386], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15227648 0.15601878 0.13392213], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15224858 0.15596654 0.13399041], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15222068 0.1559143  0.1340587 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15219278 0.15586205 0.134127  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15216487 0.1558098  0.13419531], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15213696 0.15575755 0.13426362], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15210905 0.15570529 0.13433194], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15208113 0.15565302 0.13440027], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1520532  0.15560075 0.1344686 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15202527 0.15554848 0.13453695], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15199734 0.1554962  0.1346053 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15196941 0.15544391 0.13467366], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15194147 0.15539162 0.13474202], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 50: Weights: [0.33065202 0.3280028  0.34134515]\n",
            "Gradient risk: [0.15191352 0.15533933 0.1348104 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15188557 0.15528703 0.13487878], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15185762 0.15523472 0.13494717], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15182966 0.15518242 0.13501557], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1518017  0.1551301  0.13508397], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15177374 0.15507779 0.13515238], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15174577 0.15502546 0.1352208 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15171779 0.15497314 0.13528923], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15168982 0.1549208  0.13535766], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15166183 0.15486847 0.1354261 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15163385 0.15481613 0.13549455], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15160586 0.15476378 0.13556301], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15157787 0.15471143 0.13563147], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15154987 0.15465907 0.13569995], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15152187 0.15460671 0.13576843], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15149386 0.15455435 0.13583691], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15146585 0.15450198 0.13590541], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15143784 0.15444961 0.13597391], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15140982 0.15439723 0.13604241], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1513818  0.15434485 0.13611093], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15135377 0.15429246 0.13617945], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15132574 0.15424007 0.13624798], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15129771 0.15418767 0.13631652], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15126967 0.15413527 0.13638507], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15124163 0.15408287 0.13645362], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15121358 0.15403046 0.13652218], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15118553 0.15397804 0.13659074], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15115748 0.15392562 0.13665932], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15112942 0.1538732  0.1367279 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15110136 0.15382077 0.13679649], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15107329 0.15376834 0.13686508], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15104522 0.15371591 0.13693368], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15101715 0.15366347 0.13700229], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15098907 0.15361102 0.13707091], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15096099 0.15355857 0.13713953], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15093291 0.15350612 0.13720816], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15090482 0.15345366 0.1372768 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15087672 0.1534012  0.13734544], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15084863 0.15334873 0.1374141 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15082052 0.15329626 0.13748275], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15079242 0.15324379 0.13755142], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15076431 0.15319131 0.13762009], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1507362  0.15313883 0.13768877], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15070808 0.15308634 0.13775746], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15067996 0.15303385 0.13782615], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15065184 0.15298135 0.13789485], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15062371 0.15292885 0.13796356], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15059558 0.15287635 0.13803227], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15056744 0.15282384 0.13810099], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1505393  0.15277132 0.13816972], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 100: Weights: [0.32800226 0.32275358 0.34924414]\n",
            "Gradient risk: [0.15051116 0.15271881 0.13823846], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15048301 0.15266629 0.1383072 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15045486 0.15261376 0.13837595], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15042671 0.15256123 0.1384447 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15039855 0.1525087  0.13851346], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15037038 0.15245616 0.13858223], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15034222 0.15240362 0.13865101], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15031405 0.15235107 0.13871979], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15028587 0.15229852 0.13878858], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1502577  0.15224597 0.13885737], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15022951 0.15219341 0.13892618], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15020133 0.15214085 0.13899498], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15017314 0.15208829 0.1390638 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15014495 0.15203572 0.13913262], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15011675 0.15198314 0.13920145], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15008855 0.15193057 0.13927028], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15006035 0.15187798 0.13933913], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15003214 0.1518254  0.13940797], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15000393 0.15177281 0.13947683], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14997571 0.15172022 0.13954569], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14994749 0.15166762 0.13961456], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14991927 0.15161502 0.13968343], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14989104 0.15156241 0.13975231], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14986281 0.1515098  0.1398212 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14983458 0.15145719 0.13989009], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14980634 0.15140458 0.13995899], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1497781  0.15135195 0.1400279 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14974986 0.15129933 0.14009681], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14972161 0.1512467  0.14016573], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14969336 0.15119407 0.14023466], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1496651  0.15114144 0.14030359], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14963684 0.1510888  0.14037253], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14960858 0.15103615 0.14044147], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14958031 0.15098351 0.14051043], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14955204 0.15093086 0.14057938], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14952377 0.1508782  0.14064835], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14949549 0.15082555 0.14071732], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14946721 0.15077288 0.14078629], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14943892 0.15072022 0.14085528], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14941064 0.15066755 0.14092426], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14938234 0.15061488 0.14099326], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14935405 0.1505622  0.14106226], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14932575 0.15050952 0.14113127], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14929745 0.15045684 0.14120028], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14926914 0.15040415 0.1412693 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14924083 0.15035146 0.14133833], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14921252 0.15029877 0.14140736], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1491842  0.15024607 0.14147639], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14915588 0.15019337 0.14154544], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14912755 0.15014067 0.14161449], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 150: Weights: [0.32533299 0.31748454 0.35718244]\n",
            "Gradient risk: [0.14909923 0.15008796 0.14168354], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14907089 0.15003525 0.14175261], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14904256 0.14998253 0.14182167], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14901422 0.14992981 0.14189075], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14898588 0.14987709 0.14195983], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14895753 0.14982437 0.14202891], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14892918 0.14977164 0.142098  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14890083 0.14971891 0.1421671 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14887248 0.14966617 0.14223621], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14884412 0.14961343 0.14230532], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14881575 0.14956069 0.14237443], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14878739 0.14950794 0.14244355], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14875902 0.1494552  0.14251268], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14873064 0.14940244 0.14258181], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14870227 0.14934969 0.14265095], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14867389 0.14929693 0.1427201 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1486455  0.14924417 0.14278925], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14861712 0.1491914  0.1428584 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14858873 0.14913863 0.14292756], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14856033 0.14908586 0.14299673], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14853193 0.14903308 0.1430659 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14850353 0.14898031 0.14313508], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14847513 0.14892752 0.14320427], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14844672 0.14887474 0.14327346], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14841831 0.14882195 0.14334265], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1483899  0.14876916 0.14341186], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14836148 0.14871637 0.14348106], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14833306 0.14866357 0.14355028], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14830463 0.14861077 0.14361949], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1482762  0.14855796 0.14368872], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14824777 0.14850516 0.14375795], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14821934 0.14845235 0.14382718], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1481909  0.14839953 0.14389642], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14816246 0.14834672 0.14396567], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14813401 0.1482939  0.14403492], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14810557 0.14824107 0.14410418], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14807712 0.14818825 0.14417344], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14804866 0.14813542 0.14424271], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1480202  0.14808259 0.14431198], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14799174 0.14802975 0.14438126], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14796328 0.14797692 0.14445054], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14793481 0.14792408 0.14451983], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14790634 0.14787123 0.14458913], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14787786 0.14781839 0.14465843], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14784939 0.14776554 0.14472773], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1478209  0.14771268 0.14479704], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14779242 0.14765983 0.14486636], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14776393 0.14760697 0.14493568], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14773544 0.14755411 0.14500501], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14770695 0.14750125 0.14507434], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 200: Weights: [0.32264554 0.31219892 0.36515551]\n",
            "Gradient risk: [0.14767845 0.14744838 0.14514368], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14764995 0.14739551 0.14521302], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14762145 0.14734264 0.14528237], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14759294 0.14728976 0.14535172], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14756443 0.14723688 0.14542108], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14753591 0.147184   0.14549044], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1475074  0.14713112 0.14555981], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14747888 0.14707823 0.14562918], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14745036 0.14702534 0.14569856], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14742183 0.14697245 0.14576795], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1473933  0.14691956 0.14583733], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14736477 0.14686666 0.14590673], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14733623 0.14681376 0.14597613], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14730769 0.14676086 0.14604553], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14727915 0.14670795 0.14611494], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1472506  0.14665504 0.14618435], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14722206 0.14660213 0.14625377], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1471935  0.14654922 0.14632319], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14716495 0.1464963  0.14639262], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14713639 0.14644338 0.14646206], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14710783 0.14639046 0.1465315 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14707927 0.14633754 0.14660094], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1470507  0.14628461 0.14667039], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14702213 0.14623168 0.14673984], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14699356 0.14617875 0.1468093 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14696498 0.14612582 0.14687876], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1469364  0.14607288 0.14694823], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14690782 0.14601994 0.1470177 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14687923 0.145967   0.14708718], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14685064 0.14591406 0.14715666], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14682205 0.14586111 0.14722615], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14679346 0.14580816 0.14729564], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14676486 0.14575521 0.14736513], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14673626 0.14570226 0.14743463], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14670765 0.1456493  0.14750414], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14667905 0.14559634 0.14757365], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14665044 0.14554338 0.14764316], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14662182 0.14549042 0.14771268], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14659321 0.14543745 0.14778221], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14656459 0.14538448 0.14785173], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14653596 0.14533151 0.14792127], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14650734 0.14527854 0.14799081], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14647871 0.14522557 0.14806035], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14645008 0.14517259 0.14812989], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14642145 0.14511961 0.14819945], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14639281 0.14506663 0.148269  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14636417 0.14501364 0.14833856], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14633553 0.14496066 0.14840813], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14630688 0.14490767 0.14847769], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14627823 0.14485468 0.14854727], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 250: Weights: [0.31994129 0.30689997 0.37315871]\n",
            "Gradient risk: [0.14624958 0.14480168 0.14861685], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14622092 0.14474869 0.14868643], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14619227 0.14469569 0.14875602], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14616361 0.14464269 0.14882561], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14613494 0.14458969 0.1488952 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14610628 0.14453669 0.1489648 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14607761 0.14448368 0.14903441], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14604894 0.14443067 0.14910401], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14602026 0.14437766 0.14917363], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14599158 0.14432465 0.14924324], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1459629  0.14427164 0.14931286], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14593422 0.14421862 0.14938249], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14590553 0.1441656  0.14945212], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14587684 0.14411258 0.14952175], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14584815 0.14405956 0.14959139], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14581945 0.14400653 0.14966103], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14579076 0.14395351 0.14973068], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14576205 0.14390048 0.14980033], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14573335 0.14384745 0.14986998], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14570464 0.14379442 0.14993964], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14567594 0.14374138 0.15000931], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14564722 0.14368835 0.15007897], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14561851 0.14363531 0.15014864], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14558979 0.14358227 0.15021832], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14556107 0.14352923 0.150288  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14553235 0.14347618 0.15035768], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14550362 0.14342314 0.15042737], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14547489 0.14337009 0.15049706], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14544616 0.14331704 0.15056675], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14541743 0.14326399 0.15063645], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14538869 0.14321094 0.15070615], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14535995 0.14315788 0.15077586], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14533121 0.14310482 0.15084557], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14530246 0.14305177 0.15091529], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14527371 0.14299871 0.150985  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14524496 0.14294564 0.15105473], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14521621 0.14289258 0.15112445], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14518745 0.14283952 0.15119418], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14515869 0.14278645 0.15126391], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14512993 0.14273338 0.15133365], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14510117 0.14268031 0.15140339], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1450724  0.14262724 0.15147314], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14504363 0.14257416 0.15154288], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14501486 0.14252109 0.15161264], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14498608 0.14246801 0.15168239], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1449573  0.14241493 0.15175215], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14492852 0.14236185 0.15182192], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14489974 0.14230877 0.15189168], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14487096 0.14225569 0.15196145], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14484217 0.1422026  0.15203123], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 300: Weights: [0.31722163 0.30159101 0.38118732]\n",
            "Gradient risk: [0.14481338 0.14214952 0.152101  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14478458 0.14209643 0.15217078], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14475579 0.14204334 0.15224057], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14472699 0.14199025 0.15231036], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14469819 0.14193716 0.15238015], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14466938 0.14188406 0.15244994], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14464058 0.14183097 0.15251974], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14461177 0.14177787 0.15258955], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14458295 0.14172477 0.15265935], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14455414 0.14167167 0.15272916], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14452532 0.14161857 0.15279897], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1444965  0.14156547 0.15286879], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14446768 0.14151237 0.15293861], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14443886 0.14145926 0.15300843], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14441003 0.14140616 0.15307826], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1443812  0.14135305 0.15314808], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14435237 0.14129994 0.15321792], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14432353 0.14124683 0.15328775], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1442947  0.14119372 0.15335759], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14426586 0.1411406  0.15342743], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14423701 0.14108749 0.15349728], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14420817 0.14103437 0.15356713], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14417932 0.14098126 0.15363698], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14415047 0.14092814 0.15370684], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14412162 0.14087502 0.1537767 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14409277 0.1408219  0.15384656], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14406391 0.14076878 0.15391642], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14403505 0.14071565 0.15398629], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14400619 0.14066253 0.15405616], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14397732 0.14060941 0.15412604], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14394845 0.14055628 0.15419591], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14391959 0.14050315 0.15426579], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14389071 0.14045002 0.15433568], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14386184 0.14039689 0.15440556], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14383296 0.14034376 0.15447545], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14380408 0.14029063 0.15454534], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1437752  0.1402375  0.15461524], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14374632 0.14018436 0.15468514], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14371743 0.14013123 0.15475504], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14368854 0.14007809 0.15482494], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14365965 0.14002496 0.15489485], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14363076 0.13997182 0.15496476], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14360186 0.13991868 0.15503467], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14357296 0.13986554 0.15510459], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14354406 0.1398124  0.15517451], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14351516 0.13975926 0.15524443], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14348626 0.13970611 0.15531435], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14345735 0.13965297 0.15538428], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14342844 0.13959983 0.15545421], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14339953 0.13954668 0.15552414], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 350: Weights: [0.31448799 0.29627539 0.38923659]\n",
            "Gradient risk: [0.14337061 0.13949353 0.15559408], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1433417  0.13944039 0.15566402], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14331278 0.13938724 0.15573396], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14328386 0.13933409 0.1558039 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14325493 0.13928094 0.15587385], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14322601 0.13922779 0.1559438 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14319708 0.13917464 0.15601375], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14316815 0.13912149 0.1560837 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14313921 0.13906833 0.15615366], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14311028 0.13901518 0.15622362], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14308134 0.13896203 0.15629358], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1430524  0.13890887 0.15636355], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14302346 0.13885572 0.15643351], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14299452 0.13880256 0.15650348], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14296557 0.1387494  0.15657346], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14293662 0.13869624 0.15664343], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14290767 0.13864309 0.15671341], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14287872 0.13858993 0.15678339], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14284976 0.13853677 0.15685337], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14282081 0.13848361 0.15692335], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14279185 0.13843045 0.15699334], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14276288 0.13837729 0.15706333], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14273392 0.13832412 0.15713332], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14270495 0.13827096 0.15720332], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14267599 0.1382178  0.15727331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14264702 0.13816463 0.15734331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14261804 0.13811147 0.15741331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14258907 0.1380583  0.15748331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14256009 0.13800514 0.15755332], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14253111 0.13795197 0.15762333], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14250213 0.13789881 0.15769334], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14247315 0.13784564 0.15776335], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14244416 0.13779247 0.15783337], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14241518 0.13773931 0.15790338], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14238619 0.13768614 0.1579734 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14235719 0.13763297 0.15804342], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1423282  0.1375798  0.15811345], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14229921 0.13752663 0.15818347], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14227021 0.13747346 0.1582535 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14224121 0.13742029 0.15832353], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14221221 0.13736712 0.15839356], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1421832  0.13731395 0.15846359], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1421542  0.13726078 0.15853363], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14212519 0.13720761 0.15860367], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14209618 0.13715444 0.15867371], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14206717 0.13710127 0.15874375], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14203815 0.1370481  0.15881379], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14200913 0.13699492 0.15888384], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14198012 0.13694175 0.15895388], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1419511  0.13688858 0.15902393], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 400: Weights: [0.31174181 0.29095648 0.39730168]\n",
            "Gradient risk: [0.14192207 0.13683541 0.15909399], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14189305 0.13678223 0.15916404], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14186402 0.13672906 0.15923409], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.141835   0.13667589 0.15930415], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14180597 0.13662271 0.15937421], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14177693 0.13656954 0.15944427], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1417479  0.13651637 0.15951433], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14171886 0.13646319 0.1595844 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14168983 0.13641002 0.15965446], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14166079 0.13635684 0.15972453], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14163174 0.13630367 0.1597946 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1416027  0.1362505  0.15986467], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14157365 0.13619732 0.15993475], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14154461 0.13614415 0.16000482], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14151556 0.13609097 0.1600749 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14148651 0.1360378  0.16014497], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14145745 0.13598463 0.16021505], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1414284  0.13593145 0.16028513], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14139934 0.13587828 0.16035522], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14137028 0.1358251  0.1604253 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14134122 0.13577193 0.16049539], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14131216 0.13571876 0.16056548], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14128309 0.13566558 0.16063556], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14125403 0.13561241 0.16070566], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14122496 0.13555924 0.16077575], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14119589 0.13550606 0.16084584], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14116682 0.13545289 0.16091594], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14113774 0.13539972 0.16098603], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14110867 0.13534654 0.16105613], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14107959 0.13529337 0.16112623], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14105051 0.1352402  0.16119633], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14102143 0.13518703 0.16126643], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14099235 0.13513385 0.16133654], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14096326 0.13508068 0.16140664], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14093417 0.13502751 0.16147675], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14090509 0.13497434 0.16154685], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.140876   0.13492117 0.16161696], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1408469  0.134868   0.16168707], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14081781 0.13481483 0.16175718], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14078872 0.13476166 0.1618273 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14075962 0.13470849 0.16189741], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14073052 0.13465532 0.16196753], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14070142 0.13460215 0.16203764], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14067232 0.13454898 0.16210776], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14064321 0.13449581 0.16217788], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14061411 0.13444264 0.162248  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.140585   0.13438948 0.16231812], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14055589 0.13433631 0.16238824], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14052678 0.13428314 0.16245836], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14049767 0.13422998 0.16252849], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 450: Weights: [0.30898455 0.28563765 0.40537776]\n",
            "Gradient risk: [0.14046856 0.13417681 0.16259861], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14043944 0.13412365 0.16266874], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14041032 0.13407048 0.16273887], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1403812  0.13401732 0.16280899], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14035208 0.13396416 0.16287912], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14032296 0.13391099 0.16294925], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14029384 0.13385783 0.16301939], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14026471 0.13380467 0.16308952], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14023558 0.13375151 0.16315965], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14020645 0.13369835 0.16322978], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14017732 0.13364519 0.16329992], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14014819 0.13359203 0.16337005], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14011906 0.13353887 0.16344019], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14008992 0.13348571 0.16351033], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14006079 0.13343255 0.16358047], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14003165 0.1333794  0.16365061], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14000251 0.13332624 0.16372075], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13997337 0.13327308 0.16379089], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13994422 0.13321993 0.16386103], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13991508 0.13316678 0.16393117], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13988593 0.13311362 0.16400131], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13985678 0.13306047 0.16407146], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13982764 0.13300732 0.1641416 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13979848 0.13295417 0.16421174], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13976933 0.13290102 0.16428189], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13974018 0.13284787 0.16435204], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13971102 0.13279472 0.16442218], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13968187 0.13274157 0.16449233], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13965271 0.13268843 0.16456248], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13962355 0.13263528 0.16463263], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13959439 0.13258213 0.16470278], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13956522 0.13252899 0.16477293], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13953606 0.13247585 0.16484308], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13950689 0.1324227  0.16491323], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13947773 0.13236956 0.16498338], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13944856 0.13231642 0.16505353], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13941939 0.13226328 0.16512368], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13939022 0.13221014 0.16519383], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13936104 0.13215701 0.16526399], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13933187 0.13210387 0.16533414], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13930269 0.13205074 0.16540429], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13927351 0.1319976  0.16547445], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13924434 0.13194447 0.1655446 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13921516 0.13189133 0.16561476], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13918597 0.1318382  0.16568491], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13915679 0.13178507 0.16575507], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13912761 0.13173194 0.16582522], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13909842 0.13167882 0.16589538], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13906923 0.13162569 0.16596553], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Gradient risk: [0.05333333 0.058      0.03133333], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 0: Weights: [0.33328113 0.33322937 0.33348947]\n",
            "Gradient risk: [0.05332134 0.0579793  0.03135426], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05330936 0.0579586  0.03137518], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05329737 0.05793791 0.0313961 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05328538 0.05791721 0.03141703], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05327338 0.05789651 0.03143796], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05326139 0.05787581 0.03145889], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0532494  0.0578551  0.03147982], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0532374  0.0578344  0.03150075], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05322541 0.0578137  0.03152168], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05321341 0.05779299 0.03154262], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05320141 0.05777229 0.03156355], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05318941 0.05775158 0.03158449], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05317741 0.05773087 0.03160543], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05316541 0.05771016 0.03162637], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05315341 0.05768945 0.03164731], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05314141 0.05766874 0.03166825], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05312941 0.05764803 0.03168919], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0531174  0.05762732 0.03171014], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0531054  0.05760661 0.03173108], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05309339 0.05758589 0.03175203], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05308138 0.05756518 0.03177298], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05306937 0.05754446 0.03179393], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05305737 0.05752374 0.03181488], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05304536 0.05750303 0.03183583], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05303334 0.05748231 0.03185678], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05302133 0.05746159 0.03187773], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05300932 0.05744087 0.03189869], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05299731 0.05742015 0.03191964], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05298529 0.05739943 0.0319406 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05297327 0.0573787  0.03196156], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05296126 0.05735798 0.03198252], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05294924 0.05733726 0.03200348], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05293722 0.05731653 0.03202444], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0529252  0.0572958  0.03204541], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05291318 0.05727508 0.03206637], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05290116 0.05725435 0.03208733], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05288914 0.05723362 0.0321083 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05287711 0.05721289 0.03212927], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05286509 0.05719216 0.03215024], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05285306 0.05717143 0.03217121], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05284104 0.0571507  0.03219218], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05282901 0.05712997 0.03221315], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05281698 0.05710923 0.03223412], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05280496 0.0570885  0.0322551 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05279293 0.05706776 0.03227607], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0527809  0.05704703 0.03229705], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05276886 0.05702629 0.03231803], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05275683 0.05700555 0.032339  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0527448  0.05698482 0.03235998], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05273276 0.05696408 0.03238096], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 50: Weights: [0.33066429 0.32802716 0.34130852]\n",
            "Gradient risk: [0.05272073 0.05694334 0.03240195], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05270869 0.0569226  0.03242293], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05269666 0.05690186 0.03244391], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05268462 0.05688112 0.0324649 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05267258 0.05686037 0.03248588], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05266054 0.05683963 0.03250687], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0526485  0.05681889 0.03252786], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05263646 0.05679814 0.03254885], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05262442 0.0567774  0.03256984], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05261238 0.05675665 0.03259083], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05260033 0.0567359  0.03261182], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05258829 0.05671516 0.03263281], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05257624 0.05669441 0.03265381], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0525642  0.05667366 0.0326748 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05255215 0.05665291 0.0326958 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0525401  0.05663216 0.03271679], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05252805 0.05661141 0.03273779], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.052516   0.05659066 0.03275879], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05250395 0.05656991 0.03277979], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0524919  0.05654915 0.03280079], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05247985 0.0565284  0.03282179], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0524678  0.05650765 0.03284279], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05245574 0.05648689 0.0328638 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05244369 0.05646614 0.0328848 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05243163 0.05644538 0.03290581], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05241958 0.05642463 0.03292681], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05240752 0.05640387 0.03294782], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05239546 0.05638311 0.03296883], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0523834  0.05636235 0.03298984], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05237134 0.05634159 0.03301085], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05235928 0.05632084 0.03303186], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05234722 0.05630008 0.03305287], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05233516 0.05627931 0.03307388], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0523231  0.05625855 0.0330949 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05231104 0.05623779 0.03311591], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05229897 0.05621703 0.03313693], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05228691 0.05619627 0.03315794], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05227484 0.0561755  0.03317896], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05226277 0.05615474 0.03319998], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05225071 0.05613397 0.033221  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05223864 0.05611321 0.03324202], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05222657 0.05609244 0.03326304], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0522145  0.05607168 0.03328406], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05220243 0.05605091 0.03330508], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05219036 0.05603014 0.03332611], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05217829 0.05600938 0.03334713], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05216622 0.05598861 0.03336816], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05215414 0.05596784 0.03338918], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05214207 0.05594707 0.03341021], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05212999 0.0559263  0.03343124], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 100: Weights: [0.3280347  0.32281781 0.34914746]\n",
            "Gradient risk: [0.05211792 0.05590553 0.03345226], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05210584 0.05588476 0.03347329], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05209376 0.05586399 0.03349432], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05208169 0.05584322 0.03351535], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05206961 0.05582245 0.03353638], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05205753 0.05580167 0.03355742], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05204545 0.0557809  0.03357845], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05203337 0.05576013 0.03359948], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05202129 0.05573935 0.03362052], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0520092  0.05571858 0.03364155], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05199712 0.0556978  0.03366259], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05198504 0.05567703 0.03368363], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05197295 0.05565625 0.03370466], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05196087 0.05563548 0.0337257 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05194878 0.0556147  0.03374674], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05193669 0.05559392 0.03376778], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05192461 0.05557315 0.03378882], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05191252 0.05555237 0.03380986], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05190043 0.05553159 0.0338309 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05188834 0.05551081 0.03385195], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05187625 0.05549003 0.03387299], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05186416 0.05546925 0.03389403], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05185207 0.05544847 0.03391508], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05183998 0.05542769 0.03393612], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05182788 0.05540691 0.03395717], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05181579 0.05538613 0.03397822], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0518037  0.05536535 0.03399926], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0517916  0.05534457 0.03402031], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05177951 0.05532379 0.03404136], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05176741 0.055303   0.03406241], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05175531 0.05528222 0.03408346], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05174322 0.05526144 0.03410451], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05173112 0.05524066 0.03412556], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05171902 0.05521987 0.03414662], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05170692 0.05519909 0.03416767], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05169482 0.0551783  0.03418872], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05168272 0.05515752 0.03420978], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05167062 0.05513673 0.03423083], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05165852 0.05511595 0.03425189], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05164641 0.05509516 0.03427294], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05163431 0.05507438 0.034294  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05162221 0.05505359 0.03431506], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0516101  0.05503281 0.03433611], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.051598   0.05501202 0.03435717], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05158589 0.05499123 0.03437823], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05157378 0.05497044 0.03439929], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05156168 0.05494966 0.03442035], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05154957 0.05492887 0.03444141], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05153746 0.05490808 0.03446247], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05152535 0.05488729 0.03448354], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 150: Weights: [0.32539373 0.31760451 0.35700173]\n",
            "Gradient risk: [0.05151324 0.05486651 0.0345046 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05150113 0.05484572 0.03452566], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05148902 0.05482493 0.03454672], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05147691 0.05480414 0.03456779], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0514648  0.05478335 0.03458885], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05145268 0.05476256 0.03460992], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05144057 0.05474177 0.03463098], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05142846 0.05472098 0.03465205], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05141634 0.05470019 0.03467312], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05140423 0.0546794  0.03469418], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05139211 0.05465861 0.03471525], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05138    0.05463782 0.03473632], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05136788 0.05461703 0.03475739], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05135576 0.05459624 0.03477846], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05134364 0.05457545 0.03479953], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05133152 0.05455465 0.0348206 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05131941 0.05453386 0.03484167], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05130729 0.05451307 0.03486274], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05129517 0.05449228 0.03488381], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05128304 0.05447149 0.03490489], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05127092 0.05445069 0.03492596], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0512588  0.0544299  0.03494703], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05124668 0.05440911 0.03496811], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05123456 0.05438832 0.03498918], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05122243 0.05436752 0.03501025], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05121031 0.05434673 0.03503133], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05119818 0.05432594 0.0350524 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05118606 0.05430515 0.03507348], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05117393 0.05428435 0.03509456], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05116181 0.05426356 0.03511563], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05114968 0.05424277 0.03513671], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05113755 0.05422197 0.03515779], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05112542 0.05420118 0.03517887], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0511133  0.05418039 0.03519994], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05110117 0.05415959 0.03522102], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05108904 0.0541388  0.0352421 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05107691 0.054118   0.03526318], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05106478 0.05409721 0.03528426], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05105265 0.05407642 0.03530534], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05104052 0.05405562 0.03532642], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05102838 0.05403483 0.0353475 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05101625 0.05401404 0.03536858], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05100412 0.05399324 0.03538967], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05099198 0.05397245 0.03541075], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05097985 0.05395165 0.03543183], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05096772 0.05393086 0.03545291], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05095558 0.05391007 0.035474  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05094345 0.05388927 0.03549508], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05093131 0.05386848 0.03551616], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05091917 0.05384768 0.03553725], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 200: Weights: [0.32274279 0.31239046 0.36486673]\n",
            "Gradient risk: [0.05090704 0.05382689 0.03555833], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0508949  0.05380609 0.03557942], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05088276 0.0537853  0.0356005 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05087062 0.05376451 0.03562159], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05085848 0.05374371 0.03564267], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05084635 0.05372292 0.03566376], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05083421 0.05370213 0.03568484], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05082207 0.05368133 0.03570593], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05080992 0.05366054 0.03572702], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05079778 0.05363974 0.03574811], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05078564 0.05361895 0.03576919], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0507735  0.05359816 0.03579028], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05076136 0.05357736 0.03581137], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05074922 0.05355657 0.03583246], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05073707 0.05353578 0.03585354], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05072493 0.05351498 0.03587463], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05071278 0.05349419 0.03589572], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05070064 0.0534734  0.03591681], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05068849 0.0534526  0.0359379 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05067635 0.05343181 0.03595899], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0506642  0.05341102 0.03598008], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05065206 0.05339023 0.03600117], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05063991 0.05336943 0.03602226], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05062776 0.05334864 0.03604335], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05061562 0.05332785 0.03606444], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05060347 0.05330706 0.03608553], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05059132 0.05328626 0.03610662], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05057917 0.05326547 0.03612771], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05056702 0.05324468 0.0361488 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05055487 0.05322389 0.03616989], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05054272 0.0532031  0.03619099], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05053057 0.05318231 0.03621208], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05051842 0.05316152 0.03623317], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05050627 0.05314072 0.03625426], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05049412 0.05311993 0.03627535], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05048197 0.05309914 0.03629645], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05046981 0.05307835 0.03631754], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05045766 0.05305756 0.03633863], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05044551 0.05303677 0.03635972], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05043336 0.05301598 0.03638082], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0504212  0.05299519 0.03640191], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05040905 0.0529744  0.036423  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05039689 0.05295362 0.0364441 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05038474 0.05293283 0.03646519], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05037258 0.05291204 0.03648628], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05036043 0.05289125 0.03650738], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05034827 0.05287046 0.03652847], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05033612 0.05284967 0.03654956], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05032396 0.05282889 0.03657066], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0503118  0.0528081  0.03659175], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 250: Weights: [0.32008326 0.30717887 0.37273784]\n",
            "Gradient risk: [0.05029964 0.05278731 0.03661285], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05028749 0.05276653 0.03663394], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05027533 0.05274574 0.03665503], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05026317 0.05272495 0.03667613], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05025101 0.05270417 0.03669722], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05023885 0.05268338 0.03671832], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05022669 0.0526626  0.03673941], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05021453 0.05264181 0.0367605 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05020237 0.05262103 0.0367816 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05019021 0.05260024 0.03680269], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05017805 0.05257946 0.03682379], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05016589 0.05255867 0.03684488], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05015373 0.05253789 0.03686598], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05014157 0.05251711 0.03688707], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05012941 0.05249632 0.03690817], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05011725 0.05247554 0.03692926], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05010508 0.05245476 0.03695035], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05009292 0.05243398 0.03697145], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05008076 0.0524132  0.03699254], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05006859 0.05239241 0.03701364], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05005643 0.05237163 0.03703473], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05004427 0.05235085 0.03705582], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0500321  0.05233007 0.03707692], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05001994 0.05230929 0.03709801], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05000777 0.05228851 0.03711911], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04999561 0.05226774 0.0371402 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04998344 0.05224696 0.0371613 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04997128 0.05222618 0.03718239], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04995911 0.0522054  0.03720348], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04994695 0.05218462 0.03722458], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04993478 0.05216385 0.03724567], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04992261 0.05214307 0.03726676], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04991045 0.05212229 0.03728786], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04989828 0.05210152 0.03730895], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04988611 0.05208074 0.03733004], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04987394 0.05205997 0.03735114], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04986178 0.0520392  0.03737223], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04984961 0.05201842 0.03739332], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04983744 0.05199765 0.03741442], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04982527 0.05197688 0.03743551], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0498131 0.0519561 0.0374566], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04980093 0.05193533 0.03747769], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04978877 0.05191456 0.03749879], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0497766  0.05189379 0.03751988], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04976443 0.05187302 0.03754097], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04975226 0.05185225 0.03756206], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04974009 0.05183148 0.03758315], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04972792 0.05181071 0.03760424], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04971575 0.05178994 0.03762534], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04970358 0.05176917 0.03764643], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 300: Weights: [0.31741657 0.30197297 0.38061043]\n",
            "Gradient risk: [0.0496914  0.05174841 0.03766752], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04967923 0.05172764 0.03768861], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04966706 0.05170687 0.0377097 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04965489 0.05168611 0.03773079], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04964272 0.05166534 0.03775188], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04963055 0.05164458 0.03777297], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04961837 0.05162381 0.03779406], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0496062  0.05160305 0.03781515], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04959403 0.05158228 0.03783624], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04958186 0.05156152 0.03785733], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04956968 0.05154076 0.03787842], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04955751 0.05152    0.03789951], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04954534 0.05149924 0.03792059], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04953317 0.05147848 0.03794168], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04952099 0.05145772 0.03796277], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04950882 0.05143696 0.03798386], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04949664 0.0514162  0.03800495], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04948447 0.05139544 0.03802603], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0494723  0.05137468 0.03804712], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04946012 0.05135393 0.03806821], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04944795 0.05133317 0.03808929], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04943577 0.05131242 0.03811038], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0494236  0.05129166 0.03813146], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04941142 0.05127091 0.03815255], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04939925 0.05125015 0.03817364], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04938707 0.0512294  0.03819472], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0493749  0.05120865 0.0382158 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04936272 0.0511879  0.03823689], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04935055 0.05116715 0.03825797], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04933837 0.0511464  0.03827906], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0493262  0.05112565 0.03830014], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04931402 0.0511049  0.03832122], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04930184 0.05108415 0.0383423 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04928967 0.0510634  0.03836339], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04927749 0.05104265 0.03838447], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04926532 0.05102191 0.03840555], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04925314 0.05100116 0.03842663], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04924096 0.05098042 0.03844771], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04922879 0.05095967 0.03846879], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04921661 0.05093893 0.03848987], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04920443 0.05091819 0.03851095], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04919226 0.05089745 0.03853203], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04918008 0.0508767  0.03855311], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0491679  0.05085596 0.03857419], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04915572 0.05083522 0.03859527], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04914355 0.05081448 0.03861635], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04913137 0.05079375 0.03863742], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04911919 0.05077301 0.0386585 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04910701 0.05075227 0.03867958], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04909484 0.05073154 0.03870065], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 350: Weights: [0.31474413 0.29677596 0.38847987]\n",
            "Gradient risk: [0.04908266 0.0507108  0.03872173], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04907048 0.05069007 0.0387428 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0490583  0.05066933 0.03876388], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04904613 0.0506486  0.03878495], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04903395 0.05062787 0.03880603], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04902177 0.05060714 0.0388271 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04900959 0.0505864  0.03884817], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04899741 0.05056568 0.03886925], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04898524 0.05054495 0.03889032], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04897306 0.05052422 0.03891139], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04896088 0.05050349 0.03893246], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0489487  0.05048276 0.03895353], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04893652 0.05046204 0.0389746 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04892434 0.05044131 0.03899567], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04891217 0.05042059 0.03901674], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04889999 0.05039987 0.03903781], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04888781 0.05037914 0.03905888], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04887563 0.05035842 0.03907994], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04886345 0.0503377  0.03910101], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04885127 0.05031698 0.03912208], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0488391  0.05029626 0.03914314], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04882692 0.05027555 0.03916421], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04881474 0.05025483 0.03918527], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04880256 0.05023411 0.03920634], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04879038 0.0502134  0.0392274 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0487782  0.05019268 0.03924847], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04876602 0.05017197 0.03926953], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04875385 0.05015126 0.03929059], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04874167 0.05013054 0.03931165], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04872949 0.05010983 0.03933271], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04871731 0.05008912 0.03935377], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04870513 0.05006841 0.03937483], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04869295 0.05004771 0.03939589], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04868078 0.050027   0.03941695], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0486686  0.05000629 0.03943801], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04865642 0.04998559 0.03945907], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04864424 0.04996488 0.03948012], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04863206 0.04994418 0.03950118], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04861988 0.04992348 0.03952224], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04860771 0.04990278 0.03954329], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04859553 0.04988208 0.03956435], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04858335 0.04986138 0.0395854 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04857117 0.04984068 0.03960645], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04855899 0.04981998 0.0396275 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04854682 0.04979928 0.03964856], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04853464 0.04977859 0.03966961], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04852246 0.04975789 0.03969066], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04851028 0.0497372  0.03971171], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0484981  0.04971651 0.03973276], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04848593 0.04969581 0.03975381], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 400: Weights: [0.31206738 0.29159105 0.39634154]\n",
            "Gradient risk: [0.04847375 0.04967512 0.03977485], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04846157 0.04965443 0.0397959 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04844939 0.04963375 0.03981695], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04843722 0.04961306 0.03983799], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04842504 0.04959237 0.03985904], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04841286 0.04957169 0.03988008], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04840069 0.049551   0.03990113], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04838851 0.04953032 0.03992217], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04837633 0.04950964 0.03994321], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04836415 0.04948896 0.03996425], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04835198 0.04946828 0.0399853 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0483398  0.0494476  0.04000634], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04832762 0.04942692 0.04002737], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04831545 0.04940624 0.04004841], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04830327 0.04938557 0.04006945], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0482911  0.04936489 0.04009049], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04827892 0.04934422 0.04011153], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04826674 0.04932355 0.04013256], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04825457 0.04930287 0.0401536 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04824239 0.0492822  0.04017463], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04823022 0.04926153 0.04019566], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04821804 0.04924087 0.0402167 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04820587 0.0492202  0.04023773], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04819369 0.04919953 0.04025876], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04818151 0.04917887 0.04027979], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04816934 0.04915821 0.04030082], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04815717 0.04913754 0.04032185], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04814499 0.04911688 0.04034287], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04813282 0.04909622 0.0403639 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04812064 0.04907556 0.04038493], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04810847 0.0490549  0.04040595], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04809629 0.04903425 0.04042698], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04808412 0.04901359 0.040448  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04807195 0.04899294 0.04046902], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04805977 0.04897229 0.04049005], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0480476  0.04895163 0.04051107], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04803542 0.04893098 0.04053209], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04802325 0.04891033 0.04055311], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04801108 0.04888969 0.04057413], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04799891 0.04886904 0.04059514], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04798673 0.04884839 0.04061616], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04797456 0.04882775 0.04063718], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04796239 0.0488071  0.04065819], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04795022 0.04878646 0.04067921], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04793804 0.04876582 0.04070022], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04792587 0.04874518 0.04072123], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0479137  0.04872454 0.04074224], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04790153 0.04870391 0.04076325], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04788936 0.04868327 0.04078426], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04787719 0.04866264 0.04080527], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 450: Weights: [0.30938774 0.2864214  0.40419083]\n",
            "Gradient risk: [0.04786502 0.048642   0.04082628], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04785285 0.04862137 0.04084729], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04784068 0.04860074 0.04086829], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04782851 0.04858011 0.0408893 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04781634 0.04855948 0.0409103 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04780417 0.04853885 0.04093131], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.047792   0.04851823 0.04095231], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04777983 0.0484976  0.04097331], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04776766 0.04847698 0.04099431], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04775549 0.04845636 0.04101531], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04774332 0.04843574 0.04103631], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04773115 0.04841512 0.0410573 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04771899 0.0483945  0.0410783 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04770682 0.04837389 0.0410993 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04769465 0.04835327 0.04112029], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04768248 0.04833266 0.04114128], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04767032 0.04831204 0.04116228], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04765815 0.04829143 0.04118327], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04764598 0.04827082 0.04120426], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04763382 0.04825022 0.04122525], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04762165 0.04822961 0.04124624], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04760948 0.048209   0.04126722], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04759732 0.0481884  0.04128821], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04758515 0.0481678  0.0413092 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04757299 0.04814719 0.04133018], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04756082 0.04812659 0.04135116], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04754866 0.04810599 0.04137215], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0475365  0.0480854  0.04139313], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04752433 0.0480648  0.04141411], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04751217 0.04804421 0.04143509], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0475     0.04802361 0.04145606], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04748784 0.04800302 0.04147704], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04747568 0.04798243 0.04149802], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04746352 0.04796184 0.04151899], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04745135 0.04794126 0.04153997], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04743919 0.04792067 0.04156094], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04742703 0.04790008 0.04158191], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04741487 0.0478795  0.04160288], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04740271 0.04785892 0.04162385], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04739055 0.04783834 0.04164482], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04737839 0.04781776 0.04166578], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04736623 0.04779718 0.04168675], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04735407 0.04777661 0.04170771], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04734191 0.04775603 0.04172868], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04732975 0.04773546 0.04174964], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04731759 0.04771489 0.0417706 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04730543 0.04769432 0.04179156], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04729327 0.04767375 0.04181252], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04728112 0.04765318 0.04183348], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Stress Test Results: {'Black Swan': array([0.30404941, 0.27611053, 0.41984003]), 'Liquidity Crisis': array([0.30627312, 0.28042857, 0.41329828]), 'Interest Rate Shock': array([0.30676026, 0.28137299, 0.41186672])}\n",
            "Fat-Tailed Returns Simulation (First 5 Samples): [[ 0.01267502 -0.00028619 -0.0051903 ]\n",
            " [ 0.01246463  0.00465216  0.00800077]\n",
            " [ 0.00537407  0.01400212 -0.00452129]\n",
            " [-0.04092433 -0.05856108  0.02539303]\n",
            " [ 0.01327186  0.00959086  0.00469752]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.optimize as sco\n",
        "\n",
        "# Mean-Variance Optimization\n",
        "def mean_variance_optimization(Sigma, mu, lam=0.1):\n",
        "    \"\"\"\n",
        "    Perform Mean-Variance Optimization\n",
        "    Sigma: Covariance matrix\n",
        "    mu: Expected returns\n",
        "    lam: Risk-return tradeoff parameter\n",
        "    \"\"\"\n",
        "    n = len(mu)  # Number of assets\n",
        "\n",
        "    # Objective function to minimize (Risk - Return + Kolmogorov Penalty)\n",
        "    def objective(w):\n",
        "        # Risk (variance of the portfolio)\n",
        "        risk = w.T @ Sigma @ w\n",
        "        # Return (expected return)\n",
        "        ret = w.T @ mu\n",
        "        # Kolmogorov Complexity Penalty (L1 sparsity)\n",
        "        penalty = np.sum(np.abs(w))\n",
        "\n",
        "        return risk - lam * ret + penalty  # Total objective\n",
        "\n",
        "    # Constraints: Sum of weights = 1 (portfolio fully invested)\n",
        "    def constraint(w):\n",
        "        return np.sum(w) - 1\n",
        "\n",
        "    # Bounds: Weights should be between 0 and 1 (long-only portfolio)\n",
        "    bounds = [(0, 1) for _ in range(n)]\n",
        "\n",
        "    # Initial guess: equal weights\n",
        "    w0 = np.ones(n) / n\n",
        "\n",
        "    # Optimization\n",
        "    result = sco.minimize(objective, w0, method='SLSQP', bounds=bounds, constraints={'type': 'eq', 'fun': constraint})\n",
        "\n",
        "    return result.x  # Optimal portfolio weights\n",
        "\n",
        "# Example Data (Covariance matrix and expected returns)\n",
        "Sigma = np.array([[0.02, 0.015, -0.005], [0.015, 0.03, -0.008], [-0.005, -0.008, 0.01]])\n",
        "mu = np.array([0.01, 0.005, -0.002])  # Expected returns for 3 assets\n",
        "\n",
        "# Perform the optimization\n",
        "optimized_weights = mean_variance_optimization(Sigma, mu, lam=0.1)\n",
        "\n",
        "print(\"Optimized Portfolio Weights:\", optimized_weights)\n",
        "\n",
        "# Example of Monte Carlo simulation or stress test can be added here for further testing.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9bXxrCLnuNX",
        "outputId": "1e5ed683-6885-44f7-9414-b0cd5c2946ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights: [0.24478153 0.15935004 0.59586843]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import heapq\n",
        "\n",
        "# Create a text file to save all the outputs (again, appending this time)\n",
        "log_file = open(\"portfolio_optimization_results.txt\", \"a\")\n",
        "\n",
        "# Function to log output to the text file\n",
        "def log_output(output):\n",
        "    print(output)\n",
        "    log_file.write(output + \"\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# Portfolio Optimization with Simple Gradient Descent\n",
        "# -------------------------------\n",
        "\n",
        "def optimize_weights(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, lr=1e-2, epochs=500):\n",
        "    \"\"\"\n",
        "    Simple gradient descent for portfolio optimization\n",
        "    - Sigma: Covariance matrix\n",
        "    - mu: Expected returns\n",
        "    - lam: Risk-return tradeoff parameter\n",
        "    - gamma: Sparsity regularization parameter\n",
        "    - eta: Magnitude regularization parameter\n",
        "    - lr: Learning rate\n",
        "    - epochs: Number of iterations\n",
        "    \"\"\"\n",
        "    w = np.ones(Sigma.shape[0]) / Sigma.shape[0]  # Equal distribution of weights\n",
        "\n",
        "    log_output(f\"Initial weights: {w}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        grad = grad_objective(w, Sigma, mu, lam, eta)\n",
        "        w -= lr * grad\n",
        "\n",
        "        # Apply Proximal Operator for L0 (sparsity)\n",
        "        w[np.abs(w) < gamma] = 0\n",
        "\n",
        "        # Ensure non-negative weights and sum to 1\n",
        "        w = np.maximum(w, 0)\n",
        "        w /= np.sum(w + 1e-8)\n",
        "\n",
        "        if epoch % 50 == 0:  # Log every 50th iteration\n",
        "            log_output(f\"Epoch {epoch}: Weights: {w}\")\n",
        "\n",
        "    return w\n",
        "\n",
        "# -------------------------------\n",
        "# Gradient of the Objective Function\n",
        "# -------------------------------\n",
        "\n",
        "def grad_objective(w, Sigma, mu, lam, eta):\n",
        "    \"\"\"\n",
        "    Gradient of the risk-return objective function with L0 and L1 penalties\n",
        "    \"\"\"\n",
        "    grad_risk = 2 * Sigma @ w  # Risk (variance)\n",
        "    grad_ret = lam * mu  # Return (expected return)\n",
        "    grad_l1 = eta * np.sign(w)  # L1 penalty for magnitude regularization\n",
        "\n",
        "    log_output(f\"Gradient risk: {grad_risk}, Gradient return: {grad_ret}, Gradient L1: {grad_l1}\")\n",
        "\n",
        "    return grad_risk - grad_ret + grad_l1\n",
        "\n",
        "# -------------------------------\n",
        "# Stress Testing\n",
        "# -------------------------------\n",
        "\n",
        "def extreme_market_scenarios(Sigma, mu, scenarios):\n",
        "    \"\"\"\n",
        "    Perform stress tests by modifying Sigma and mu to simulate market scenarios.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for scenario in scenarios:\n",
        "        stressed_Sigma = Sigma + scenario['sigma_change']  # Add shocks to covariance\n",
        "        stressed_mu = mu + scenario['mu_change']  # Modify expected returns based on the scenario\n",
        "        optimized_weights = optimize_weights(stressed_Sigma, stressed_mu)\n",
        "        results[scenario['name']] = optimized_weights\n",
        "\n",
        "    return results\n",
        "\n",
        "# Define market stress scenarios\n",
        "extreme_scenarios = [\n",
        "    {'name': 'Market Bubble', 'sigma_change': 0.1 * np.identity(3), 'mu_change': -0.05 * np.ones(3)},\n",
        "    {'name': 'Liquidity Crisis', 'sigma_change': 0.2 * np.identity(3), 'mu_change': 0.0 * np.ones(3)},\n",
        "    {'name': 'Interest Rate Shock', 'sigma_change': 0.05 * np.identity(3), 'mu_change': 0.02 * np.ones(3)}\n",
        "]\n",
        "\n",
        "# Run the stress tests\n",
        "stress_test_results = extreme_market_scenarios(Sigma, mu, extreme_scenarios)\n",
        "\n",
        "# Log the results\n",
        "log_output(f\"Stress Test Results: {stress_test_results}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Monte Carlo Simulation\n",
        "# -------------------------------\n",
        "\n",
        "def monte_carlo_testing_large(Sigma, mu, n_samples=1000, n_assets=100, lr=1e-3, epochs=500):\n",
        "    \"\"\"\n",
        "    Perform Monte Carlo simulation on large portfolios with randomized covariances and returns.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for _ in range(n_samples):\n",
        "        # Randomly perturb the Sigma and mu to simulate uncertainty\n",
        "        Sigma_randomized = Sigma + 0.05 * np.random.randn(n_assets, n_assets)\n",
        "        mu_randomized = mu + 0.01 * np.random.randn(n_assets)\n",
        "\n",
        "        # Run optimization on the perturbed data\n",
        "        optimized_weights = optimize_weights(Sigma_randomized, mu_randomized)\n",
        "        results.append(optimized_weights)\n",
        "\n",
        "    return np.array(results)\n",
        "\n",
        "# Run the Monte Carlo Simulation\n",
        "monte_carlo_results = monte_carlo_testing_large(Sigma, mu, n_samples=1000, n_assets=100)\n",
        "\n",
        "# Log the results\n",
        "log_output(f\"Monte Carlo Simulation Results for Large Portfolio: {monte_carlo_results[:5]}\")  # Show first 5 samples for brevity\n",
        "\n",
        "# -------------------------------\n",
        "# Sensitivity Analysis\n",
        "# -------------------------------\n",
        "\n",
        "def sensitivity_analysis(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, perturbation=0.05):\n",
        "    original_weights = optimize_weights(Sigma, mu, lam, gamma, eta)\n",
        "\n",
        "    sensitivities = {}\n",
        "    # Sensitivity analysis for mu (expected returns)\n",
        "    for i in range(len(mu)):\n",
        "        perturbed_mu = mu.copy()\n",
        "        perturbed_mu[i] += perturbation\n",
        "        weights_perturbed = optimize_weights(Sigma, perturbed_mu, lam, gamma, eta)\n",
        "        sensitivity = np.abs(weights_perturbed - original_weights)\n",
        "        sensitivities[i] = sensitivity\n",
        "\n",
        "    # Sensitivity analysis for Sigma (covariance matrix)\n",
        "    cov_sensitivities = {}\n",
        "    for i in range(Sigma.shape[0]):\n",
        "        for j in range(Sigma.shape[1]):\n",
        "            perturbed_Sigma = Sigma.copy()\n",
        "            perturbed_Sigma[i, j] += perturbation\n",
        "            weights_perturbed = optimize_weights(perturbed_Sigma, mu, lam, gamma, eta)\n",
        "            cov_sensitivities[(i, j)] = np.abs(weights_perturbed - original_weights)\n",
        "\n",
        "    return sensitivities, cov_sensitivities\n",
        "\n",
        "# Perform Sensitivity Analysis\n",
        "sensitivity_results, cov_sensitivity_results = sensitivity_analysis(Sigma, mu)\n",
        "\n",
        "# Log the results\n",
        "log_output(f\"Sensitivity Analysis (1st Order): {sensitivity_results}\")\n",
        "log_output(f\"Covariance Sensitivity Analysis: {cov_sensitivity_results}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Running the Optimization and Stress Tests\n",
        "# -------------------------------\n",
        "\n",
        "# Example Data (Covariance matrix and expected returns)\n",
        "Sigma = np.array([[0.02, 0.015, -0.005], [0.015, 0.03, -0.008], [-0.005, -0.008, 0.01]])\n",
        "mu = np.array([0.01, 0.005, -0.002])  # Expected returns for 3 assets\n",
        "\n",
        "# Run the Portfolio Optimization\n",
        "optimized_weights = optimize_weights(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2)\n",
        "\n",
        "log_output(f\"Optimized Weights: {optimized_weights}\")\n",
        "\n",
        "# Perform Stress Tests\n",
        "stress_test_results = extreme_market_scenarios(Sigma, mu, extreme_scenarios)\n",
        "\n",
        "log_output(f\"Stress Test Results: {stress_test_results}\")\n",
        "\n",
        "# Simulate Fat-Tailed Returns\n",
        "fat_tailed_returns = simulate_fat_tailed_returns(Sigma, mu)\n",
        "\n",
        "log_output(f\"Fat-Tailed Returns Simulation (First 5 Samples): {fat_tailed_returns[:5]}\")\n",
        "\n",
        "# Close the log file after writing all results\n",
        "log_file.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "27J9jsPCoA_5",
        "outputId": "4594cdcd-a3c5-424a-ea0c-e1c868d7552f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Gradient risk: [0.08666667 0.09133333 0.06466667], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 0: Weights: [0.33328106 0.33322925 0.33348966]\n",
            "Gradient risk: [0.08664944 0.0913022  0.06470325], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0866322  0.09127106 0.06473984], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08661496 0.09123992 0.06477644], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08659772 0.09120877 0.06481305], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08658047 0.09117761 0.06484967], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08656322 0.09114644 0.06488631], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08654596 0.09111526 0.06492295], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08652869 0.09108408 0.0649596 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08651142 0.09105288 0.06499626], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08649415 0.09102168 0.06503293], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08647687 0.09099048 0.06506961], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08645958 0.09095926 0.0651063 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08644229 0.09092804 0.06514301], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.086425   0.0908968  0.06517972], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08640769 0.09086556 0.06521644], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08639039 0.09083432 0.06525317], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08637308 0.09080306 0.06528991], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08635576 0.0907718  0.06532666], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08633844 0.09074053 0.06536342], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08632111 0.09070925 0.06540019], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08630378 0.09067796 0.06543697], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08628644 0.09064666 0.06547376], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0862691  0.09061536 0.06551056], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08625175 0.09058405 0.06554737], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0862344  0.09055273 0.06558419], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08621704 0.09052141 0.06562102], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08619968 0.09049007 0.06565786], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08618231 0.09045873 0.06569471], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08616494 0.09042738 0.06573157], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08614756 0.09039602 0.06576844], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08613018 0.09036466 0.06580532], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08611279 0.09033329 0.06584221], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0860954  0.09030191 0.0658791 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.086078   0.09027052 0.06591601], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08606059 0.09023912 0.06595293], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08604318 0.09020772 0.06598986], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08602577 0.0901763  0.06602679], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08600835 0.09014489 0.06606374], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08599093 0.09011346 0.0661007 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0859735  0.09008202 0.06613767], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08595606 0.09005058 0.06617464], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08593862 0.09001913 0.06621163], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08592118 0.08998767 0.06624862], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08590373 0.08995621 0.06628563], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08588627 0.08992473 0.06632264], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08586881 0.08989325 0.06635967], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08585135 0.08986176 0.0663967 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08583388 0.08983026 0.06643375], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0858164  0.08979876 0.0664708 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08579892 0.08976725 0.06650787], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 50: Weights: [0.33064691 0.32799264 0.34136042]\n",
            "Gradient risk: [0.08578143 0.08973573 0.06654494], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08576394 0.0897042  0.06658202], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08574645 0.08967267 0.06661912], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08572895 0.08964112 0.06665622], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08571144 0.08960957 0.06669333], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08569393 0.08957801 0.06673045], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08567641 0.08954645 0.06676758], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08565889 0.08951487 0.06680473], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08564136 0.08948329 0.06684188], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08562383 0.0894517  0.06687904], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0856063  0.08942011 0.06691621], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08558876 0.0893885  0.06695339], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08557121 0.08935689 0.06699058], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08555366 0.08932527 0.06702777], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0855361  0.08929365 0.06706498], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08551854 0.08926201 0.0671022 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08550097 0.08923037 0.06713943], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0854834  0.08919872 0.06717667], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08546582 0.08916706 0.06721391], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08544824 0.0891354  0.06725117], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08543065 0.08910373 0.06728843], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08541306 0.08907205 0.06732571], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08539546 0.08904036 0.06736299], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08537786 0.08900867 0.06740029], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08536026 0.08897696 0.06743759], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08534264 0.08894525 0.06747491], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08532503 0.08891354 0.06751223], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0853074  0.08888181 0.06754956], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08528978 0.08885008 0.0675869 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08527214 0.08881834 0.06762426], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08525451 0.08878659 0.06766162], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08523686 0.08875484 0.06769899], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08521922 0.08872307 0.06773637], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08520157 0.0886913  0.06777376], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08518391 0.08865953 0.06781116], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08516625 0.08862774 0.06784856], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08514858 0.08859595 0.06788598], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08513091 0.08856415 0.06792341], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08511323 0.08853234 0.06796085], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08509555 0.08850053 0.06799829], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08507786 0.08846871 0.06803575], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08506017 0.08843688 0.06807321], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08504247 0.08840504 0.06811069], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08502477 0.0883732  0.06814817], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08500706 0.08834134 0.06818566], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08498935 0.08830948 0.06822317], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08497163 0.08827762 0.06826068], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08495391 0.08824574 0.0682982 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08493618 0.08821386 0.06833573], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08491845 0.08818197 0.06837327], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 100: Weights: [0.32797191 0.32269337 0.34933469]\n",
            "Gradient risk: [0.08490071 0.08815008 0.06841082], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08488297 0.08811817 0.06844838], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08486522 0.08808626 0.06848595], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08484747 0.08805435 0.06852352], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08482971 0.08802242 0.06856111], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08481195 0.08799049 0.06859871], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08479419 0.08795855 0.06863631], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08477641 0.0879266  0.06867393], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08475864 0.08789464 0.06871155], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08474086 0.08786268 0.06874918], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08472307 0.08783071 0.06878683], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08470528 0.08779874 0.06882448], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08468748 0.08776675 0.06886214], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08466968 0.08773476 0.06889981], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08465187 0.08770276 0.06893749], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08463406 0.08767076 0.06897518], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08461625 0.08763874 0.06901288], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08459843 0.08760672 0.06905058], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0845806  0.08757469 0.0690883 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08456277 0.08754266 0.06912603], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08454493 0.08751062 0.06916376], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08452709 0.08747857 0.06920151], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08450925 0.08744651 0.06923926], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0844914  0.08741445 0.06927702], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08447354 0.08738238 0.0693148 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08445568 0.0873503  0.06935258], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08443782 0.08731821 0.06939037], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08441995 0.08728612 0.06942817], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08440207 0.08725402 0.06946598], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08438419 0.08722191 0.06950379], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08436631 0.0871898  0.06954162], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08434842 0.08715767 0.06957946], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08433052 0.08712555 0.0696173 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08431262 0.08709341 0.06965516], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08429472 0.08706127 0.06969302], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08427681 0.08702912 0.06973089], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0842589  0.08699696 0.06976877], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08424098 0.08696479 0.06980666], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08422305 0.08693262 0.06984456], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08420513 0.08690044 0.06988247], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08418719 0.08686826 0.06992039], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08416925 0.08683607 0.06995832], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08415131 0.08680387 0.06999626], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08413336 0.08677166 0.0700342 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08411541 0.08673944 0.07007216], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08409745 0.08670722 0.07011012], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08407949 0.08667499 0.07014809], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08406152 0.08664276 0.07018607], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08404355 0.08661052 0.07022406], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08402557 0.08657827 0.07026206], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 150: Weights: [0.32525693 0.31733402 0.35740902]\n",
            "Gradient risk: [0.08400759 0.08654601 0.07030007], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08398961 0.08651374 0.07033809], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08397162 0.08648147 0.07037612], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08395362 0.0864492  0.07041415], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08393562 0.08641691 0.0704522 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08391761 0.08638462 0.07049025], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0838996  0.08635232 0.07052831], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08388159 0.08632001 0.07056638], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08386357 0.0862877  0.07060446], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08384554 0.08625538 0.07064255], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08382751 0.08622305 0.07068065], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08380948 0.08619072 0.07071876], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08379144 0.08615838 0.07075687], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08377339 0.08612603 0.070795  ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08375534 0.08609368 0.07083313], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08373729 0.08606131 0.07087128], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08371923 0.08602895 0.07090943], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08370117 0.08599657 0.07094759], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0836831  0.08596419 0.07098576], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08366503 0.0859318  0.07102394], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08364695 0.0858994  0.07106212], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08362887 0.085867   0.07110032], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08361078 0.08583459 0.07113853], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08359269 0.08580217 0.07117674], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08357459 0.08576975 0.07121496], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08355649 0.08573732 0.07125319], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08353839 0.08570488 0.07129143], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08352027 0.08567243 0.07132968], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08350216 0.08563998 0.07136794], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08348404 0.08560752 0.07140621], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08346591 0.08557506 0.07144448], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08344778 0.08554258 0.07148277], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08342965 0.08551011 0.07152106], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08341151 0.08547762 0.07155936], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08339337 0.08544513 0.07159767], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08337522 0.08541263 0.07163599], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08335706 0.08538012 0.07167432], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08333891 0.08534761 0.07171266], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08332074 0.08531509 0.071751  ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08330258 0.08528256 0.07178936], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0832844  0.08525003 0.07182772], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08326623 0.08521749 0.07186609], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08324804 0.08518494 0.07190447], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08322986 0.08515239 0.07194286], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08321167 0.08511983 0.07198126], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08319347 0.08508726 0.07201967], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08317527 0.08505468 0.07205808], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08315707 0.0850221  0.07209651], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08313886 0.08498952 0.07213494], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08312064 0.08495692 0.07217338], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 200: Weights: [0.32250291 0.31191734 0.36557972]\n",
            "Gradient risk: [0.08310242 0.08492432 0.07221183], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0830842  0.08489171 0.07225029], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08306597 0.0848591  0.07228876], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08304774 0.08482648 0.07232723], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0830295  0.08479385 0.07236572], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08301126 0.08476121 0.07240421], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08299301 0.08472857 0.07244271], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08297476 0.08469593 0.07248122], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0829565  0.08466327 0.07251974], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08293824 0.08463061 0.07255827], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08291997 0.08459794 0.0725968 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0829017  0.08456527 0.07263535], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08288343 0.08453259 0.0726739 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08286515 0.0844999  0.07271246], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08284686 0.0844672  0.07275103], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08282858 0.0844345  0.07278961], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08281028 0.0844018  0.0728282 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08279198 0.08436908 0.07286679], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08277368 0.08433636 0.0729054 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08275537 0.08430363 0.07294401], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08273706 0.0842709  0.07298263], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08271875 0.08423816 0.07302126], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08270042 0.08420541 0.0730599 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0826821  0.08417266 0.07309855], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08266377 0.0841399  0.0731372 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08264543 0.08410713 0.07317587], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08262709 0.08407436 0.07321454], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08260875 0.08404158 0.07325322], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0825904  0.08400879 0.07329191], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08257205 0.083976   0.0733306 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08255369 0.0839432  0.07336931], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08253533 0.08391039 0.07340802], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08251696 0.08387758 0.07344675], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08249859 0.08384476 0.07348548], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08248021 0.08381194 0.07352422], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08246183 0.08377911 0.07356297], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08244344 0.08374627 0.07360172], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08242505 0.08371342 0.07364049], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08240666 0.08368057 0.07367926], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08238826 0.08364771 0.07371804], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08236986 0.08361485 0.07375683], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08235145 0.08358198 0.07379563], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08233304 0.0835491  0.07383443], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08231462 0.08351622 0.07387325], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0822962  0.08348333 0.07391207], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08227777 0.08345043 0.0739509 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08225934 0.08341753 0.07398974], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0822409  0.08338462 0.07402859], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08222246 0.08335171 0.07406744], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08220402 0.08331879 0.07410631], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 250: Weights: [0.31971089 0.30644622 0.37384286]\n",
            "Gradient risk: [0.08218557 0.08328586 0.07414518], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08216712 0.08325292 0.07418406], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08214866 0.08321998 0.07422295], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0821302  0.08318704 0.07426185], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08211173 0.08315408 0.07430075], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08209326 0.08312112 0.07433967], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08207478 0.08308816 0.07437859], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0820563  0.08305519 0.07441752], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08203782 0.08302221 0.07445646], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08201933 0.08298922 0.0744954 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08200083 0.08295623 0.07453436], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08198233 0.08292323 0.07457332], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08196383 0.08289023 0.07461229], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08194532 0.08285722 0.07465127], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08192681 0.0828242  0.07469026], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0819083  0.08279118 0.07472925], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08188978 0.08275815 0.07476826], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08187125 0.08272512 0.07480727], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08185272 0.08269208 0.07484629], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08183419 0.08265903 0.07488532], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08181565 0.08262597 0.07492435], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0817971  0.08259291 0.0749634 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08177856 0.08255985 0.07500245], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08176    0.08252678 0.07504151], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08174145 0.0824937  0.07508058], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08172289 0.08246061 0.07511965], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08170432 0.08242752 0.07515874], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08168575 0.08239443 0.07519783], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08166718 0.08236132 0.07523693], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0816486  0.08232821 0.07527604], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08163002 0.0822951  0.07531515], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08161143 0.08226198 0.07535428], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08159284 0.08222885 0.07539341], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08157424 0.08219571 0.07543255], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08155564 0.08216257 0.0754717 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08153703 0.08212943 0.07551086], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08151843 0.08209628 0.07555002], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08149981 0.08206312 0.07558919], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08148119 0.08202995 0.07562837], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08146257 0.08199678 0.07566756], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08144394 0.08196361 0.07570676], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08142531 0.08193042 0.07574596], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08140668 0.08189724 0.07578517], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08138804 0.08186404 0.07582439], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08136939 0.08183084 0.07586362], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08135074 0.08179763 0.07590286], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08133209 0.08176442 0.0759421 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08131343 0.0817312  0.07598135], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08129477 0.08169798 0.07602061], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0812761  0.08166475 0.07605988], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 300: Weights: [0.31688194 0.3009237  0.38219433]\n",
            "Gradient risk: [0.08125743 0.08163151 0.07609915], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08123876 0.08159827 0.07613844], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08122008 0.08156502 0.07617773], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08120139 0.08153176 0.07621703], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08118271 0.0814985  0.07625633], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08116401 0.08146524 0.07629565], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08114532 0.08143196 0.07633497], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08112662 0.08139869 0.0763743 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08110791 0.0813654  0.07641364], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0810892  0.08133211 0.07645298], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08107049 0.08129882 0.07649234], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08105177 0.08126551 0.0765317 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08103305 0.08123221 0.07657107], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08101432 0.08119889 0.07661044], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08099559 0.08116557 0.07664983], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08097685 0.08113225 0.07668922], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08095811 0.08109891 0.07672862], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08093937 0.08106558 0.07676803], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08092062 0.08103223 0.07680744], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08090187 0.08099888 0.07684686], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08088311 0.08096553 0.07688629], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08086435 0.08093217 0.07692573], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08084558 0.0808988  0.07696518], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08082681 0.08086543 0.07700463], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08080804 0.08083205 0.07704409], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08078926 0.08079867 0.07708356], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08077048 0.08076528 0.07712304], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08075169 0.08073188 0.07716252], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0807329  0.08069848 0.07720201], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0807141  0.08066507 0.07724151], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0806953  0.08063166 0.07728102], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0806765  0.08059824 0.07732053], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08065769 0.08056482 0.07736005], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08063888 0.08053139 0.07739958], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08062006 0.08049795 0.07743912], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08060124 0.08046451 0.07747867], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08058242 0.08043106 0.07751822], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08056359 0.08039761 0.07755778], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08054475 0.08036415 0.07759735], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08052592 0.08033069 0.07763692], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08050707 0.08029722 0.0776765 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08048823 0.08026374 0.07771609], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08046938 0.08023026 0.07775569], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08045052 0.08019677 0.07779529], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08043166 0.08016328 0.07783491], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0804128  0.08012978 0.07787453], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08039393 0.08009628 0.07791415], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08037506 0.08006277 0.07795379], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08035619 0.08002925 0.07799343], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08033731 0.07999573 0.07803308], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 350: Weights: [0.31401722 0.29535294 0.3906298 ]\n",
            "Gradient risk: [0.08031842 0.0799622  0.07807274], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08029954 0.07992867 0.0781124 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08028064 0.07989513 0.07815207], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08026175 0.07986159 0.07819175], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08024285 0.07982804 0.07823144], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08022394 0.07979449 0.07827113], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08020503 0.07976093 0.07831084], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08018612 0.07972736 0.07835054], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0801672  0.07969379 0.07839026], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08014828 0.07966022 0.07842998], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08012936 0.07962663 0.07846971], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08011043 0.07959305 0.07850945], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08009149 0.07955945 0.0785492 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08007256 0.07952585 0.07858895], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08005361 0.07949225 0.07862871], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08003467 0.07945864 0.07866848], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.08001572 0.07942502 0.07870825], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07999676 0.0793914  0.07874803], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0799778  0.07935778 0.07878782], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07995884 0.07932415 0.07882762], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07993988 0.07929051 0.07886742], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0799209  0.07925687 0.07890724], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07990193 0.07922322 0.07894705], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07988295 0.07918957 0.07898688], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07986397 0.07915591 0.07902671], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07984498 0.07912224 0.07906655], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07982599 0.07908857 0.0791064 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07980699 0.0790549  0.07914625], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07978799 0.07902122 0.07918611], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07976899 0.07898753 0.07922598], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07974998 0.07895384 0.07926586], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07973097 0.07892015 0.07930574], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07971196 0.07888644 0.07934563], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07969294 0.07885274 0.07938553], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07967391 0.07881903 0.07942543], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07965489 0.07878531 0.07946535], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07963585 0.07875158 0.07950526], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07961682 0.07871786 0.07954519], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07959778 0.07868412 0.07958512], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07957873 0.07865038 0.07962506], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07955969 0.07861664 0.07966501], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07954063 0.07858289 0.07970496], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07952158 0.07854914 0.07974492], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07950252 0.07851538 0.07978489], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07948345 0.07848161 0.07982487], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07946439 0.07844784 0.07986485], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07944531 0.07841406 0.07990484], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07942624 0.07838028 0.07994484], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07940716 0.0783465  0.07998484], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07938807 0.07831271 0.08002485], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 400: Weights: [0.31111798 0.28973725 0.39914474]\n",
            "Gradient risk: [0.07936899 0.07827891 0.08006487], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07934989 0.07824511 0.08010489], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0793308  0.0782113  0.08014492], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0793117  0.07817749 0.08018496], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07929259 0.07814367 0.080225  ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07927349 0.07810985 0.08026506], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07925437 0.07807602 0.08030512], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07923526 0.07804219 0.08034518], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07921614 0.07800835 0.08038525], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07919701 0.07797451 0.08042533], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07917789 0.07794066 0.08046542], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07915876 0.07790681 0.08050551], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07913962 0.07787295 0.08054561], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07912048 0.07783908 0.08058572], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07910134 0.07780522 0.08062584], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07908219 0.07777134 0.08066596], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07906304 0.07773746 0.08070608], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07904388 0.07770358 0.08074622], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07902472 0.07766969 0.08078636], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07900556 0.0776358  0.08082651], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07898639 0.0776019  0.08086666], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07896722 0.07756799 0.08090683], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07894805 0.07753409 0.08094699], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07892887 0.07750017 0.08098717], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07890969 0.07746625 0.08102735], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0788905  0.07743233 0.08106754], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07887131 0.0773984  0.08110774], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07885211 0.07736447 0.08114794], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07883292 0.07733053 0.08118815], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07881371 0.07729658 0.08122836], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07879451 0.07726263 0.08126859], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0787753  0.07722868 0.08130882], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07875608 0.07719472 0.08134905], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07873687 0.07716076 0.08138929], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07871764 0.07712679 0.08142954], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07869842 0.07709282 0.0814698 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07867919 0.07705884 0.08151006], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07865996 0.07702485 0.08155033], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07864072 0.07699087 0.08159061], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07862148 0.07695687 0.08163089], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07860223 0.07692287 0.08167118], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07858299 0.07688887 0.08171148], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07856373 0.07685486 0.08175178], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07854448 0.07682085 0.08179209], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07852522 0.07678683 0.0818324 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07850595 0.07675281 0.08187272], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07848669 0.07671879 0.08191305], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07846742 0.07668475 0.08195339], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07844814 0.07665072 0.08199373], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07842886 0.07661668 0.08203408], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 450: Weights: [0.30818551 0.28408006 0.40773441]\n",
            "Gradient risk: [0.07840958 0.07658263 0.08207443], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07839029 0.07654858 0.08211479], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.078371   0.07651452 0.08215516], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07835171 0.07648046 0.08219554], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07833241 0.0764464  0.08223592], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07831311 0.07641233 0.08227631], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0782938  0.07637825 0.0823167 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07827449 0.07634417 0.0823571 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07825518 0.07631009 0.08239751], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07823586 0.076276   0.08243792], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07821654 0.07624191 0.08247834], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07819722 0.07620781 0.08251876], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07817789 0.07617371 0.0825592 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07815856 0.0761396  0.08259964], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07813922 0.07610549 0.08264008], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07811988 0.07607137 0.08268053], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07810054 0.07603725 0.08272099], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07808119 0.07600312 0.08276146], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07806184 0.07596899 0.08280193], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07804249 0.07593486 0.0828424 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07802313 0.07590072 0.08288289], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07800377 0.07586657 0.08292338], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07798441 0.07583242 0.08296387], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07796504 0.07579827 0.08300437], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07794566 0.07576411 0.08304488], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07792629 0.07572995 0.0830854 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07790691 0.07569578 0.08312592], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07788752 0.07566161 0.08316645], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07786814 0.07562743 0.08320698], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07784874 0.07559325 0.08324752], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07782935 0.07555906 0.08328807], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07780995 0.07552487 0.08332862], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07779055 0.07549068 0.08336918], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07777114 0.07545648 0.08340974], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07775173 0.07542227 0.08345031], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07773232 0.07538807 0.08349089], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0777129  0.07535385 0.08353148], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07769348 0.07531964 0.08357206], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07767406 0.07528541 0.08361266], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07765463 0.07525119 0.08365326], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0776352  0.07521696 0.08369387], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07761576 0.07518272 0.08373449], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07759633 0.07514848 0.08377511], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07757688 0.07511424 0.08381573], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07755744 0.07507999 0.08385636], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07753799 0.07504574 0.083897  ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07751853 0.07501148 0.08393765], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07749908 0.07497722 0.0839783 ], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.07747962 0.07494295 0.08401896], Gradient return: [-0.004  -0.0045 -0.0052], Gradient L1: [0.01 0.01 0.01]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Gradient risk: [0.15333333 0.158      0.13133333], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 0: Weights: [0.33328097 0.33322905 0.33348995]\n",
            "Gradient risk: [0.1533056  0.15794795 0.1314013 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15327786 0.15789591 0.13146929], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15325012 0.15784386 0.13153728], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15322238 0.1577918  0.13160528], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15319463 0.15773974 0.13167329], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15316687 0.15768767 0.13174131], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15313911 0.1576356  0.13180933], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15311135 0.15758352 0.13187736], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15308358 0.15753144 0.13194541], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15305581 0.15747935 0.13201346], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15302804 0.15742725 0.13208151], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15300026 0.15737516 0.13214958], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15297247 0.15732305 0.13221766], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15294469 0.15727094 0.13228574], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15291689 0.15721883 0.13235383], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1528891  0.15716671 0.13242193], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15286129 0.15711458 0.13249003], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15283349 0.15706245 0.13255815], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15280568 0.15701032 0.13262627], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15277786 0.15695818 0.1326944 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15275005 0.15690603 0.13276254], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15272222 0.15685388 0.13283069], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1526944  0.15680172 0.13289885], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15266656 0.15674956 0.13296701], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15263873 0.1566974  0.13303518], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15261089 0.15664522 0.13310336], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15258304 0.15659305 0.13317155], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1525552  0.15654087 0.13323974], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15252734 0.15648868 0.13330795], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15249949 0.15643649 0.13337616], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15247162 0.15638429 0.13344438], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15244376 0.15633209 0.1335126 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15241589 0.15627988 0.13358084], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15238802 0.15622767 0.13364908], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15236014 0.15617546 0.13371733], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15233226 0.15612323 0.13378559], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15230437 0.15607101 0.13385386], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15227648 0.15601878 0.13392213], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15224858 0.15596654 0.13399041], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15222068 0.1559143  0.1340587 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15219278 0.15586205 0.134127  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15216487 0.1558098  0.13419531], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15213696 0.15575755 0.13426362], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15210905 0.15570529 0.13433194], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15208113 0.15565302 0.13440027], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1520532  0.15560075 0.1344686 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15202527 0.15554848 0.13453695], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15199734 0.1554962  0.1346053 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15196941 0.15544391 0.13467366], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15194147 0.15539162 0.13474202], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 50: Weights: [0.33065202 0.3280028  0.34134515]\n",
            "Gradient risk: [0.15191352 0.15533933 0.1348104 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15188557 0.15528703 0.13487878], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15185762 0.15523472 0.13494717], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15182966 0.15518242 0.13501557], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1518017  0.1551301  0.13508397], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15177374 0.15507779 0.13515238], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15174577 0.15502546 0.1352208 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15171779 0.15497314 0.13528923], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15168982 0.1549208  0.13535766], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15166183 0.15486847 0.1354261 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15163385 0.15481613 0.13549455], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15160586 0.15476378 0.13556301], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15157787 0.15471143 0.13563147], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15154987 0.15465907 0.13569995], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15152187 0.15460671 0.13576843], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15149386 0.15455435 0.13583691], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15146585 0.15450198 0.13590541], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15143784 0.15444961 0.13597391], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15140982 0.15439723 0.13604241], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1513818  0.15434485 0.13611093], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15135377 0.15429246 0.13617945], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15132574 0.15424007 0.13624798], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15129771 0.15418767 0.13631652], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15126967 0.15413527 0.13638507], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15124163 0.15408287 0.13645362], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15121358 0.15403046 0.13652218], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15118553 0.15397804 0.13659074], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15115748 0.15392562 0.13665932], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15112942 0.1538732  0.1367279 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15110136 0.15382077 0.13679649], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15107329 0.15376834 0.13686508], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15104522 0.15371591 0.13693368], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15101715 0.15366347 0.13700229], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15098907 0.15361102 0.13707091], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15096099 0.15355857 0.13713953], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15093291 0.15350612 0.13720816], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15090482 0.15345366 0.1372768 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15087672 0.1534012  0.13734544], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15084863 0.15334873 0.1374141 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15082052 0.15329626 0.13748275], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15079242 0.15324379 0.13755142], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15076431 0.15319131 0.13762009], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1507362  0.15313883 0.13768877], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15070808 0.15308634 0.13775746], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15067996 0.15303385 0.13782615], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15065184 0.15298135 0.13789485], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15062371 0.15292885 0.13796356], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15059558 0.15287635 0.13803227], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15056744 0.15282384 0.13810099], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1505393  0.15277132 0.13816972], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 100: Weights: [0.32800226 0.32275358 0.34924414]\n",
            "Gradient risk: [0.15051116 0.15271881 0.13823846], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15048301 0.15266629 0.1383072 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15045486 0.15261376 0.13837595], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15042671 0.15256123 0.1384447 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15039855 0.1525087  0.13851346], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15037038 0.15245616 0.13858223], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15034222 0.15240362 0.13865101], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15031405 0.15235107 0.13871979], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15028587 0.15229852 0.13878858], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1502577  0.15224597 0.13885737], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15022951 0.15219341 0.13892618], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15020133 0.15214085 0.13899498], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15017314 0.15208829 0.1390638 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15014495 0.15203572 0.13913262], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15011675 0.15198314 0.13920145], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15008855 0.15193057 0.13927028], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15006035 0.15187798 0.13933913], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15003214 0.1518254  0.13940797], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.15000393 0.15177281 0.13947683], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14997571 0.15172022 0.13954569], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14994749 0.15166762 0.13961456], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14991927 0.15161502 0.13968343], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14989104 0.15156241 0.13975231], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14986281 0.1515098  0.1398212 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14983458 0.15145719 0.13989009], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14980634 0.15140458 0.13995899], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1497781  0.15135195 0.1400279 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14974986 0.15129933 0.14009681], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14972161 0.1512467  0.14016573], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14969336 0.15119407 0.14023466], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1496651  0.15114144 0.14030359], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14963684 0.1510888  0.14037253], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14960858 0.15103615 0.14044147], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14958031 0.15098351 0.14051043], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14955204 0.15093086 0.14057938], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14952377 0.1508782  0.14064835], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14949549 0.15082555 0.14071732], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14946721 0.15077288 0.14078629], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14943892 0.15072022 0.14085528], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14941064 0.15066755 0.14092426], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14938234 0.15061488 0.14099326], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14935405 0.1505622  0.14106226], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14932575 0.15050952 0.14113127], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14929745 0.15045684 0.14120028], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14926914 0.15040415 0.1412693 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14924083 0.15035146 0.14133833], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14921252 0.15029877 0.14140736], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1491842  0.15024607 0.14147639], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14915588 0.15019337 0.14154544], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14912755 0.15014067 0.14161449], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 150: Weights: [0.32533299 0.31748454 0.35718244]\n",
            "Gradient risk: [0.14909923 0.15008796 0.14168354], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14907089 0.15003525 0.14175261], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14904256 0.14998253 0.14182167], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14901422 0.14992981 0.14189075], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14898588 0.14987709 0.14195983], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14895753 0.14982437 0.14202891], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14892918 0.14977164 0.142098  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14890083 0.14971891 0.1421671 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14887248 0.14966617 0.14223621], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14884412 0.14961343 0.14230532], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14881575 0.14956069 0.14237443], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14878739 0.14950794 0.14244355], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14875902 0.1494552  0.14251268], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14873064 0.14940244 0.14258181], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14870227 0.14934969 0.14265095], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14867389 0.14929693 0.1427201 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1486455  0.14924417 0.14278925], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14861712 0.1491914  0.1428584 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14858873 0.14913863 0.14292756], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14856033 0.14908586 0.14299673], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14853193 0.14903308 0.1430659 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14850353 0.14898031 0.14313508], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14847513 0.14892752 0.14320427], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14844672 0.14887474 0.14327346], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14841831 0.14882195 0.14334265], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1483899  0.14876916 0.14341186], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14836148 0.14871637 0.14348106], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14833306 0.14866357 0.14355028], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14830463 0.14861077 0.14361949], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1482762  0.14855796 0.14368872], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14824777 0.14850516 0.14375795], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14821934 0.14845235 0.14382718], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1481909  0.14839953 0.14389642], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14816246 0.14834672 0.14396567], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14813401 0.1482939  0.14403492], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14810557 0.14824107 0.14410418], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14807712 0.14818825 0.14417344], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14804866 0.14813542 0.14424271], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1480202  0.14808259 0.14431198], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14799174 0.14802975 0.14438126], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14796328 0.14797692 0.14445054], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14793481 0.14792408 0.14451983], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14790634 0.14787123 0.14458913], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14787786 0.14781839 0.14465843], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14784939 0.14776554 0.14472773], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1478209  0.14771268 0.14479704], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14779242 0.14765983 0.14486636], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14776393 0.14760697 0.14493568], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14773544 0.14755411 0.14500501], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14770695 0.14750125 0.14507434], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 200: Weights: [0.32264554 0.31219892 0.36515551]\n",
            "Gradient risk: [0.14767845 0.14744838 0.14514368], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14764995 0.14739551 0.14521302], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14762145 0.14734264 0.14528237], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14759294 0.14728976 0.14535172], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14756443 0.14723688 0.14542108], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14753591 0.147184   0.14549044], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1475074  0.14713112 0.14555981], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14747888 0.14707823 0.14562918], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14745036 0.14702534 0.14569856], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14742183 0.14697245 0.14576795], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1473933  0.14691956 0.14583733], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14736477 0.14686666 0.14590673], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14733623 0.14681376 0.14597613], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14730769 0.14676086 0.14604553], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14727915 0.14670795 0.14611494], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1472506  0.14665504 0.14618435], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14722206 0.14660213 0.14625377], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1471935  0.14654922 0.14632319], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14716495 0.1464963  0.14639262], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14713639 0.14644338 0.14646206], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14710783 0.14639046 0.1465315 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14707927 0.14633754 0.14660094], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1470507  0.14628461 0.14667039], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14702213 0.14623168 0.14673984], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14699356 0.14617875 0.1468093 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14696498 0.14612582 0.14687876], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1469364  0.14607288 0.14694823], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14690782 0.14601994 0.1470177 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14687923 0.145967   0.14708718], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14685064 0.14591406 0.14715666], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14682205 0.14586111 0.14722615], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14679346 0.14580816 0.14729564], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14676486 0.14575521 0.14736513], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14673626 0.14570226 0.14743463], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14670765 0.1456493  0.14750414], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14667905 0.14559634 0.14757365], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14665044 0.14554338 0.14764316], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14662182 0.14549042 0.14771268], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14659321 0.14543745 0.14778221], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14656459 0.14538448 0.14785173], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14653596 0.14533151 0.14792127], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14650734 0.14527854 0.14799081], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14647871 0.14522557 0.14806035], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14645008 0.14517259 0.14812989], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14642145 0.14511961 0.14819945], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14639281 0.14506663 0.148269  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14636417 0.14501364 0.14833856], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14633553 0.14496066 0.14840813], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14630688 0.14490767 0.14847769], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14627823 0.14485468 0.14854727], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 250: Weights: [0.31994129 0.30689997 0.37315871]\n",
            "Gradient risk: [0.14624958 0.14480168 0.14861685], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14622092 0.14474869 0.14868643], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14619227 0.14469569 0.14875602], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14616361 0.14464269 0.14882561], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14613494 0.14458969 0.1488952 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14610628 0.14453669 0.1489648 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14607761 0.14448368 0.14903441], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14604894 0.14443067 0.14910401], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14602026 0.14437766 0.14917363], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14599158 0.14432465 0.14924324], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1459629  0.14427164 0.14931286], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14593422 0.14421862 0.14938249], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14590553 0.1441656  0.14945212], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14587684 0.14411258 0.14952175], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14584815 0.14405956 0.14959139], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14581945 0.14400653 0.14966103], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14579076 0.14395351 0.14973068], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14576205 0.14390048 0.14980033], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14573335 0.14384745 0.14986998], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14570464 0.14379442 0.14993964], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14567594 0.14374138 0.15000931], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14564722 0.14368835 0.15007897], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14561851 0.14363531 0.15014864], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14558979 0.14358227 0.15021832], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14556107 0.14352923 0.150288  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14553235 0.14347618 0.15035768], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14550362 0.14342314 0.15042737], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14547489 0.14337009 0.15049706], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14544616 0.14331704 0.15056675], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14541743 0.14326399 0.15063645], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14538869 0.14321094 0.15070615], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14535995 0.14315788 0.15077586], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14533121 0.14310482 0.15084557], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14530246 0.14305177 0.15091529], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14527371 0.14299871 0.150985  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14524496 0.14294564 0.15105473], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14521621 0.14289258 0.15112445], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14518745 0.14283952 0.15119418], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14515869 0.14278645 0.15126391], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14512993 0.14273338 0.15133365], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14510117 0.14268031 0.15140339], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1450724  0.14262724 0.15147314], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14504363 0.14257416 0.15154288], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14501486 0.14252109 0.15161264], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14498608 0.14246801 0.15168239], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1449573  0.14241493 0.15175215], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14492852 0.14236185 0.15182192], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14489974 0.14230877 0.15189168], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14487096 0.14225569 0.15196145], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14484217 0.1422026  0.15203123], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 300: Weights: [0.31722163 0.30159101 0.38118732]\n",
            "Gradient risk: [0.14481338 0.14214952 0.152101  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14478458 0.14209643 0.15217078], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14475579 0.14204334 0.15224057], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14472699 0.14199025 0.15231036], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14469819 0.14193716 0.15238015], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14466938 0.14188406 0.15244994], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14464058 0.14183097 0.15251974], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14461177 0.14177787 0.15258955], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14458295 0.14172477 0.15265935], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14455414 0.14167167 0.15272916], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14452532 0.14161857 0.15279897], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1444965  0.14156547 0.15286879], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14446768 0.14151237 0.15293861], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14443886 0.14145926 0.15300843], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14441003 0.14140616 0.15307826], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1443812  0.14135305 0.15314808], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14435237 0.14129994 0.15321792], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14432353 0.14124683 0.15328775], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1442947  0.14119372 0.15335759], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14426586 0.1411406  0.15342743], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14423701 0.14108749 0.15349728], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14420817 0.14103437 0.15356713], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14417932 0.14098126 0.15363698], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14415047 0.14092814 0.15370684], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14412162 0.14087502 0.1537767 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14409277 0.1408219  0.15384656], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14406391 0.14076878 0.15391642], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14403505 0.14071565 0.15398629], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14400619 0.14066253 0.15405616], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14397732 0.14060941 0.15412604], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14394845 0.14055628 0.15419591], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14391959 0.14050315 0.15426579], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14389071 0.14045002 0.15433568], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14386184 0.14039689 0.15440556], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14383296 0.14034376 0.15447545], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14380408 0.14029063 0.15454534], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1437752  0.1402375  0.15461524], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14374632 0.14018436 0.15468514], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14371743 0.14013123 0.15475504], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14368854 0.14007809 0.15482494], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14365965 0.14002496 0.15489485], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14363076 0.13997182 0.15496476], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14360186 0.13991868 0.15503467], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14357296 0.13986554 0.15510459], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14354406 0.1398124  0.15517451], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14351516 0.13975926 0.15524443], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14348626 0.13970611 0.15531435], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14345735 0.13965297 0.15538428], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14342844 0.13959983 0.15545421], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14339953 0.13954668 0.15552414], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 350: Weights: [0.31448799 0.29627539 0.38923659]\n",
            "Gradient risk: [0.14337061 0.13949353 0.15559408], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1433417  0.13944039 0.15566402], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14331278 0.13938724 0.15573396], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14328386 0.13933409 0.1558039 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14325493 0.13928094 0.15587385], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14322601 0.13922779 0.1559438 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14319708 0.13917464 0.15601375], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14316815 0.13912149 0.1560837 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14313921 0.13906833 0.15615366], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14311028 0.13901518 0.15622362], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14308134 0.13896203 0.15629358], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1430524  0.13890887 0.15636355], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14302346 0.13885572 0.15643351], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14299452 0.13880256 0.15650348], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14296557 0.1387494  0.15657346], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14293662 0.13869624 0.15664343], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14290767 0.13864309 0.15671341], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14287872 0.13858993 0.15678339], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14284976 0.13853677 0.15685337], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14282081 0.13848361 0.15692335], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14279185 0.13843045 0.15699334], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14276288 0.13837729 0.15706333], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14273392 0.13832412 0.15713332], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14270495 0.13827096 0.15720332], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14267599 0.1382178  0.15727331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14264702 0.13816463 0.15734331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14261804 0.13811147 0.15741331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14258907 0.1380583  0.15748331], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14256009 0.13800514 0.15755332], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14253111 0.13795197 0.15762333], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14250213 0.13789881 0.15769334], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14247315 0.13784564 0.15776335], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14244416 0.13779247 0.15783337], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14241518 0.13773931 0.15790338], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14238619 0.13768614 0.1579734 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14235719 0.13763297 0.15804342], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1423282  0.1375798  0.15811345], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14229921 0.13752663 0.15818347], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14227021 0.13747346 0.1582535 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14224121 0.13742029 0.15832353], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14221221 0.13736712 0.15839356], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1421832  0.13731395 0.15846359], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1421542  0.13726078 0.15853363], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14212519 0.13720761 0.15860367], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14209618 0.13715444 0.15867371], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14206717 0.13710127 0.15874375], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14203815 0.1370481  0.15881379], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14200913 0.13699492 0.15888384], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14198012 0.13694175 0.15895388], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1419511  0.13688858 0.15902393], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 400: Weights: [0.31174181 0.29095648 0.39730168]\n",
            "Gradient risk: [0.14192207 0.13683541 0.15909399], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14189305 0.13678223 0.15916404], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14186402 0.13672906 0.15923409], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.141835   0.13667589 0.15930415], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14180597 0.13662271 0.15937421], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14177693 0.13656954 0.15944427], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1417479  0.13651637 0.15951433], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14171886 0.13646319 0.1595844 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14168983 0.13641002 0.15965446], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14166079 0.13635684 0.15972453], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14163174 0.13630367 0.1597946 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1416027  0.1362505  0.15986467], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14157365 0.13619732 0.15993475], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14154461 0.13614415 0.16000482], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14151556 0.13609097 0.1600749 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14148651 0.1360378  0.16014497], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14145745 0.13598463 0.16021505], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1414284  0.13593145 0.16028513], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14139934 0.13587828 0.16035522], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14137028 0.1358251  0.1604253 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14134122 0.13577193 0.16049539], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14131216 0.13571876 0.16056548], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14128309 0.13566558 0.16063556], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14125403 0.13561241 0.16070566], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14122496 0.13555924 0.16077575], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14119589 0.13550606 0.16084584], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14116682 0.13545289 0.16091594], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14113774 0.13539972 0.16098603], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14110867 0.13534654 0.16105613], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14107959 0.13529337 0.16112623], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14105051 0.1352402  0.16119633], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14102143 0.13518703 0.16126643], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14099235 0.13513385 0.16133654], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14096326 0.13508068 0.16140664], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14093417 0.13502751 0.16147675], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14090509 0.13497434 0.16154685], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.140876   0.13492117 0.16161696], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1408469  0.134868   0.16168707], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14081781 0.13481483 0.16175718], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14078872 0.13476166 0.1618273 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14075962 0.13470849 0.16189741], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14073052 0.13465532 0.16196753], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14070142 0.13460215 0.16203764], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14067232 0.13454898 0.16210776], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14064321 0.13449581 0.16217788], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14061411 0.13444264 0.162248  ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.140585   0.13438948 0.16231812], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14055589 0.13433631 0.16238824], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14052678 0.13428314 0.16245836], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14049767 0.13422998 0.16252849], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 450: Weights: [0.30898455 0.28563765 0.40537776]\n",
            "Gradient risk: [0.14046856 0.13417681 0.16259861], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14043944 0.13412365 0.16266874], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14041032 0.13407048 0.16273887], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.1403812  0.13401732 0.16280899], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14035208 0.13396416 0.16287912], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14032296 0.13391099 0.16294925], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14029384 0.13385783 0.16301939], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14026471 0.13380467 0.16308952], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14023558 0.13375151 0.16315965], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14020645 0.13369835 0.16322978], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14017732 0.13364519 0.16329992], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14014819 0.13359203 0.16337005], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14011906 0.13353887 0.16344019], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14008992 0.13348571 0.16351033], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14006079 0.13343255 0.16358047], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14003165 0.1333794  0.16365061], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.14000251 0.13332624 0.16372075], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13997337 0.13327308 0.16379089], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13994422 0.13321993 0.16386103], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13991508 0.13316678 0.16393117], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13988593 0.13311362 0.16400131], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13985678 0.13306047 0.16407146], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13982764 0.13300732 0.1641416 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13979848 0.13295417 0.16421174], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13976933 0.13290102 0.16428189], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13974018 0.13284787 0.16435204], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13971102 0.13279472 0.16442218], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13968187 0.13274157 0.16449233], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13965271 0.13268843 0.16456248], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13962355 0.13263528 0.16463263], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13959439 0.13258213 0.16470278], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13956522 0.13252899 0.16477293], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13953606 0.13247585 0.16484308], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13950689 0.1324227  0.16491323], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13947773 0.13236956 0.16498338], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13944856 0.13231642 0.16505353], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13941939 0.13226328 0.16512368], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13939022 0.13221014 0.16519383], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13936104 0.13215701 0.16526399], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13933187 0.13210387 0.16533414], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13930269 0.13205074 0.16540429], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13927351 0.1319976  0.16547445], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13924434 0.13194447 0.1655446 ], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13921516 0.13189133 0.16561476], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13918597 0.1318382  0.16568491], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13915679 0.13178507 0.16575507], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13912761 0.13173194 0.16582522], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13909842 0.13167882 0.16589538], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.13906923 0.13162569 0.16596553], Gradient return: [ 0.001   0.0005 -0.0002], Gradient L1: [0.01 0.01 0.01]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Gradient risk: [0.05333333 0.058      0.03133333], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 0: Weights: [0.33328113 0.33322937 0.33348947]\n",
            "Gradient risk: [0.05332134 0.0579793  0.03135426], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05330936 0.0579586  0.03137518], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05329737 0.05793791 0.0313961 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05328538 0.05791721 0.03141703], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05327338 0.05789651 0.03143796], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05326139 0.05787581 0.03145889], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0532494  0.0578551  0.03147982], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0532374  0.0578344  0.03150075], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05322541 0.0578137  0.03152168], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05321341 0.05779299 0.03154262], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05320141 0.05777229 0.03156355], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05318941 0.05775158 0.03158449], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05317741 0.05773087 0.03160543], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05316541 0.05771016 0.03162637], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05315341 0.05768945 0.03164731], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05314141 0.05766874 0.03166825], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05312941 0.05764803 0.03168919], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0531174  0.05762732 0.03171014], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0531054  0.05760661 0.03173108], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05309339 0.05758589 0.03175203], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05308138 0.05756518 0.03177298], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05306937 0.05754446 0.03179393], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05305737 0.05752374 0.03181488], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05304536 0.05750303 0.03183583], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05303334 0.05748231 0.03185678], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05302133 0.05746159 0.03187773], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05300932 0.05744087 0.03189869], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05299731 0.05742015 0.03191964], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05298529 0.05739943 0.0319406 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05297327 0.0573787  0.03196156], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05296126 0.05735798 0.03198252], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05294924 0.05733726 0.03200348], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05293722 0.05731653 0.03202444], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0529252  0.0572958  0.03204541], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05291318 0.05727508 0.03206637], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05290116 0.05725435 0.03208733], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05288914 0.05723362 0.0321083 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05287711 0.05721289 0.03212927], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05286509 0.05719216 0.03215024], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05285306 0.05717143 0.03217121], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05284104 0.0571507  0.03219218], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05282901 0.05712997 0.03221315], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05281698 0.05710923 0.03223412], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05280496 0.0570885  0.0322551 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05279293 0.05706776 0.03227607], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0527809  0.05704703 0.03229705], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05276886 0.05702629 0.03231803], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05275683 0.05700555 0.032339  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0527448  0.05698482 0.03235998], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05273276 0.05696408 0.03238096], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 50: Weights: [0.33066429 0.32802716 0.34130852]\n",
            "Gradient risk: [0.05272073 0.05694334 0.03240195], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05270869 0.0569226  0.03242293], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05269666 0.05690186 0.03244391], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05268462 0.05688112 0.0324649 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05267258 0.05686037 0.03248588], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05266054 0.05683963 0.03250687], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0526485  0.05681889 0.03252786], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05263646 0.05679814 0.03254885], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05262442 0.0567774  0.03256984], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05261238 0.05675665 0.03259083], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05260033 0.0567359  0.03261182], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05258829 0.05671516 0.03263281], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05257624 0.05669441 0.03265381], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0525642  0.05667366 0.0326748 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05255215 0.05665291 0.0326958 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0525401  0.05663216 0.03271679], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05252805 0.05661141 0.03273779], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.052516   0.05659066 0.03275879], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05250395 0.05656991 0.03277979], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0524919  0.05654915 0.03280079], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05247985 0.0565284  0.03282179], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0524678  0.05650765 0.03284279], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05245574 0.05648689 0.0328638 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05244369 0.05646614 0.0328848 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05243163 0.05644538 0.03290581], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05241958 0.05642463 0.03292681], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05240752 0.05640387 0.03294782], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05239546 0.05638311 0.03296883], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0523834  0.05636235 0.03298984], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05237134 0.05634159 0.03301085], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05235928 0.05632084 0.03303186], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05234722 0.05630008 0.03305287], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05233516 0.05627931 0.03307388], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0523231  0.05625855 0.0330949 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05231104 0.05623779 0.03311591], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05229897 0.05621703 0.03313693], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05228691 0.05619627 0.03315794], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05227484 0.0561755  0.03317896], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05226277 0.05615474 0.03319998], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05225071 0.05613397 0.033221  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05223864 0.05611321 0.03324202], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05222657 0.05609244 0.03326304], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0522145  0.05607168 0.03328406], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05220243 0.05605091 0.03330508], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05219036 0.05603014 0.03332611], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05217829 0.05600938 0.03334713], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05216622 0.05598861 0.03336816], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05215414 0.05596784 0.03338918], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05214207 0.05594707 0.03341021], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05212999 0.0559263  0.03343124], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 100: Weights: [0.3280347  0.32281781 0.34914746]\n",
            "Gradient risk: [0.05211792 0.05590553 0.03345226], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05210584 0.05588476 0.03347329], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05209376 0.05586399 0.03349432], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05208169 0.05584322 0.03351535], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05206961 0.05582245 0.03353638], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05205753 0.05580167 0.03355742], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05204545 0.0557809  0.03357845], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05203337 0.05576013 0.03359948], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05202129 0.05573935 0.03362052], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0520092  0.05571858 0.03364155], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05199712 0.0556978  0.03366259], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05198504 0.05567703 0.03368363], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05197295 0.05565625 0.03370466], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05196087 0.05563548 0.0337257 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05194878 0.0556147  0.03374674], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05193669 0.05559392 0.03376778], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05192461 0.05557315 0.03378882], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05191252 0.05555237 0.03380986], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05190043 0.05553159 0.0338309 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05188834 0.05551081 0.03385195], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05187625 0.05549003 0.03387299], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05186416 0.05546925 0.03389403], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05185207 0.05544847 0.03391508], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05183998 0.05542769 0.03393612], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05182788 0.05540691 0.03395717], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05181579 0.05538613 0.03397822], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0518037  0.05536535 0.03399926], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0517916  0.05534457 0.03402031], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05177951 0.05532379 0.03404136], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05176741 0.055303   0.03406241], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05175531 0.05528222 0.03408346], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05174322 0.05526144 0.03410451], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05173112 0.05524066 0.03412556], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05171902 0.05521987 0.03414662], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05170692 0.05519909 0.03416767], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05169482 0.0551783  0.03418872], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05168272 0.05515752 0.03420978], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05167062 0.05513673 0.03423083], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05165852 0.05511595 0.03425189], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05164641 0.05509516 0.03427294], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05163431 0.05507438 0.034294  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05162221 0.05505359 0.03431506], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0516101  0.05503281 0.03433611], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.051598   0.05501202 0.03435717], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05158589 0.05499123 0.03437823], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05157378 0.05497044 0.03439929], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05156168 0.05494966 0.03442035], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05154957 0.05492887 0.03444141], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05153746 0.05490808 0.03446247], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05152535 0.05488729 0.03448354], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 150: Weights: [0.32539373 0.31760451 0.35700173]\n",
            "Gradient risk: [0.05151324 0.05486651 0.0345046 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05150113 0.05484572 0.03452566], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05148902 0.05482493 0.03454672], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05147691 0.05480414 0.03456779], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0514648  0.05478335 0.03458885], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05145268 0.05476256 0.03460992], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05144057 0.05474177 0.03463098], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05142846 0.05472098 0.03465205], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05141634 0.05470019 0.03467312], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05140423 0.0546794  0.03469418], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05139211 0.05465861 0.03471525], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05138    0.05463782 0.03473632], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05136788 0.05461703 0.03475739], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05135576 0.05459624 0.03477846], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05134364 0.05457545 0.03479953], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05133152 0.05455465 0.0348206 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05131941 0.05453386 0.03484167], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05130729 0.05451307 0.03486274], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05129517 0.05449228 0.03488381], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05128304 0.05447149 0.03490489], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05127092 0.05445069 0.03492596], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0512588  0.0544299  0.03494703], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05124668 0.05440911 0.03496811], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05123456 0.05438832 0.03498918], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05122243 0.05436752 0.03501025], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05121031 0.05434673 0.03503133], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05119818 0.05432594 0.0350524 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05118606 0.05430515 0.03507348], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05117393 0.05428435 0.03509456], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05116181 0.05426356 0.03511563], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05114968 0.05424277 0.03513671], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05113755 0.05422197 0.03515779], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05112542 0.05420118 0.03517887], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0511133  0.05418039 0.03519994], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05110117 0.05415959 0.03522102], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05108904 0.0541388  0.0352421 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05107691 0.054118   0.03526318], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05106478 0.05409721 0.03528426], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05105265 0.05407642 0.03530534], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05104052 0.05405562 0.03532642], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05102838 0.05403483 0.0353475 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05101625 0.05401404 0.03536858], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05100412 0.05399324 0.03538967], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05099198 0.05397245 0.03541075], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05097985 0.05395165 0.03543183], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05096772 0.05393086 0.03545291], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05095558 0.05391007 0.035474  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05094345 0.05388927 0.03549508], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05093131 0.05386848 0.03551616], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05091917 0.05384768 0.03553725], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 200: Weights: [0.32274279 0.31239046 0.36486673]\n",
            "Gradient risk: [0.05090704 0.05382689 0.03555833], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0508949  0.05380609 0.03557942], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05088276 0.0537853  0.0356005 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05087062 0.05376451 0.03562159], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05085848 0.05374371 0.03564267], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05084635 0.05372292 0.03566376], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05083421 0.05370213 0.03568484], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05082207 0.05368133 0.03570593], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05080992 0.05366054 0.03572702], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05079778 0.05363974 0.03574811], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05078564 0.05361895 0.03576919], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0507735  0.05359816 0.03579028], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05076136 0.05357736 0.03581137], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05074922 0.05355657 0.03583246], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05073707 0.05353578 0.03585354], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05072493 0.05351498 0.03587463], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05071278 0.05349419 0.03589572], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05070064 0.0534734  0.03591681], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05068849 0.0534526  0.0359379 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05067635 0.05343181 0.03595899], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0506642  0.05341102 0.03598008], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05065206 0.05339023 0.03600117], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05063991 0.05336943 0.03602226], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05062776 0.05334864 0.03604335], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05061562 0.05332785 0.03606444], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05060347 0.05330706 0.03608553], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05059132 0.05328626 0.03610662], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05057917 0.05326547 0.03612771], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05056702 0.05324468 0.0361488 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05055487 0.05322389 0.03616989], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05054272 0.0532031  0.03619099], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05053057 0.05318231 0.03621208], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05051842 0.05316152 0.03623317], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05050627 0.05314072 0.03625426], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05049412 0.05311993 0.03627535], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05048197 0.05309914 0.03629645], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05046981 0.05307835 0.03631754], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05045766 0.05305756 0.03633863], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05044551 0.05303677 0.03635972], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05043336 0.05301598 0.03638082], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0504212  0.05299519 0.03640191], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05040905 0.0529744  0.036423  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05039689 0.05295362 0.0364441 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05038474 0.05293283 0.03646519], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05037258 0.05291204 0.03648628], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05036043 0.05289125 0.03650738], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05034827 0.05287046 0.03652847], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05033612 0.05284967 0.03654956], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05032396 0.05282889 0.03657066], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0503118  0.0528081  0.03659175], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 250: Weights: [0.32008326 0.30717887 0.37273784]\n",
            "Gradient risk: [0.05029964 0.05278731 0.03661285], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05028749 0.05276653 0.03663394], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05027533 0.05274574 0.03665503], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05026317 0.05272495 0.03667613], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05025101 0.05270417 0.03669722], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05023885 0.05268338 0.03671832], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05022669 0.0526626  0.03673941], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05021453 0.05264181 0.0367605 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05020237 0.05262103 0.0367816 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05019021 0.05260024 0.03680269], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05017805 0.05257946 0.03682379], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05016589 0.05255867 0.03684488], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05015373 0.05253789 0.03686598], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05014157 0.05251711 0.03688707], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05012941 0.05249632 0.03690817], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05011725 0.05247554 0.03692926], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05010508 0.05245476 0.03695035], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05009292 0.05243398 0.03697145], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05008076 0.0524132  0.03699254], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05006859 0.05239241 0.03701364], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05005643 0.05237163 0.03703473], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05004427 0.05235085 0.03705582], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0500321  0.05233007 0.03707692], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05001994 0.05230929 0.03709801], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.05000777 0.05228851 0.03711911], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04999561 0.05226774 0.0371402 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04998344 0.05224696 0.0371613 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04997128 0.05222618 0.03718239], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04995911 0.0522054  0.03720348], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04994695 0.05218462 0.03722458], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04993478 0.05216385 0.03724567], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04992261 0.05214307 0.03726676], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04991045 0.05212229 0.03728786], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04989828 0.05210152 0.03730895], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04988611 0.05208074 0.03733004], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04987394 0.05205997 0.03735114], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04986178 0.0520392  0.03737223], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04984961 0.05201842 0.03739332], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04983744 0.05199765 0.03741442], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04982527 0.05197688 0.03743551], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0498131 0.0519561 0.0374566], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04980093 0.05193533 0.03747769], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04978877 0.05191456 0.03749879], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0497766  0.05189379 0.03751988], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04976443 0.05187302 0.03754097], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04975226 0.05185225 0.03756206], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04974009 0.05183148 0.03758315], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04972792 0.05181071 0.03760424], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04971575 0.05178994 0.03762534], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04970358 0.05176917 0.03764643], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 300: Weights: [0.31741657 0.30197297 0.38061043]\n",
            "Gradient risk: [0.0496914  0.05174841 0.03766752], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04967923 0.05172764 0.03768861], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04966706 0.05170687 0.0377097 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04965489 0.05168611 0.03773079], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04964272 0.05166534 0.03775188], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04963055 0.05164458 0.03777297], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04961837 0.05162381 0.03779406], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0496062  0.05160305 0.03781515], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04959403 0.05158228 0.03783624], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04958186 0.05156152 0.03785733], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04956968 0.05154076 0.03787842], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04955751 0.05152    0.03789951], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04954534 0.05149924 0.03792059], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04953317 0.05147848 0.03794168], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04952099 0.05145772 0.03796277], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04950882 0.05143696 0.03798386], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04949664 0.0514162  0.03800495], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04948447 0.05139544 0.03802603], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0494723  0.05137468 0.03804712], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04946012 0.05135393 0.03806821], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04944795 0.05133317 0.03808929], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04943577 0.05131242 0.03811038], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0494236  0.05129166 0.03813146], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04941142 0.05127091 0.03815255], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04939925 0.05125015 0.03817364], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04938707 0.0512294  0.03819472], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0493749  0.05120865 0.0382158 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04936272 0.0511879  0.03823689], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04935055 0.05116715 0.03825797], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04933837 0.0511464  0.03827906], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0493262  0.05112565 0.03830014], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04931402 0.0511049  0.03832122], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04930184 0.05108415 0.0383423 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04928967 0.0510634  0.03836339], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04927749 0.05104265 0.03838447], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04926532 0.05102191 0.03840555], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04925314 0.05100116 0.03842663], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04924096 0.05098042 0.03844771], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04922879 0.05095967 0.03846879], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04921661 0.05093893 0.03848987], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04920443 0.05091819 0.03851095], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04919226 0.05089745 0.03853203], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04918008 0.0508767  0.03855311], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0491679  0.05085596 0.03857419], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04915572 0.05083522 0.03859527], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04914355 0.05081448 0.03861635], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04913137 0.05079375 0.03863742], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04911919 0.05077301 0.0386585 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04910701 0.05075227 0.03867958], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04909484 0.05073154 0.03870065], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 350: Weights: [0.31474413 0.29677596 0.38847987]\n",
            "Gradient risk: [0.04908266 0.0507108  0.03872173], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04907048 0.05069007 0.0387428 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0490583  0.05066933 0.03876388], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04904613 0.0506486  0.03878495], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04903395 0.05062787 0.03880603], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04902177 0.05060714 0.0388271 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04900959 0.0505864  0.03884817], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04899741 0.05056568 0.03886925], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04898524 0.05054495 0.03889032], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04897306 0.05052422 0.03891139], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04896088 0.05050349 0.03893246], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0489487  0.05048276 0.03895353], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04893652 0.05046204 0.0389746 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04892434 0.05044131 0.03899567], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04891217 0.05042059 0.03901674], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04889999 0.05039987 0.03903781], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04888781 0.05037914 0.03905888], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04887563 0.05035842 0.03907994], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04886345 0.0503377  0.03910101], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04885127 0.05031698 0.03912208], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0488391  0.05029626 0.03914314], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04882692 0.05027555 0.03916421], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04881474 0.05025483 0.03918527], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04880256 0.05023411 0.03920634], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04879038 0.0502134  0.0392274 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0487782  0.05019268 0.03924847], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04876602 0.05017197 0.03926953], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04875385 0.05015126 0.03929059], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04874167 0.05013054 0.03931165], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04872949 0.05010983 0.03933271], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04871731 0.05008912 0.03935377], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04870513 0.05006841 0.03937483], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04869295 0.05004771 0.03939589], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04868078 0.050027   0.03941695], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0486686  0.05000629 0.03943801], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04865642 0.04998559 0.03945907], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04864424 0.04996488 0.03948012], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04863206 0.04994418 0.03950118], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04861988 0.04992348 0.03952224], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04860771 0.04990278 0.03954329], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04859553 0.04988208 0.03956435], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04858335 0.04986138 0.0395854 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04857117 0.04984068 0.03960645], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04855899 0.04981998 0.0396275 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04854682 0.04979928 0.03964856], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04853464 0.04977859 0.03966961], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04852246 0.04975789 0.03969066], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04851028 0.0497372  0.03971171], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0484981  0.04971651 0.03973276], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04848593 0.04969581 0.03975381], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 400: Weights: [0.31206738 0.29159105 0.39634154]\n",
            "Gradient risk: [0.04847375 0.04967512 0.03977485], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04846157 0.04965443 0.0397959 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04844939 0.04963375 0.03981695], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04843722 0.04961306 0.03983799], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04842504 0.04959237 0.03985904], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04841286 0.04957169 0.03988008], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04840069 0.049551   0.03990113], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04838851 0.04953032 0.03992217], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04837633 0.04950964 0.03994321], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04836415 0.04948896 0.03996425], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04835198 0.04946828 0.0399853 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0483398  0.0494476  0.04000634], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04832762 0.04942692 0.04002737], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04831545 0.04940624 0.04004841], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04830327 0.04938557 0.04006945], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0482911  0.04936489 0.04009049], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04827892 0.04934422 0.04011153], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04826674 0.04932355 0.04013256], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04825457 0.04930287 0.0401536 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04824239 0.0492822  0.04017463], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04823022 0.04926153 0.04019566], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04821804 0.04924087 0.0402167 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04820587 0.0492202  0.04023773], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04819369 0.04919953 0.04025876], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04818151 0.04917887 0.04027979], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04816934 0.04915821 0.04030082], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04815717 0.04913754 0.04032185], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04814499 0.04911688 0.04034287], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04813282 0.04909622 0.0403639 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04812064 0.04907556 0.04038493], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04810847 0.0490549  0.04040595], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04809629 0.04903425 0.04042698], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04808412 0.04901359 0.040448  ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04807195 0.04899294 0.04046902], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04805977 0.04897229 0.04049005], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0480476  0.04895163 0.04051107], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04803542 0.04893098 0.04053209], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04802325 0.04891033 0.04055311], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04801108 0.04888969 0.04057413], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04799891 0.04886904 0.04059514], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04798673 0.04884839 0.04061616], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04797456 0.04882775 0.04063718], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04796239 0.0488071  0.04065819], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04795022 0.04878646 0.04067921], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04793804 0.04876582 0.04070022], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04792587 0.04874518 0.04072123], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0479137  0.04872454 0.04074224], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04790153 0.04870391 0.04076325], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04788936 0.04868327 0.04078426], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04787719 0.04866264 0.04080527], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Epoch 450: Weights: [0.30938774 0.2864214  0.40419083]\n",
            "Gradient risk: [0.04786502 0.048642   0.04082628], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04785285 0.04862137 0.04084729], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04784068 0.04860074 0.04086829], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04782851 0.04858011 0.0408893 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04781634 0.04855948 0.0409103 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04780417 0.04853885 0.04093131], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.047792   0.04851823 0.04095231], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04777983 0.0484976  0.04097331], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04776766 0.04847698 0.04099431], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04775549 0.04845636 0.04101531], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04774332 0.04843574 0.04103631], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04773115 0.04841512 0.0410573 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04771899 0.0483945  0.0410783 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04770682 0.04837389 0.0410993 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04769465 0.04835327 0.04112029], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04768248 0.04833266 0.04114128], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04767032 0.04831204 0.04116228], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04765815 0.04829143 0.04118327], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04764598 0.04827082 0.04120426], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04763382 0.04825022 0.04122525], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04762165 0.04822961 0.04124624], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04760948 0.048209   0.04126722], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04759732 0.0481884  0.04128821], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04758515 0.0481678  0.0413092 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04757299 0.04814719 0.04133018], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04756082 0.04812659 0.04135116], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04754866 0.04810599 0.04137215], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0475365  0.0480854  0.04139313], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04752433 0.0480648  0.04141411], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04751217 0.04804421 0.04143509], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.0475     0.04802361 0.04145606], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04748784 0.04800302 0.04147704], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04747568 0.04798243 0.04149802], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04746352 0.04796184 0.04151899], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04745135 0.04794126 0.04153997], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04743919 0.04792067 0.04156094], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04742703 0.04790008 0.04158191], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04741487 0.0478795  0.04160288], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04740271 0.04785892 0.04162385], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04739055 0.04783834 0.04164482], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04737839 0.04781776 0.04166578], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04736623 0.04779718 0.04168675], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04735407 0.04777661 0.04170771], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04734191 0.04775603 0.04172868], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04732975 0.04773546 0.04174964], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04731759 0.04771489 0.0417706 ], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04730543 0.04769432 0.04179156], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04729327 0.04767375 0.04181252], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Gradient risk: [0.04728112 0.04765318 0.04183348], Gradient return: [0.003  0.0025 0.0018], Gradient L1: [0.01 0.01 0.01]\n",
            "Stress Test Results: {'Market Bubble': array([0.30528075, 0.27849916, 0.41622006]), 'Liquidity Crisis': array([0.30627312, 0.28042857, 0.41329828]), 'Interest Rate Shock': array([0.30676026, 0.28137299, 0.41186672])}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (3,3) (100,100) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-110-106176619.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;31m# Run the Monte Carlo Simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0mmonte_carlo_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonte_carlo_testing_large\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_assets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# Log the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-110-106176619.py\u001b[0m in \u001b[0;36mmonte_carlo_testing_large\u001b[0;34m(Sigma, mu, n_samples, n_assets, lr, epochs)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Randomly perturb the Sigma and mu to simulate uncertainty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mSigma_randomized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSigma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.05\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_assets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_assets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mmu_randomized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_assets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,3) (100,100) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Define the device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the main and adversary models\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self, input_size=8):\n",
        "        super(MainModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class AdversaryModel(nn.Module):\n",
        "    def __init__(self, input_size=8):\n",
        "        super(AdversaryModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Training function with AdamW optimizer and class weights\n",
        "def train_model(main_model, adversary_model, train_loader, num_epochs=10, lr_main=0.01, lr_adv=0.01):\n",
        "    main_optimizer = optim.AdamW(main_model.parameters(), lr=lr_main)\n",
        "    adversary_optimizer = optim.AdamW(adversary_model.parameters(), lr=lr_adv)\n",
        "\n",
        "    class_weights = torch.tensor([1.0, 2.0]).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss(weight=class_weights)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        main_model.train()\n",
        "        adversary_model.train()\n",
        "\n",
        "        main_loss_total = 0\n",
        "        adversary_loss_total = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            if target.dim() == 1:\n",
        "                target = target.unsqueeze(1)\n",
        "            target = target.float()\n",
        "\n",
        "            main_optimizer.zero_grad()\n",
        "            output_main = main_model(data)\n",
        "            main_loss = criterion(output_main.squeeze(), target.squeeze())\n",
        "            main_loss.backward()\n",
        "            main_optimizer.step()\n",
        "            main_loss_total += main_loss.item()\n",
        "\n",
        "            adversary_optimizer.zero_grad()\n",
        "            output_adv = adversary_model(data)\n",
        "            adversary_loss = criterion(output_adv.squeeze(), target.squeeze())\n",
        "            adversary_loss.backward()\n",
        "            adversary_optimizer.step()\n",
        "            adversary_loss_total += adversary_loss.item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Main Loss: {main_loss.item()}, Adversary Loss: {adversary_loss.item()}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Main Loss: {main_loss_total/len(train_loader)}, Adversary Loss: {adversary_loss_total/len(train_loader)}\")\n",
        "\n",
        "# Monte Carlo Simulation for Large Portfolio\n",
        "def monte_carlo_testing_large(Sigma, mu, n_samples=1000, n_assets=3, lr=1e-3, epochs=500):\n",
        "    results = []\n",
        "    for _ in range(n_samples):\n",
        "        Sigma_randomized = Sigma + 0.05 * np.random.randn(n_assets, n_assets)\n",
        "        mu_randomized = mu + 0.01 * np.random.randn(n_assets)\n",
        "        # Simulate further actions with the randomized Sigma and mu\n",
        "        # Perform training on the randomized data\n",
        "        # Model training code...\n",
        "        results.append(Sigma_randomized)  # Placeholder for simulation results\n",
        "    return results\n",
        "\n",
        "# Simulate Fat-Tailed Returns (Student's T-distribution)\n",
        "def simulate_fat_tailed_returns(Sigma, mu, dof=4, n_samples=1000):\n",
        "    t_returns = np.random.standard_t(dof, size=(n_samples, len(mu)))\n",
        "    fat_tailed_returns = np.dot(t_returns, Sigma) + mu\n",
        "    return fat_tailed_returns\n",
        "\n",
        "# Running the model\n",
        "Sigma = np.array([[0.02, 0.015, -0.005], [0.015, 0.03, -0.008], [-0.005, -0.008, 0.01]])\n",
        "mu = np.array([0.01, 0.005, -0.002])\n",
        "\n",
        "# Running Stress Tests (This part is removed as per the last request)\n",
        "print(f\"Running stress tests...\")\n",
        "# Removed stress tests code here\n",
        "\n",
        "# Simulate Fat-Tailed Returns\n",
        "print(f\"Simulating fat-tailed returns...\")\n",
        "fat_tailed_returns = simulate_fat_tailed_returns(Sigma, mu)\n",
        "\n",
        "# Monte Carlo Simulation\n",
        "print(f\"Running Monte Carlo simulation...\")\n",
        "monte_carlo_results = monte_carlo_testing_large(Sigma, mu, n_samples=1000, n_assets=3)\n",
        "\n",
        "# Log everything to file (removed file I/O code to prevent issues)\n",
        "print(f\"Stress Test Results: Not logged due to removal\")\n",
        "print(f\"Fat-Tailed Returns Simulation (First 5 Samples): {fat_tailed_returns[:5]}\")\n",
        "print(f\"Monte Carlo Simulation Results: {monte_carlo_results[:5]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwCUhn-AqLrD",
        "outputId": "2e1c6695-89bb-4cf3-e5c2-729252c3dde5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running stress tests...\n",
            "Simulating fat-tailed returns...\n",
            "Running Monte Carlo simulation...\n",
            "Stress Test Results: Not logged due to removal\n",
            "Fat-Tailed Returns Simulation (First 5 Samples): [[-0.0068737  -0.02516619  0.00801347]\n",
            " [-0.01301376  0.01871296 -0.02217811]\n",
            " [ 0.00451194  0.01808637  0.00125363]\n",
            " [ 0.00659795 -0.00554436  0.00095572]\n",
            " [-0.04043782 -0.05602465  0.02659924]]\n",
            "Monte Carlo Simulation Results: [array([[ 0.01872837, -0.04920408, -0.01067983],\n",
            "       [-0.01732166,  0.05626294,  0.09684963],\n",
            "       [-0.00348613, -0.01135062,  0.07152931]]), array([[-0.09213218, -0.00120358, -0.0032978 ],\n",
            "       [-0.06542539, -0.05878452, -0.04961221],\n",
            "       [-0.06003418, -0.03221896, -0.03456912]]), array([[ 0.0655105 ,  0.00737327, -0.07364112],\n",
            "       [ 0.07885221,  0.10042863, -0.0537096 ],\n",
            "       [-0.04910273,  0.05793937,  0.05992888]]), array([[ 0.06473144, -0.0356834 ,  0.06568829],\n",
            "       [ 0.08815781,  0.03222421, -0.01290678],\n",
            "       [-0.01648431, -0.0459503 ,  0.02808319]]), array([[-0.05759484,  0.05725413, -0.03629563],\n",
            "       [-0.00244595, -0.01574625, -0.07440822],\n",
            "       [ 0.05820936, -0.00983797,  0.0566032 ]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create a text file to save essential outputs (in a summarized form)\n",
        "log_file = open(\"/content/portfolio_optimization_results_reduced.txt\", \"w\")\n",
        "\n",
        "# Function to log critical output\n",
        "def log_output(output):\n",
        "    print(output)\n",
        "    log_file.write(output + \"\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# Portfolio Optimization with Simple Gradient Descent\n",
        "# -------------------------------\n",
        "\n",
        "def optimize_weights(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2, lr=1e-2, epochs=500):\n",
        "    \"\"\"\n",
        "    Optimizes portfolio weights using gradient descent with regularization\n",
        "    \"\"\"\n",
        "    w = np.ones(Sigma.shape[0]) / Sigma.shape[0]  # Equal distribution of weights\n",
        "    log_output(f\"Initial weights: {w}\")\n",
        "\n",
        "    # Perform optimization and log essential updates\n",
        "    for epoch in range(epochs):\n",
        "        grad = grad_objective(w, Sigma, mu, lam, eta)\n",
        "        w -= lr * grad\n",
        "        w[np.abs(w) < gamma] = 0  # Apply sparsity\n",
        "\n",
        "        # Ensure non-negative weights and normalize them\n",
        "        w = np.maximum(w, 0)\n",
        "        w /= np.sum(w + 1e-8)\n",
        "\n",
        "        # Log progress every 50th epoch\n",
        "        if epoch % 50 == 0:\n",
        "            log_output(f\"Epoch {epoch}, Weights: {w[:3]}\")  # Log only first 3 for brevity\n",
        "\n",
        "    return w\n",
        "\n",
        "# -------------------------------\n",
        "# Gradient of the Objective Function\n",
        "# -------------------------------\n",
        "\n",
        "def grad_objective(w, Sigma, mu, lam, eta):\n",
        "    \"\"\"\n",
        "    Calculate gradient for risk-return optimization with L1 regularization\n",
        "    \"\"\"\n",
        "    grad_risk = 2 * Sigma @ w  # Risk (variance)\n",
        "    grad_ret = lam * mu  # Return (expected return)\n",
        "    grad_l1 = eta * np.sign(w)  # L1 penalty for magnitude regularization\n",
        "    return grad_risk - grad_ret + grad_l1\n",
        "\n",
        "# -------------------------------\n",
        "# Stress Testing\n",
        "# -------------------------------\n",
        "\n",
        "def extreme_market_scenarios(Sigma, mu, scenarios):\n",
        "    \"\"\"\n",
        "    Perform stress tests on the portfolio under different market conditions.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for scenario in scenarios:\n",
        "        stressed_Sigma = Sigma + scenario['sigma_change']\n",
        "        stressed_mu = mu + scenario['mu_change']\n",
        "        optimized_weights = optimize_weights(stressed_Sigma, stressed_mu)\n",
        "        results[scenario['name']] = optimized_weights\n",
        "\n",
        "    # Log summaries of stress test results\n",
        "    log_output(f\"Stress Test Results (Summary): {dict(list(results.items())[:3])}\")  # Log first 3 results\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------------------\n",
        "# Simulate Fat-Tailed Returns (Student's T-distribution)\n",
        "# -------------------------------\n",
        "\n",
        "def simulate_fat_tailed_returns(Sigma, mu, dof=4, n_samples=1000):\n",
        "    \"\"\"\n",
        "    Simulate returns from a fat-tailed distribution (Student's t-distribution).\n",
        "    \"\"\"\n",
        "    t_returns = np.random.standard_t(dof, size=(n_samples, len(mu)))\n",
        "    fat_tailed_returns = np.dot(t_returns, Sigma) + mu\n",
        "    return fat_tailed_returns\n",
        "\n",
        "# -------------------------------\n",
        "# Monte Carlo Simulation for Large Portfolio\n",
        "# -------------------------------\n",
        "\n",
        "def monte_carlo_testing_large(Sigma, mu, n_samples=1000, n_assets=3, lr=1e-3, epochs=500):\n",
        "    \"\"\"\n",
        "    Perform Monte Carlo Simulation with randomized Sigma and mu.\n",
        "    \"\"\"\n",
        "    monte_carlo_results = []\n",
        "    for _ in range(n_samples):\n",
        "        Sigma_randomized = Sigma + 0.05 * np.random.randn(n_assets, n_assets)\n",
        "        mu_randomized = mu + 0.01 * np.random.randn(n_assets)\n",
        "        weights = optimize_weights(Sigma_randomized, mu_randomized, lam=0.1, gamma=1e-2, eta=1e-2, lr=lr, epochs=epochs)\n",
        "        monte_carlo_results.append(weights)\n",
        "\n",
        "    # Log Monte Carlo results summary (first 5 results)\n",
        "    log_output(f\"Monte Carlo Simulation Results (Summary): {monte_carlo_results[:5]}\")  # First 5 samples\n",
        "\n",
        "    return monte_carlo_results\n",
        "\n",
        "# -------------------------------\n",
        "# Running All Experiments\n",
        "# -------------------------------\n",
        "\n",
        "# Example Data (Covariance matrix and expected returns)\n",
        "Sigma = np.array([[0.02, 0.015, -0.005], [0.015, 0.03, -0.008], [-0.005, -0.008, 0.01]])\n",
        "mu = np.array([0.01, 0.005, -0.002])  # Expected returns for 3 assets\n",
        "\n",
        "# Run the Portfolio Optimization\n",
        "optimized_weights = optimize_weights(Sigma, mu, lam=0.1, gamma=1e-2, eta=1e-2)\n",
        "log_output(f\"Optimized Weights: {optimized_weights}\")\n",
        "\n",
        "# Stress Test Scenarios (market conditions)\n",
        "extreme_scenarios = [\n",
        "    {'name': 'Black Swan', 'sigma_change': 0.1 * np.identity(3), 'mu_change': -0.05 * np.ones(3)},\n",
        "    {'name': 'Liquidity Crisis', 'sigma_change': 0.2 * np.identity(3), 'mu_change': 0.0 * np.ones(3)},\n",
        "    {'name': 'Interest Rate Shock', 'sigma_change': 0.05 * np.identity(3), 'mu_change': 0.02 * np.ones(3)}\n",
        "]\n",
        "\n",
        "# Run Stress Tests\n",
        "stress_test_results = extreme_market_scenarios(Sigma, mu, extreme_scenarios)\n",
        "\n",
        "# Simulate Fat-Tailed Returns\n",
        "fat_tailed_returns = simulate_fat_tailed_returns(Sigma, mu)\n",
        "\n",
        "# Run Monte Carlo Simulation\n",
        "monte_carlo_results = monte_carlo_testing_large(Sigma, mu, n_samples=1000, n_assets=3)\n",
        "\n",
        "# Close the log file after writing all results\n",
        "log_file.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XWkTSqfq6SS",
        "outputId": "2cb19403-ecb0-4f22-ed63-7e9d9638fabd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 250, Weights: [0.3047642  0.32402748 0.37120829]\n",
            "Epoch 300, Weights: [0.29907477 0.32232181 0.37860339]\n",
            "Epoch 350, Weights: [0.29338504 0.32066487 0.38595005]\n",
            "Epoch 400, Weights: [0.28769466 0.31905647 0.39324883]\n",
            "Epoch 450, Weights: [0.28200328 0.31749642 0.40050026]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337279 0.3333289  0.33329828]\n",
            "Epoch 50, Weights: [0.33534129 0.3331035  0.33155518]\n",
            "Epoch 100, Weights: [0.33730007 0.33286996 0.32982994]\n",
            "Epoch 150, Weights: [0.33924916 0.33262834 0.32812248]\n",
            "Epoch 200, Weights: [0.34118859 0.33237869 0.32643269]\n",
            "Epoch 250, Weights: [0.34311838 0.33212108 0.32476051]\n",
            "Epoch 300, Weights: [0.34503857 0.33185557 0.32310583]\n",
            "Epoch 350, Weights: [0.34694918 0.33158221 0.32146858]\n",
            "Epoch 400, Weights: [0.34885025 0.33130106 0.31984866]\n",
            "Epoch 450, Weights: [0.35074181 0.33101219 0.31824597]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33326598 0.33333172 0.33340227]\n",
            "Epoch 50, Weights: [0.32988462 0.3332537  0.33686165]\n",
            "Epoch 100, Weights: [0.32647491 0.33317991 0.34034515]\n",
            "Epoch 150, Weights: [0.32303669 0.33311043 0.34385285]\n",
            "Epoch 200, Weights: [0.31956979 0.33304534 0.34738483]\n",
            "Epoch 250, Weights: [0.31607406 0.33298474 0.35094117]\n",
            "Epoch 300, Weights: [0.31254931 0.33292871 0.35452195]\n",
            "Epoch 350, Weights: [0.30899539 0.33287734 0.35812724]\n",
            "Epoch 400, Weights: [0.30541212 0.33283073 0.36175711]\n",
            "Epoch 450, Weights: [0.30179935 0.33278897 0.36541166]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332565 0.33329124 0.33338308]\n",
            "Epoch 50, Weights: [0.33295729 0.33118055 0.33586213]\n",
            "Epoch 100, Weights: [0.33261908 0.32905672 0.33832417]\n",
            "Epoch 150, Weights: [0.3323109  0.32691967 0.3407694 ]\n",
            "Epoch 200, Weights: [0.3320326  0.32476932 0.34319804]\n",
            "Epoch 250, Weights: [0.33178407 0.32260557 0.34561032]\n",
            "Epoch 300, Weights: [0.33156518 0.32042834 0.34800645]\n",
            "Epoch 350, Weights: [0.33137581 0.31823753 0.35038663]\n",
            "Epoch 400, Weights: [0.33121582 0.31603305 0.35275109]\n",
            "Epoch 450, Weights: [0.33108512 0.31381481 0.35510004]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335699 0.3333466  0.33329638]\n",
            "Epoch 50, Weights: [0.33454108 0.33401002 0.33144887]\n",
            "Epoch 100, Weights: [0.3357266  0.334673   0.32960038]\n",
            "Epoch 150, Weights: [0.33691363 0.33533556 0.32775079]\n",
            "Epoch 200, Weights: [0.33810226 0.33599774 0.32589997]\n",
            "Epoch 250, Weights: [0.33929259 0.33665956 0.32404781]\n",
            "Epoch 300, Weights: [0.34048471 0.33732108 0.32219418]\n",
            "Epoch 350, Weights: [0.34167872 0.33798231 0.32033894]\n",
            "Epoch 400, Weights: [0.3428747  0.33864329 0.31848198]\n",
            "Epoch 450, Weights: [0.34407275 0.33930406 0.31662316]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33343652 0.33327844 0.33328501]\n",
            "Epoch 50, Weights: [0.33858785 0.33054925 0.33086286]\n",
            "Epoch 100, Weights: [0.34372307 0.32784883 0.32842808]\n",
            "Epoch 150, Weights: [0.34884264 0.32517669 0.32598065]\n",
            "Epoch 200, Weights: [0.35394704 0.32253236 0.32352057]\n",
            "Epoch 250, Weights: [0.35903675 0.3199154  0.32104783]\n",
            "Epoch 300, Weights: [0.36411224 0.31732532 0.31856241]\n",
            "Epoch 350, Weights: [0.36917398 0.3147617  0.31606429]\n",
            "Epoch 400, Weights: [0.37422244 0.31222408 0.31355345]\n",
            "Epoch 450, Weights: [0.37925809 0.30971203 0.31102985]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332446 0.33333399 0.33334152]\n",
            "Epoch 50, Weights: [0.33287929 0.33337044 0.33375024]\n",
            "Epoch 100, Weights: [0.33242999 0.33341311 0.33415687]\n",
            "Epoch 150, Weights: [0.33197646 0.33346218 0.33456134]\n",
            "Epoch 200, Weights: [0.33151861 0.3335178  0.33496356]\n",
            "Epoch 250, Weights: [0.33105636 0.33358015 0.33536346]\n",
            "Epoch 300, Weights: [0.3305896  0.33364941 0.33576096]\n",
            "Epoch 350, Weights: [0.33011824 0.33372575 0.33615598]\n",
            "Epoch 400, Weights: [0.32964218 0.33380937 0.33654842]\n",
            "Epoch 450, Weights: [0.32916132 0.33390045 0.3369382 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328661 0.33332879 0.33338457]\n",
            "Epoch 50, Weights: [0.33096178 0.33309954 0.33593865]\n",
            "Epoch 100, Weights: [0.32865852 0.33286507 0.33847638]\n",
            "Epoch 150, Weights: [0.32637673 0.33262546 0.34099779]\n",
            "Epoch 200, Weights: [0.32411634 0.33238075 0.34350288]\n",
            "Epoch 250, Weights: [0.32187726 0.33213103 0.34599168]\n",
            "Epoch 300, Weights: [0.31965942 0.33187634 0.34846421]\n",
            "Epoch 350, Weights: [0.31746273 0.33161675 0.35092049]\n",
            "Epoch 400, Weights: [0.3152871  0.33135232 0.35336056]\n",
            "Epoch 450, Weights: [0.31313244 0.33108311 0.35578442]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33339262 0.33327893 0.33332842]\n",
            "Epoch 50, Weights: [0.33636389 0.33055047 0.33308561]\n",
            "Epoch 100, Weights: [0.33934775 0.32780505 0.33284717]\n",
            "Epoch 150, Weights: [0.3423437  0.3250431  0.33261317]\n",
            "Epoch 200, Weights: [0.34535124 0.32226507 0.33238365]\n",
            "Epoch 250, Weights: [0.34836986 0.31947144 0.33215868]\n",
            "Epoch 300, Weights: [0.35139902 0.31666266 0.33193829]\n",
            "Epoch 350, Weights: [0.35443819 0.31383923 0.33172255]\n",
            "Epoch 400, Weights: [0.35748684 0.31100164 0.33151149]\n",
            "Epoch 450, Weights: [0.3605444  0.3081504  0.33130517]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338835 0.3333073  0.33330432]\n",
            "Epoch 50, Weights: [0.33614677 0.33200528 0.33184792]\n",
            "Epoch 100, Weights: [0.33891957 0.33070147 0.33037894]\n",
            "Epoch 150, Weights: [0.3417069  0.32939584 0.32889723]\n",
            "Epoch 200, Weights: [0.34450893 0.32808838 0.32740266]\n",
            "Epoch 250, Weights: [0.34732582 0.32677907 0.32589509]\n",
            "Epoch 300, Weights: [0.35015771 0.32546789 0.32437437]\n",
            "Epoch 350, Weights: [0.35300479 0.32415482 0.32284036]\n",
            "Epoch 400, Weights: [0.35586721 0.32283985 0.32129291]\n",
            "Epoch 450, Weights: [0.35874514 0.32152296 0.31973188]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333175 0.33328931 0.33337891]\n",
            "Epoch 50, Weights: [0.33325834 0.33109634 0.33564529]\n",
            "Epoch 100, Weights: [0.33319487 0.32891874 0.33788636]\n",
            "Epoch 150, Weights: [0.33314127 0.32675647 0.34010223]\n",
            "Epoch 200, Weights: [0.33309748 0.3246095  0.34229299]\n",
            "Epoch 250, Weights: [0.33306343 0.32247778 0.34445876]\n",
            "Epoch 300, Weights: [0.33303906 0.32036129 0.34659962]\n",
            "Epoch 350, Weights: [0.3330243  0.31825997 0.3487157 ]\n",
            "Epoch 400, Weights: [0.33301909 0.31617378 0.3508071 ]\n",
            "Epoch 450, Weights: [0.33302336 0.31410267 0.35287393]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328815 0.33336847 0.33334335]\n",
            "Epoch 50, Weights: [0.33101517 0.33512973 0.33385507]\n",
            "Epoch 100, Weights: [0.3287138  0.33689884 0.33438733]\n",
            "Epoch 150, Weights: [0.32638362 0.33867583 0.33494052]\n",
            "Epoch 200, Weights: [0.32402423 0.34046069 0.33551505]\n",
            "Epoch 250, Weights: [0.32163518 0.34225344 0.33611135]\n",
            "Epoch 300, Weights: [0.31921606 0.34405407 0.33672983]\n",
            "Epoch 350, Weights: [0.31676643 0.34586261 0.33737093]\n",
            "Epoch 400, Weights: [0.31428584 0.34767904 0.3380351 ]\n",
            "Epoch 450, Weights: [0.31177384 0.34950336 0.33872277]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329522 0.33343221 0.33327254]\n",
            "Epoch 50, Weights: [0.33137715 0.33837481 0.330248  ]\n",
            "Epoch 100, Weights: [0.32943352 0.34331486 0.32725159]\n",
            "Epoch 150, Weights: [0.32746404 0.34825302 0.32428291]\n",
            "Epoch 200, Weights: [0.32546845 0.35318995 0.32134157]\n",
            "Epoch 250, Weights: [0.32344644 0.35812633 0.3184272 ]\n",
            "Epoch 300, Weights: [0.32139771 0.36306283 0.31553943]\n",
            "Epoch 350, Weights: [0.31932194 0.36800014 0.31267789]\n",
            "Epoch 400, Weights: [0.31721882 0.37293894 0.30984222]\n",
            "Epoch 450, Weights: [0.31508799 0.37787992 0.30703206]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334706 0.33330188 0.33335104]\n",
            "Epoch 50, Weights: [0.33403335 0.33172805 0.33423857]\n",
            "Epoch 100, Weights: [0.33471864 0.33015137 0.33512996]\n",
            "Epoch 150, Weights: [0.33540292 0.3285718  0.33602525]\n",
            "Epoch 200, Weights: [0.33608617 0.32698928 0.33692452]\n",
            "Epoch 250, Weights: [0.33676837 0.32540378 0.33782783]\n",
            "Epoch 300, Weights: [0.3374495  0.32381524 0.33873523]\n",
            "Epoch 350, Weights: [0.33812956 0.32222362 0.33964679]\n",
            "Epoch 400, Weights: [0.33880852 0.32062888 0.34056257]\n",
            "Epoch 450, Weights: [0.33948637 0.31903097 0.34148264]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33342934 0.33320818 0.33336244]\n",
            "Epoch 50, Weights: [0.33821166 0.32695129 0.33483703]\n",
            "Epoch 100, Weights: [0.34295744 0.32069463 0.3363479 ]\n",
            "Epoch 150, Weights: [0.34766688 0.31443841 0.33789467]\n",
            "Epoch 200, Weights: [0.35234018 0.30818282 0.33947697]\n",
            "Epoch 250, Weights: [0.35697752 0.30192805 0.3410944 ]\n",
            "Epoch 300, Weights: [0.36157909 0.29567428 0.34274661]\n",
            "Epoch 350, Weights: [0.36614508 0.28942169 0.3444332 ]\n",
            "Epoch 400, Weights: [0.3706757  0.28317046 0.34615381]\n",
            "Epoch 450, Weights: [0.37517113 0.27692077 0.34790807]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331089 0.33337285 0.33331623]\n",
            "Epoch 50, Weights: [0.33218187 0.33535724 0.33246086]\n",
            "Epoch 100, Weights: [0.33103855 0.33735735 0.33160407]\n",
            "Epoch 150, Weights: [0.32988082 0.33937327 0.33074588]\n",
            "Epoch 200, Weights: [0.32870861 0.34140506 0.3298863 ]\n",
            "Epoch 250, Weights: [0.32752183 0.34345281 0.32902534]\n",
            "Epoch 300, Weights: [0.32632037 0.34551659 0.32816301]\n",
            "Epoch 350, Weights: [0.32510416 0.34759648 0.32729933]\n",
            "Epoch 400, Weights: [0.32387311 0.34969256 0.3264343 ]\n",
            "Epoch 450, Weights: [0.32262712 0.3518049  0.32556795]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333493  0.3333606  0.33329007]\n",
            "Epoch 50, Weights: [0.33415339 0.33473434 0.33111224]\n",
            "Epoch 100, Weights: [0.33496749 0.33612813 0.32890435]\n",
            "Epoch 150, Weights: [0.33579176 0.33754228 0.32666593]\n",
            "Epoch 200, Weights: [0.33662634 0.33897715 0.32439648]\n",
            "Epoch 250, Weights: [0.33747139 0.34043308 0.3220955 ]\n",
            "Epoch 300, Weights: [0.33832707 0.34191041 0.31976248]\n",
            "Epoch 350, Weights: [0.33919355 0.34340952 0.3173969 ]\n",
            "Epoch 400, Weights: [0.340071   0.34493077 0.31499821]\n",
            "Epoch 450, Weights: [0.34095957 0.34647452 0.31256588]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33340717 0.33332741 0.33326538]\n",
            "Epoch 50, Weights: [0.33709132 0.33301721 0.32989144]\n",
            "Epoch 100, Weights: [0.34075888 0.33267829 0.3265628 ]\n",
            "Epoch 150, Weights: [0.34440974 0.33231078 0.32327946]\n",
            "Epoch 200, Weights: [0.34804377 0.33191479 0.32004141]\n",
            "Epoch 250, Weights: [0.35166088 0.33149047 0.31684862]\n",
            "Epoch 300, Weights: [0.35526095 0.33103794 0.31370107]\n",
            "Epoch 350, Weights: [0.3588439  0.33055736 0.31059871]\n",
            "Epoch 400, Weights: [0.36240962 0.33004886 0.30754149]\n",
            "Epoch 450, Weights: [0.36595802 0.3295126  0.30452934]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333397  0.333353   0.33330727]\n",
            "Epoch 50, Weights: [0.33366367 0.33433276 0.33200354]\n",
            "Epoch 100, Weights: [0.33399745 0.33530507 0.33069746]\n",
            "Epoch 150, Weights: [0.33434102 0.33626995 0.329389  ]\n",
            "Epoch 200, Weights: [0.33469439 0.33722745 0.32807813]\n",
            "Epoch 250, Weights: [0.33505754 0.3381776  0.32676483]\n",
            "Epoch 300, Weights: [0.33543045 0.33912044 0.32544907]\n",
            "Epoch 350, Weights: [0.33581313 0.34005601 0.32413082]\n",
            "Epoch 400, Weights: [0.33620557 0.34098434 0.32281006]\n",
            "Epoch 450, Weights: [0.33660775 0.34190548 0.32148674]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330121 0.33331319 0.33338557]\n",
            "Epoch 50, Weights: [0.33168569 0.33229101 0.33602328]\n",
            "Epoch 100, Weights: [0.33005079 0.3312382  0.33871099]\n",
            "Epoch 150, Weights: [0.32839604 0.33015424 0.34144969]\n",
            "Epoch 200, Weights: [0.32672097 0.32903861 0.3442404 ]\n",
            "Epoch 250, Weights: [0.32502509 0.32789075 0.34708413]\n",
            "Epoch 300, Weights: [0.32330793 0.32671011 0.34998193]\n",
            "Epoch 350, Weights: [0.32156898 0.32549613 0.35293486]\n",
            "Epoch 400, Weights: [0.31980773 0.32424823 0.35594401]\n",
            "Epoch 450, Weights: [0.31802367 0.32296583 0.35901047]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334035 0.33328613 0.33337349]\n",
            "Epoch 50, Weights: [0.33369601 0.33092088 0.33538308]\n",
            "Epoch 100, Weights: [0.33406039 0.32854486 0.33739472]\n",
            "Epoch 150, Weights: [0.33443358 0.32615808 0.3394083 ]\n",
            "Epoch 200, Weights: [0.33481567 0.32376056 0.34142374]\n",
            "Epoch 250, Weights: [0.33520675 0.32135228 0.34344094]\n",
            "Epoch 300, Weights: [0.33560691 0.31893328 0.34545978]\n",
            "Epoch 350, Weights: [0.33601625 0.31650354 0.34748017]\n",
            "Epoch 400, Weights: [0.33643487 0.31406309 0.34950202]\n",
            "Epoch 450, Weights: [0.33686284 0.31161193 0.3515252 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329863 0.33332687 0.33337447]\n",
            "Epoch 50, Weights: [0.33156199 0.33300621 0.33543177]\n",
            "Epoch 100, Weights: [0.32982108 0.33268944 0.33748944]\n",
            "Epoch 150, Weights: [0.32807575 0.33237653 0.33954769]\n",
            "Epoch 200, Weights: [0.32632582 0.33206743 0.34160672]\n",
            "Epoch 250, Weights: [0.32457113 0.3317621  0.34366674]\n",
            "Epoch 300, Weights: [0.32281151 0.3314605  0.34572796]\n",
            "Epoch 350, Weights: [0.3210468  0.33116258 0.34779059]\n",
            "Epoch 400, Weights: [0.31927682 0.33086833 0.34985483]\n",
            "Epoch 450, Weights: [0.31750139 0.33057768 0.3519209 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335178 0.33332423 0.33332396]\n",
            "Epoch 50, Weights: [0.33427879 0.33286804 0.33285314]\n",
            "Epoch 100, Weights: [0.3352142  0.33240884 0.33237693]\n",
            "Epoch 150, Weights: [0.33615812 0.33194658 0.33189528]\n",
            "Epoch 200, Weights: [0.33711067 0.3314812  0.3314081 ]\n",
            "Epoch 250, Weights: [0.33807198 0.33101265 0.33091534]\n",
            "Epoch 300, Weights: [0.33904216 0.33054089 0.33041692]\n",
            "Epoch 350, Weights: [0.34002135 0.33006586 0.32991276]\n",
            "Epoch 400, Weights: [0.34100966 0.32958752 0.32940279]\n",
            "Epoch 450, Weights: [0.34200725 0.32910579 0.32888693]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332905 0.33336645 0.33330447]\n",
            "Epoch 50, Weights: [0.33311286 0.33503067 0.33185644]\n",
            "Epoch 100, Weights: [0.33289203 0.33671012 0.33039782]\n",
            "Epoch 150, Weights: [0.3326665  0.33840497 0.3289285 ]\n",
            "Epoch 200, Weights: [0.33243623 0.34011538 0.32744837]\n",
            "Epoch 250, Weights: [0.33220116 0.3418415  0.3259573 ]\n",
            "Epoch 300, Weights: [0.33196126 0.34358351 0.3244552 ]\n",
            "Epoch 350, Weights: [0.33171646 0.34534158 0.32294193]\n",
            "Epoch 400, Weights: [0.33146673 0.34711586 0.32141738]\n",
            "Epoch 450, Weights: [0.331212   0.34890654 0.31988142]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335727 0.33321483 0.33342787]\n",
            "Epoch 50, Weights: [0.33456606 0.32727472 0.33815919]\n",
            "Epoch 100, Weights: [0.33579719 0.32130477 0.342898  ]\n",
            "Epoch 150, Weights: [0.33705075 0.31530542 0.34764381]\n",
            "Epoch 200, Weights: [0.33832682 0.30927709 0.35239607]\n",
            "Epoch 250, Weights: [0.33962548 0.30322023 0.35715426]\n",
            "Epoch 300, Weights: [0.34094682 0.29713531 0.36191784]\n",
            "Epoch 350, Weights: [0.3422909  0.29102279 0.36668628]\n",
            "Epoch 400, Weights: [0.34365779 0.28488315 0.37145903]\n",
            "Epoch 450, Weights: [0.34504755 0.27871688 0.37623554]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331566 0.33329438 0.33338993]\n",
            "Epoch 50, Weights: [0.3324304  0.33134734 0.33622223]\n",
            "Epoch 100, Weights: [0.33154072 0.32940063 0.33905862]\n",
            "Epoch 150, Weights: [0.33064661 0.32745418 0.34189917]\n",
            "Epoch 200, Weights: [0.32974802 0.32550796 0.34474398]\n",
            "Epoch 250, Weights: [0.32884492 0.32356191 0.34759314]\n",
            "Epoch 300, Weights: [0.32793727 0.32161596 0.35044674]\n",
            "Epoch 350, Weights: [0.32702503 0.31967008 0.35330486]\n",
            "Epoch 400, Weights: [0.32610817 0.31772419 0.35616761]\n",
            "Epoch 450, Weights: [0.32518665 0.31577826 0.35903506]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33340795 0.33334028 0.33325174]\n",
            "Epoch 50, Weights: [0.33712824 0.33367665 0.32919508]\n",
            "Epoch 100, Weights: [0.34082673 0.3339911  0.32518214]\n",
            "Epoch 150, Weights: [0.34450372 0.33428398 0.32121227]\n",
            "Epoch 200, Weights: [0.34815951 0.33455563 0.31728483]\n",
            "Epoch 250, Weights: [0.3517944  0.3348064  0.31339917]\n",
            "Epoch 300, Weights: [0.35540867 0.33503662 0.30955469]\n",
            "Epoch 350, Weights: [0.3590026  0.33524661 0.30575076]\n",
            "Epoch 400, Weights: [0.36257647 0.33543671 0.3019868 ]\n",
            "Epoch 450, Weights: [0.36613055 0.33560722 0.2982622 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335376 0.33336012 0.33328608]\n",
            "Epoch 50, Weights: [0.33438354 0.33470359 0.33091285]\n",
            "Epoch 100, Weights: [0.33542892 0.33605375 0.3285173 ]\n",
            "Epoch 150, Weights: [0.33649018 0.33741065 0.32609914]\n",
            "Epoch 200, Weights: [0.33756761 0.33877432 0.32365804]\n",
            "Epoch 250, Weights: [0.33866147 0.3401448  0.32119369]\n",
            "Epoch 300, Weights: [0.33977208 0.34152213 0.31870576]\n",
            "Epoch 350, Weights: [0.34089971 0.34290635 0.3161939 ]\n",
            "Epoch 400, Weights: [0.34204469 0.3442975  0.31365779]\n",
            "Epoch 450, Weights: [0.3432073  0.34569561 0.31109706]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33339115 0.33330401 0.33330481]\n",
            "Epoch 50, Weights: [0.33628506 0.33183924 0.33187567]\n",
            "Epoch 100, Weights: [0.33918361 0.33037632 0.33044004]\n",
            "Epoch 150, Weights: [0.3420865  0.32891538 0.32899809]\n",
            "Epoch 200, Weights: [0.34499341 0.32745658 0.32754998]\n",
            "Epoch 250, Weights: [0.34790405 0.32600005 0.32609587]\n",
            "Epoch 300, Weights: [0.3508181  0.32454594 0.32463593]\n",
            "Epoch 350, Weights: [0.35373524 0.32309439 0.32317033]\n",
            "Epoch 400, Weights: [0.35665518 0.32164555 0.32169924]\n",
            "Epoch 450, Weights: [0.3595776  0.32019954 0.32022283]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333455  0.33328894 0.33336553]\n",
            "Epoch 50, Weights: [0.3339481  0.33106342 0.33498845]\n",
            "Epoch 100, Weights: [0.33453829 0.32882522 0.33663646]\n",
            "Epoch 150, Weights: [0.33511593 0.32657438 0.33830966]\n",
            "Epoch 200, Weights: [0.33568089 0.32431097 0.34000811]\n",
            "Epoch 250, Weights: [0.33623303 0.32203506 0.34173188]\n",
            "Epoch 300, Weights: [0.33677222 0.3197467  0.34348106]\n",
            "Epoch 350, Weights: [0.33729832 0.31744595 0.34525571]\n",
            "Epoch 400, Weights: [0.33781119 0.31513288 0.3470559 ]\n",
            "Epoch 450, Weights: [0.33831072 0.31280755 0.3488817 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336369 0.33328301 0.33335327]\n",
            "Epoch 50, Weights: [0.33487698 0.3307664  0.33435659]\n",
            "Epoch 100, Weights: [0.33638002 0.32824797 0.33537199]\n",
            "Epoch 150, Weights: [0.33787276 0.32572776 0.33639945]\n",
            "Epoch 200, Weights: [0.33935519 0.32320582 0.33743896]\n",
            "Epoch 250, Weights: [0.34082726 0.32068222 0.33849049]\n",
            "Epoch 300, Weights: [0.34228895 0.318157   0.33955402]\n",
            "Epoch 350, Weights: [0.34374022 0.31563021 0.34062954]\n",
            "Epoch 400, Weights: [0.34518104 0.31310192 0.34171701]\n",
            "Epoch 450, Weights: [0.34661137 0.31057218 0.34281642]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329878 0.33334571 0.33335548]\n",
            "Epoch 50, Weights: [0.33156748 0.33396952 0.33446297]\n",
            "Epoch 100, Weights: [0.32982825 0.33460172 0.33557001]\n",
            "Epoch 150, Weights: [0.32808107 0.33524233 0.33667656]\n",
            "Epoch 200, Weights: [0.32632596 0.3358914  0.33778261]\n",
            "Epoch 250, Weights: [0.32456292 0.33654893 0.33888812]\n",
            "Epoch 300, Weights: [0.32279193 0.33721496 0.33999308]\n",
            "Epoch 350, Weights: [0.321013   0.33788951 0.34109745]\n",
            "Epoch 400, Weights: [0.31922614 0.33857262 0.34220122]\n",
            "Epoch 450, Weights: [0.31743133 0.33926429 0.34330435]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333693 0.33338138 0.33328167]\n",
            "Epoch 50, Weights: [0.33352685 0.33577053 0.33070259]\n",
            "Epoch 100, Weights: [0.33373575 0.33813343 0.32813079]\n",
            "Epoch 150, Weights: [0.33396345 0.34047051 0.32556601]\n",
            "Epoch 200, Weights: [0.33420979 0.34278215 0.32300802]\n",
            "Epoch 250, Weights: [0.33447461 0.34506877 0.32045659]\n",
            "Epoch 300, Weights: [0.33475774 0.34733076 0.31791147]\n",
            "Epoch 350, Weights: [0.33505902 0.34956852 0.31537243]\n",
            "Epoch 400, Weights: [0.3353783  0.35178242 0.31283925]\n",
            "Epoch 450, Weights: [0.33571543 0.35397286 0.31031168]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335712 0.33331648 0.33332636]\n",
            "Epoch 50, Weights: [0.33454359 0.33248189 0.33297449]\n",
            "Epoch 100, Weights: [0.33572306 0.33166185 0.33261506]\n",
            "Epoch 150, Weights: [0.3368955  0.33085637 0.3322481 ]\n",
            "Epoch 200, Weights: [0.33806087 0.33006545 0.33187365]\n",
            "Epoch 250, Weights: [0.33921913 0.3292891  0.33149173]\n",
            "Epoch 300, Weights: [0.34037024 0.32852734 0.33110239]\n",
            "Epoch 350, Weights: [0.34151417 0.32778014 0.33070566]\n",
            "Epoch 400, Weights: [0.34265088 0.32704754 0.33030156]\n",
            "Epoch 450, Weights: [0.34378033 0.32632951 0.32989013]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330581 0.3332691  0.33342506]\n",
            "Epoch 50, Weights: [0.3319008  0.33008046 0.33801871]\n",
            "Epoch 100, Weights: [0.33043801 0.32693609 0.34262587]\n",
            "Epoch 150, Weights: [0.32891736 0.32383652 0.34724609]\n",
            "Epoch 200, Weights: [0.32733877 0.32078228 0.35187892]\n",
            "Epoch 250, Weights: [0.32570219 0.31777387 0.35652392]\n",
            "Epoch 300, Weights: [0.32400756 0.31481178 0.36118063]\n",
            "Epoch 350, Weights: [0.32225487 0.31189651 0.36584859]\n",
            "Epoch 400, Weights: [0.32044408 0.30902854 0.37052734]\n",
            "Epoch 450, Weights: [0.3185752  0.30620834 0.37521643]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332123 0.33332809 0.33335065]\n",
            "Epoch 50, Weights: [0.33271001 0.33306738 0.33422258]\n",
            "Epoch 100, Weights: [0.33208563 0.33280867 0.33510568]\n",
            "Epoch 150, Weights: [0.33144785 0.3325521  0.33600003]\n",
            "Epoch 200, Weights: [0.33079645 0.3322978  0.33690573]\n",
            "Epoch 250, Weights: [0.33013121 0.3320459  0.33782286]\n",
            "Epoch 300, Weights: [0.32945189 0.33179655 0.33875153]\n",
            "Epoch 350, Weights: [0.32875826 0.33154988 0.33969182]\n",
            "Epoch 400, Weights: [0.32805009 0.33130605 0.34064383]\n",
            "Epoch 450, Weights: [0.32732713 0.3310652  0.34160764]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336648 0.3332575  0.333376  ]\n",
            "Epoch 50, Weights: [0.33502911 0.32946526 0.3355056 ]\n",
            "Epoch 100, Weights: [0.33670152 0.32567119 0.33762725]\n",
            "Epoch 150, Weights: [0.3383837  0.32187529 0.33974097]\n",
            "Epoch 200, Weights: [0.34007565 0.31807754 0.34184678]\n",
            "Epoch 250, Weights: [0.34177735 0.31427793 0.34394469]\n",
            "Epoch 300, Weights: [0.34348881 0.31047643 0.34603473]\n",
            "Epoch 350, Weights: [0.34521002 0.30667304 0.34811691]\n",
            "Epoch 400, Weights: [0.34694098 0.30286773 0.35019126]\n",
            "Epoch 450, Weights: [0.34868168 0.2990605  0.35225779]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333491 0.33334977 0.33331529]\n",
            "Epoch 50, Weights: [0.33341538 0.33417439 0.3324102 ]\n",
            "Epoch 100, Weights: [0.33349836 0.33500341 0.3314982 ]\n",
            "Epoch 150, Weights: [0.33358388 0.33583685 0.33057924]\n",
            "Epoch 200, Weights: [0.33367199 0.33667474 0.32965325]\n",
            "Epoch 250, Weights: [0.33376271 0.3375171  0.32872016]\n",
            "Epoch 300, Weights: [0.33385609 0.33836395 0.32777992]\n",
            "Epoch 350, Weights: [0.33395218 0.33921533 0.32683246]\n",
            "Epoch 400, Weights: [0.33405099 0.34007127 0.32587771]\n",
            "Epoch 450, Weights: [0.33415259 0.34093177 0.32491561]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329694 0.33330617 0.33339687]\n",
            "Epoch 50, Weights: [0.33147704 0.33194124 0.33658169]\n",
            "Epoch 100, Weights: [0.32965619 0.33056224 0.33978154]\n",
            "Epoch 150, Weights: [0.32783455 0.32916903 0.34299639]\n",
            "Epoch 200, Weights: [0.32601225 0.32776154 0.34622618]\n",
            "Epoch 250, Weights: [0.32418946 0.32633964 0.34947087]\n",
            "Epoch 300, Weights: [0.32236633 0.32490323 0.35273041]\n",
            "Epoch 350, Weights: [0.32054301 0.32345221 0.35600475]\n",
            "Epoch 400, Weights: [0.31871967 0.32198648 0.35929383]\n",
            "Epoch 450, Weights: [0.31689647 0.32050591 0.36259759]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.333353   0.3333331  0.33331386]\n",
            "Epoch 50, Weights: [0.33433913 0.33331996 0.33234088]\n",
            "Epoch 100, Weights: [0.33532942 0.33330274 0.33136781]\n",
            "Epoch 150, Weights: [0.33632394 0.33328141 0.33039462]\n",
            "Epoch 200, Weights: [0.33732273 0.33325593 0.32942131]\n",
            "Epoch 250, Weights: [0.33832585 0.33322627 0.32844785]\n",
            "Epoch 300, Weights: [0.33933336 0.33319239 0.32747422]\n",
            "Epoch 350, Weights: [0.34034531 0.33315426 0.32650041]\n",
            "Epoch 400, Weights: [0.34136174 0.33311183 0.32552639]\n",
            "Epoch 450, Weights: [0.34238273 0.33306509 0.32455215]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338896 0.33338625 0.33322475]\n",
            "Epoch 50, Weights: [0.33617243 0.33602336 0.32780418]\n",
            "Epoch 100, Weights: [0.33895894 0.33864191 0.32239912]\n",
            "Epoch 150, Weights: [0.3417485  0.34124183 0.31700964]\n",
            "Epoch 200, Weights: [0.34454113 0.34382303 0.31163581]\n",
            "Epoch 250, Weights: [0.34733685 0.34638545 0.30627767]\n",
            "Epoch 300, Weights: [0.35013566 0.34892901 0.30093529]\n",
            "Epoch 350, Weights: [0.3529376  0.35145364 0.29560873]\n",
            "Epoch 400, Weights: [0.35574266 0.35395926 0.29029804]\n",
            "Epoch 450, Weights: [0.35855088 0.35644581 0.28500328]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334095 0.33329292 0.3333661 ]\n",
            "Epoch 50, Weights: [0.33372069 0.33127947 0.33499981]\n",
            "Epoch 100, Weights: [0.33409726 0.32927907 0.33662364]\n",
            "Epoch 150, Weights: [0.33447069 0.32729161 0.33823768]\n",
            "Epoch 200, Weights: [0.33484098 0.325317   0.33984199]\n",
            "Epoch 250, Weights: [0.33520818 0.32335515 0.34143665]\n",
            "Epoch 300, Weights: [0.33557229 0.32140595 0.34302173]\n",
            "Epoch 350, Weights: [0.33593334 0.31946932 0.34459731]\n",
            "Epoch 400, Weights: [0.33629134 0.31754516 0.34616347]\n",
            "Epoch 450, Weights: [0.33664633 0.31563338 0.34772026]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335149 0.33333147 0.33331701]\n",
            "Epoch 50, Weights: [0.33425936 0.33324027 0.33250034]\n",
            "Epoch 100, Weights: [0.33516607 0.33315244 0.33168146]\n",
            "Epoch 150, Weights: [0.33607163 0.33306795 0.33086039]\n",
            "Epoch 200, Weights: [0.33697599 0.33298681 0.33003717]\n",
            "Epoch 250, Weights: [0.33787914 0.33290899 0.32921183]\n",
            "Epoch 300, Weights: [0.33878107 0.33283449 0.32838441]\n",
            "Epoch 350, Weights: [0.33968174 0.33276329 0.32755494]\n",
            "Epoch 400, Weights: [0.34058115 0.33269538 0.32672344]\n",
            "Epoch 450, Weights: [0.34147926 0.33263076 0.32588995]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33339574 0.33327339 0.33333084]\n",
            "Epoch 50, Weights: [0.3365299  0.3302619  0.33320817]\n",
            "Epoch 100, Weights: [0.33968986 0.32722143 0.33308868]\n",
            "Epoch 150, Weights: [0.34287576 0.3241518  0.33297241]\n",
            "Epoch 200, Weights: [0.34608771 0.32105283 0.33285944]\n",
            "Epoch 250, Weights: [0.34932582 0.31792434 0.33274981]\n",
            "Epoch 300, Weights: [0.35259022 0.31476615 0.3326436 ]\n",
            "Epoch 350, Weights: [0.35588102 0.31157808 0.33254087]\n",
            "Epoch 400, Weights: [0.35919833 0.30835997 0.33244167]\n",
            "Epoch 450, Weights: [0.36254226 0.30511163 0.33234608]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328853 0.33333453 0.3333769 ]\n",
            "Epoch 50, Weights: [0.33104316 0.33339741 0.3355594 ]\n",
            "Epoch 100, Weights: [0.32878617 0.3334651  0.3377487 ]\n",
            "Epoch 150, Weights: [0.32651765 0.33353765 0.33994466]\n",
            "Epoch 200, Weights: [0.32423769 0.33361512 0.34214717]\n",
            "Epoch 250, Weights: [0.32194637 0.33369753 0.34435607]\n",
            "Epoch 300, Weights: [0.3196438  0.33378493 0.34657124]\n",
            "Epoch 350, Weights: [0.31733008 0.33387737 0.34879253]\n",
            "Epoch 400, Weights: [0.31500529 0.33397488 0.3510198 ]\n",
            "Epoch 450, Weights: [0.31266956 0.33407751 0.3532529 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33325992 0.33336432 0.33337573]\n",
            "Epoch 50, Weights: [0.32958616 0.33491222 0.33550159]\n",
            "Epoch 100, Weights: [0.32590532 0.33645662 0.33763803]\n",
            "Epoch 150, Weights: [0.32221761 0.33799738 0.33978498]\n",
            "Epoch 200, Weights: [0.31852328 0.33953432 0.34194237]\n",
            "Epoch 250, Weights: [0.31482253 0.34106731 0.34411013]\n",
            "Epoch 300, Weights: [0.31111559 0.34259619 0.34628819]\n",
            "Epoch 350, Weights: [0.30740269 0.3441208  0.34847648]\n",
            "Epoch 400, Weights: [0.30368405 0.345641   0.35067492]\n",
            "Epoch 450, Weights: [0.29995991 0.34715662 0.35288344]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335964 0.33334304 0.33329729]\n",
            "Epoch 50, Weights: [0.33467743 0.33383104 0.3314915 ]\n",
            "Epoch 100, Weights: [0.33599868 0.33432334 0.32967796]\n",
            "Epoch 150, Weights: [0.33732337 0.33481993 0.32785668]\n",
            "Epoch 200, Weights: [0.33865149 0.33532081 0.32602767]\n",
            "Epoch 250, Weights: [0.33998303 0.33582598 0.32419096]\n",
            "Epoch 300, Weights: [0.34131798 0.33633543 0.32234656]\n",
            "Epoch 350, Weights: [0.34265632 0.33684916 0.3204945 ]\n",
            "Epoch 400, Weights: [0.34399804 0.33736715 0.31863478]\n",
            "Epoch 450, Weights: [0.34534313 0.33788941 0.31676742]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33339133 0.33327158 0.33333706]\n",
            "Epoch 50, Weights: [0.33630201 0.33017895 0.33351901]\n",
            "Epoch 100, Weights: [0.33923294 0.32707556 0.33369147]\n",
            "Epoch 150, Weights: [0.34218439 0.32396112 0.33385446]\n",
            "Epoch 200, Weights: [0.34515663 0.32083533 0.33400801]\n",
            "Epoch 250, Weights: [0.34814996 0.3176979  0.33415211]\n",
            "Epoch 300, Weights: [0.35116465 0.31454852 0.3342868 ]\n",
            "Epoch 350, Weights: [0.354201   0.3113869  0.33441207]\n",
            "Epoch 400, Weights: [0.35725932 0.30821272 0.33452794]\n",
            "Epoch 450, Weights: [0.36033991 0.30502565 0.33463441]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335817 0.33326566 0.33337614]\n",
            "Epoch 50, Weights: [0.3346062  0.3298733  0.33552047]\n",
            "Epoch 100, Weights: [0.33586548 0.32646279 0.3376717 ]\n",
            "Epoch 150, Weights: [0.33713605 0.32303416 0.33982976]\n",
            "Epoch 200, Weights: [0.33841796 0.31958742 0.34199459]\n",
            "Epoch 250, Weights: [0.33971125 0.31612259 0.34416613]\n",
            "Epoch 300, Weights: [0.34101595 0.3126397  0.34634432]\n",
            "Epoch 350, Weights: [0.34233211 0.30913877 0.34852909]\n",
            "Epoch 400, Weights: [0.34365977 0.30561983 0.35072036]\n",
            "Epoch 450, Weights: [0.34499897 0.30208292 0.35291808]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33324826 0.33327415 0.33347756]\n",
            "Epoch 50, Weights: [0.32900356 0.33028389 0.34071252]\n",
            "Epoch 100, Weights: [0.32477573 0.32723066 0.34799359]\n",
            "Epoch 150, Weights: [0.32056498 0.32411344 0.35532155]\n",
            "Epoch 200, Weights: [0.31637154 0.32093124 0.3626972 ]\n",
            "Epoch 250, Weights: [0.31219563 0.317683   0.37012134]\n",
            "Epoch 300, Weights: [0.30803749 0.31436768 0.37759479]\n",
            "Epoch 350, Weights: [0.30389738 0.31098421 0.38511838]\n",
            "Epoch 400, Weights: [0.29977555 0.30753148 0.39269294]\n",
            "Epoch 450, Weights: [0.29567227 0.30400838 0.40031932]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333164 0.33328564 0.3333827 ]\n",
            "Epoch 50, Weights: [0.33325305 0.33088925 0.33585768]\n",
            "Epoch 100, Weights: [0.33318585 0.32846894 0.33834518]\n",
            "Epoch 150, Weights: [0.33313024 0.32602449 0.34084524]\n",
            "Epoch 200, Weights: [0.33308646 0.32355566 0.34335785]\n",
            "Epoch 250, Weights: [0.33305472 0.32106222 0.34588303]\n",
            "Epoch 300, Weights: [0.33303525 0.31854393 0.34842078]\n",
            "Epoch 350, Weights: [0.33302828 0.31600056 0.35097113]\n",
            "Epoch 400, Weights: [0.33303404 0.31343187 0.35353406]\n",
            "Epoch 450, Weights: [0.33305276 0.31083761 0.3561096 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335819 0.33332988 0.33331191]\n",
            "Epoch 50, Weights: [0.33459929 0.33315504 0.33224564]\n",
            "Epoch 100, Weights: [0.33583632 0.33297541 0.33118824]\n",
            "Epoch 150, Weights: [0.33706936 0.33279098 0.33013962]\n",
            "Epoch 200, Weights: [0.3382985  0.33260176 0.32909971]\n",
            "Epoch 250, Weights: [0.33952383 0.33240773 0.32806842]\n",
            "Epoch 300, Weights: [0.34074542 0.33220889 0.32704566]\n",
            "Epoch 350, Weights: [0.34196338 0.33200523 0.32603136]\n",
            "Epoch 400, Weights: [0.34317777 0.33179676 0.32502544]\n",
            "Epoch 450, Weights: [0.34438869 0.33158346 0.32402782]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329877 0.33337838 0.33332282]\n",
            "Epoch 50, Weights: [0.33156545 0.3356504  0.33278412]\n",
            "Epoch 100, Weights: [0.3298212  0.33796028 0.33221849]\n",
            "Epoch 150, Weights: [0.32806592 0.34030833 0.33162572]\n",
            "Epoch 200, Weights: [0.32629955 0.34269487 0.33100555]\n",
            "Epoch 250, Weights: [0.32452201 0.34512022 0.33035774]\n",
            "Epoch 300, Weights: [0.32273323 0.34758471 0.32968203]\n",
            "Epoch 350, Weights: [0.32093311 0.35008866 0.32897819]\n",
            "Epoch 400, Weights: [0.31912159 0.35263242 0.32824596]\n",
            "Epoch 450, Weights: [0.31729858 0.35521631 0.32748507]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328992 0.33333852 0.33337153]\n",
            "Epoch 50, Weights: [0.33111259 0.33359407 0.33529331]\n",
            "Epoch 100, Weights: [0.32892131 0.33384148 0.33723718]\n",
            "Epoch 150, Weights: [0.32671613 0.33408062 0.33920322]\n",
            "Epoch 200, Weights: [0.3244971  0.33431138 0.34119149]\n",
            "Epoch 250, Weights: [0.32226429 0.33453362 0.34320206]\n",
            "Epoch 300, Weights: [0.32001775 0.33474723 0.34523499]\n",
            "Epoch 350, Weights: [0.31775755 0.33495208 0.34729034]\n",
            "Epoch 400, Weights: [0.31548376 0.33514805 0.34936816]\n",
            "Epoch 450, Weights: [0.31319644 0.33533502 0.35146851]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334159 0.33332087 0.33333752]\n",
            "Epoch 50, Weights: [0.33375276 0.33269651 0.3335507 ]\n",
            "Epoch 100, Weights: [0.3341601  0.33206936 0.3337705 ]\n",
            "Epoch 150, Weights: [0.33456356 0.33143951 0.3339969 ]\n",
            "Epoch 200, Weights: [0.33496308 0.33080704 0.33422985]\n",
            "Epoch 250, Weights: [0.33535863 0.33017201 0.33446933]\n",
            "Epoch 300, Weights: [0.33575017 0.32953452 0.33471529]\n",
            "Epoch 350, Weights: [0.33613764 0.32889464 0.3349677 ]\n",
            "Epoch 400, Weights: [0.33652101 0.32825245 0.33522651]\n",
            "Epoch 450, Weights: [0.33690023 0.32760804 0.3354917 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33327795 0.3333569  0.33336512]\n",
            "Epoch 50, Weights: [0.33050199 0.33453845 0.33495953]\n",
            "Epoch 100, Weights: [0.3277118  0.33572491 0.33656327]\n",
            "Epoch 150, Weights: [0.32490747 0.33691623 0.33817627]\n",
            "Epoch 200, Weights: [0.32208911 0.33811237 0.33979849]\n",
            "Epoch 250, Weights: [0.31925683 0.33931329 0.34142986]\n",
            "Epoch 300, Weights: [0.31641072 0.34051894 0.34307031]\n",
            "Epoch 350, Weights: [0.3135509  0.34172928 0.34471979]\n",
            "Epoch 400, Weights: [0.31067748 0.34294426 0.34637823]\n",
            "Epoch 450, Weights: [0.30779059 0.34416383 0.34804556]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332957 0.33329351 0.33337689]\n",
            "Epoch 50, Weights: [0.3331495  0.33129109 0.33555938]\n",
            "Epoch 100, Weights: [0.33298432 0.32926552 0.33775013]\n",
            "Epoch 150, Weights: [0.33283418 0.32721647 0.33994932]\n",
            "Epoch 200, Weights: [0.33269922 0.3251436  0.34215715]\n",
            "Epoch 250, Weights: [0.3325796  0.32304656 0.3443738 ]\n",
            "Epoch 300, Weights: [0.33247548 0.320925   0.3465995 ]\n",
            "Epoch 350, Weights: [0.33238699 0.31877855 0.34883443]\n",
            "Epoch 400, Weights: [0.33231431 0.31660684 0.35107882]\n",
            "Epoch 450, Weights: [0.3322576  0.3144095  0.35333286]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332184 0.33327824 0.33339989]\n",
            "Epoch 50, Weights: [0.33275763 0.33050561 0.33673673]\n",
            "Epoch 100, Weights: [0.33221275 0.32769662 0.3400906 ]\n",
            "Epoch 150, Weights: [0.33168729 0.32485125 0.34346143]\n",
            "Epoch 200, Weights: [0.33118135 0.32196948 0.34684914]\n",
            "Epoch 250, Weights: [0.33069499 0.31905132 0.35025366]\n",
            "Epoch 300, Weights: [0.3302283  0.31609674 0.35367493]\n",
            "Epoch 350, Weights: [0.32978136 0.31310576 0.35711285]\n",
            "Epoch 400, Weights: [0.32935423 0.31007836 0.36056738]\n",
            "Epoch 450, Weights: [0.328947   0.30701454 0.36403843]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330286 0.33323076 0.33346635]\n",
            "Epoch 50, Weights: [0.33177609 0.32810537 0.34011851]\n",
            "Epoch 100, Weights: [0.33024262 0.3229856  0.34677174]\n",
            "Epoch 150, Weights: [0.32870264 0.31787231 0.35342502]\n",
            "Epoch 200, Weights: [0.32715635 0.3127663  0.36007733]\n",
            "Epoch 250, Weights: [0.32560393 0.30766841 0.36672763]\n",
            "Epoch 300, Weights: [0.3240456  0.30257948 0.3733749 ]\n",
            "Epoch 350, Weights: [0.32248155 0.29750031 0.38001811]\n",
            "Epoch 400, Weights: [0.320912   0.29243174 0.38665624]\n",
            "Epoch 450, Weights: [0.31933714 0.28737457 0.39328826]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335172 0.33332036 0.33332789]\n",
            "Epoch 50, Weights: [0.33427306 0.33266953 0.33305738]\n",
            "Epoch 100, Weights: [0.33519728 0.33201337 0.33278932]\n",
            "Epoch 150, Weights: [0.33612438 0.33135184 0.33252376]\n",
            "Epoch 200, Weights: [0.33705436 0.3306849  0.33226072]\n",
            "Epoch 250, Weights: [0.33798721 0.33001252 0.33200025]\n",
            "Epoch 300, Weights: [0.33892292 0.32933467 0.33174238]\n",
            "Epoch 350, Weights: [0.3398615  0.32865131 0.33148717]\n",
            "Epoch 400, Weights: [0.34080292 0.32796241 0.33123464]\n",
            "Epoch 450, Weights: [0.3417472  0.32726793 0.33098484]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333737  0.33329207 0.3333342 ]\n",
            "Epoch 50, Weights: [0.33539423 0.33122101 0.33338474]\n",
            "Epoch 100, Weights: [0.33741831 0.32913354 0.33344812]\n",
            "Epoch 150, Weights: [0.33944585 0.32702966 0.33352445]\n",
            "Epoch 200, Weights: [0.34147676 0.32490938 0.33361383]\n",
            "Epoch 250, Weights: [0.34351094 0.32277268 0.33371635]\n",
            "Epoch 300, Weights: [0.34554828 0.32061959 0.3338321 ]\n",
            "Epoch 350, Weights: [0.3475887  0.31845011 0.33396116]\n",
            "Epoch 400, Weights: [0.34963207 0.31626426 0.33410364]\n",
            "Epoch 450, Weights: [0.3516783  0.31406205 0.33425962]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328989 0.33334606 0.33336402]\n",
            "Epoch 50, Weights: [0.33111804 0.33398139 0.33490054]\n",
            "Epoch 100, Weights: [0.32894569 0.334614   0.33644028]\n",
            "Epoch 150, Weights: [0.32677293 0.33524385 0.33798319]\n",
            "Epoch 200, Weights: [0.32459983 0.33587091 0.33952923]\n",
            "Epoch 250, Weights: [0.32242649 0.33649514 0.34107835]\n",
            "Epoch 300, Weights: [0.32025298 0.33711649 0.34263049]\n",
            "Epoch 350, Weights: [0.3180794  0.33773495 0.34418562]\n",
            "Epoch 400, Weights: [0.31590583 0.33835046 0.34574369]\n",
            "Epoch 450, Weights: [0.31373235 0.33896299 0.34730464]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331036 0.3332923  0.33339731]\n",
            "Epoch 50, Weights: [0.33216359 0.33125137 0.33658501]\n",
            "Epoch 100, Weights: [0.33101984 0.32923038 0.33974975]\n",
            "Epoch 150, Weights: [0.32987913 0.3272293  0.34289154]\n",
            "Epoch 200, Weights: [0.3287415  0.32524811 0.34601036]\n",
            "Epoch 250, Weights: [0.32760696 0.3232868  0.34910621]\n",
            "Epoch 300, Weights: [0.32647553 0.32134533 0.3521791 ]\n",
            "Epoch 350, Weights: [0.32534725 0.31942368 0.35522903]\n",
            "Epoch 400, Weights: [0.32422214 0.31752183 0.358256  ]\n",
            "Epoch 450, Weights: [0.32310022 0.31563973 0.36126003]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33324399 0.33338995 0.33336602]\n",
            "Epoch 50, Weights: [0.32877543 0.3362187  0.33500584]\n",
            "Epoch 100, Weights: [0.3243028  0.3390421  0.33665507]\n",
            "Epoch 150, Weights: [0.31982616 0.34186007 0.33831374]\n",
            "Epoch 200, Weights: [0.31534557 0.34467254 0.33998186]\n",
            "Epoch 250, Weights: [0.31086106 0.34747944 0.34165947]\n",
            "Epoch 300, Weights: [0.30637267 0.35028071 0.34334658]\n",
            "Epoch 350, Weights: [0.30188045 0.35307628 0.34504323]\n",
            "Epoch 400, Weights: [0.29738443 0.35586609 0.34674945]\n",
            "Epoch 450, Weights: [0.29288463 0.35865009 0.34846526]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332009 0.33332399 0.33335589]\n",
            "Epoch 50, Weights: [0.33265606 0.33286118 0.33448273]\n",
            "Epoch 100, Weights: [0.33198782 0.3324058  0.33560635]\n",
            "Epoch 150, Weights: [0.33131536 0.33195793 0.33672667]\n",
            "Epoch 200, Weights: [0.33063868 0.33151766 0.33784363]\n",
            "Epoch 250, Weights: [0.32995776 0.33108508 0.33895713]\n",
            "Epoch 300, Weights: [0.3292726  0.33066027 0.3400671 ]\n",
            "Epoch 350, Weights: [0.32858319 0.33024331 0.34117347]\n",
            "Epoch 400, Weights: [0.32788953 0.32983428 0.34227615]\n",
            "Epoch 450, Weights: [0.3271916  0.32943328 0.34337508]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334409 0.33336153 0.33329436]\n",
            "Epoch 50, Weights: [0.33388668 0.33478023 0.33133306]\n",
            "Epoch 100, Weights: [0.33443805 0.33621569 0.32934623]\n",
            "Epoch 150, Weights: [0.33499835 0.33766817 0.32733345]\n",
            "Epoch 200, Weights: [0.33556774 0.33913792 0.32529431]\n",
            "Epoch 250, Weights: [0.3361464  0.34062518 0.3232284 ]\n",
            "Epoch 300, Weights: [0.33673449 0.34213021 0.32113526]\n",
            "Epoch 350, Weights: [0.3373322  0.34365329 0.31901448]\n",
            "Epoch 400, Weights: [0.33793971 0.34519468 0.31686558]\n",
            "Epoch 450, Weights: [0.33855719 0.34675465 0.31468812]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33339566 0.33333374 0.33327056]\n",
            "Epoch 50, Weights: [0.33651978 0.33335796 0.33012223]\n",
            "Epoch 100, Weights: [0.3396578  0.33338855 0.32695362]\n",
            "Epoch 150, Weights: [0.34280978 0.33342552 0.32376467]\n",
            "Epoch 200, Weights: [0.34597576 0.33346888 0.32055533]\n",
            "Epoch 250, Weights: [0.34915578 0.33351864 0.31732555]\n",
            "Epoch 300, Weights: [0.35234988 0.33357482 0.31407527]\n",
            "Epoch 350, Weights: [0.35555811 0.33363743 0.31080443]\n",
            "Epoch 400, Weights: [0.35878051 0.33370649 0.30751297]\n",
            "Epoch 450, Weights: [0.36201714 0.333782   0.30420084]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333288 0.33327517 0.33339193]\n",
            "Epoch 50, Weights: [0.33332486 0.33036428 0.33631083]\n",
            "Epoch 100, Weights: [0.33334487 0.32744748 0.33920762]\n",
            "Epoch 150, Weights: [0.33339292 0.32452488 0.34208216]\n",
            "Epoch 200, Weights: [0.33346906 0.32159662 0.34493429]\n",
            "Epoch 250, Weights: [0.3335733  0.31866281 0.34776386]\n",
            "Epoch 300, Weights: [0.33370566 0.31572357 0.35057075]\n",
            "Epoch 350, Weights: [0.33386614 0.31277901 0.35335482]\n",
            "Epoch 400, Weights: [0.33405477 0.30982924 0.35611595]\n",
            "Epoch 450, Weights: [0.33427154 0.30687438 0.35885405]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33342323 0.33328985 0.3332869 ]\n",
            "Epoch 50, Weights: [0.33791382 0.3311201  0.33096605]\n",
            "Epoch 100, Weights: [0.34239556 0.32895846 0.32864595]\n",
            "Epoch 150, Weights: [0.34686854 0.32680485 0.32632658]\n",
            "Epoch 200, Weights: [0.35133285 0.32465921 0.32400791]\n",
            "Epoch 250, Weights: [0.35578858 0.32252148 0.32168992]\n",
            "Epoch 300, Weights: [0.36023582 0.32039158 0.31937257]\n",
            "Epoch 350, Weights: [0.36467466 0.31826945 0.31705586]\n",
            "Epoch 400, Weights: [0.3691052  0.31615503 0.31473974]\n",
            "Epoch 450, Weights: [0.37352752 0.31404824 0.3124242 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330577 0.33337084 0.33332336]\n",
            "Epoch 50, Weights: [0.33193378 0.33524716 0.33281903]\n",
            "Epoch 100, Weights: [0.33057245 0.33712451 0.33230301]\n",
            "Epoch 150, Weights: [0.32922183 0.33900287 0.33177526]\n",
            "Epoch 200, Weights: [0.32788194 0.34088226 0.33123577]\n",
            "Epoch 250, Weights: [0.3265528  0.34276267 0.3306845 ]\n",
            "Epoch 300, Weights: [0.32523444 0.34464411 0.33012142]\n",
            "Epoch 350, Weights: [0.32392689 0.34652656 0.32954651]\n",
            "Epoch 400, Weights: [0.32263018 0.34841004 0.32895975]\n",
            "Epoch 450, Weights: [0.32134433 0.35029455 0.3283611 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33340348 0.33326764 0.33332885]\n",
            "Epoch 50, Weights: [0.33690463 0.32999755 0.33309779]\n",
            "Epoch 100, Weights: [0.3403932  0.32675469 0.33285209]\n",
            "Epoch 150, Weights: [0.34386935 0.32353897 0.33259165]\n",
            "Epoch 200, Weights: [0.34733325 0.32035029 0.33231643]\n",
            "Epoch 250, Weights: [0.35078509 0.31718856 0.33202633]\n",
            "Epoch 300, Weights: [0.35422502 0.31405367 0.33172128]\n",
            "Epoch 350, Weights: [0.35765323 0.31094554 0.3314012 ]\n",
            "Epoch 400, Weights: [0.36106989 0.30786405 0.33106602]\n",
            "Epoch 450, Weights: [0.36447519 0.30480912 0.33071566]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332439 0.33335011 0.33332546]\n",
            "Epoch 50, Weights: [0.33287765 0.33419035 0.33293197]\n",
            "Epoch 100, Weights: [0.33243069 0.33503194 0.33253734]\n",
            "Epoch 150, Weights: [0.33198351 0.33587489 0.33214158]\n",
            "Epoch 200, Weights: [0.3315361  0.3367192  0.33174467]\n",
            "Epoch 250, Weights: [0.33108846 0.3375649  0.33134661]\n",
            "Epoch 300, Weights: [0.3306406  0.338412   0.33094738]\n",
            "Epoch 350, Weights: [0.3301925  0.3392605  0.33054697]\n",
            "Epoch 400, Weights: [0.32974417 0.34011041 0.33014539]\n",
            "Epoch 450, Weights: [0.32929561 0.34096176 0.3297426 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328726 0.33339189 0.33332082]\n",
            "Epoch 50, Weights: [0.33097534 0.33633164 0.33269299]\n",
            "Epoch 100, Weights: [0.32864624 0.3392939  0.33205983]\n",
            "Epoch 150, Weights: [0.32629963 0.3422791  0.33142123]\n",
            "Epoch 200, Weights: [0.32393518 0.34528769 0.3307771 ]\n",
            "Epoch 250, Weights: [0.32155255 0.3483201  0.33012732]\n",
            "Epoch 300, Weights: [0.3191514  0.35137678 0.32947179]\n",
            "Epoch 350, Weights: [0.31673135 0.35445822 0.3288104 ]\n",
            "Epoch 400, Weights: [0.31429207 0.35756487 0.32814303]\n",
            "Epoch 450, Weights: [0.31183316 0.36069723 0.32746957]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335187 0.33331064 0.33333746]\n",
            "Epoch 50, Weights: [0.33428242 0.33217668 0.33354087]\n",
            "Epoch 100, Weights: [0.3352192  0.33104361 0.33373716]\n",
            "Epoch 150, Weights: [0.33616219 0.32991149 0.33392629]\n",
            "Epoch 200, Weights: [0.33711138 0.32878039 0.33410821]\n",
            "Epoch 250, Weights: [0.33806675 0.32765036 0.33428287]\n",
            "Epoch 300, Weights: [0.33902827 0.32652148 0.33445022]\n",
            "Epoch 350, Weights: [0.33999594 0.3253938  0.33461023]\n",
            "Epoch 400, Weights: [0.34096972 0.32426741 0.33476284]\n",
            "Epoch 450, Weights: [0.3419496  0.32314236 0.33490801]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335111 0.33335632 0.33329254]\n",
            "Epoch 50, Weights: [0.33423791 0.33450816 0.33125389]\n",
            "Epoch 100, Weights: [0.33511994 0.33566408 0.32921595]\n",
            "Epoch 150, Weights: [0.33599724 0.33682403 0.32717871]\n",
            "Epoch 200, Weights: [0.33686986 0.33798797 0.32514214]\n",
            "Epoch 250, Weights: [0.33773787 0.33915585 0.32310625]\n",
            "Epoch 300, Weights: [0.33860131 0.34032763 0.32107102]\n",
            "Epoch 350, Weights: [0.33946024 0.34150328 0.31903645]\n",
            "Epoch 400, Weights: [0.34031471 0.34268275 0.31700251]\n",
            "Epoch 450, Weights: [0.34116476 0.343866   0.3149692 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33342014 0.33327921 0.33330062]\n",
            "Epoch 50, Weights: [0.33776808 0.33056706 0.33166483]\n",
            "Epoch 100, Weights: [0.3421297  0.32784184 0.33002843]\n",
            "Epoch 150, Weights: [0.34650462 0.32510377 0.32839157]\n",
            "Epoch 200, Weights: [0.35089247 0.32235308 0.32675442]\n",
            "Epoch 250, Weights: [0.35529284 0.31958999 0.32511714]\n",
            "Epoch 300, Weights: [0.35970536 0.31681472 0.3234799 ]\n",
            "Epoch 350, Weights: [0.36412961 0.31402751 0.32184286]\n",
            "Epoch 400, Weights: [0.36856519 0.31122859 0.3202062 ]\n",
            "Epoch 450, Weights: [0.37301169 0.3084182  0.31857008]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337988 0.33322747 0.33339262]\n",
            "Epoch 50, Weights: [0.33571345 0.32794583 0.33634069]\n",
            "Epoch 100, Weights: [0.33805848 0.32268543 0.33925605]\n",
            "Epoch 150, Weights: [0.3404151  0.3174459  0.34213897]\n",
            "Epoch 200, Weights: [0.34278341 0.31222687 0.34498969]\n",
            "Epoch 250, Weights: [0.34516354 0.30702798 0.34780845]\n",
            "Epoch 300, Weights: [0.34755561 0.30184887 0.35059549]\n",
            "Epoch 350, Weights: [0.34995973 0.29668921 0.35335104]\n",
            "Epoch 400, Weights: [0.35237602 0.29154865 0.3560753 ]\n",
            "Epoch 450, Weights: [0.3548046  0.28642688 0.35876849]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337557 0.33332571 0.33329869]\n",
            "Epoch 50, Weights: [0.33550046 0.33294111 0.33155839]\n",
            "Epoch 100, Weights: [0.33765022 0.33254889 0.32980086]\n",
            "Epoch 150, Weights: [0.3398252  0.33214891 0.32802586]\n",
            "Epoch 200, Weights: [0.34202576 0.33174108 0.32623313]\n",
            "Epoch 250, Weights: [0.34425229 0.33132526 0.32442242]\n",
            "Epoch 300, Weights: [0.34650515 0.33090133 0.32259349]\n",
            "Epoch 350, Weights: [0.34878475 0.33046917 0.32074606]\n",
            "Epoch 400, Weights: [0.35109146 0.33002864 0.31887987]\n",
            "Epoch 450, Weights: [0.35342569 0.32957963 0.31699465]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336555 0.33329014 0.33334428]\n",
            "Epoch 50, Weights: [0.33498938 0.33111805 0.33389254]\n",
            "Epoch 100, Weights: [0.33663792 0.32892007 0.33444198]\n",
            "Epoch 150, Weights: [0.33831146 0.32669592 0.33499259]\n",
            "Epoch 200, Weights: [0.34001027 0.3244453  0.33554439]\n",
            "Epoch 250, Weights: [0.34173466 0.32216792 0.33609739]\n",
            "Epoch 300, Weights: [0.3434849  0.31986348 0.33665159]\n",
            "Epoch 350, Weights: [0.34526129 0.31753168 0.337207  ]\n",
            "Epoch 400, Weights: [0.34706412 0.31517222 0.33776363]\n",
            "Epoch 450, Weights: [0.34889369 0.31278479 0.33832149]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333469 0.33327674 0.33338854]\n",
            "Epoch 50, Weights: [0.33340983 0.33043622 0.33615392]\n",
            "Epoch 100, Weights: [0.33349867 0.32757342 0.33892788]\n",
            "Epoch 150, Weights: [0.33360129 0.32468862 0.34171006]\n",
            "Epoch 200, Weights: [0.33371777 0.32178207 0.34450013]\n",
            "Epoch 250, Weights: [0.33384818 0.31885407 0.34729772]\n",
            "Epoch 300, Weights: [0.33399261 0.31590489 0.35010247]\n",
            "Epoch 350, Weights: [0.33415112 0.31293484 0.35291401]\n",
            "Epoch 400, Weights: [0.33432378 0.30994424 0.35573196]\n",
            "Epoch 450, Weights: [0.33451064 0.30693339 0.35855594]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330589 0.33335771 0.33333637]\n",
            "Epoch 50, Weights: [0.33193025 0.33457744 0.33349228]\n",
            "Epoch 100, Weights: [0.33054698 0.33579815 0.33365484]\n",
            "Epoch 150, Weights: [0.32915601 0.33701987 0.33382409]\n",
            "Epoch 200, Weights: [0.32775725 0.33824264 0.33400008]\n",
            "Epoch 250, Weights: [0.3263506  0.33946649 0.33418287]\n",
            "Epoch 300, Weights: [0.32493599 0.34069147 0.33437251]\n",
            "Epoch 350, Weights: [0.32351332 0.3419176  0.33456905]\n",
            "Epoch 400, Weights: [0.3220825  0.34314493 0.33477254]\n",
            "Epoch 450, Weights: [0.32064344 0.34437349 0.33498304]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330808 0.33332315 0.33336874]\n",
            "Epoch 50, Weights: [0.33204009 0.33281801 0.33514187]\n",
            "Epoch 100, Weights: [0.33076069 0.33231955 0.33691973]\n",
            "Epoch 150, Weights: [0.32946992 0.3318278  0.33870225]\n",
            "Epoch 200, Weights: [0.3281678  0.33134278 0.34048939]\n",
            "Epoch 250, Weights: [0.32685439 0.3308645  0.34228108]\n",
            "Epoch 300, Weights: [0.32552971 0.330393   0.34407726]\n",
            "Epoch 350, Weights: [0.32419381 0.32992827 0.34587789]\n",
            "Epoch 400, Weights: [0.32284672 0.32947035 0.34768289]\n",
            "Epoch 450, Weights: [0.32148849 0.32901925 0.34949222]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33339422 0.33322105 0.3333847 ]\n",
            "Epoch 50, Weights: [0.33645428 0.32761067 0.33593502]\n",
            "Epoch 100, Weights: [0.33954493 0.32200672 0.33844832]\n",
            "Epoch 150, Weights: [0.34266653 0.31640926 0.34092418]\n",
            "Epoch 200, Weights: [0.3458194  0.31081838 0.34336219]\n",
            "Epoch 250, Weights: [0.34900389 0.30523419 0.34576189]\n",
            "Epoch 300, Weights: [0.35222032 0.29965683 0.34812282]\n",
            "Epoch 350, Weights: [0.35546901 0.29408646 0.35044451]\n",
            "Epoch 400, Weights: [0.35875027 0.28852325 0.35272645]\n",
            "Epoch 450, Weights: [0.36206439 0.28296742 0.35496815]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333093 0.33330035 0.33336869]\n",
            "Epoch 50, Weights: [0.33320998 0.33165215 0.33513784]\n",
            "Epoch 100, Weights: [0.33308623 0.33000496 0.33690878]\n",
            "Epoch 150, Weights: [0.3329597  0.32835887 0.3386814 ]\n",
            "Epoch 200, Weights: [0.33283038 0.32671399 0.3404556 ]\n",
            "Epoch 250, Weights: [0.3326983  0.32507041 0.34223126]\n",
            "Epoch 300, Weights: [0.33256345 0.32342823 0.3440083 ]\n",
            "Epoch 350, Weights: [0.33242584 0.32178754 0.34578659]\n",
            "Epoch 400, Weights: [0.33228549 0.32014845 0.34756603]\n",
            "Epoch 450, Weights: [0.33214239 0.31851106 0.34934652]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33321457 0.33333878 0.33344661]\n",
            "Epoch 50, Weights: [0.32727599 0.33360821 0.33911576]\n",
            "Epoch 100, Weights: [0.32133404 0.33387049 0.34479544]\n",
            "Epoch 150, Weights: [0.31538697 0.3341256  0.3504874 ]\n",
            "Epoch 200, Weights: [0.30943302 0.3343735  0.35619345]\n",
            "Epoch 250, Weights: [0.30347045 0.33461416 0.36191536]\n",
            "Epoch 300, Weights: [0.29749748 0.33484752 0.36765497]\n",
            "Epoch 350, Weights: [0.29151232 0.33507354 0.37341411]\n",
            "Epoch 400, Weights: [0.28551317 0.33529217 0.37919464]\n",
            "Epoch 450, Weights: [0.27949821 0.33550332 0.38499844]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33326222 0.33334351 0.33339424]\n",
            "Epoch 50, Weights: [0.32971762 0.33384197 0.33644038]\n",
            "Epoch 100, Weights: [0.32619416 0.33431869 0.33948713]\n",
            "Epoch 150, Weights: [0.32269173 0.33477379 0.34253445]\n",
            "Epoch 200, Weights: [0.31921025 0.33520739 0.34558233]\n",
            "Epoch 250, Weights: [0.3157496  0.33561961 0.34863076]\n",
            "Epoch 300, Weights: [0.31230968 0.33601056 0.35167972]\n",
            "Epoch 350, Weights: [0.30889038 0.33638037 0.35472922]\n",
            "Epoch 400, Weights: [0.30549158 0.33672914 0.35777925]\n",
            "Epoch 450, Weights: [0.30211315 0.337057   0.36082982]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33342155 0.33324198 0.33333644]\n",
            "Epoch 50, Weights: [0.33781585 0.32870591 0.33347822]\n",
            "Epoch 100, Weights: [0.34217736 0.3242301  0.33359251]\n",
            "Epoch 150, Weights: [0.34650676 0.31981344 0.33367977]\n",
            "Epoch 200, Weights: [0.35080474 0.31545482 0.33374041]\n",
            "Epoch 250, Weights: [0.35507197 0.31115316 0.33377484]\n",
            "Epoch 300, Weights: [0.3593091  0.3069074  0.33378347]\n",
            "Epoch 350, Weights: [0.3635168  0.30271647 0.3337667 ]\n",
            "Epoch 400, Weights: [0.3676957  0.29857937 0.3337249 ]\n",
            "Epoch 450, Weights: [0.37184645 0.29449506 0.33365846]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334843 0.33325522 0.33339631]\n",
            "Epoch 50, Weights: [0.33410336 0.3293406  0.33655601]\n",
            "Epoch 100, Weights: [0.33485699 0.32540682 0.33973616]\n",
            "Epoch 150, Weights: [0.33560936 0.32145364 0.34293697]\n",
            "Epoch 200, Weights: [0.33636048 0.31748081 0.34615867]\n",
            "Epoch 250, Weights: [0.33711037 0.31348809 0.34940151]\n",
            "Epoch 300, Weights: [0.33785906 0.3094752  0.35266571]\n",
            "Epoch 350, Weights: [0.33860656 0.30544191 0.3559515 ]\n",
            "Epoch 400, Weights: [0.33935288 0.30138795 0.35925914]\n",
            "Epoch 450, Weights: [0.34009806 0.29731305 0.36258886]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33327025 0.33330904 0.33342069]\n",
            "Epoch 50, Weights: [0.33009875 0.3321157  0.33778552]\n",
            "Epoch 100, Weights: [0.32689309 0.33096381 0.34214307]\n",
            "Epoch 150, Weights: [0.32365372 0.3298537  0.34649255]\n",
            "Epoch 200, Weights: [0.32038113 0.3287857  0.35083314]\n",
            "Epoch 250, Weights: [0.3170758  0.32776009 0.35516408]\n",
            "Epoch 300, Weights: [0.31373823 0.32677716 0.35948458]\n",
            "Epoch 350, Weights: [0.31036892 0.32583716 0.36379388]\n",
            "Epoch 400, Weights: [0.30696839 0.32494034 0.36809124]\n",
            "Epoch 450, Weights: [0.30353715 0.32408691 0.37237591]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33327146 0.33331027 0.33341824]\n",
            "Epoch 50, Weights: [0.33015993 0.33217405 0.33766599]\n",
            "Epoch 100, Weights: [0.32701247 0.33107062 0.34191688]\n",
            "Epoch 150, Weights: [0.32382894 0.33000052 0.34617052]\n",
            "Epoch 200, Weights: [0.3206092  0.32896429 0.35042648]\n",
            "Epoch 250, Weights: [0.31735314 0.32796247 0.35468435]\n",
            "Epoch 300, Weights: [0.31406062 0.3269956  0.35894375]\n",
            "Epoch 350, Weights: [0.31073148 0.32606421 0.36320428]\n",
            "Epoch 400, Weights: [0.3073656  0.32516884 0.36746553]\n",
            "Epoch 450, Weights: [0.30396281 0.32431002 0.37172714]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329841 0.33335314 0.33334842]\n",
            "Epoch 50, Weights: [0.33154421 0.33434469 0.33411107]\n",
            "Epoch 100, Weights: [0.32977306 0.33533731 0.3348896 ]\n",
            "Epoch 150, Weights: [0.32798479 0.33633103 0.33568415]\n",
            "Epoch 200, Weights: [0.32617926 0.33732585 0.33649485]\n",
            "Epoch 250, Weights: [0.32435632 0.33832179 0.33732186]\n",
            "Epoch 300, Weights: [0.32251581 0.33931886 0.3381653 ]\n",
            "Epoch 350, Weights: [0.32065758 0.34031706 0.33902533]\n",
            "Epoch 400, Weights: [0.31878147 0.34131641 0.33990209]\n",
            "Epoch 450, Weights: [0.31688732 0.34231692 0.34079572]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334475 0.33335623 0.333299  ]\n",
            "Epoch 50, Weights: [0.33391355 0.33450924 0.33157717]\n",
            "Epoch 100, Weights: [0.33447776 0.33567779 0.32984442]\n",
            "Epoch 150, Weights: [0.33503733 0.33686207 0.32810056]\n",
            "Epoch 200, Weights: [0.33559224 0.33806232 0.32634542]\n",
            "Epoch 250, Weights: [0.33614243 0.33927874 0.3245788 ]\n",
            "Epoch 300, Weights: [0.33668787 0.34051158 0.32280052]\n",
            "Epoch 350, Weights: [0.33722853 0.34176106 0.32101038]\n",
            "Epoch 400, Weights: [0.33776436 0.34302741 0.3192082 ]\n",
            "Epoch 450, Weights: [0.33829532 0.34431089 0.31739376]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333911 0.33331212 0.33334874]\n",
            "Epoch 50, Weights: [0.33363144 0.33224974 0.33411879]\n",
            "Epoch 100, Weights: [0.33392959 0.33118323 0.33488715]\n",
            "Epoch 150, Weights: [0.33423358 0.33011262 0.33565377]\n",
            "Epoch 200, Weights: [0.33454343 0.32903795 0.33641859]\n",
            "Epoch 250, Weights: [0.33485913 0.32795925 0.33718158]\n",
            "Epoch 300, Weights: [0.33518072 0.32687656 0.33794269]\n",
            "Epoch 350, Weights: [0.3355082  0.32578991 0.33870186]\n",
            "Epoch 400, Weights: [0.33584158 0.32469934 0.33945906]\n",
            "Epoch 450, Weights: [0.33618087 0.32360488 0.34021422]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331235 0.33325043 0.3334372 ]\n",
            "Epoch 50, Weights: [0.3322509  0.32908065 0.33866842]\n",
            "Epoch 100, Weights: [0.33116431 0.32486209 0.34397356]\n",
            "Epoch 150, Weights: [0.3300523  0.3205947  0.34935296]\n",
            "Epoch 200, Weights: [0.3289146  0.31627845 0.35480692]\n",
            "Epoch 250, Weights: [0.32775094 0.31191333 0.3603357 ]\n",
            "Epoch 300, Weights: [0.32656105 0.30749935 0.36593957]\n",
            "Epoch 350, Weights: [0.32534468 0.30303654 0.37161875]\n",
            "Epoch 400, Weights: [0.32410158 0.29852494 0.37737345]\n",
            "Epoch 450, Weights: [0.3228315  0.29396462 0.38320385]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33326211 0.33337671 0.33336114]\n",
            "Epoch 50, Weights: [0.32970431 0.33554495 0.33475071]\n",
            "Epoch 100, Weights: [0.32615169 0.33771088 0.3361374 ]\n",
            "Epoch 150, Weights: [0.32260405 0.33987464 0.33752128]\n",
            "Epoch 200, Weights: [0.31906118 0.34203634 0.33890245]\n",
            "Epoch 250, Weights: [0.31552289 0.34419611 0.34028098]\n",
            "Epoch 300, Weights: [0.31198895 0.34635407 0.34165695]\n",
            "Epoch 350, Weights: [0.30845918 0.34851035 0.34303044]\n",
            "Epoch 400, Weights: [0.30493337 0.35066507 0.34440153]\n",
            "Epoch 450, Weights: [0.30141131 0.35281835 0.34577031]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330095 0.33332615 0.33337287]\n",
            "Epoch 50, Weights: [0.33168971 0.33296408 0.33534617]\n",
            "Epoch 100, Weights: [0.33009297 0.33259552 0.33731149]\n",
            "Epoch 150, Weights: [0.32851072 0.3322205  0.33926875]\n",
            "Epoch 200, Weights: [0.326943   0.33183907 0.3412179 ]\n",
            "Epoch 250, Weights: [0.3253898  0.3314513  0.34315887]\n",
            "Epoch 300, Weights: [0.32385114 0.33105723 0.34509161]\n",
            "Epoch 350, Weights: [0.32232701 0.33065691 0.34701606]\n",
            "Epoch 400, Weights: [0.32081742 0.33025039 0.34893216]\n",
            "Epoch 450, Weights: [0.31932236 0.32983774 0.35083987]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33326217 0.33331376 0.33342404]\n",
            "Epoch 50, Weights: [0.32970797 0.33233876 0.33795324]\n",
            "Epoch 100, Weights: [0.32616069 0.33137003 0.34246925]\n",
            "Epoch 150, Weights: [0.32262038 0.3304076  0.346972  ]\n",
            "Epoch 200, Weights: [0.31908705 0.32945146 0.35146145]\n",
            "Epoch 250, Weights: [0.31556076 0.32850164 0.35593757]\n",
            "Epoch 300, Weights: [0.31204152 0.32755814 0.3604003 ]\n",
            "Epoch 350, Weights: [0.30852938 0.32662098 0.36484961]\n",
            "Epoch 400, Weights: [0.30502437 0.32569015 0.36928545]\n",
            "Epoch 450, Weights: [0.30152651 0.32476568 0.37370778]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332119 0.33340118 0.33327761]\n",
            "Epoch 50, Weights: [0.33271378 0.33678977 0.33049642]\n",
            "Epoch 100, Weights: [0.33210516 0.34017088 0.32772393]\n",
            "Epoch 150, Weights: [0.33149516 0.34354531 0.32495949]\n",
            "Epoch 200, Weights: [0.33088366 0.34691386 0.32220245]\n",
            "Epoch 250, Weights: [0.33027048 0.35027732 0.31945217]\n",
            "Epoch 300, Weights: [0.32965549 0.35363648 0.31670801]\n",
            "Epoch 350, Weights: [0.32903853 0.35699211 0.31396933]\n",
            "Epoch 400, Weights: [0.32841944 0.360345   0.31123553]\n",
            "Epoch 450, Weights: [0.32779809 0.36369592 0.30850597]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332551 0.33333267 0.33334179]\n",
            "Epoch 50, Weights: [0.33293504 0.3332993  0.33376562]\n",
            "Epoch 100, Weights: [0.33254528 0.33326441 0.33419028]\n",
            "Epoch 150, Weights: [0.33215622 0.33322798 0.33461578]\n",
            "Epoch 200, Weights: [0.33176788 0.33318999 0.3350421 ]\n",
            "Epoch 250, Weights: [0.33138029 0.33315042 0.33546926]\n",
            "Epoch 300, Weights: [0.33099346 0.33310925 0.33589726]\n",
            "Epoch 350, Weights: [0.33060741 0.33306647 0.33632609]\n",
            "Epoch 400, Weights: [0.33022215 0.33302206 0.33675576]\n",
            "Epoch 450, Weights: [0.32983771 0.33297599 0.33718627]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333412  0.33331721 0.33334155]\n",
            "Epoch 50, Weights: [0.33373609 0.33251092 0.33375296]\n",
            "Epoch 100, Weights: [0.33413283 0.33170311 0.33416403]\n",
            "Epoch 150, Weights: [0.33453143 0.33089378 0.33457477]\n",
            "Epoch 200, Weights: [0.33493189 0.33008291 0.33498516]\n",
            "Epoch 250, Weights: [0.33533423 0.32927052 0.33539522]\n",
            "Epoch 300, Weights: [0.33573846 0.32845658 0.33580493]\n",
            "Epoch 350, Weights: [0.33614458 0.32764109 0.3362143 ]\n",
            "Epoch 400, Weights: [0.3365526  0.32682405 0.33662332]\n",
            "Epoch 450, Weights: [0.33696253 0.32600545 0.33703199]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335784 0.33329957 0.33334257]\n",
            "Epoch 50, Weights: [0.33458432 0.33160239 0.33381326]\n",
            "Epoch 100, Weights: [0.33581213 0.32988685 0.33430099]\n",
            "Epoch 150, Weights: [0.33704123 0.32815273 0.33480602]\n",
            "Epoch 200, Weights: [0.33827157 0.32639979 0.33532861]\n",
            "Epoch 250, Weights: [0.3395031  0.32462781 0.33586906]\n",
            "Epoch 300, Weights: [0.34073579 0.32283656 0.33642762]\n",
            "Epoch 350, Weights: [0.34196959 0.32102581 0.33700457]\n",
            "Epoch 400, Weights: [0.34320445 0.31919532 0.33760021]\n",
            "Epoch 450, Weights: [0.34444031 0.31734486 0.3382148 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337317 0.33329774 0.33332905]\n",
            "Epoch 50, Weights: [0.33537915 0.33151389 0.33310693]\n",
            "Epoch 100, Weights: [0.33741167 0.32972034 0.33286796]\n",
            "Epoch 150, Weights: [0.3394711  0.327917   0.33261188]\n",
            "Epoch 200, Weights: [0.34155778 0.32610378 0.33233841]\n",
            "Epoch 250, Weights: [0.34367207 0.32428062 0.33204729]\n",
            "Epoch 300, Weights: [0.34581433 0.3224474  0.33173823]\n",
            "Epoch 350, Weights: [0.34798494 0.32060406 0.33141097]\n",
            "Epoch 400, Weights: [0.35018425 0.3187505  0.33106522]\n",
            "Epoch 450, Weights: [0.35241264 0.31688663 0.33070069]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33326324 0.33334847 0.33338826]\n",
            "Epoch 50, Weights: [0.32976937 0.33410292 0.33612768]\n",
            "Epoch 100, Weights: [0.32629549 0.33485181 0.33885267]\n",
            "Epoch 150, Weights: [0.32284174 0.33559513 0.34156311]\n",
            "Epoch 200, Weights: [0.31940822 0.33633285 0.3442589 ]\n",
            "Epoch 250, Weights: [0.31599505 0.33706498 0.34693994]\n",
            "Epoch 300, Weights: [0.31260233 0.3377915  0.34960614]\n",
            "Epoch 350, Weights: [0.30923017 0.3385124  0.3522574 ]\n",
            "Epoch 400, Weights: [0.30587865 0.33922768 0.35489364]\n",
            "Epoch 450, Weights: [0.30254788 0.33993733 0.35751476]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3332823  0.33326961 0.33344806]\n",
            "Epoch 50, Weights: [0.33070425 0.33007027 0.33922546]\n",
            "Epoch 100, Weights: [0.32807268 0.32684428 0.34508302]\n",
            "Epoch 150, Weights: [0.32538647 0.32359146 0.35102204]\n",
            "Epoch 200, Weights: [0.32264445 0.32031167 0.35704385]\n",
            "Epoch 250, Weights: [0.31984544 0.31700472 0.36314981]\n",
            "Epoch 300, Weights: [0.31698822 0.31367043 0.36934132]\n",
            "Epoch 350, Weights: [0.31407154 0.31030863 0.3756198 ]\n",
            "Epoch 400, Weights: [0.31109411 0.30691913 0.38198673]\n",
            "Epoch 450, Weights: [0.30805462 0.30350173 0.38844362]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333507  0.33332213 0.33332714]\n",
            "Epoch 50, Weights: [0.33422151 0.3327593  0.33301916]\n",
            "Epoch 100, Weights: [0.33509608 0.33218996 0.33271393]\n",
            "Epoch 150, Weights: [0.3359744  0.3316141  0.33241147]\n",
            "Epoch 200, Weights: [0.33685649 0.33103167 0.33211181]\n",
            "Epoch 250, Weights: [0.33774234 0.33044266 0.33181497]\n",
            "Epoch 300, Weights: [0.33863195 0.32984704 0.33152099]\n",
            "Epoch 350, Weights: [0.33952532 0.32924477 0.33122988]\n",
            "Epoch 400, Weights: [0.34042246 0.32863582 0.33094168]\n",
            "Epoch 450, Weights: [0.34132337 0.32802018 0.33065642]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329983 0.33327306 0.33342708]\n",
            "Epoch 50, Weights: [0.33160033 0.33025782 0.33814182]\n",
            "Epoch 100, Weights: [0.32985176 0.32723865 0.34290956]\n",
            "Epoch 150, Weights: [0.32805324 0.32421565 0.34773108]\n",
            "Epoch 200, Weights: [0.3262039  0.32118891 0.35260716]\n",
            "Epoch 250, Weights: [0.32430285 0.31815851 0.35753861]\n",
            "Epoch 300, Weights: [0.32234916 0.31512455 0.36252626]\n",
            "Epoch 350, Weights: [0.32034191 0.31208712 0.36757094]\n",
            "Epoch 400, Weights: [0.31828016 0.3090463  0.37267351]\n",
            "Epoch 450, Weights: [0.31616292 0.3060022  0.37783485]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328757 0.33332924 0.33338315]\n",
            "Epoch 50, Weights: [0.33099067 0.33312477 0.33588453]\n",
            "Epoch 100, Weights: [0.32867524 0.33291941 0.33840532]\n",
            "Epoch 150, Weights: [0.32634122 0.33271316 0.3409456 ]\n",
            "Epoch 200, Weights: [0.32398854 0.33250602 0.34350541]\n",
            "Epoch 250, Weights: [0.32161715 0.332298   0.34608482]\n",
            "Epoch 300, Weights: [0.31922699 0.3320891  0.34868388]\n",
            "Epoch 350, Weights: [0.31681799 0.33187932 0.35130265]\n",
            "Epoch 400, Weights: [0.31439011 0.33166868 0.35394118]\n",
            "Epoch 450, Weights: [0.31194329 0.33145716 0.35659952]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33339915 0.33327973 0.33332109]\n",
            "Epoch 50, Weights: [0.33673293 0.33057755 0.33268949]\n",
            "Epoch 100, Weights: [0.34015112 0.327831   0.33201785]\n",
            "Epoch 150, Weights: [0.34365551 0.32503931 0.33130515]\n",
            "Epoch 200, Weights: [0.34724794 0.32220169 0.33055034]\n",
            "Epoch 250, Weights: [0.35093027 0.31931734 0.32975237]\n",
            "Epoch 300, Weights: [0.35470438 0.31638545 0.32891014]\n",
            "Epoch 350, Weights: [0.35857221 0.3134052  0.32802256]\n",
            "Epoch 400, Weights: [0.3625357  0.31037577 0.3270885 ]\n",
            "Epoch 450, Weights: [0.36659684 0.30729629 0.32610684]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33327619 0.33333257 0.33339121]\n",
            "Epoch 50, Weights: [0.33041519 0.33329588 0.3362889 ]\n",
            "Epoch 100, Weights: [0.32754579 0.33326127 0.33919291]\n",
            "Epoch 150, Weights: [0.32466767 0.33322877 0.34210353]\n",
            "Epoch 200, Weights: [0.32178052 0.33319838 0.34502106]\n",
            "Epoch 250, Weights: [0.31888404 0.33317012 0.34794581]\n",
            "Epoch 300, Weights: [0.31597791 0.33314399 0.35087807]\n",
            "Epoch 350, Weights: [0.3130618  0.33312002 0.35381816]\n",
            "Epoch 400, Weights: [0.31013539 0.3330982  0.35676638]\n",
            "Epoch 450, Weights: [0.30719835 0.33307856 0.35972305]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332094 0.33326945 0.33340959]\n",
            "Epoch 50, Weights: [0.33271134 0.33007202 0.33721661]\n",
            "Epoch 100, Weights: [0.33212089 0.32686731 0.34101177]\n",
            "Epoch 150, Weights: [0.33154945 0.32365533 0.34479518]\n",
            "Epoch 200, Weights: [0.33099692 0.32043612 0.34856693]\n",
            "Epoch 250, Weights: [0.33046315 0.31720969 0.35232712]\n",
            "Epoch 300, Weights: [0.32994805 0.31397607 0.35607585]\n",
            "Epoch 350, Weights: [0.3294515  0.31073526 0.35981321]\n",
            "Epoch 400, Weights: [0.32897337 0.3074873  0.3635393 ]\n",
            "Epoch 450, Weights: [0.32851356 0.30423219 0.36725423]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335328 0.33324007 0.33340662]\n",
            "Epoch 50, Weights: [0.33438077 0.32855961 0.33705958]\n",
            "Epoch 100, Weights: [0.33546674 0.32384393 0.34068929]\n",
            "Epoch 150, Weights: [0.33661215 0.31909203 0.34429579]\n",
            "Epoch 200, Weights: [0.33781799 0.31430288 0.3478791 ]\n",
            "Epoch 250, Weights: [0.33908528 0.30947544 0.35143925]\n",
            "Epoch 300, Weights: [0.34041507 0.30460867 0.35497624]\n",
            "Epoch 350, Weights: [0.34180844 0.29970145 0.35849008]\n",
            "Epoch 400, Weights: [0.34326653 0.29475269 0.36198076]\n",
            "Epoch 450, Weights: [0.34479047 0.28976123 0.36544827]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330808 0.33331276 0.33337913]\n",
            "Epoch 50, Weights: [0.33205257 0.33229511 0.33565229]\n",
            "Epoch 100, Weights: [0.33081007 0.33129819 0.33789171]\n",
            "Epoch 150, Weights: [0.3295804  0.3303216  0.34009797]\n",
            "Epoch 200, Weights: [0.32836335 0.32936497 0.34227165]\n",
            "Epoch 250, Weights: [0.32715873 0.32842793 0.3444133 ]\n",
            "Epoch 300, Weights: [0.32596637 0.32751011 0.34652348]\n",
            "Epoch 350, Weights: [0.32478608 0.32661116 0.34860273]\n",
            "Epoch 400, Weights: [0.32361767 0.32573073 0.35065156]\n",
            "Epoch 450, Weights: [0.32246099 0.32486848 0.3526705 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330911 0.3333247  0.33336615]\n",
            "Epoch 50, Weights: [0.33210386 0.3328911  0.335005  ]\n",
            "Epoch 100, Weights: [0.3309092  0.33245228 0.33663848]\n",
            "Epoch 150, Weights: [0.3297251  0.3320083  0.33826657]\n",
            "Epoch 200, Weights: [0.32855152 0.33155923 0.33988922]\n",
            "Epoch 250, Weights: [0.32738845 0.33110511 0.34150641]\n",
            "Epoch 300, Weights: [0.32623585 0.33064601 0.34311811]\n",
            "Epoch 350, Weights: [0.32509369 0.33018199 0.34472428]\n",
            "Epoch 400, Weights: [0.32396196 0.32971312 0.34632489]\n",
            "Epoch 450, Weights: [0.32284062 0.32923945 0.3479199 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337701 0.33325976 0.3333632 ]\n",
            "Epoch 50, Weights: [0.33558157 0.32955648 0.33486192]\n",
            "Epoch 100, Weights: [0.33782599 0.32580416 0.33636982]\n",
            "Epoch 150, Weights: [0.34011056 0.32200281 0.3378866 ]\n",
            "Epoch 200, Weights: [0.34243556 0.31815245 0.33941196]\n",
            "Epoch 250, Weights: [0.34480124 0.31425316 0.34094557]\n",
            "Epoch 300, Weights: [0.34720785 0.31030503 0.3424871 ]\n",
            "Epoch 350, Weights: [0.34965561 0.30630816 0.3440362 ]\n",
            "Epoch 400, Weights: [0.35214474 0.30226271 0.34559252]\n",
            "Epoch 450, Weights: [0.35467542 0.29816887 0.34715568]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.333314   0.33329364 0.33339233]\n",
            "Epoch 50, Weights: [0.33235434 0.3313055  0.33634013]\n",
            "Epoch 100, Weights: [0.3314073  0.32930932 0.33928335]\n",
            "Epoch 150, Weights: [0.33047279 0.32730525 0.34222193]\n",
            "Epoch 200, Weights: [0.32955071 0.32529343 0.34515583]\n",
            "Epoch 250, Weights: [0.32864098 0.32327401 0.34808498]\n",
            "Epoch 300, Weights: [0.3277435  0.32124713 0.35100934]\n",
            "Epoch 350, Weights: [0.32685819 0.31921294 0.35392884]\n",
            "Epoch 400, Weights: [0.32598496 0.31717159 0.35684343]\n",
            "Epoch 450, Weights: [0.3251237  0.31512322 0.35975305]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330425 0.33328681 0.33340891]\n",
            "Epoch 50, Weights: [0.33186281 0.33096718 0.33716997]\n",
            "Epoch 100, Weights: [0.33044534 0.3286592  0.34089543]\n",
            "Epoch 150, Weights: [0.32905165 0.32636289 0.34458544]\n",
            "Epoch 200, Weights: [0.32768155 0.32407826 0.34824016]\n",
            "Epoch 250, Weights: [0.32633487 0.32180535 0.35185976]\n",
            "Epoch 300, Weights: [0.32501141 0.31954417 0.35544439]\n",
            "Epoch 350, Weights: [0.32371101 0.31729474 0.35899422]\n",
            "Epoch 400, Weights: [0.32243348 0.3150571  0.3625094 ]\n",
            "Epoch 450, Weights: [0.32117863 0.31283125 0.36599009]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33327284 0.33331193 0.3334152 ]\n",
            "Epoch 50, Weights: [0.33023496 0.33226341 0.3375016 ]\n",
            "Epoch 100, Weights: [0.32716978 0.33125667 0.34157352]\n",
            "Epoch 150, Weights: [0.32407695 0.33029236 0.34563065]\n",
            "Epoch 200, Weights: [0.32095615 0.32937114 0.34967269]\n",
            "Epoch 250, Weights: [0.317807   0.32849364 0.35369933]\n",
            "Epoch 300, Weights: [0.31462915 0.32766052 0.3577103 ]\n",
            "Epoch 350, Weights: [0.31142223 0.32687244 0.3617053 ]\n",
            "Epoch 400, Weights: [0.30818589 0.32613003 0.36568405]\n",
            "Epoch 450, Weights: [0.30491973 0.32543396 0.36964628]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335318 0.3332611  0.33338569]\n",
            "Epoch 50, Weights: [0.33435355 0.32963035 0.33601607]\n",
            "Epoch 100, Weights: [0.33536877 0.32596156 0.33866965]\n",
            "Epoch 150, Weights: [0.33639886 0.32225477 0.34134635]\n",
            "Epoch 200, Weights: [0.33744385 0.31851005 0.34404607]\n",
            "Epoch 250, Weights: [0.33850375 0.3147275  0.34676871]\n",
            "Epoch 300, Weights: [0.33957859 0.31090721 0.34951417]\n",
            "Epoch 350, Weights: [0.34066838 0.30704929 0.3522823 ]\n",
            "Epoch 400, Weights: [0.34177311 0.30315387 0.35507299]\n",
            "Epoch 450, Weights: [0.34289279 0.29922109 0.35788609]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332351 0.33333489 0.33334157]\n",
            "Epoch 50, Weights: [0.33283024 0.33341532 0.33375441]\n",
            "Epoch 100, Weights: [0.33233176 0.33349979 0.33416842]\n",
            "Epoch 150, Weights: [0.33182801 0.33358839 0.33458357]\n",
            "Epoch 200, Weights: [0.33131895 0.33368119 0.33499983]\n",
            "Epoch 250, Weights: [0.33080453 0.33377827 0.33541718]\n",
            "Epoch 300, Weights: [0.3302847  0.33387969 0.33583558]\n",
            "Epoch 350, Weights: [0.32975942 0.33398555 0.336255  ]\n",
            "Epoch 400, Weights: [0.32922864 0.33409591 0.33667542]\n",
            "Epoch 450, Weights: [0.32869232 0.33421086 0.33709679]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333875 0.33331227 0.33334896]\n",
            "Epoch 50, Weights: [0.33361306 0.33225127 0.33413564]\n",
            "Epoch 100, Weights: [0.33389376 0.33117423 0.33493198]\n",
            "Epoch 150, Weights: [0.33418096 0.33008092 0.33573809]\n",
            "Epoch 200, Weights: [0.33447479 0.3289711  0.33655408]\n",
            "Epoch 250, Weights: [0.33477537 0.32784454 0.33738006]\n",
            "Epoch 300, Weights: [0.33508282 0.32670101 0.33821614]\n",
            "Epoch 350, Weights: [0.33539728 0.32554027 0.33906242]\n",
            "Epoch 400, Weights: [0.33571887 0.32436207 0.33991903]\n",
            "Epoch 450, Weights: [0.33604774 0.32316617 0.34078606]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33319314 0.33337963 0.33342721]\n",
            "Epoch 50, Weights: [0.32619603 0.33572517 0.33807876]\n",
            "Epoch 100, Weights: [0.31922287 0.33812992 0.34264718]\n",
            "Epoch 150, Weights: [0.31227389 0.34059311 0.34713297]\n",
            "Epoch 200, Weights: [0.30534928 0.34311401 0.35153668]\n",
            "Epoch 250, Weights: [0.29844923 0.34569188 0.35585887]\n",
            "Epoch 300, Weights: [0.29157389 0.34832598 0.3601001 ]\n",
            "Epoch 350, Weights: [0.28472342 0.35101559 0.36426096]\n",
            "Epoch 400, Weights: [0.27789793 0.35376    0.36834204]\n",
            "Epoch 450, Weights: [0.27109753 0.3565585  0.37234394]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330608 0.33330331 0.33339058]\n",
            "Epoch 50, Weights: [0.33193921 0.33180089 0.33625987]\n",
            "Epoch 100, Weights: [0.33056262 0.33029498 0.33914237]\n",
            "Epoch 150, Weights: [0.32917625 0.32878555 0.34203817]\n",
            "Epoch 200, Weights: [0.32777999 0.32727257 0.34494741]\n",
            "Epoch 250, Weights: [0.32637376 0.32575603 0.34787018]\n",
            "Epoch 300, Weights: [0.32495747 0.32423589 0.35080661]\n",
            "Epoch 350, Weights: [0.32353102 0.32271213 0.35375682]\n",
            "Epoch 400, Weights: [0.32209434 0.32118472 0.35672091]\n",
            "Epoch 450, Weights: [0.32064732 0.31965364 0.35969901]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333255  0.33330011 0.33337437]\n",
            "Epoch 50, Weights: [0.3329432  0.33164375 0.33541302]\n",
            "Epoch 100, Weights: [0.33257851 0.32999618 0.33742528]\n",
            "Epoch 150, Weights: [0.33223113 0.32835759 0.33941125]\n",
            "Epoch 200, Weights: [0.33190075 0.32672819 0.34137103]\n",
            "Epoch 250, Weights: [0.33158705 0.32510818 0.34330474]\n",
            "Epoch 300, Weights: [0.33128975 0.32349773 0.34521249]\n",
            "Epoch 350, Weights: [0.33100853 0.32189703 0.34709441]\n",
            "Epoch 400, Weights: [0.33074309 0.32030627 0.34895062]\n",
            "Epoch 450, Weights: [0.33049313 0.31872559 0.35078125]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33340511 0.3332387  0.33335616]\n",
            "Epoch 50, Weights: [0.33702252 0.3285076  0.33446985]\n",
            "Epoch 100, Weights: [0.34069533 0.32377694 0.33552771]\n",
            "Epoch 150, Weights: [0.3444237  0.31904729 0.33652898]\n",
            "Epoch 200, Weights: [0.34820781 0.31431926 0.3374729 ]\n",
            "Epoch 250, Weights: [0.35204783 0.30959343 0.33835871]\n",
            "Epoch 300, Weights: [0.3559439  0.3048704  0.33918567]\n",
            "Epoch 350, Weights: [0.35989616 0.30015077 0.33995304]\n",
            "Epoch 400, Weights: [0.36390476 0.29543513 0.34066008]\n",
            "Epoch 450, Weights: [0.36796981 0.2907241  0.34130606]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328802 0.33333314 0.33337881]\n",
            "Epoch 50, Weights: [0.33102258 0.33332325 0.33565414]\n",
            "Epoch 100, Weights: [0.32875598 0.33331243 0.33793155]\n",
            "Epoch 150, Weights: [0.32648798 0.33330068 0.34021131]\n",
            "Epoch 200, Weights: [0.32421833 0.33328796 0.34249368]\n",
            "Epoch 250, Weights: [0.32194678 0.33327427 0.34477891]\n",
            "Epoch 300, Weights: [0.31967308 0.33325961 0.34706729]\n",
            "Epoch 350, Weights: [0.31739697 0.33324394 0.34935906]\n",
            "Epoch 400, Weights: [0.3151182  0.33322727 0.3516545 ]\n",
            "Epoch 450, Weights: [0.31283653 0.33320957 0.35395387]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333893 0.33333101 0.33333003]\n",
            "Epoch 50, Weights: [0.33362062 0.33321542 0.33316392]\n",
            "Epoch 100, Weights: [0.33390486 0.33309991 0.3329952 ]\n",
            "Epoch 150, Weights: [0.33419168 0.33298446 0.33282383]\n",
            "Epoch 200, Weights: [0.3344811  0.33286908 0.33264979]\n",
            "Epoch 250, Weights: [0.33477315 0.33275375 0.33247307]\n",
            "Epoch 300, Weights: [0.33506786 0.33263847 0.33229363]\n",
            "Epoch 350, Weights: [0.33536527 0.33252324 0.33211146]\n",
            "Epoch 400, Weights: [0.33566539 0.33240805 0.33192653]\n",
            "Epoch 450, Weights: [0.33596827 0.33229289 0.33173881]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336007 0.33335906 0.33328084]\n",
            "Epoch 50, Weights: [0.33471075 0.33465774 0.33063148]\n",
            "Epoch 100, Weights: [0.33608801 0.33598004 0.32793192]\n",
            "Epoch 150, Weights: [0.33749251 0.33732654 0.32518092]\n",
            "Epoch 200, Weights: [0.33892495 0.33869784 0.32237717]\n",
            "Epoch 250, Weights: [0.34038606 0.34009455 0.31951935]\n",
            "Epoch 300, Weights: [0.34187658 0.34151731 0.31660608]\n",
            "Epoch 350, Weights: [0.34339728 0.34296676 0.31363593]\n",
            "Epoch 400, Weights: [0.34494896 0.34444358 0.31060743]\n",
            "Epoch 450, Weights: [0.34653243 0.34594848 0.30751906]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328709 0.33341111 0.33330177]\n",
            "Epoch 50, Weights: [0.33096586 0.33728025 0.33175386]\n",
            "Epoch 100, Weights: [0.32862639 0.34111042 0.33026316]\n",
            "Epoch 150, Weights: [0.32626939 0.34490193 0.32882865]\n",
            "Epoch 200, Weights: [0.32389559 0.34865511 0.32744928]\n",
            "Epoch 250, Weights: [0.32150567 0.35237026 0.32612404]\n",
            "Epoch 300, Weights: [0.31910033 0.35604772 0.32485192]\n",
            "Epoch 350, Weights: [0.31668027 0.35968779 0.32363191]\n",
            "Epoch 400, Weights: [0.31424616 0.36329077 0.32246304]\n",
            "Epoch 450, Weights: [0.31179868 0.36685697 0.32134432]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33339574 0.33333141 0.33327282]\n",
            "Epoch 50, Weights: [0.3365344  0.33323078 0.33023479]\n",
            "Epoch 100, Weights: [0.33970846 0.33312023 0.32717128]\n",
            "Epoch 150, Weights: [0.34291833 0.33299961 0.32408204]\n",
            "Epoch 200, Weights: [0.3461644  0.33286876 0.32096681]\n",
            "Epoch 250, Weights: [0.34944709 0.33272755 0.31782534]\n",
            "Epoch 300, Weights: [0.35276681 0.3325758  0.31465736]\n",
            "Epoch 350, Weights: [0.35612398 0.33241338 0.31146262]\n",
            "Epoch 400, Weights: [0.35951901 0.33224011 0.30824085]\n",
            "Epoch 450, Weights: [0.36295233 0.33205584 0.3049918 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337808 0.33320596 0.33341593]\n",
            "Epoch 50, Weights: [0.33562711 0.326825   0.33754786]\n",
            "Epoch 100, Weights: [0.33789795 0.32041955 0.34168247]\n",
            "Epoch 150, Weights: [0.34019051 0.3139901  0.34581936]\n",
            "Epoch 200, Weights: [0.34250466 0.30753717 0.34995814]\n",
            "Epoch 250, Weights: [0.34484028 0.30106127 0.35409841]\n",
            "Epoch 300, Weights: [0.34719726 0.29456293 0.35823978]\n",
            "Epoch 350, Weights: [0.34957544 0.28804269 0.36238184]\n",
            "Epoch 400, Weights: [0.3519747  0.28150108 0.3665242 ]\n",
            "Epoch 450, Weights: [0.35439488 0.27493864 0.37066644]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333333  0.33330431 0.33336236]\n",
            "Epoch 50, Weights: [0.33333082 0.33185155 0.3348176 ]\n",
            "Epoch 100, Weights: [0.33332579 0.330395   0.33627918]\n",
            "Epoch 150, Weights: [0.3333182  0.32893473 0.33774704]\n",
            "Epoch 200, Weights: [0.33330804 0.32747079 0.33922114]\n",
            "Epoch 250, Weights: [0.3332953  0.32600323 0.34070144]\n",
            "Epoch 300, Weights: [0.33327998 0.32453211 0.34218788]\n",
            "Epoch 350, Weights: [0.33326207 0.32305748 0.34368042]\n",
            "Epoch 400, Weights: [0.33324156 0.32157941 0.345179  ]\n",
            "Epoch 450, Weights: [0.33321844 0.32009794 0.34668358]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337048 0.33327166 0.33335782]\n",
            "Epoch 50, Weights: [0.33522536 0.33018849 0.33458611]\n",
            "Epoch 100, Weights: [0.33707393 0.32710518 0.33582086]\n",
            "Epoch 150, Weights: [0.33891587 0.32402209 0.33706201]\n",
            "Epoch 200, Weights: [0.34075086 0.32093961 0.3383095 ]\n",
            "Epoch 250, Weights: [0.34257858 0.31785812 0.33956327]\n",
            "Epoch 300, Weights: [0.34439871 0.31477799 0.34082327]\n",
            "Epoch 350, Weights: [0.34621094 0.31169959 0.34208943]\n",
            "Epoch 400, Weights: [0.34801495 0.30862331 0.3433617 ]\n",
            "Epoch 450, Weights: [0.34981043 0.30554952 0.34464002]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335861 0.33336524 0.33327612]\n",
            "Epoch 50, Weights: [0.33462163 0.33497082 0.33040752]\n",
            "Epoch 100, Weights: [0.33588187 0.33659551 0.32752259]\n",
            "Epoch 150, Weights: [0.33713941 0.33823963 0.32462092]\n",
            "Epoch 200, Weights: [0.33839437 0.33990351 0.32170209]\n",
            "Epoch 250, Weights: [0.33964685 0.34158746 0.31876567]\n",
            "Epoch 300, Weights: [0.34089694 0.34329181 0.31581122]\n",
            "Epoch 350, Weights: [0.34214474 0.34501691 0.31283831]\n",
            "Epoch 400, Weights: [0.34339037 0.3467631  0.3098465 ]\n",
            "Epoch 450, Weights: [0.3446339  0.34853073 0.30683534]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336524 0.33330842 0.33332631]\n",
            "Epoch 50, Weights: [0.33496679 0.33205616 0.33297701]\n",
            "Epoch 100, Weights: [0.33657941 0.33079029 0.33263027]\n",
            "Epoch 150, Weights: [0.33820325 0.32951059 0.33228613]\n",
            "Epoch 200, Weights: [0.3398385  0.32821683 0.33194464]\n",
            "Epoch 250, Weights: [0.34148532 0.32690881 0.33160585]\n",
            "Epoch 300, Weights: [0.34314389 0.32558629 0.3312698 ]\n",
            "Epoch 350, Weights: [0.34481439 0.32424904 0.33093654]\n",
            "Epoch 400, Weights: [0.34649701 0.32289683 0.33060613]\n",
            "Epoch 450, Weights: [0.34819193 0.32152943 0.33027861]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331866 0.33331638 0.33336493]\n",
            "Epoch 50, Weights: [0.33257625 0.33247633 0.33494739]\n",
            "Epoch 100, Weights: [0.33181524 0.33165024 0.33653449]\n",
            "Epoch 150, Weights: [0.33103558 0.33083823 0.33812615]\n",
            "Epoch 200, Weights: [0.33023724 0.33004042 0.33972231]\n",
            "Epoch 250, Weights: [0.32942016 0.32925692 0.34132289]\n",
            "Epoch 300, Weights: [0.32858429 0.32848785 0.34292784]\n",
            "Epoch 350, Weights: [0.32772958 0.32773333 0.34453707]\n",
            "Epoch 400, Weights: [0.32685598 0.32699347 0.34615052]\n",
            "Epoch 450, Weights: [0.32596344 0.3262684  0.34776813]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331188 0.33333621 0.33335188]\n",
            "Epoch 50, Weights: [0.33223555 0.33348632 0.3342781 ]\n",
            "Epoch 100, Weights: [0.33115124 0.33364809 0.33520064]\n",
            "Epoch 150, Weights: [0.33005891 0.33382158 0.33611948]\n",
            "Epoch 200, Weights: [0.32895853 0.33400685 0.33703459]\n",
            "Epoch 250, Weights: [0.32785007 0.33420395 0.33794595]\n",
            "Epoch 300, Weights: [0.32673351 0.33441296 0.33885351]\n",
            "Epoch 350, Weights: [0.32560881 0.33463391 0.33975725]\n",
            "Epoch 400, Weights: [0.32447595 0.33486688 0.34065714]\n",
            "Epoch 450, Weights: [0.32333491 0.33511192 0.34155314]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331044 0.33337803 0.33331149]\n",
            "Epoch 50, Weights: [0.33215728 0.33562323 0.33221945]\n",
            "Epoch 100, Weights: [0.33098611 0.33788747 0.33112639]\n",
            "Epoch 150, Weights: [0.32979667 0.34017101 0.33003229]\n",
            "Epoch 200, Weights: [0.32858871 0.34247412 0.32893713]\n",
            "Epoch 250, Weights: [0.32736199 0.34479709 0.3278409 ]\n",
            "Epoch 300, Weights: [0.32611623 0.34714018 0.32674356]\n",
            "Epoch 350, Weights: [0.32485118 0.3495037  0.32564509]\n",
            "Epoch 400, Weights: [0.32356656 0.35188792 0.32454549]\n",
            "Epoch 450, Weights: [0.3222621  0.35429315 0.32344472]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335492 0.33322998 0.33341508]\n",
            "Epoch 50, Weights: [0.33443933 0.32809969 0.33746096]\n",
            "Epoch 100, Weights: [0.33553264 0.32304224 0.34142509]\n",
            "Epoch 150, Weights: [0.33663428 0.31805792 0.34530777]\n",
            "Epoch 200, Weights: [0.3377437  0.31314694 0.34910933]\n",
            "Epoch 250, Weights: [0.33886036 0.30830945 0.35283016]\n",
            "Epoch 300, Weights: [0.33998371 0.30354554 0.35647072]\n",
            "Epoch 350, Weights: [0.34111322 0.29885524 0.36003151]\n",
            "Epoch 400, Weights: [0.34224837 0.29423851 0.36351309]\n",
            "Epoch 450, Weights: [0.34338865 0.28969527 0.36691606]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332166 0.33331411 0.33336421]\n",
            "Epoch 50, Weights: [0.33273329 0.33234803 0.33491865]\n",
            "Epoch 100, Weights: [0.33213497 0.33137176 0.33649324]\n",
            "Epoch 150, Weights: [0.33152659 0.33038521 0.33808817]\n",
            "Epoch 200, Weights: [0.330908   0.32938832 0.33970366]\n",
            "Epoch 250, Weights: [0.33027909 0.32838099 0.3413399 ]\n",
            "Epoch 300, Weights: [0.32963973 0.32736314 0.3429971 ]\n",
            "Epoch 350, Weights: [0.32898979 0.32633471 0.34467548]\n",
            "Epoch 400, Weights: [0.32832914 0.3252956  0.34637523]\n",
            "Epoch 450, Weights: [0.32765766 0.32424574 0.34809657]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334418 0.33326833 0.33338746]\n",
            "Epoch 50, Weights: [0.33388723 0.33001565 0.33609708]\n",
            "Epoch 100, Weights: [0.33443032 0.32675761 0.33881204]\n",
            "Epoch 150, Weights: [0.3349734  0.32349445 0.34153212]\n",
            "Epoch 200, Weights: [0.33551643 0.32022642 0.34425713]\n",
            "Epoch 250, Weights: [0.33605935 0.31695378 0.34698684]\n",
            "Epoch 300, Weights: [0.33660214 0.31367678 0.34972106]\n",
            "Epoch 350, Weights: [0.33714473 0.31039568 0.35245956]\n",
            "Epoch 400, Weights: [0.33768708 0.30711074 0.35520215]\n",
            "Epoch 450, Weights: [0.33822916 0.30382222 0.3579486 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328716 0.33339606 0.33331675]\n",
            "Epoch 50, Weights: [0.33095366 0.33656409 0.33248222]\n",
            "Epoch 100, Weights: [0.32856981 0.33979402 0.33163614]\n",
            "Epoch 150, Weights: [0.32613472 0.34308683 0.33077842]\n",
            "Epoch 200, Weights: [0.3236475  0.3464435  0.32990897]\n",
            "Epoch 250, Weights: [0.32110725 0.34986502 0.32902769]\n",
            "Epoch 300, Weights: [0.31851308 0.3533524  0.32813449]\n",
            "Epoch 350, Weights: [0.31586407 0.35690663 0.32722928]\n",
            "Epoch 400, Weights: [0.31315931 0.3605287  0.32631196]\n",
            "Epoch 450, Weights: [0.31039787 0.36421963 0.32538247]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333118 0.3333944  0.33327439]\n",
            "Epoch 50, Weights: [0.33322193 0.33647044 0.3303076 ]\n",
            "Epoch 100, Weights: [0.3331083  0.33959111 0.32730056]\n",
            "Epoch 150, Weights: [0.33299022 0.34275724 0.32425251]\n",
            "Epoch 200, Weights: [0.33286759 0.34596974 0.32116265]\n",
            "Epoch 250, Weights: [0.33274031 0.34922949 0.31803017]\n",
            "Epoch 300, Weights: [0.33260828 0.35253744 0.31485425]\n",
            "Epoch 350, Weights: [0.33247141 0.35589452 0.31163404]\n",
            "Epoch 400, Weights: [0.33232959 0.35930172 0.30836865]\n",
            "Epoch 450, Weights: [0.33218272 0.36276004 0.30505721]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333837 0.33334066 0.33332094]\n",
            "Epoch 50, Weights: [0.3335897  0.33370564 0.33270463]\n",
            "Epoch 100, Weights: [0.33383858 0.3340671  0.33209429]\n",
            "Epoch 150, Weights: [0.33408505 0.33442507 0.33148985]\n",
            "Epoch 200, Weights: [0.33432914 0.33477959 0.33089124]\n",
            "Epoch 250, Weights: [0.33457087 0.3351307  0.3302984 ]\n",
            "Epoch 300, Weights: [0.33481027 0.33547844 0.32971126]\n",
            "Epoch 350, Weights: [0.33504736 0.33582284 0.32912977]\n",
            "Epoch 400, Weights: [0.33528217 0.33616394 0.32855386]\n",
            "Epoch 450, Weights: [0.33551472 0.33650178 0.32798348]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3332563  0.33328567 0.33345801]\n",
            "Epoch 50, Weights: [0.32941339 0.33092494 0.33966165]\n",
            "Epoch 100, Weights: [0.32558663 0.32860736 0.34580598]\n",
            "Epoch 150, Weights: [0.32177578 0.3263326  0.35189159]\n",
            "Epoch 200, Weights: [0.31798055 0.32410034 0.35791908]\n",
            "Epoch 250, Weights: [0.3142007  0.32191026 0.363889  ]\n",
            "Epoch 300, Weights: [0.31043597 0.31976205 0.36980195]\n",
            "Epoch 350, Weights: [0.30668611 0.31765539 0.37565847]\n",
            "Epoch 400, Weights: [0.30295089 0.31558997 0.38145911]\n",
            "Epoch 450, Weights: [0.29923004 0.3135655  0.38720442]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333034 0.33327586 0.33339376]\n",
            "Epoch 50, Weights: [0.33317167 0.33040091 0.33642739]\n",
            "Epoch 100, Weights: [0.33299407 0.32752215 0.33948375]\n",
            "Epoch 150, Weights: [0.33279746 0.32463971 0.34256279]\n",
            "Epoch 200, Weights: [0.33258176 0.32175372 0.34566448]\n",
            "Epoch 250, Weights: [0.33234689 0.3188643  0.34878878]\n",
            "Epoch 300, Weights: [0.33209277 0.31597157 0.35193563]\n",
            "Epoch 350, Weights: [0.33181931 0.31307566 0.355105  ]\n",
            "Epoch 400, Weights: [0.33152645 0.31017669 0.35829683]\n",
            "Epoch 450, Weights: [0.3312141  0.3072748  0.36151106]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336739 0.33328183 0.33335074]\n",
            "Epoch 50, Weights: [0.33508367 0.33068053 0.33423577]\n",
            "Epoch 100, Weights: [0.33682543 0.32802575 0.33514879]\n",
            "Epoch 150, Weights: [0.33859318 0.32531624 0.33609055]\n",
            "Epoch 200, Weights: [0.34038743 0.32255073 0.33706181]\n",
            "Epoch 250, Weights: [0.34220872 0.31972787 0.33806338]\n",
            "Epoch 300, Weights: [0.34405759 0.31684633 0.33909605]\n",
            "Epoch 350, Weights: [0.34593459 0.31390469 0.34016069]\n",
            "Epoch 400, Weights: [0.34784031 0.31090152 0.34125814]\n",
            "Epoch 450, Weights: [0.34977534 0.30783534 0.34238929]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337439 0.33335126 0.33327432]\n",
            "Epoch 50, Weights: [0.33543376 0.33425232 0.33031389]\n",
            "Epoch 100, Weights: [0.33750502 0.33516206 0.32733289]\n",
            "Epoch 150, Weights: [0.33958838 0.33608061 0.32433098]\n",
            "Epoch 200, Weights: [0.34168407 0.33700811 0.32130779]\n",
            "Epoch 250, Weights: [0.34379233 0.33794468 0.31826295]\n",
            "Epoch 300, Weights: [0.3459134  0.33889048 0.31519609]\n",
            "Epoch 350, Weights: [0.34804751 0.33984564 0.31210682]\n",
            "Epoch 400, Weights: [0.35019492 0.3408103  0.30899475]\n",
            "Epoch 450, Weights: [0.35235587 0.34178462 0.30585948]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334762 0.33336639 0.33328597]\n",
            "Epoch 50, Weights: [0.33406039 0.33503104 0.33090854]\n",
            "Epoch 100, Weights: [0.33476968 0.33671857 0.32851172]\n",
            "Epoch 150, Weights: [0.3354756  0.33842933 0.32609504]\n",
            "Epoch 200, Weights: [0.33617824 0.34016372 0.32365801]\n",
            "Epoch 250, Weights: [0.33687771 0.34192212 0.32120014]\n",
            "Epoch 300, Weights: [0.33757412 0.34370495 0.3187209 ]\n",
            "Epoch 350, Weights: [0.33826757 0.34551263 0.31621977]\n",
            "Epoch 400, Weights: [0.33895817 0.34734559 0.31369621]\n",
            "Epoch 450, Weights: [0.33964602 0.34920428 0.31114967]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333511  0.33330049 0.33334838]\n",
            "Epoch 50, Weights: [0.33423777 0.33166059 0.33410161]\n",
            "Epoch 100, Weights: [0.33512045 0.3300237  0.33485582]\n",
            "Epoch 150, Weights: [0.33599919 0.32838971 0.33561106]\n",
            "Epoch 200, Weights: [0.33687408 0.32675851 0.33636739]\n",
            "Epoch 250, Weights: [0.33774516 0.32512997 0.33712484]\n",
            "Epoch 300, Weights: [0.33861251 0.32350399 0.33788347]\n",
            "Epoch 350, Weights: [0.33947619 0.32188044 0.33864333]\n",
            "Epoch 400, Weights: [0.34033627 0.32025923 0.33940447]\n",
            "Epoch 450, Weights: [0.34119279 0.31864023 0.34016695]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333784 0.3332839  0.33337822]\n",
            "Epoch 50, Weights: [0.33355695 0.33081917 0.33562385]\n",
            "Epoch 100, Weights: [0.33376259 0.32836673 0.33787064]\n",
            "Epoch 150, Weights: [0.33395481 0.32592658 0.34011857]\n",
            "Epoch 200, Weights: [0.33413365 0.32349872 0.3423676 ]\n",
            "Epoch 250, Weights: [0.33429914 0.32108315 0.34461768]\n",
            "Epoch 300, Weights: [0.33445131 0.31867987 0.34686879]\n",
            "Epoch 350, Weights: [0.33459021 0.31628889 0.34912087]\n",
            "Epoch 400, Weights: [0.33471587 0.31391021 0.35137389]\n",
            "Epoch 450, Weights: [0.33482833 0.31154383 0.35362781]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33326552 0.33334312 0.33339132]\n",
            "Epoch 50, Weights: [0.32989073 0.33383206 0.33627719]\n",
            "Epoch 100, Weights: [0.32654545 0.33431902 0.3391355 ]\n",
            "Epoch 150, Weights: [0.3232293  0.33480406 0.34196661]\n",
            "Epoch 200, Weights: [0.31994186 0.33528723 0.34477088]\n",
            "Epoch 250, Weights: [0.31668274 0.33576858 0.34754865]\n",
            "Epoch 300, Weights: [0.31345157 0.33624815 0.35030025]\n",
            "Epoch 350, Weights: [0.31024796 0.336726   0.35302601]\n",
            "Epoch 400, Weights: [0.30707155 0.33720216 0.35572626]\n",
            "Epoch 450, Weights: [0.30392196 0.33767669 0.35840131]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330872 0.33334048 0.33335077]\n",
            "Epoch 50, Weights: [0.33207892 0.33370133 0.33421973]\n",
            "Epoch 100, Weights: [0.33085002 0.33406837 0.33508157]\n",
            "Epoch 150, Weights: [0.32962201 0.33444164 0.33593632]\n",
            "Epoch 200, Weights: [0.32839485 0.33482115 0.33678397]\n",
            "Epoch 250, Weights: [0.32716852 0.33520692 0.33762453]\n",
            "Epoch 300, Weights: [0.325943   0.33559896 0.33845802]\n",
            "Epoch 350, Weights: [0.32471825 0.3359973  0.33928442]\n",
            "Epoch 400, Weights: [0.32349425 0.33640195 0.34010376]\n",
            "Epoch 450, Weights: [0.32227099 0.33681294 0.34091604]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328949 0.33336905 0.33334143]\n",
            "Epoch 50, Weights: [0.33110149 0.33515755 0.33374092]\n",
            "Epoch 100, Weights: [0.32892067 0.33695075 0.33412855]\n",
            "Epoch 150, Weights: [0.3267469  0.33874867 0.3345044 ]\n",
            "Epoch 200, Weights: [0.32458006 0.34055135 0.33486855]\n",
            "Epoch 250, Weights: [0.32242003 0.34235884 0.3352211 ]\n",
            "Epoch 300, Weights: [0.32026669 0.34417118 0.3355621 ]\n",
            "Epoch 350, Weights: [0.31811991 0.3459884  0.33589166]\n",
            "Epoch 400, Weights: [0.31597959 0.34781054 0.33620984]\n",
            "Epoch 450, Weights: [0.31384559 0.34963765 0.33651673]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333146  0.3333578  0.33332757]\n",
            "Epoch 50, Weights: [0.3323705  0.33458726 0.3330422 ]\n",
            "Epoch 100, Weights: [0.33141019 0.33582822 0.33276155]\n",
            "Epoch 150, Weights: [0.33043341 0.33708077 0.33248578]\n",
            "Epoch 200, Weights: [0.3294399  0.338345   0.33221507]\n",
            "Epoch 250, Weights: [0.32842938 0.33962101 0.33194958]\n",
            "Epoch 300, Weights: [0.32740159 0.34090889 0.3316895 ]\n",
            "Epoch 350, Weights: [0.32635623 0.34220874 0.331435  ]\n",
            "Epoch 400, Weights: [0.32529304 0.34352066 0.33118628]\n",
            "Epoch 450, Weights: [0.32421172 0.34484473 0.33094351]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336979 0.33331845 0.33331173]\n",
            "Epoch 50, Weights: [0.33519416 0.33257683 0.33222898]\n",
            "Epoch 100, Weights: [0.337021   0.33183898 0.33113999]\n",
            "Epoch 150, Weights: [0.3388504  0.33110484 0.33004474]\n",
            "Epoch 200, Weights: [0.34068241 0.33037435 0.32894321]\n",
            "Epoch 250, Weights: [0.34251713 0.32964747 0.32783537]\n",
            "Epoch 300, Weights: [0.34435464 0.32892413 0.3267212 ]\n",
            "Epoch 350, Weights: [0.34619501 0.32820428 0.32560068]\n",
            "Epoch 400, Weights: [0.34803833 0.32748786 0.32447378]\n",
            "Epoch 450, Weights: [0.34988467 0.32677483 0.32334047]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329951 0.33332872 0.33337175]\n",
            "Epoch 50, Weights: [0.33160755 0.33310939 0.33528304]\n",
            "Epoch 100, Weights: [0.3299132  0.33291148 0.33717529]\n",
            "Epoch 150, Weights: [0.32821653 0.33273465 0.3390488 ]\n",
            "Epoch 200, Weights: [0.32651757 0.33257857 0.34090383]\n",
            "Epoch 250, Weights: [0.32481638 0.33244291 0.34274067]\n",
            "Epoch 300, Weights: [0.32311303 0.33232736 0.34455959]\n",
            "Epoch 350, Weights: [0.32140755 0.33223158 0.34636083]\n",
            "Epoch 400, Weights: [0.31970001 0.33215528 0.34814467]\n",
            "Epoch 450, Weights: [0.31799046 0.33209815 0.34991136]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337649 0.33321739 0.33340609]\n",
            "Epoch 50, Weights: [0.33553196 0.32741833 0.33704968]\n",
            "Epoch 100, Weights: [0.33768186 0.32161425 0.34070386]\n",
            "Epoch 150, Weights: [0.33982645 0.31580427 0.34436925]\n",
            "Epoch 200, Weights: [0.34196595 0.30998754 0.34804648]\n",
            "Epoch 250, Weights: [0.34410062 0.30416316 0.35173618]\n",
            "Epoch 300, Weights: [0.3462307  0.29833028 0.35543899]\n",
            "Epoch 350, Weights: [0.34835642 0.29248801 0.35915554]\n",
            "Epoch 400, Weights: [0.35047803 0.28663545 0.36288648]\n",
            "Epoch 450, Weights: [0.35259576 0.28077172 0.36663249]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328457 0.33336661 0.33334879]\n",
            "Epoch 50, Weights: [0.33086659 0.33501778 0.3341156 ]\n",
            "Epoch 100, Weights: [0.3284871  0.3366434  0.33486947]\n",
            "Epoch 150, Weights: [0.32614567 0.33824373 0.33561057]\n",
            "Epoch 200, Weights: [0.32384184 0.33981908 0.33633905]\n",
            "Epoch 250, Weights: [0.32157515 0.34136974 0.33705508]\n",
            "Epoch 300, Weights: [0.31934517 0.34289599 0.33775881]\n",
            "Epoch 350, Weights: [0.31715143 0.34439813 0.33845041]\n",
            "Epoch 400, Weights: [0.31499348 0.34587645 0.33913003]\n",
            "Epoch 450, Weights: [0.31287089 0.34733125 0.33979784]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331679 0.33332953 0.33335365]\n",
            "Epoch 50, Weights: [0.33249065 0.33314022 0.3343691 ]\n",
            "Epoch 100, Weights: [0.33166508 0.33295203 0.33538286]\n",
            "Epoch 150, Weights: [0.33084006 0.33276495 0.33639496]\n",
            "Epoch 200, Weights: [0.33001556 0.33257898 0.33740543]\n",
            "Epoch 250, Weights: [0.32919158 0.33239409 0.3384143 ]\n",
            "Epoch 300, Weights: [0.32836809 0.3322103  0.33942158]\n",
            "Epoch 350, Weights: [0.32754508 0.33202758 0.34042731]\n",
            "Epoch 400, Weights: [0.32672253 0.33184593 0.34143151]\n",
            "Epoch 450, Weights: [0.32590042 0.33166534 0.34243421]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333352 0.33337057 0.33329588]\n",
            "Epoch 50, Weights: [0.33334264 0.33523372 0.33142361]\n",
            "Epoch 100, Weights: [0.33335072 0.33709805 0.3295512 ]\n",
            "Epoch 150, Weights: [0.33335775 0.33896349 0.32767873]\n",
            "Epoch 200, Weights: [0.33336373 0.34082996 0.32580628]\n",
            "Epoch 250, Weights: [0.33336867 0.34269739 0.32393392]\n",
            "Epoch 300, Weights: [0.33337255 0.3445657  0.32206171]\n",
            "Epoch 350, Weights: [0.33337539 0.34643483 0.32018975]\n",
            "Epoch 400, Weights: [0.33337718 0.34830469 0.3183181 ]\n",
            "Epoch 450, Weights: [0.33337791 0.35017522 0.31644684]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.333315   0.33336617 0.3333188 ]\n",
            "Epoch 50, Weights: [0.33240494 0.33500728 0.33258775]\n",
            "Epoch 100, Weights: [0.33150631 0.33664639 0.33184727]\n",
            "Epoch 150, Weights: [0.33061904 0.33828344 0.33109749]\n",
            "Epoch 200, Weights: [0.32974306 0.33991836 0.33033855]\n",
            "Epoch 250, Weights: [0.3288783  0.34155108 0.32957059]\n",
            "Epoch 300, Weights: [0.32802471 0.34318154 0.32879373]\n",
            "Epoch 350, Weights: [0.3271822  0.34480966 0.32800811]\n",
            "Epoch 400, Weights: [0.32635072 0.34643539 0.32721386]\n",
            "Epoch 450, Weights: [0.3255302  0.34805866 0.32641111]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33327961 0.33336053 0.33335983]\n",
            "Epoch 50, Weights: [0.33060977 0.33471637 0.33467383]\n",
            "Epoch 100, Weights: [0.32797101 0.33606328 0.33596568]\n",
            "Epoch 150, Weights: [0.32536322 0.33740124 0.3372355 ]\n",
            "Epoch 200, Weights: [0.32278631 0.33873026 0.33848339]\n",
            "Epoch 250, Weights: [0.32024016 0.34005033 0.33970948]\n",
            "Epoch 300, Weights: [0.31772464 0.34136146 0.34091387]\n",
            "Epoch 350, Weights: [0.31523963 0.34266364 0.3420967 ]\n",
            "Epoch 400, Weights: [0.312785   0.34395688 0.34325809]\n",
            "Epoch 450, Weights: [0.31036061 0.34524118 0.34439818]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331299 0.33335293 0.33333406]\n",
            "Epoch 50, Weights: [0.33229835 0.33432843 0.33337318]\n",
            "Epoch 100, Weights: [0.33128801 0.33529474 0.33341723]\n",
            "Epoch 150, Weights: [0.33028192 0.33625191 0.33346614]\n",
            "Epoch 200, Weights: [0.32928005 0.33720004 0.33351988]\n",
            "Epoch 250, Weights: [0.32828237 0.3381392  0.3335784 ]\n",
            "Epoch 300, Weights: [0.32728885 0.33906945 0.33364166]\n",
            "Epoch 350, Weights: [0.32629947 0.33999088 0.33370962]\n",
            "Epoch 400, Weights: [0.32531418 0.34090356 0.33378223]\n",
            "Epoch 450, Weights: [0.32433296 0.34180756 0.33385945]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333117 0.33339882 0.33326997]\n",
            "Epoch 50, Weights: [0.3332246  0.33666706 0.3301083 ]\n",
            "Epoch 100, Weights: [0.33311969 0.33992211 0.32695816]\n",
            "Epoch 150, Weights: [0.33301644 0.34316387 0.32381966]\n",
            "Epoch 200, Weights: [0.33291485 0.34639221 0.32069291]\n",
            "Epoch 250, Weights: [0.33281491 0.34960705 0.31757801]\n",
            "Epoch 300, Weights: [0.33271663 0.35280828 0.31447506]\n",
            "Epoch 350, Weights: [0.33262001 0.3559958  0.31138416]\n",
            "Epoch 400, Weights: [0.33252504 0.35916952 0.30830542]\n",
            "Epoch 450, Weights: [0.33243171 0.36232934 0.30523892]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33327963 0.33337019 0.33335015]\n",
            "Epoch 50, Weights: [0.33058365 0.33522015 0.33419617]\n",
            "Epoch 100, Weights: [0.32786548 0.33708345 0.33505104]\n",
            "Epoch 150, Weights: [0.32512519 0.33896003 0.33591475]\n",
            "Epoch 200, Weights: [0.32236284 0.34084985 0.33678728]\n",
            "Epoch 250, Weights: [0.31957852 0.34275284 0.33766861]\n",
            "Epoch 300, Weights: [0.31677231 0.34466893 0.33855872]\n",
            "Epoch 350, Weights: [0.31394432 0.34659806 0.33945759]\n",
            "Epoch 400, Weights: [0.31109463 0.34854015 0.34036518]\n",
            "Epoch 450, Weights: [0.30822337 0.35049513 0.34128147]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330426 0.33338259 0.33331312]\n",
            "Epoch 50, Weights: [0.33184922 0.33584698 0.33230376]\n",
            "Epoch 100, Weights: [0.33039011 0.33831406 0.3312958 ]\n",
            "Epoch 150, Weights: [0.32892681 0.34078397 0.33028919]\n",
            "Epoch 200, Weights: [0.32745921 0.34325689 0.32928387]\n",
            "Epoch 250, Weights: [0.3259872  0.34573297 0.3282798 ]\n",
            "Epoch 300, Weights: [0.32451066 0.34821239 0.32727692]\n",
            "Epoch 350, Weights: [0.32302947 0.3506953  0.3262752 ]\n",
            "Epoch 400, Weights: [0.32154353 0.35318187 0.32527457]\n",
            "Epoch 450, Weights: [0.3200527  0.35567227 0.324275  ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332145 0.33335158 0.33332694]\n",
            "Epoch 50, Weights: [0.3327271  0.33426446 0.33300841]\n",
            "Epoch 100, Weights: [0.3321319  0.33517701 0.33269106]\n",
            "Epoch 150, Weights: [0.33153583 0.33608929 0.33237485]\n",
            "Epoch 200, Weights: [0.33093887 0.33700134 0.33205977]\n",
            "Epoch 250, Weights: [0.330341   0.33791319 0.33174578]\n",
            "Epoch 300, Weights: [0.32974222 0.33882489 0.33143285]\n",
            "Epoch 350, Weights: [0.3291425  0.33973649 0.33112097]\n",
            "Epoch 400, Weights: [0.32854184 0.34064802 0.33081011]\n",
            "Epoch 450, Weights: [0.3279402  0.34155953 0.33050024]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332629 0.33333949 0.33333419]\n",
            "Epoch 50, Weights: [0.33297141 0.33365004 0.33337851]\n",
            "Epoch 100, Weights: [0.33261018 0.33396525 0.33342453]\n",
            "Epoch 150, Weights: [0.33224249 0.3342852  0.33347228]\n",
            "Epoch 200, Weights: [0.33186823 0.33460995 0.33352179]\n",
            "Epoch 250, Weights: [0.33148728 0.33493959 0.33357309]\n",
            "Epoch 300, Weights: [0.33109954 0.33527422 0.33362622]\n",
            "Epoch 350, Weights: [0.33070488 0.3356139  0.33368119]\n",
            "Epoch 400, Weights: [0.33030319 0.33595873 0.33373805]\n",
            "Epoch 450, Weights: [0.32989434 0.3363088  0.33379683]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331802 0.33336639 0.33331555]\n",
            "Epoch 50, Weights: [0.33255005 0.33502262 0.3324273 ]\n",
            "Epoch 100, Weights: [0.33177608 0.33668447 0.33153942]\n",
            "Epoch 150, Weights: [0.33099606 0.33835197 0.33065194]\n",
            "Epoch 200, Weights: [0.33020991 0.34002515 0.32976491]\n",
            "Epoch 250, Weights: [0.32941757 0.34170404 0.32887836]\n",
            "Epoch 300, Weights: [0.32861898 0.34338868 0.32799231]\n",
            "Epoch 350, Weights: [0.32781405 0.34507911 0.32710681]\n",
            "Epoch 400, Weights: [0.32700273 0.34677535 0.32622189]\n",
            "Epoch 450, Weights: [0.32618494 0.34847743 0.3253376 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33341717 0.33336588 0.33321693]\n",
            "Epoch 50, Weights: [0.33762977 0.33496301 0.32740719]\n",
            "Epoch 100, Weights: [0.34188254 0.33650027 0.32161716]\n",
            "Epoch 150, Weights: [0.34617576 0.33797748 0.31584673]\n",
            "Epoch 200, Weights: [0.35050971 0.33939444 0.31009583]\n",
            "Epoch 250, Weights: [0.35488465 0.34075094 0.30436437]\n",
            "Epoch 300, Weights: [0.35930085 0.34204679 0.29865233]\n",
            "Epoch 350, Weights: [0.36375853 0.34328177 0.29295967]\n",
            "Epoch 400, Weights: [0.36825793 0.34445566 0.28728638]\n",
            "Epoch 450, Weights: [0.37279926 0.34556824 0.28163248]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336582 0.33326992 0.33336423]\n",
            "Epoch 50, Weights: [0.33498815 0.33009364 0.33491818]\n",
            "Epoch 100, Weights: [0.33660589 0.32690526 0.33648882]\n",
            "Epoch 150, Weights: [0.338219   0.32370468 0.33807629]\n",
            "Epoch 200, Weights: [0.33982748 0.32049179 0.33968071]\n",
            "Epoch 250, Weights: [0.3414313  0.31726647 0.3413022 ]\n",
            "Epoch 300, Weights: [0.34303043 0.31402863 0.3429409 ]\n",
            "Epoch 350, Weights: [0.34462487 0.31077816 0.34459694]\n",
            "Epoch 400, Weights: [0.34621458 0.30751496 0.34627043]\n",
            "Epoch 450, Weights: [0.34779954 0.30423891 0.34796152]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33326483 0.33333916 0.33339598]\n",
            "Epoch 50, Weights: [0.3298252  0.33363504 0.33653973]\n",
            "Epoch 100, Weights: [0.32635592 0.33393938 0.33970467]\n",
            "Epoch 150, Weights: [0.32285656 0.33425229 0.34289112]\n",
            "Epoch 200, Weights: [0.31932669 0.33457388 0.3460994 ]\n",
            "Epoch 250, Weights: [0.31576585 0.33490428 0.34932984]\n",
            "Epoch 300, Weights: [0.31217358 0.33524361 0.35258277]\n",
            "Epoch 350, Weights: [0.30854943 0.335592   0.35585854]\n",
            "Epoch 400, Weights: [0.30489292 0.33594956 0.35915749]\n",
            "Epoch 450, Weights: [0.30120355 0.33631644 0.36247998]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33341834 0.33324945 0.33333218]\n",
            "Epoch 50, Weights: [0.3376769  0.32904452 0.33327855]\n",
            "Epoch 100, Weights: [0.34195043 0.3248173  0.33323224]\n",
            "Epoch 150, Weights: [0.34623872 0.32056793 0.33319332]\n",
            "Epoch 200, Weights: [0.35054154 0.3162966  0.33316183]\n",
            "Epoch 250, Weights: [0.35485868 0.31200346 0.33313783]\n",
            "Epoch 300, Weights: [0.35918991 0.30768869 0.33312137]\n",
            "Epoch 350, Weights: [0.36353499 0.30335247 0.33311251]\n",
            "Epoch 400, Weights: [0.36789368 0.29899499 0.3331113 ]\n",
            "Epoch 450, Weights: [0.37226575 0.29461643 0.33311779]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337068 0.33333619 0.33329309]\n",
            "Epoch 50, Weights: [0.33521658 0.33349331 0.33129008]\n",
            "Epoch 100, Weights: [0.33701957 0.33367679 0.32930361]\n",
            "Epoch 150, Weights: [0.33878024 0.3338861  0.32733363]\n",
            "Epoch 200, Weights: [0.34049919 0.33412069 0.32538009]\n",
            "Epoch 250, Weights: [0.34217698 0.33438004 0.32344295]\n",
            "Epoch 300, Weights: [0.34381418 0.33466363 0.32152216]\n",
            "Epoch 350, Weights: [0.34541135 0.33497094 0.31961768]\n",
            "Epoch 400, Weights: [0.34696904 0.33530148 0.31772945]\n",
            "Epoch 450, Weights: [0.34848778 0.33565474 0.31585745]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33327167 0.33335046 0.33337784]\n",
            "Epoch 50, Weights: [0.33017612 0.33420878 0.33561506]\n",
            "Epoch 100, Weights: [0.32705539 0.33506968 0.3378749 ]\n",
            "Epoch 150, Weights: [0.32390929 0.33593317 0.34015751]\n",
            "Epoch 200, Weights: [0.32073767 0.33679923 0.34246307]\n",
            "Epoch 250, Weights: [0.31754035 0.33766787 0.34479175]\n",
            "Epoch 300, Weights: [0.31431718 0.33853908 0.34714371]\n",
            "Epoch 350, Weights: [0.31106797 0.33941287 0.34951913]\n",
            "Epoch 400, Weights: [0.30779257 0.34028922 0.35191818]\n",
            "Epoch 450, Weights: [0.30449081 0.34116814 0.35434102]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338352 0.33330596 0.3333105 ]\n",
            "Epoch 50, Weights: [0.33590044 0.33192882 0.33217071]\n",
            "Epoch 100, Weights: [0.33843179 0.33053444 0.33103374]\n",
            "Epoch 150, Weights: [0.34097778 0.32912262 0.32989957]\n",
            "Epoch 200, Weights: [0.34353861 0.32769318 0.32876818]\n",
            "Epoch 250, Weights: [0.3461145  0.32624591 0.32763957]\n",
            "Epoch 300, Weights: [0.34870565 0.32478062 0.32651371]\n",
            "Epoch 350, Weights: [0.35131228 0.3232971  0.32539059]\n",
            "Epoch 400, Weights: [0.35393462 0.32179516 0.32427019]\n",
            "Epoch 450, Weights: [0.35657288 0.32027458 0.32315251]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335519 0.33319009 0.33345469]\n",
            "Epoch 50, Weights: [0.33439613 0.32605203 0.33955181]\n",
            "Epoch 100, Weights: [0.33533507 0.3189589  0.345706  ]\n",
            "Epoch 150, Weights: [0.33617292 0.31190882 0.35191823]\n",
            "Epoch 200, Weights: [0.33691053 0.30489993 0.35818951]\n",
            "Epoch 250, Weights: [0.33754866 0.29793042 0.36452089]\n",
            "Epoch 300, Weights: [0.338088   0.29099849 0.37091348]\n",
            "Epoch 350, Weights: [0.33852916 0.28410237 0.37736845]\n",
            "Epoch 400, Weights: [0.33887267 0.27724029 0.38388701]\n",
            "Epoch 450, Weights: [0.33911901 0.27041052 0.39047044]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338976 0.33323854 0.33337168]\n",
            "Epoch 50, Weights: [0.33622171 0.32850635 0.33527191]\n",
            "Epoch 100, Weights: [0.33907362 0.32378796 0.33713838]\n",
            "Epoch 150, Weights: [0.34194545 0.31908304 0.33897148]\n",
            "Epoch 200, Weights: [0.34483715 0.31439121 0.34077161]\n",
            "Epoch 250, Weights: [0.3477487  0.30971211 0.34253916]\n",
            "Epoch 300, Weights: [0.35068008 0.30504538 0.34427451]\n",
            "Epoch 350, Weights: [0.35363128 0.30039064 0.34597805]\n",
            "Epoch 400, Weights: [0.3566023  0.2957475  0.34765018]\n",
            "Epoch 450, Weights: [0.35959315 0.29111557 0.34929126]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33339097 0.33326868 0.33334032]\n",
            "Epoch 50, Weights: [0.33628994 0.33002329 0.33368674]\n",
            "Epoch 100, Weights: [0.33922155 0.32675207 0.33402636]\n",
            "Epoch 150, Weights: [0.34218597 0.32345493 0.33435908]\n",
            "Epoch 200, Weights: [0.34518338 0.3201318  0.3346848 ]\n",
            "Epoch 250, Weights: [0.34821396 0.31678259 0.33500342]\n",
            "Epoch 300, Weights: [0.35127788 0.31340724 0.33531485]\n",
            "Epoch 350, Weights: [0.35437531 0.31000568 0.33561898]\n",
            "Epoch 400, Weights: [0.35750641 0.30657785 0.33591571]\n",
            "Epoch 450, Weights: [0.36067134 0.30312368 0.33620494]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334429 0.33331026 0.33334542]\n",
            "Epoch 50, Weights: [0.33389482 0.3321509  0.33395425]\n",
            "Epoch 100, Weights: [0.33445005 0.33097923 0.33457069]\n",
            "Epoch 150, Weights: [0.33501002 0.32979513 0.33519482]\n",
            "Epoch 200, Weights: [0.33557478 0.32859849 0.33582669]\n",
            "Epoch 250, Weights: [0.33614439 0.32738919 0.33646639]\n",
            "Epoch 300, Weights: [0.33671889 0.3261671  0.33711398]\n",
            "Epoch 350, Weights: [0.33729833 0.32493211 0.33776952]\n",
            "Epoch 400, Weights: [0.33788277 0.32368411 0.3384331 ]\n",
            "Epoch 450, Weights: [0.33847225 0.32242295 0.33910477]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328916 0.33337099 0.33333982]\n",
            "Epoch 50, Weights: [0.33107112 0.335268   0.33366085]\n",
            "Epoch 100, Weights: [0.32883358 0.33719196 0.33397443]\n",
            "Epoch 150, Weights: [0.32657652 0.33914312 0.33428033]\n",
            "Epoch 200, Weights: [0.32429997 0.34112172 0.33457827]\n",
            "Epoch 250, Weights: [0.32200394 0.343128   0.33486803]\n",
            "Epoch 300, Weights: [0.31968845 0.34516219 0.33514933]\n",
            "Epoch 350, Weights: [0.31735352 0.34722453 0.33542191]\n",
            "Epoch 400, Weights: [0.31499919 0.34931525 0.33568553]\n",
            "Epoch 450, Weights: [0.31262548 0.35143458 0.33593991]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332596 0.33330954 0.33336447]\n",
            "Epoch 50, Weights: [0.3329591  0.33212757 0.3349133 ]\n",
            "Epoch 100, Weights: [0.33259526 0.33095968 0.33644503]\n",
            "Epoch 150, Weights: [0.33223443 0.32980582 0.33795972]\n",
            "Epoch 200, Weights: [0.33187661 0.32866591 0.33945745]\n",
            "Epoch 250, Weights: [0.33152181 0.32753987 0.34093829]\n",
            "Epoch 300, Weights: [0.33117003 0.32642763 0.34240231]\n",
            "Epoch 350, Weights: [0.33082126 0.32532911 0.3438496 ]\n",
            "Epoch 400, Weights: [0.33047549 0.32424423 0.34528024]\n",
            "Epoch 450, Weights: [0.33013274 0.32317291 0.34669432]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33340356 0.3333058  0.33329061]\n",
            "Epoch 50, Weights: [0.33691509 0.33192863 0.33115626]\n",
            "Epoch 100, Weights: [0.34042592 0.33054992 0.32902412]\n",
            "Epoch 150, Weights: [0.34393605 0.32916968 0.32689423]\n",
            "Epoch 200, Weights: [0.34744545 0.32778792 0.3247666 ]\n",
            "Epoch 250, Weights: [0.35095411 0.32640463 0.32264123]\n",
            "Epoch 300, Weights: [0.35446199 0.32501983 0.32051815]\n",
            "Epoch 350, Weights: [0.3579691  0.32363351 0.31839736]\n",
            "Epoch 400, Weights: [0.3614754  0.32224568 0.31627889]\n",
            "Epoch 450, Weights: [0.36498088 0.32085636 0.31416273]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334189 0.33328298 0.3333751 ]\n",
            "Epoch 50, Weights: [0.33376611 0.33075735 0.33547652]\n",
            "Epoch 100, Weights: [0.33418204 0.3282152  0.33760273]\n",
            "Epoch 150, Weights: [0.33458948 0.3256564  0.33975409]\n",
            "Epoch 200, Weights: [0.33498823 0.32308081 0.34193093]\n",
            "Epoch 250, Weights: [0.33537808 0.32048827 0.34413361]\n",
            "Epoch 300, Weights: [0.33575883 0.31787865 0.34636249]\n",
            "Epoch 350, Weights: [0.33613026 0.31525179 0.34861793]\n",
            "Epoch 400, Weights: [0.33649214 0.31260753 0.3509003 ]\n",
            "Epoch 450, Weights: [0.33684427 0.30994572 0.35320998]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334135 0.3333257  0.33333293]\n",
            "Epoch 50, Weights: [0.33374355 0.33294434 0.33331208]\n",
            "Epoch 100, Weights: [0.33414787 0.3325629  0.3332892 ]\n",
            "Epoch 150, Weights: [0.33455429 0.33218139 0.3332643 ]\n",
            "Epoch 200, Weights: [0.33496283 0.33179978 0.33323736]\n",
            "Epoch 250, Weights: [0.33537349 0.33141809 0.33320839]\n",
            "Epoch 300, Weights: [0.33578629 0.33103631 0.33317737]\n",
            "Epoch 350, Weights: [0.33620123 0.33065443 0.33314432]\n",
            "Epoch 400, Weights: [0.33661831 0.33027244 0.33310921]\n",
            "Epoch 450, Weights: [0.33703755 0.32989036 0.33307206]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333462  0.33329865 0.33335512]\n",
            "Epoch 50, Weights: [0.33399494 0.33156082 0.3344442 ]\n",
            "Epoch 100, Weights: [0.33465293 0.32981485 0.33553219]\n",
            "Epoch 150, Weights: [0.33532016 0.32806076 0.33661905]\n",
            "Epoch 200, Weights: [0.33599661 0.3262986  0.33770477]\n",
            "Epoch 250, Weights: [0.33668225 0.32452839 0.33878933]\n",
            "Epoch 300, Weights: [0.33737708 0.32275016 0.33987273]\n",
            "Epoch 350, Weights: [0.33808107 0.32096395 0.34095495]\n",
            "Epoch 400, Weights: [0.33879422 0.31916979 0.34203596]\n",
            "Epoch 450, Weights: [0.33951649 0.31736771 0.34311576]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331517 0.33342315 0.33326165]\n",
            "Epoch 50, Weights: [0.33242733 0.33792749 0.32964516]\n",
            "Epoch 100, Weights: [0.33157881 0.34245772 0.32596344]\n",
            "Epoch 150, Weights: [0.33077038 0.34701435 0.32221524]\n",
            "Epoch 200, Weights: [0.33000281 0.35159789 0.31839927]\n",
            "Epoch 250, Weights: [0.32927687 0.35620889 0.31451421]\n",
            "Epoch 300, Weights: [0.32859338 0.36084787 0.31055871]\n",
            "Epoch 350, Weights: [0.32795315 0.36551543 0.30653139]\n",
            "Epoch 400, Weights: [0.32735702 0.37021213 0.30243082]\n",
            "Epoch 450, Weights: [0.32680585 0.37493858 0.29825554]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330756 0.33334556 0.33334685]\n",
            "Epoch 50, Weights: [0.33201431 0.33395973 0.33402593]\n",
            "Epoch 100, Weights: [0.33071116 0.33457878 0.33471003]\n",
            "Epoch 150, Weights: [0.32939806 0.33520275 0.33539916]\n",
            "Epoch 200, Weights: [0.32807498 0.33583163 0.33609335]\n",
            "Epoch 250, Weights: [0.3267419  0.33646546 0.33679261]\n",
            "Epoch 300, Weights: [0.32539879 0.33710424 0.33749694]\n",
            "Epoch 350, Weights: [0.32404561 0.33774799 0.33820637]\n",
            "Epoch 400, Weights: [0.32268234 0.33839672 0.33892091]\n",
            "Epoch 450, Weights: [0.32130894 0.33905045 0.33964057]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333428 0.33333183 0.33333385]\n",
            "Epoch 50, Weights: [0.33338221 0.33325729 0.33336047]\n",
            "Epoch 100, Weights: [0.3334299  0.33318251 0.33338756]\n",
            "Epoch 150, Weights: [0.33347737 0.33310749 0.33341511]\n",
            "Epoch 200, Weights: [0.33352461 0.33303223 0.33344313]\n",
            "Epoch 250, Weights: [0.33357163 0.33295672 0.33347161]\n",
            "Epoch 300, Weights: [0.33361844 0.33288096 0.33350057]\n",
            "Epoch 350, Weights: [0.33366503 0.33280495 0.33352999]\n",
            "Epoch 400, Weights: [0.33371141 0.33272867 0.33355988]\n",
            "Epoch 450, Weights: [0.33375759 0.33265213 0.33359025]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331447 0.33327645 0.33340904]\n",
            "Epoch 50, Weights: [0.33236653 0.33041516 0.33721829]\n",
            "Epoch 100, Weights: [0.33140775 0.32751875 0.34107347]\n",
            "Epoch 150, Weights: [0.33043802 0.32458681 0.34497514]\n",
            "Epoch 200, Weights: [0.32945723 0.32161891 0.34892382]\n",
            "Epoch 250, Weights: [0.32846525 0.31861463 0.35292008]\n",
            "Epoch 300, Weights: [0.32746197 0.31557353 0.35696447]\n",
            "Epoch 350, Weights: [0.32644726 0.31249517 0.36105754]\n",
            "Epoch 400, Weights: [0.32542101 0.30937909 0.36519987]\n",
            "Epoch 450, Weights: [0.32438308 0.30622487 0.36939202]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333131  0.33331018 0.33337669]\n",
            "Epoch 50, Weights: [0.33228906 0.33215293 0.33555798]\n",
            "Epoch 100, Weights: [0.33123931 0.33099559 0.33776508]\n",
            "Epoch 150, Weights: [0.33016343 0.32983833 0.33999821]\n",
            "Epoch 200, Weights: [0.32906102 0.32868135 0.34225761]\n",
            "Epoch 250, Weights: [0.32793165 0.32752484 0.34454348]\n",
            "Epoch 300, Weights: [0.3267749  0.32636902 0.34685605]\n",
            "Epoch 350, Weights: [0.32559036 0.32521408 0.34919553]\n",
            "Epoch 400, Weights: [0.32437759 0.32406024 0.35156213]\n",
            "Epoch 450, Weights: [0.32313617 0.32290772 0.35395608]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33339642 0.33327651 0.33332703]\n",
            "Epoch 50, Weights: [0.33653442 0.33044683 0.33301873]\n",
            "Epoch 100, Weights: [0.33963917 0.3276383  0.33272251]\n",
            "Epoch 150, Weights: [0.34271093 0.32485079 0.33243825]\n",
            "Epoch 200, Weights: [0.34574999 0.32208415 0.33216583]\n",
            "Epoch 250, Weights: [0.34875659 0.31933824 0.33190514]\n",
            "Epoch 300, Weights: [0.35173101 0.31661292 0.33165605]\n",
            "Epoch 350, Weights: [0.35467349 0.31390804 0.33141844]\n",
            "Epoch 400, Weights: [0.3575843  0.31122347 0.33119219]\n",
            "Epoch 450, Weights: [0.36046369 0.30855908 0.3309772 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331496 0.33328766 0.33339735]\n",
            "Epoch 50, Weights: [0.33239918 0.33100634 0.33659446]\n",
            "Epoch 100, Weights: [0.33148804 0.32872878 0.33978315]\n",
            "Epoch 150, Weights: [0.33058167 0.32645521 0.34296309]\n",
            "Epoch 200, Weights: [0.32968016 0.32418586 0.34613395]\n",
            "Epoch 250, Weights: [0.32878364 0.32192093 0.3492954 ]\n",
            "Epoch 300, Weights: [0.3278922  0.31966066 0.35244711]\n",
            "Epoch 350, Weights: [0.32700595 0.31740525 0.35558877]\n",
            "Epoch 400, Weights: [0.32612501 0.31515491 0.35872005]\n",
            "Epoch 450, Weights: [0.32524946 0.31290986 0.36184065]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333653  0.3332931  0.33334157]\n",
            "Epoch 50, Weights: [0.3349755  0.33126245 0.33376201]\n",
            "Epoch 100, Weights: [0.33660812 0.32919326 0.33419859]\n",
            "Epoch 150, Weights: [0.33826337 0.32708493 0.33465167]\n",
            "Epoch 200, Weights: [0.3399415  0.32493685 0.33512163]\n",
            "Epoch 250, Weights: [0.34164272 0.32274841 0.33560884]\n",
            "Epoch 300, Weights: [0.34336728 0.320519   0.33611369]\n",
            "Epoch 350, Weights: [0.3451154  0.31824799 0.33663658]\n",
            "Epoch 400, Weights: [0.34688731 0.31593477 0.33717789]\n",
            "Epoch 450, Weights: [0.34868323 0.3135787  0.33773804]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329766 0.33326813 0.33343418]\n",
            "Epoch 50, Weights: [0.33153866 0.3299843  0.33847701]\n",
            "Epoch 100, Weights: [0.3298268  0.32665304 0.34352014]\n",
            "Epoch 150, Weights: [0.32816222 0.32327364 0.3485641 ]\n",
            "Epoch 200, Weights: [0.32654509 0.3198454  0.35360948]\n",
            "Epoch 250, Weights: [0.32497556 0.31636758 0.35865683]\n",
            "Epoch 300, Weights: [0.32345382 0.31283941 0.36370675]\n",
            "Epoch 350, Weights: [0.32198004 0.3092601  0.36875982]\n",
            "Epoch 400, Weights: [0.32055445 0.30562886 0.37381666]\n",
            "Epoch 450, Weights: [0.31917725 0.30194485 0.37887787]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334096 0.3332795  0.33337951]\n",
            "Epoch 50, Weights: [0.33372179 0.33060687 0.33567131]\n",
            "Epoch 100, Weights: [0.33410086 0.32797011 0.337929  ]\n",
            "Epoch 150, Weights: [0.33447816 0.32536892 0.34015289]\n",
            "Epoch 200, Weights: [0.33485368 0.32280297 0.34234331]\n",
            "Epoch 250, Weights: [0.33522743 0.32027195 0.34450059]\n",
            "Epoch 300, Weights: [0.3355994  0.31777553 0.34662504]\n",
            "Epoch 350, Weights: [0.33596959 0.31531339 0.34871699]\n",
            "Epoch 400, Weights: [0.336338   0.31288519 0.35077678]\n",
            "Epoch 450, Weights: [0.33670463 0.3104906  0.35280473]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333289 0.3332783  0.33338878]\n",
            "Epoch 50, Weights: [0.33330966 0.33052395 0.33616637]\n",
            "Epoch 100, Weights: [0.33328334 0.32776309 0.33895354]\n",
            "Epoch 150, Weights: [0.33325394 0.32499601 0.34175002]\n",
            "Epoch 200, Weights: [0.33322143 0.32222299 0.34455555]\n",
            "Epoch 250, Weights: [0.33318579 0.31944433 0.34736984]\n",
            "Epoch 300, Weights: [0.33314703 0.31666032 0.35019262]\n",
            "Epoch 350, Weights: [0.33310511 0.31387126 0.35302361]\n",
            "Epoch 400, Weights: [0.33306003 0.31107743 0.35586251]\n",
            "Epoch 450, Weights: [0.33301179 0.30827914 0.35870905]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336392 0.33334918 0.33328687]\n",
            "Epoch 50, Weights: [0.33489248 0.33414729 0.3309602 ]\n",
            "Epoch 100, Weights: [0.33641835 0.33495568 0.32862594]\n",
            "Epoch 150, Weights: [0.3379415  0.33577438 0.32628409]\n",
            "Epoch 200, Weights: [0.33946187 0.33660345 0.32393466]\n",
            "Epoch 250, Weights: [0.3409794  0.33744293 0.32157764]\n",
            "Epoch 300, Weights: [0.34249404 0.33829288 0.31921306]\n",
            "Epoch 350, Weights: [0.34400573 0.33915333 0.31684091]\n",
            "Epoch 400, Weights: [0.34551442 0.34002435 0.3144612 ]\n",
            "Epoch 450, Weights: [0.34702005 0.34090596 0.31207395]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336613 0.33332421 0.33330964]\n",
            "Epoch 50, Weights: [0.33501239 0.33286847 0.33211911]\n",
            "Epoch 100, Weights: [0.33667052 0.33241296 0.33091649]\n",
            "Epoch 150, Weights: [0.33834067 0.33195764 0.32970166]\n",
            "Epoch 200, Weights: [0.34002299 0.33150249 0.32847449]\n",
            "Epoch 250, Weights: [0.34171762 0.33104748 0.32723486]\n",
            "Epoch 300, Weights: [0.34342472 0.3305926  0.32598264]\n",
            "Epoch 350, Weights: [0.34514445 0.33013782 0.3247177 ]\n",
            "Epoch 400, Weights: [0.34687696 0.32968312 0.32343989]\n",
            "Epoch 450, Weights: [0.34862241 0.32922846 0.3221491 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334431 0.3333381  0.33331756]\n",
            "Epoch 50, Weights: [0.33389272 0.33357883 0.33252842]\n",
            "Epoch 100, Weights: [0.33443943 0.33382366 0.33173688]\n",
            "Epoch 150, Weights: [0.33498443 0.33407261 0.33094293]\n",
            "Epoch 200, Weights: [0.33552772 0.33432575 0.3301465 ]\n",
            "Epoch 250, Weights: [0.33606929 0.3345831  0.32934758]\n",
            "Epoch 300, Weights: [0.33660912 0.33484473 0.32854612]\n",
            "Epoch 350, Weights: [0.33714723 0.33511068 0.32774207]\n",
            "Epoch 400, Weights: [0.33768359 0.33538098 0.3269354 ]\n",
            "Epoch 450, Weights: [0.3382182  0.33565571 0.32612607]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33325283 0.33333748 0.33340966]\n",
            "Epoch 50, Weights: [0.32924177 0.33352953 0.33722867]\n",
            "Epoch 100, Weights: [0.32525736 0.33369078 0.34105183]\n",
            "Epoch 150, Weights: [0.32129979 0.33382149 0.34487869]\n",
            "Epoch 200, Weights: [0.31736925 0.33392193 0.34870879]\n",
            "Epoch 250, Weights: [0.3134659  0.33399236 0.35254171]\n",
            "Epoch 300, Weights: [0.30958991 0.33403307 0.35637699]\n",
            "Epoch 350, Weights: [0.30574144 0.33404434 0.36021419]\n",
            "Epoch 400, Weights: [0.30192063 0.33402646 0.36405288]\n",
            "Epoch 450, Weights: [0.29812764 0.33397973 0.3678926 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332431 0.33330426 0.33337141]\n",
            "Epoch 50, Weights: [0.3328778  0.3318475  0.33527467]\n",
            "Epoch 100, Weights: [0.33243974 0.33038392 0.33717631]\n",
            "Epoch 150, Weights: [0.33201002 0.32891342 0.33907653]\n",
            "Epoch 200, Weights: [0.33158854 0.3274359  0.34097553]\n",
            "Epoch 250, Weights: [0.33117522 0.32595124 0.3428735 ]\n",
            "Epoch 300, Weights: [0.33076997 0.32445935 0.34477065]\n",
            "Epoch 350, Weights: [0.33037269 0.32296012 0.34666716]\n",
            "Epoch 400, Weights: [0.32998329 0.32145344 0.34856324]\n",
            "Epoch 450, Weights: [0.32960169 0.31993921 0.35045907]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332256 0.33329345 0.33338397]\n",
            "Epoch 50, Weights: [0.33279182 0.33130218 0.33590597]\n",
            "Epoch 100, Weights: [0.33227584 0.32931595 0.33840818]\n",
            "Epoch 150, Weights: [0.33177455 0.32733472 0.3408907 ]\n",
            "Epoch 200, Weights: [0.33128784 0.32535848 0.34335364]\n",
            "Epoch 250, Weights: [0.33081563 0.3233872  0.34579713]\n",
            "Epoch 300, Weights: [0.33035783 0.32142086 0.34822128]\n",
            "Epoch 350, Weights: [0.32991435 0.31945944 0.35062619]\n",
            "Epoch 400, Weights: [0.32948509 0.31750291 0.35301197]\n",
            "Epoch 450, Weights: [0.32906997 0.31555126 0.35537874]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329558 0.33334936 0.33335503]\n",
            "Epoch 50, Weights: [0.33140677 0.33415056 0.33444265]\n",
            "Epoch 100, Weights: [0.32951483 0.33495023 0.33553491]\n",
            "Epoch 150, Weights: [0.32761978 0.33574834 0.33663185]\n",
            "Epoch 200, Weights: [0.32572165 0.33654482 0.3377335 ]\n",
            "Epoch 250, Weights: [0.32382044 0.33733961 0.33883992]\n",
            "Epoch 300, Weights: [0.32191619 0.33813266 0.33995112]\n",
            "Epoch 350, Weights: [0.32000891 0.33892389 0.34106717]\n",
            "Epoch 400, Weights: [0.31809861 0.33971327 0.34218809]\n",
            "Epoch 450, Weights: [0.31618532 0.34050072 0.34331393]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332182 0.33325741 0.33342074]\n",
            "Epoch 50, Weights: [0.33274305 0.32944755 0.33780937]\n",
            "Epoch 100, Weights: [0.33215701 0.32560996 0.342233  ]\n",
            "Epoch 150, Weights: [0.33156362 0.32174418 0.34669217]\n",
            "Epoch 200, Weights: [0.3309628  0.31784977 0.3511874 ]\n",
            "Epoch 250, Weights: [0.33035443 0.31392628 0.35571926]\n",
            "Epoch 300, Weights: [0.32973843 0.30997324 0.3602883 ]\n",
            "Epoch 350, Weights: [0.32911471 0.30599017 0.36489509]\n",
            "Epoch 400, Weights: [0.32848315 0.3019766  0.36954022]\n",
            "Epoch 450, Weights: [0.32784366 0.29793203 0.37422429]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33341162 0.33335357 0.33323478]\n",
            "Epoch 50, Weights: [0.3373392  0.33435007 0.3283107 ]\n",
            "Epoch 100, Weights: [0.34129181 0.33531488 0.32339329]\n",
            "Epoch 150, Weights: [0.34526956 0.33624761 0.3184828 ]\n",
            "Epoch 200, Weights: [0.34927258 0.33714787 0.31357951]\n",
            "Epoch 250, Weights: [0.35330102 0.33801527 0.30868368]\n",
            "Epoch 300, Weights: [0.35735498 0.33884939 0.30379559]\n",
            "Epoch 350, Weights: [0.36143461 0.33964984 0.29891552]\n",
            "Epoch 400, Weights: [0.36554001 0.34041621 0.29404375]\n",
            "Epoch 450, Weights: [0.36967132 0.34114808 0.28918057]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337443 0.33329364 0.3333319 ]\n",
            "Epoch 50, Weights: [0.3354098  0.33132261 0.33326755]\n",
            "Epoch 100, Weights: [0.33740666 0.32937736 0.33321595]\n",
            "Epoch 150, Weights: [0.3393657  0.32745746 0.33317682]\n",
            "Epoch 200, Weights: [0.34128761 0.32556247 0.3331499 ]\n",
            "Epoch 250, Weights: [0.34317307 0.32369198 0.33313492]\n",
            "Epoch 300, Weights: [0.34502273 0.3218456  0.33313164]\n",
            "Epoch 350, Weights: [0.34683725 0.32002293 0.33313979]\n",
            "Epoch 400, Weights: [0.34861726 0.31822357 0.33315914]\n",
            "Epoch 450, Weights: [0.35036337 0.31644715 0.33318945]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333781 0.33335155 0.33331061]\n",
            "Epoch 50, Weights: [0.33356291 0.33426017 0.33217689]\n",
            "Epoch 100, Weights: [0.33378943 0.33516354 0.33104699]\n",
            "Epoch 150, Weights: [0.33401739 0.33606169 0.32992089]\n",
            "Epoch 200, Weights: [0.33424679 0.33695463 0.32879855]\n",
            "Epoch 250, Weights: [0.33447764 0.33784239 0.32767994]\n",
            "Epoch 300, Weights: [0.33470993 0.33872499 0.32656505]\n",
            "Epoch 350, Weights: [0.33494368 0.33960246 0.32545383]\n",
            "Epoch 400, Weights: [0.3351789  0.34047481 0.32434626]\n",
            "Epoch 450, Weights: [0.33541558 0.34134208 0.32324231]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335278 0.33332776 0.33331943]\n",
            "Epoch 50, Weights: [0.33432813 0.33304669 0.33262515]\n",
            "Epoch 100, Weights: [0.3353087  0.33275951 0.33193176]\n",
            "Epoch 150, Weights: [0.33629455 0.33246609 0.33123933]\n",
            "Epoch 200, Weights: [0.33728572 0.33216631 0.33054794]\n",
            "Epoch 250, Weights: [0.33828228 0.33186005 0.32985765]\n",
            "Epoch 300, Weights: [0.33928427 0.33154717 0.32916854]\n",
            "Epoch 350, Weights: [0.34029175 0.33122755 0.32848068]\n",
            "Epoch 400, Weights: [0.34130477 0.33090104 0.32779416]\n",
            "Epoch 450, Weights: [0.3423234  0.33056752 0.32710905]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33322146 0.33332666 0.33345185]\n",
            "Epoch 50, Weights: [0.32761601 0.33297817 0.33940579]\n",
            "Epoch 100, Weights: [0.321986   0.33259917 0.3454148 ]\n",
            "Epoch 150, Weights: [0.31633056 0.33218928 0.35148013]\n",
            "Epoch 200, Weights: [0.31064878 0.33174812 0.35760307]\n",
            "Epoch 250, Weights: [0.30493975 0.33127528 0.36378495]\n",
            "Epoch 300, Weights: [0.29920254 0.33077034 0.37002708]\n",
            "Epoch 350, Weights: [0.29343623 0.33023289 0.37633085]\n",
            "Epoch 400, Weights: [0.28763987 0.32966246 0.38269764]\n",
            "Epoch 450, Weights: [0.2818125  0.32905861 0.38912887]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336836 0.33330242 0.33332918]\n",
            "Epoch 50, Weights: [0.33511114 0.33175951 0.33312932]\n",
            "Epoch 100, Weights: [0.33683606 0.33022065 0.33294326]\n",
            "Epoch 150, Weights: [0.33854315 0.32868601 0.33277081]\n",
            "Epoch 200, Weights: [0.34023244 0.32715575 0.33261178]\n",
            "Epoch 250, Weights: [0.34190398 0.32563003 0.33246596]\n",
            "Epoch 300, Weights: [0.34355781 0.324109   0.33233317]\n",
            "Epoch 350, Weights: [0.34519397 0.32259281 0.3322132 ]\n",
            "Epoch 400, Weights: [0.34681252 0.3210816  0.33210585]\n",
            "Epoch 450, Weights: [0.3484135  0.31957553 0.33201094]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328217 0.33336423 0.33335358]\n",
            "Epoch 50, Weights: [0.33070923 0.33492257 0.33436817]\n",
            "Epoch 100, Weights: [0.32810652 0.33650698 0.33538647]\n",
            "Epoch 150, Weights: [0.32547378 0.33811784 0.33640835]\n",
            "Epoch 200, Weights: [0.32281074 0.33975554 0.33743368]\n",
            "Epoch 250, Weights: [0.32011715 0.34142048 0.33846234]\n",
            "Epoch 300, Weights: [0.31739273 0.34311305 0.33949419]\n",
            "Epoch 350, Weights: [0.31463723 0.34483365 0.34052909]\n",
            "Epoch 400, Weights: [0.31185037 0.3465827  0.3415669 ]\n",
            "Epoch 450, Weights: [0.3090319  0.3483606  0.34260747]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33341008 0.33323901 0.33335088]\n",
            "Epoch 50, Weights: [0.3372646  0.32850304 0.33423234]\n",
            "Epoch 100, Weights: [0.3411513  0.32372733 0.33512134]\n",
            "Epoch 150, Weights: [0.34507006 0.31891205 0.33601786]\n",
            "Epoch 200, Weights: [0.34902071 0.31405741 0.33692185]\n",
            "Epoch 250, Weights: [0.35300309 0.3091636  0.33783328]\n",
            "Epoch 300, Weights: [0.35701701 0.30423084 0.33875212]\n",
            "Epoch 350, Weights: [0.36106228 0.29925937 0.33967832]\n",
            "Epoch 400, Weights: [0.3651387  0.29424944 0.34061183]\n",
            "Epoch 450, Weights: [0.36924606 0.2892013  0.34155261]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333388 0.33321612 0.33344998]\n",
            "Epoch 50, Weights: [0.33336797 0.32737252 0.33925948]\n",
            "Epoch 100, Weights: [0.33341446 0.32156127 0.34502423]\n",
            "Epoch 150, Weights: [0.33347319 0.31578178 0.350745  ]\n",
            "Epoch 200, Weights: [0.33354398 0.31003345 0.35642254]\n",
            "Epoch 250, Weights: [0.33362668 0.30431573 0.36205757]\n",
            "Epoch 300, Weights: [0.33372112 0.29862804 0.36765081]\n",
            "Epoch 350, Weights: [0.33382715 0.29296983 0.37320299]\n",
            "Epoch 400, Weights: [0.33394462 0.28734055 0.3787148 ]\n",
            "Epoch 450, Weights: [0.33407339 0.28173966 0.38418693]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332254 0.33336428 0.33331315]\n",
            "Epoch 50, Weights: [0.33278133 0.33490951 0.33230914]\n",
            "Epoch 100, Weights: [0.33223623 0.33644917 0.33131456]\n",
            "Epoch 150, Weights: [0.33168733 0.33798317 0.33032947]\n",
            "Epoch 200, Weights: [0.3311347  0.33951137 0.3293539 ]\n",
            "Epoch 250, Weights: [0.33057841 0.34103367 0.32838789]\n",
            "Epoch 300, Weights: [0.33001854 0.34254996 0.32743147]\n",
            "Epoch 350, Weights: [0.32945517 0.34406011 0.32648468]\n",
            "Epoch 400, Weights: [0.32888838 0.34556403 0.32554756]\n",
            "Epoch 450, Weights: [0.32831823 0.3470616  0.32462013]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331345 0.33329037 0.33339615]\n",
            "Epoch 50, Weights: [0.33230675 0.33114636 0.33654686]\n",
            "Epoch 100, Weights: [0.33127415 0.32900975 0.33971607]\n",
            "Epoch 150, Weights: [0.33021559 0.32688085 0.34290352]\n",
            "Epoch 200, Weights: [0.32913104 0.32475998 0.34610895]\n",
            "Epoch 250, Weights: [0.32802045 0.32264742 0.34933209]\n",
            "Epoch 300, Weights: [0.32688381 0.3205435  0.35257266]\n",
            "Epoch 350, Weights: [0.32572107 0.31844851 0.35583038]\n",
            "Epoch 400, Weights: [0.32453224 0.31636277 0.35910496]\n",
            "Epoch 450, Weights: [0.3233173  0.31428658 0.36239609]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336391 0.33335561 0.33328045]\n",
            "Epoch 50, Weights: [0.33488918 0.33446166 0.33064913]\n",
            "Epoch 100, Weights: [0.33640635 0.33555126 0.32804236]\n",
            "Epoch 150, Weights: [0.33791546 0.33662449 0.32546003]\n",
            "Epoch 200, Weights: [0.33941652 0.33768141 0.32290205]\n",
            "Epoch 250, Weights: [0.34090955 0.33872211 0.32036831]\n",
            "Epoch 300, Weights: [0.34239458 0.33974667 0.31785873]\n",
            "Epoch 350, Weights: [0.34387163 0.34075515 0.31537319]\n",
            "Epoch 400, Weights: [0.34534073 0.34174766 0.31291158]\n",
            "Epoch 450, Weights: [0.34680191 0.34272426 0.3104738 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33325692 0.33335303 0.33339002]\n",
            "Epoch 50, Weights: [0.32942563 0.33433937 0.33623498]\n",
            "Epoch 100, Weights: [0.3255724  0.33532734 0.33910023]\n",
            "Epoch 150, Weights: [0.32169718 0.33631692 0.34198587]\n",
            "Epoch 200, Weights: [0.31779991 0.33730807 0.34489198]\n",
            "Epoch 250, Weights: [0.31388055 0.33830078 0.34781864]\n",
            "Epoch 300, Weights: [0.30993903 0.339295   0.35076594]\n",
            "Epoch 350, Weights: [0.3059753  0.34029072 0.35373395]\n",
            "Epoch 400, Weights: [0.30198931 0.3412879  0.35672276]\n",
            "Epoch 450, Weights: [0.29798101 0.34228651 0.35973246]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33325896 0.3334212  0.33331981]\n",
            "Epoch 50, Weights: [0.32954133 0.33780046 0.33265818]\n",
            "Epoch 100, Weights: [0.32582447 0.34215132 0.33202418]\n",
            "Epoch 150, Weights: [0.32210861 0.34647362 0.33141773]\n",
            "Epoch 200, Weights: [0.31839399 0.35076723 0.33083875]\n",
            "Epoch 250, Weights: [0.31468082 0.35503199 0.33028716]\n",
            "Epoch 300, Weights: [0.31096933 0.35926777 0.32976287]\n",
            "Epoch 350, Weights: [0.30725975 0.36347444 0.32926579]\n",
            "Epoch 400, Weights: [0.30355228 0.36765187 0.32879582]\n",
            "Epoch 450, Weights: [0.29984715 0.37179996 0.32835286]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33325583 0.33337663 0.33336751]\n",
            "Epoch 50, Weights: [0.32941347 0.33553425 0.33505225]\n",
            "Epoch 100, Weights: [0.32563383 0.33767723 0.33668892]\n",
            "Epoch 150, Weights: [0.32191643 0.33980551 0.33827803]\n",
            "Epoch 200, Weights: [0.3182608  0.34191908 0.3398201 ]\n",
            "Epoch 250, Weights: [0.3146664  0.34401791 0.34131566]\n",
            "Epoch 300, Weights: [0.31113273 0.346102   0.34276525]\n",
            "Epoch 350, Weights: [0.30765922 0.34817134 0.34416942]\n",
            "Epoch 400, Weights: [0.30424532 0.35022594 0.34552871]\n",
            "Epoch 450, Weights: [0.30089046 0.35226581 0.3468437 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329305 0.33329207 0.33341485]\n",
            "Epoch 50, Weights: [0.33127671 0.33125125 0.33747201]\n",
            "Epoch 100, Weights: [0.32925531 0.32925345 0.34149121]\n",
            "Epoch 150, Weights: [0.32722925 0.32729822 0.3454725 ]\n",
            "Epoch 200, Weights: [0.32519892 0.32538513 0.34941591]\n",
            "Epoch 250, Weights: [0.32316472 0.32351375 0.35332151]\n",
            "Epoch 300, Weights: [0.32112701 0.32168361 0.35718935]\n",
            "Epoch 350, Weights: [0.31908618 0.31989427 0.36101952]\n",
            "Epoch 400, Weights: [0.31704258 0.31814526 0.36481213]\n",
            "Epoch 450, Weights: [0.31499659 0.31643612 0.36856726]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333183  0.33332722 0.33335445]\n",
            "Epoch 50, Weights: [0.33256433 0.33302408 0.33441156]\n",
            "Epoch 100, Weights: [0.33180514 0.33272505 0.33546978]\n",
            "Epoch 150, Weights: [0.33104077 0.3324301  0.3365291 ]\n",
            "Epoch 200, Weights: [0.33027122 0.3321392  0.33758955]\n",
            "Epoch 250, Weights: [0.32949651 0.33185234 0.33865111]\n",
            "Epoch 300, Weights: [0.32871667 0.3315695  0.3397138 ]\n",
            "Epoch 350, Weights: [0.3279317  0.33129066 0.34077762]\n",
            "Epoch 400, Weights: [0.32714163 0.33101578 0.34184256]\n",
            "Epoch 450, Weights: [0.32634647 0.33074487 0.34290863]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332573 0.33326029 0.33341395]\n",
            "Epoch 50, Weights: [0.33294121 0.32961147 0.33744729]\n",
            "Epoch 100, Weights: [0.33254765 0.32596779 0.34148452]\n",
            "Epoch 150, Weights: [0.33214502 0.32232935 0.3455256 ]\n",
            "Epoch 200, Weights: [0.33173329 0.31869623 0.34957045]\n",
            "Epoch 250, Weights: [0.33131243 0.31506852 0.35361902]\n",
            "Epoch 300, Weights: [0.33088242 0.3114463  0.35767125]\n",
            "Epoch 350, Weights: [0.33044322 0.30782969 0.36172707]\n",
            "Epoch 400, Weights: [0.32999481 0.30421875 0.3657864 ]\n",
            "Epoch 450, Weights: [0.32953717 0.3006136  0.3698492 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331694 0.33323929 0.33344374]\n",
            "Epoch 50, Weights: [0.3325073  0.3285296  0.33896307]\n",
            "Epoch 100, Weights: [0.33171681 0.3238043  0.34447886]\n",
            "Epoch 150, Weights: [0.33094573 0.3190636  0.34999064]\n",
            "Epoch 200, Weights: [0.33019432 0.31430774 0.35549792]\n",
            "Epoch 250, Weights: [0.32946284 0.30953692 0.36100021]\n",
            "Epoch 300, Weights: [0.32875153 0.3047514  0.36649703]\n",
            "Epoch 350, Weights: [0.32806067 0.2999514  0.3719879 ]\n",
            "Epoch 400, Weights: [0.3273905  0.29513717 0.37747231]\n",
            "Epoch 450, Weights: [0.32674126 0.29030894 0.38294976]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33340462 0.3332461  0.33334925]\n",
            "Epoch 50, Weights: [0.33695326 0.32889312 0.33415359]\n",
            "Epoch 100, Weights: [0.34047089 0.32455603 0.33497305]\n",
            "Epoch 150, Weights: [0.34395786 0.32023468 0.33580744]\n",
            "Epoch 200, Weights: [0.3474145  0.31592893 0.33665654]\n",
            "Epoch 250, Weights: [0.35084116 0.31163866 0.33752015]\n",
            "Epoch 300, Weights: [0.35423819 0.30736372 0.33839806]\n",
            "Epoch 350, Weights: [0.35760592 0.30310397 0.33929008]\n",
            "Epoch 400, Weights: [0.36094469 0.29885929 0.34019599]\n",
            "Epoch 450, Weights: [0.36425484 0.29462952 0.34111561]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335775 0.33324343 0.3333988 ]\n",
            "Epoch 50, Weights: [0.33456654 0.32876162 0.3366718 ]\n",
            "Epoch 100, Weights: [0.33575138 0.32430541 0.33994318]\n",
            "Epoch 150, Weights: [0.33691244 0.31987458 0.34321295]\n",
            "Epoch 200, Weights: [0.33804992 0.31546895 0.34648111]\n",
            "Epoch 250, Weights: [0.33916399 0.31108832 0.34974766]\n",
            "Epoch 300, Weights: [0.34025486 0.30673248 0.35301262]\n",
            "Epoch 350, Weights: [0.34132271 0.30240125 0.35627601]\n",
            "Epoch 400, Weights: [0.34236772 0.29809442 0.35953783]\n",
            "Epoch 450, Weights: [0.34339008 0.29381179 0.36279809]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33341243 0.33321912 0.33336842]\n",
            "Epoch 50, Weights: [0.33734521 0.32752089 0.33513387]\n",
            "Epoch 100, Weights: [0.34123401 0.32184602 0.33691995]\n",
            "Epoch 150, Weights: [0.34507959 0.31619387 0.33872651]\n",
            "Epoch 200, Weights: [0.34888271 0.31056383 0.34055344]\n",
            "Epoch 250, Weights: [0.3526441  0.30495528 0.3424006 ]\n",
            "Epoch 300, Weights: [0.35636447 0.29936763 0.34426787]\n",
            "Epoch 350, Weights: [0.36004452 0.2938003  0.34615514]\n",
            "Epoch 400, Weights: [0.36368493 0.28825275 0.3480623 ]\n",
            "Epoch 450, Weights: [0.36728634 0.28272441 0.34998921]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33320171 0.33339282 0.33340544]\n",
            "Epoch 50, Weights: [0.32661754 0.33637138 0.33701105]\n",
            "Epoch 100, Weights: [0.32002553 0.33935745 0.34061698]\n",
            "Epoch 150, Weights: [0.31342484 0.34235141 0.34422372]\n",
            "Epoch 200, Weights: [0.30681461 0.34535363 0.34783173]\n",
            "Epoch 250, Weights: [0.30019399 0.34836448 0.3514415 ]\n",
            "Epoch 300, Weights: [0.29356213 0.35138435 0.35505349]\n",
            "Epoch 350, Weights: [0.28691815 0.35441363 0.3586682 ]\n",
            "Epoch 400, Weights: [0.28026119 0.35745269 0.36228609]\n",
            "Epoch 450, Weights: [0.27359037 0.36050193 0.36590767]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338361 0.33327629 0.33334007]\n",
            "Epoch 50, Weights: [0.33588925 0.33044512 0.3336656 ]\n",
            "Epoch 100, Weights: [0.33837852 0.32765351 0.33396795]\n",
            "Epoch 150, Weights: [0.3408518  0.32490103 0.33424715]\n",
            "Epoch 200, Weights: [0.34330949 0.32218724 0.33450324]\n",
            "Epoch 250, Weights: [0.34575198 0.31951173 0.33473625]\n",
            "Epoch 300, Weights: [0.34817968 0.31687407 0.33494622]\n",
            "Epoch 350, Weights: [0.35059295 0.31427385 0.33513317]\n",
            "Epoch 400, Weights: [0.35299221 0.31171064 0.33529712]\n",
            "Epoch 450, Weights: [0.35537783 0.30918406 0.33543808]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33326177 0.33330677 0.33343144]\n",
            "Epoch 50, Weights: [0.32966823 0.33198393 0.33834781]\n",
            "Epoch 100, Weights: [0.32604342 0.33067113 0.34328542]\n",
            "Epoch 150, Weights: [0.32238694 0.32936851 0.34824452]\n",
            "Epoch 200, Weights: [0.31869837 0.32807624 0.35322536]\n",
            "Epoch 250, Weights: [0.31497729 0.32679449 0.35822819]\n",
            "Epoch 300, Weights: [0.31122328 0.32552344 0.36325325]\n",
            "Epoch 350, Weights: [0.30743593 0.32426325 0.36830079]\n",
            "Epoch 400, Weights: [0.30361479 0.32301412 0.37337106]\n",
            "Epoch 450, Weights: [0.29975945 0.32177623 0.3784643 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329596 0.33337697 0.33332704]\n",
            "Epoch 50, Weights: [0.33142755 0.33555257 0.33301985]\n",
            "Epoch 100, Weights: [0.32955869 0.33771538 0.3327259 ]\n",
            "Epoch 150, Weights: [0.32768926 0.33986564 0.33244507]\n",
            "Epoch 200, Weights: [0.32581915 0.3420036  0.33217722]\n",
            "Epoch 250, Weights: [0.32394821 0.3441295  0.33192226]\n",
            "Epoch 300, Weights: [0.32207633 0.34624358 0.33168006]\n",
            "Epoch 350, Weights: [0.32020339 0.34834606 0.33145052]\n",
            "Epoch 400, Weights: [0.31832926 0.35043719 0.33123352]\n",
            "Epoch 450, Weights: [0.31645383 0.35251718 0.33102896]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3332319  0.33335203 0.33341604]\n",
            "Epoch 50, Weights: [0.32814661 0.33430972 0.33754363]\n",
            "Epoch 100, Weights: [0.32303334 0.33531122 0.34165541]\n",
            "Epoch 150, Weights: [0.31789202 0.33635654 0.34575141]\n",
            "Epoch 200, Weights: [0.31272256 0.3374457  0.34983171]\n",
            "Epoch 250, Weights: [0.30752488 0.33857871 0.35389638]\n",
            "Epoch 300, Weights: [0.30229888 0.33975561 0.35794548]\n",
            "Epoch 350, Weights: [0.29704446 0.3409764  0.36197911]\n",
            "Epoch 400, Weights: [0.2917615  0.34224112 0.36599736]\n",
            "Epoch 450, Weights: [0.28644986 0.34354978 0.37000033]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330151 0.33336288 0.33333558]\n",
            "Epoch 50, Weights: [0.33170917 0.3348436  0.3334472 ]\n",
            "Epoch 100, Weights: [0.33011354 0.33633029 0.33355614]\n",
            "Epoch 150, Weights: [0.32851454 0.33782299 0.33366244]\n",
            "Epoch 200, Weights: [0.32691212 0.33932175 0.3337661 ]\n",
            "Epoch 250, Weights: [0.32530622 0.34082658 0.33386717]\n",
            "Epoch 300, Weights: [0.32369678 0.34233754 0.33396566]\n",
            "Epoch 350, Weights: [0.32208372 0.34385465 0.3340616 ]\n",
            "Epoch 400, Weights: [0.32046699 0.34537796 0.33415502]\n",
            "Epoch 450, Weights: [0.31884652 0.34690752 0.33424593]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336232 0.33335704 0.33328061]\n",
            "Epoch 50, Weights: [0.33482662 0.33453807 0.33063528]\n",
            "Epoch 100, Weights: [0.33631975 0.33570951 0.32797071]\n",
            "Epoch 150, Weights: [0.33784197 0.33687113 0.32528687]\n",
            "Epoch 200, Weights: [0.33939358 0.33802265 0.32258374]\n",
            "Epoch 250, Weights: [0.34097485 0.33916384 0.31986128]\n",
            "Epoch 300, Weights: [0.34258606 0.34029442 0.31711948]\n",
            "Epoch 350, Weights: [0.34422751 0.34141416 0.31435831]\n",
            "Epoch 400, Weights: [0.34589946 0.34252277 0.31157774]\n",
            "Epoch 450, Weights: [0.34760219 0.34362001 0.30877776]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336516 0.3332978  0.333337  ]\n",
            "Epoch 50, Weights: [0.33495852 0.33153214 0.33350932]\n",
            "Epoch 100, Weights: [0.33655439 0.32978653 0.33365904]\n",
            "Epoch 150, Weights: [0.33815273 0.32806076 0.33378648]\n",
            "Epoch 200, Weights: [0.33975347 0.32635457 0.33389193]\n",
            "Epoch 250, Weights: [0.34135654 0.32466774 0.33397568]\n",
            "Epoch 300, Weights: [0.3429619  0.32300003 0.33403804]\n",
            "Epoch 350, Weights: [0.34456949 0.3213512  0.33407928]\n",
            "Epoch 400, Weights: [0.34617924 0.31972103 0.3340997 ]\n",
            "Epoch 450, Weights: [0.3477911  0.31810928 0.33409959]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3332991  0.33340915 0.33329173]\n",
            "Epoch 50, Weights: [0.33159844 0.33718457 0.33121696]\n",
            "Epoch 100, Weights: [0.32991869 0.34092896 0.32915233]\n",
            "Epoch 150, Weights: [0.32825968 0.34464239 0.3270979 ]\n",
            "Epoch 200, Weights: [0.32662122 0.34832499 0.32505376]\n",
            "Epoch 250, Weights: [0.32500314 0.35197684 0.32301998]\n",
            "Epoch 300, Weights: [0.32340526 0.35559806 0.32099664]\n",
            "Epoch 350, Weights: [0.3218274  0.35918877 0.31898381]\n",
            "Epoch 400, Weights: [0.32026937 0.36274906 0.31698154]\n",
            "Epoch 450, Weights: [0.318731   0.36627906 0.31498991]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333259  0.33335193 0.33332214]\n",
            "Epoch 50, Weights: [0.33295177 0.33428105 0.33276716]\n",
            "Epoch 100, Weights: [0.33257142 0.33520802 0.33222052]\n",
            "Epoch 150, Weights: [0.33218481 0.3361329  0.33168226]\n",
            "Epoch 200, Weights: [0.33179185 0.33705575 0.33115237]\n",
            "Epoch 250, Weights: [0.33139248 0.3379766  0.33063089]\n",
            "Epoch 300, Weights: [0.33098664 0.33889551 0.33011782]\n",
            "Epoch 350, Weights: [0.33057425 0.33981254 0.32961318]\n",
            "Epoch 400, Weights: [0.33015524 0.34072773 0.329117  ]\n",
            "Epoch 450, Weights: [0.32972954 0.34164114 0.32862929]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336048 0.33331822 0.33332127]\n",
            "Epoch 50, Weights: [0.33471823 0.33256062 0.33272112]\n",
            "Epoch 100, Weights: [0.33607627 0.33179815 0.33212556]\n",
            "Epoch 150, Weights: [0.33743459 0.33103082 0.33153456]\n",
            "Epoch 200, Weights: [0.33879322 0.33025864 0.33094811]\n",
            "Epoch 250, Weights: [0.34015214 0.32948163 0.3303662 ]\n",
            "Epoch 300, Weights: [0.34151137 0.32869981 0.3297888 ]\n",
            "Epoch 350, Weights: [0.3428709  0.32791318 0.32921589]\n",
            "Epoch 400, Weights: [0.34423075 0.32712176 0.32864746]\n",
            "Epoch 450, Weights: [0.34559092 0.32632556 0.32808349]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3334061  0.33324779 0.33334609]\n",
            "Epoch 50, Weights: [0.3370601  0.32895082 0.33398905]\n",
            "Epoch 100, Weights: [0.34074393 0.32461448 0.33464157]\n",
            "Epoch 150, Weights: [0.34445759 0.32023868 0.33530371]\n",
            "Epoch 200, Weights: [0.34820107 0.31582336 0.33597553]\n",
            "Epoch 250, Weights: [0.35197439 0.31136849 0.33665709]\n",
            "Epoch 300, Weights: [0.35577751 0.306874   0.33734845]\n",
            "Epoch 350, Weights: [0.35961043 0.30233987 0.33804967]\n",
            "Epoch 400, Weights: [0.36347312 0.29776605 0.3387608 ]\n",
            "Epoch 450, Weights: [0.36736555 0.29315254 0.33948188]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330046 0.33338061 0.33331889]\n",
            "Epoch 50, Weights: [0.33163649 0.33576259 0.33260089]\n",
            "Epoch 100, Weights: [0.32993072 0.33817934 0.33188991]\n",
            "Epoch 150, Weights: [0.32818231 0.34063128 0.33118638]\n",
            "Epoch 200, Weights: [0.32639042 0.34311885 0.33049069]\n",
            "Epoch 250, Weights: [0.32455419 0.3456425  0.32980328]\n",
            "Epoch 300, Weights: [0.32267273 0.34820266 0.32912458]\n",
            "Epoch 350, Weights: [0.32074515 0.35079979 0.32845503]\n",
            "Epoch 400, Weights: [0.31877055 0.35343433 0.32779509]\n",
            "Epoch 450, Weights: [0.31674801 0.35610675 0.32714522]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329885 0.33327507 0.33342605]\n",
            "Epoch 50, Weights: [0.33155643 0.3303769  0.33806665]\n",
            "Epoch 100, Weights: [0.32977651 0.32750764 0.34271583]\n",
            "Epoch 150, Weights: [0.32795811 0.32466745 0.34737441]\n",
            "Epoch 200, Weights: [0.32610022 0.32185652 0.35204323]\n",
            "Epoch 250, Weights: [0.3242018  0.31907503 0.35672314]\n",
            "Epoch 300, Weights: [0.32226178 0.31632318 0.36141501]\n",
            "Epoch 350, Weights: [0.32027904 0.31360119 0.36611974]\n",
            "Epoch 400, Weights: [0.31825245 0.31090929 0.37083824]\n",
            "Epoch 450, Weights: [0.31618083 0.3082477  0.37557143]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33323567 0.33326916 0.33349514]\n",
            "Epoch 50, Weights: [0.32837359 0.33005084 0.34157554]\n",
            "Epoch 100, Weights: [0.32355302 0.32681343 0.34963352]\n",
            "Epoch 150, Weights: [0.31877543 0.32355834 0.3576662 ]\n",
            "Epoch 200, Weights: [0.31404222 0.32028702 0.36567073]\n",
            "Epoch 250, Weights: [0.30935477 0.31700094 0.37364426]\n",
            "Epoch 300, Weights: [0.30471442 0.31370155 0.381584  ]\n",
            "Epoch 350, Weights: [0.30012247 0.31039033 0.38948718]\n",
            "Epoch 400, Weights: [0.29558015 0.30706876 0.39735106]\n",
            "Epoch 450, Weights: [0.29108869 0.30373832 0.40517296]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33326456 0.33335771 0.3333777 ]\n",
            "Epoch 50, Weights: [0.3297982  0.33459664 0.33560514]\n",
            "Epoch 100, Weights: [0.32627627 0.33587441 0.33784929]\n",
            "Epoch 150, Weights: [0.32269821 0.33719156 0.3401102 ]\n",
            "Epoch 200, Weights: [0.31906348 0.33854859 0.3423879 ]\n",
            "Epoch 250, Weights: [0.31537154 0.33994603 0.3446824 ]\n",
            "Epoch 300, Weights: [0.31162184 0.34138439 0.34699375]\n",
            "Epoch 350, Weights: [0.30781384 0.34286418 0.34932194]\n",
            "Epoch 400, Weights: [0.30394703 0.34438593 0.35166701]\n",
            "Epoch 450, Weights: [0.30002088 0.34595014 0.35402895]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337576 0.3332835  0.33334072]\n",
            "Epoch 50, Weights: [0.33549771 0.33079219 0.33371007]\n",
            "Epoch 100, Weights: [0.33762052 0.32830091 0.33407854]\n",
            "Epoch 150, Weights: [0.33974443 0.32580936 0.33444618]\n",
            "Epoch 200, Weights: [0.34186966 0.32331729 0.33481302]\n",
            "Epoch 250, Weights: [0.34399645 0.32082442 0.33517911]\n",
            "Epoch 300, Weights: [0.34612503 0.31833047 0.33554447]\n",
            "Epoch 350, Weights: [0.34825565 0.31583517 0.33590915]\n",
            "Epoch 400, Weights: [0.35038853 0.31333825 0.33627318]\n",
            "Epoch 450, Weights: [0.35252393 0.31083944 0.33663661]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334361 0.33330818 0.33334818]\n",
            "Epoch 50, Weights: [0.3338577  0.33204859 0.33409369]\n",
            "Epoch 100, Weights: [0.33437104 0.33078454 0.33484439]\n",
            "Epoch 150, Weights: [0.33488361 0.32951602 0.33560034]\n",
            "Epoch 200, Weights: [0.33539537 0.32824302 0.33636158]\n",
            "Epoch 250, Weights: [0.3359063  0.32696552 0.33712815]\n",
            "Epoch 300, Weights: [0.33641637 0.3256835  0.3379001 ]\n",
            "Epoch 350, Weights: [0.33692554 0.32439696 0.33867747]\n",
            "Epoch 400, Weights: [0.33743378 0.32310589 0.3394603 ]\n",
            "Epoch 450, Weights: [0.33794106 0.32181026 0.34024865]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335194 0.33328369 0.33336435]\n",
            "Epoch 50, Weights: [0.33427877 0.33079895 0.33492225]\n",
            "Epoch 100, Weights: [0.33519792 0.32830855 0.3364935 ]\n",
            "Epoch 150, Weights: [0.33610937 0.32581249 0.33807812]\n",
            "Epoch 200, Weights: [0.33701308 0.32331076 0.33967613]\n",
            "Epoch 250, Weights: [0.33790904 0.32080337 0.34128756]\n",
            "Epoch 300, Weights: [0.33879723 0.31829032 0.34291242]\n",
            "Epoch 350, Weights: [0.3396776  0.31577162 0.34455075]\n",
            "Epoch 400, Weights: [0.34055016 0.31324727 0.34620255]\n",
            "Epoch 450, Weights: [0.34141485 0.31071727 0.34786785]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329955 0.33330406 0.33339636]\n",
            "Epoch 50, Weights: [0.33161383 0.33183159 0.33655455]\n",
            "Epoch 100, Weights: [0.32993415 0.33034039 0.33972543]\n",
            "Epoch 150, Weights: [0.32826076 0.3288305  0.3429087 ]\n",
            "Epoch 200, Weights: [0.32659394 0.32730195 0.34610408]\n",
            "Epoch 250, Weights: [0.32493393 0.32575477 0.34931126]\n",
            "Epoch 300, Weights: [0.32328101 0.324189   0.35252996]\n",
            "Epoch 350, Weights: [0.32163541 0.32260468 0.35575987]\n",
            "Epoch 400, Weights: [0.31999741 0.32100186 0.3590007 ]\n",
            "Epoch 450, Weights: [0.31836725 0.31938058 0.36225214]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330666 0.33329829 0.33339501]\n",
            "Epoch 50, Weights: [0.33197877 0.3315376  0.3364836 ]\n",
            "Epoch 100, Weights: [0.33066075 0.32975901 0.3395802 ]\n",
            "Epoch 150, Weights: [0.32935264 0.32796243 0.3426849 ]\n",
            "Epoch 200, Weights: [0.32805445 0.32614775 0.34579776]\n",
            "Epoch 250, Weights: [0.32676622 0.32431488 0.34891887]\n",
            "Epoch 300, Weights: [0.32548796 0.32246371 0.3520483 ]\n",
            "Epoch 350, Weights: [0.32421971 0.32059413 0.35518613]\n",
            "Epoch 400, Weights: [0.32296149 0.31870605 0.35833243]\n",
            "Epoch 450, Weights: [0.32171333 0.31679936 0.36148727]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329592 0.33340544 0.33329862]\n",
            "Epoch 50, Weights: [0.3314246  0.33700883 0.33156654]\n",
            "Epoch 100, Weights: [0.32955134 0.34060763 0.32984101]\n",
            "Epoch 150, Weights: [0.32767621 0.34420164 0.32812212]\n",
            "Epoch 200, Weights: [0.32579929 0.3477907  0.32640998]\n",
            "Epoch 250, Weights: [0.32392066 0.35137461 0.3247047 ]\n",
            "Epoch 300, Weights: [0.32204041 0.3549532  0.32300636]\n",
            "Epoch 350, Weights: [0.3201586  0.3585263  0.32131507]\n",
            "Epoch 400, Weights: [0.31827533 0.36209371 0.31963092]\n",
            "Epoch 450, Weights: [0.31639067 0.36565527 0.31795402]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338023 0.33333089 0.33328885]\n",
            "Epoch 50, Weights: [0.33572417 0.33321174 0.33106406]\n",
            "Epoch 100, Weights: [0.33806509 0.33309768 0.3288372 ]\n",
            "Epoch 150, Weights: [0.34040306 0.33298871 0.3266082 ]\n",
            "Epoch 200, Weights: [0.34273816 0.33288483 0.32437698]\n",
            "Epoch 250, Weights: [0.34507045 0.33278605 0.32214348]\n",
            "Epoch 300, Weights: [0.34739999 0.33269235 0.31990762]\n",
            "Epoch 350, Weights: [0.34972687 0.33260375 0.31766936]\n",
            "Epoch 400, Weights: [0.35205113 0.33252024 0.3154286 ]\n",
            "Epoch 450, Weights: [0.35437284 0.33244183 0.3131853 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332357 0.33334054 0.33333586]\n",
            "Epoch 50, Weights: [0.33283329 0.33370217 0.33346452]\n",
            "Epoch 100, Weights: [0.33233806 0.33406542 0.3335965 ]\n",
            "Epoch 150, Weights: [0.33183784 0.3344303  0.33373183]\n",
            "Epoch 200, Weights: [0.33133259 0.33479685 0.33387053]\n",
            "Epoch 250, Weights: [0.33082227 0.33516506 0.33401263]\n",
            "Epoch 300, Weights: [0.33030683 0.33553498 0.33415816]\n",
            "Epoch 350, Weights: [0.32978624 0.33590661 0.33430712]\n",
            "Epoch 400, Weights: [0.32926044 0.33627998 0.33445955]\n",
            "Epoch 450, Weights: [0.32872939 0.3366551  0.33461547]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335421 0.333315   0.33333075]\n",
            "Epoch 50, Weights: [0.33439879 0.33240201 0.33319918]\n",
            "Epoch 100, Weights: [0.33544369 0.33149473 0.33306155]\n",
            "Epoch 150, Weights: [0.33648887 0.33059316 0.33291794]\n",
            "Epoch 200, Weights: [0.33753426 0.32969731 0.3327684 ]\n",
            "Epoch 250, Weights: [0.33857984 0.32880715 0.33261299]\n",
            "Epoch 300, Weights: [0.33962553 0.32792268 0.33245176]\n",
            "Epoch 350, Weights: [0.34067129 0.32704389 0.33228479]\n",
            "Epoch 400, Weights: [0.34171707 0.32617078 0.33211212]\n",
            "Epoch 450, Weights: [0.34276282 0.32530333 0.33193382]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33341224 0.33331365 0.33327408]\n",
            "Epoch 50, Weights: [0.33735271 0.33234931 0.33029795]\n",
            "Epoch 100, Weights: [0.3412828  0.33142208 0.32729509]\n",
            "Epoch 150, Weights: [0.34520262 0.33053155 0.32426581]\n",
            "Epoch 200, Weights: [0.34911226 0.32967729 0.32121042]\n",
            "Epoch 250, Weights: [0.35301184 0.32885889 0.31812924]\n",
            "Epoch 300, Weights: [0.35690147 0.32807594 0.31502256]\n",
            "Epoch 350, Weights: [0.36078125 0.32732803 0.31189069]\n",
            "Epoch 400, Weights: [0.36465131 0.32661476 0.30873389]\n",
            "Epoch 450, Weights: [0.36851177 0.32593573 0.30555247]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3332881  0.3333479  0.33336397]\n",
            "Epoch 50, Weights: [0.33101974 0.33408182 0.33489841]\n",
            "Epoch 100, Weights: [0.32873761 0.33482561 0.33643675]\n",
            "Epoch 150, Weights: [0.32644163 0.33557938 0.33797897]\n",
            "Epoch 200, Weights: [0.32413172 0.33634319 0.33952506]\n",
            "Epoch 250, Weights: [0.32180781 0.33711716 0.34107501]\n",
            "Epoch 300, Weights: [0.31946981 0.33790135 0.34262881]\n",
            "Epoch 350, Weights: [0.31711765 0.33869588 0.34418644]\n",
            "Epoch 400, Weights: [0.31475125 0.33950082 0.34574789]\n",
            "Epoch 450, Weights: [0.31237053 0.34031628 0.34731316]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335553 0.33331655 0.33332788]\n",
            "Epoch 50, Weights: [0.33447272 0.33247199 0.33305527]\n",
            "Epoch 100, Weights: [0.3356031  0.3316156  0.33278128]\n",
            "Epoch 150, Weights: [0.33674679 0.33074726 0.33250591]\n",
            "Epoch 200, Weights: [0.33790393 0.32986686 0.33222918]\n",
            "Epoch 250, Weights: [0.33907464 0.32897425 0.33195108]\n",
            "Epoch 300, Weights: [0.34025904 0.32806932 0.33167161]\n",
            "Epoch 350, Weights: [0.34145726 0.32715193 0.33139078]\n",
            "Epoch 400, Weights: [0.34266943 0.32622196 0.33110858]\n",
            "Epoch 450, Weights: [0.34389566 0.32527928 0.33082503]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334574 0.3333266  0.33332763]\n",
            "Epoch 50, Weights: [0.33396584 0.33299187 0.33304227]\n",
            "Epoch 100, Weights: [0.334585   0.33265951 0.33275546]\n",
            "Epoch 150, Weights: [0.33520326 0.3323295  0.33246722]\n",
            "Epoch 200, Weights: [0.33582064 0.3320018  0.33217752]\n",
            "Epoch 250, Weights: [0.33643719 0.3316764  0.33188638]\n",
            "Epoch 300, Weights: [0.33705295 0.33135325 0.33159377]\n",
            "Epoch 350, Weights: [0.33766793 0.33103233 0.33129971]\n",
            "Epoch 400, Weights: [0.33828219 0.3307136  0.33100418]\n",
            "Epoch 450, Weights: [0.33889575 0.33039704 0.33070719]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33325891 0.33338157 0.33335949]\n",
            "Epoch 50, Weights: [0.32952951 0.33579665 0.33467381]\n",
            "Epoch 100, Weights: [0.32578308 0.33821736 0.33599953]\n",
            "Epoch 150, Weights: [0.32201926 0.34064395 0.33733676]\n",
            "Epoch 200, Weights: [0.31823769 0.34307664 0.33868564]\n",
            "Epoch 250, Weights: [0.314438   0.34551567 0.3400463 ]\n",
            "Epoch 300, Weights: [0.31061984 0.34796126 0.34141887]\n",
            "Epoch 350, Weights: [0.30678283 0.35041367 0.34280347]\n",
            "Epoch 400, Weights: [0.3029266  0.35287312 0.34420026]\n",
            "Epoch 450, Weights: [0.29905076 0.35533985 0.34560936]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33342554 0.33322437 0.33335007]\n",
            "Epoch 50, Weights: [0.33804972 0.32777329 0.33417695]\n",
            "Epoch 100, Weights: [0.34270107 0.32231507 0.33498383]\n",
            "Epoch 150, Weights: [0.34738065 0.31684839 0.33577093]\n",
            "Epoch 200, Weights: [0.35208954 0.31137193 0.33653849]\n",
            "Epoch 250, Weights: [0.35682884 0.3058844  0.33728673]\n",
            "Epoch 300, Weights: [0.36159966 0.30038446 0.33801585]\n",
            "Epoch 350, Weights: [0.36640313 0.29487078 0.33872606]\n",
            "Epoch 400, Weights: [0.37124041 0.28934203 0.33941752]\n",
            "Epoch 450, Weights: [0.37611267 0.28379686 0.34009043]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333641  0.33333183 0.33330404]\n",
            "Epoch 50, Weights: [0.33491105 0.33325033 0.33183858]\n",
            "Epoch 100, Weights: [0.33647379 0.3331553  0.33037088]\n",
            "Epoch 150, Weights: [0.33805242 0.3330466  0.32890096]\n",
            "Epoch 200, Weights: [0.33964705 0.33292408 0.32742885]\n",
            "Epoch 250, Weights: [0.34125779 0.33278761 0.32595458]\n",
            "Epoch 300, Weights: [0.34288475 0.33263705 0.32447818]\n",
            "Epoch 350, Weights: [0.34452804 0.33247225 0.32299967]\n",
            "Epoch 400, Weights: [0.34618778 0.33229309 0.3215191 ]\n",
            "Epoch 450, Weights: [0.34786409 0.33209941 0.32003647]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333304 0.33333404 0.33333288]\n",
            "Epoch 50, Weights: [0.33331901 0.33336997 0.33331099]\n",
            "Epoch 100, Weights: [0.33330479 0.33340594 0.33328924]\n",
            "Epoch 150, Weights: [0.33329038 0.33344195 0.33326763]\n",
            "Epoch 200, Weights: [0.3332758  0.333478   0.33324616]\n",
            "Epoch 250, Weights: [0.33326104 0.33351409 0.33322483]\n",
            "Epoch 300, Weights: [0.33324611 0.33355022 0.33320364]\n",
            "Epoch 350, Weights: [0.333231   0.33358639 0.33318258]\n",
            "Epoch 400, Weights: [0.33321571 0.3336226  0.33316166]\n",
            "Epoch 450, Weights: [0.33320025 0.33365885 0.33314086]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33326822 0.33339797 0.33333378]\n",
            "Epoch 50, Weights: [0.3300047  0.33663143 0.33336384]\n",
            "Epoch 100, Weights: [0.32672432 0.3398675  0.33340815]\n",
            "Epoch 150, Weights: [0.32342682 0.34310647 0.33346668]\n",
            "Epoch 200, Weights: [0.32011194 0.34634866 0.33353937]\n",
            "Epoch 250, Weights: [0.31677943 0.34959436 0.33362618]\n",
            "Epoch 300, Weights: [0.31342901 0.35284388 0.33372709]\n",
            "Epoch 350, Weights: [0.31006041 0.35609751 0.33384205]\n",
            "Epoch 400, Weights: [0.30667337 0.35935557 0.33397103]\n",
            "Epoch 450, Weights: [0.30326761 0.36261835 0.334114  ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33327624 0.3333492  0.33337453]\n",
            "Epoch 50, Weights: [0.33041067 0.33414464 0.33544466]\n",
            "Epoch 100, Weights: [0.32752271 0.3349429  0.33753436]\n",
            "Epoch 150, Weights: [0.32461238 0.33574396 0.33964362]\n",
            "Epoch 200, Weights: [0.32167971 0.3365478  0.34177246]\n",
            "Epoch 250, Weights: [0.31872473 0.33735437 0.34392087]\n",
            "Epoch 300, Weights: [0.31574749 0.33816364 0.34608884]\n",
            "Epoch 350, Weights: [0.31274803 0.33897559 0.34827635]\n",
            "Epoch 400, Weights: [0.3097264  0.33979016 0.3504834 ]\n",
            "Epoch 450, Weights: [0.30668267 0.34060734 0.35270996]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328977 0.33335965 0.33335054]\n",
            "Epoch 50, Weights: [0.33111558 0.33466524 0.33421915]\n",
            "Epoch 100, Weights: [0.32894819 0.33594936 0.33510242]\n",
            "Epoch 150, Weights: [0.32678787 0.33721206 0.33600004]\n",
            "Epoch 200, Weights: [0.32463485 0.33845341 0.33691171]\n",
            "Epoch 250, Weights: [0.32248939 0.33967346 0.33783713]\n",
            "Epoch 300, Weights: [0.32035172 0.34087227 0.33877598]\n",
            "Epoch 350, Weights: [0.3182221  0.34204991 0.33972796]\n",
            "Epoch 400, Weights: [0.31610075 0.34320647 0.34069275]\n",
            "Epoch 450, Weights: [0.31398791 0.34434201 0.34167005]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332939 0.33334948 0.3333211 ]\n",
            "Epoch 50, Weights: [0.33313165 0.33415955 0.33270878]\n",
            "Epoch 100, Weights: [0.33293188 0.33497421 0.33209388]\n",
            "Epoch 150, Weights: [0.33273007 0.33579349 0.33147641]\n",
            "Epoch 200, Weights: [0.3325262  0.33661742 0.33085634]\n",
            "Epoch 250, Weights: [0.33232027 0.33744602 0.33023368]\n",
            "Epoch 300, Weights: [0.33211225 0.33827932 0.32960839]\n",
            "Epoch 350, Weights: [0.33190214 0.33911735 0.32898048]\n",
            "Epoch 400, Weights: [0.33168993 0.33996012 0.32834992]\n",
            "Epoch 450, Weights: [0.33147559 0.34080766 0.32771672]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336092 0.33332574 0.33331331]\n",
            "Epoch 50, Weights: [0.33475342 0.33294314 0.33230341]\n",
            "Epoch 100, Weights: [0.33617067 0.33255382 0.33127548]\n",
            "Epoch 150, Weights: [0.33761322 0.33215763 0.33022912]\n",
            "Epoch 200, Weights: [0.33908165 0.33175441 0.32916391]\n",
            "Epoch 250, Weights: [0.34057653 0.33134401 0.32807943]\n",
            "Epoch 300, Weights: [0.34209844 0.33092627 0.32697526]\n",
            "Epoch 350, Weights: [0.34364802 0.33050101 0.32585094]\n",
            "Epoch 400, Weights: [0.34522589 0.33006807 0.32470601]\n",
            "Epoch 450, Weights: [0.34683269 0.32962728 0.32354   ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333798 0.3333937  0.33326829]\n",
            "Epoch 50, Weights: [0.33356394 0.33640384 0.33003219]\n",
            "Epoch 100, Weights: [0.33377624 0.33939714 0.32682659]\n",
            "Epoch 150, Weights: [0.3339751  0.342374   0.32365088]\n",
            "Epoch 200, Weights: [0.33416073 0.3453348  0.32050444]\n",
            "Epoch 250, Weights: [0.33433334 0.34827993 0.3173867 ]\n",
            "Epoch 300, Weights: [0.33449313 0.35120977 0.31429707]\n",
            "Epoch 350, Weights: [0.33464029 0.35412469 0.31123499]\n",
            "Epoch 400, Weights: [0.33477502 0.35702504 0.30819991]\n",
            "Epoch 450, Weights: [0.33489749 0.35991117 0.3051913 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329281 0.33331499 0.33339217]\n",
            "Epoch 50, Weights: [0.33125326 0.33240246 0.33634424]\n",
            "Epoch 100, Weights: [0.32918631 0.331498   0.33931566]\n",
            "Epoch 150, Weights: [0.32709174 0.33060196 0.34230628]\n",
            "Epoch 200, Weights: [0.32496934 0.32971471 0.34531593]\n",
            "Epoch 250, Weights: [0.3228189  0.32883662 0.34834445]\n",
            "Epoch 300, Weights: [0.32064023 0.32796807 0.35139167]\n",
            "Epoch 350, Weights: [0.31843312 0.32710944 0.35445741]\n",
            "Epoch 400, Weights: [0.31619737 0.32626111 0.35754149]\n",
            "Epoch 450, Weights: [0.31393279 0.32542347 0.36064371]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330639 0.33340624 0.33328734]\n",
            "Epoch 50, Weights: [0.33197029 0.33702487 0.33100481]\n",
            "Epoch 100, Weights: [0.33065444 0.34059113 0.3287544 ]\n",
            "Epoch 150, Weights: [0.3293585  0.34410586 0.32653561]\n",
            "Epoch 200, Weights: [0.32808216 0.34756988 0.32434794]\n",
            "Epoch 250, Weights: [0.32682508 0.350984   0.32219089]\n",
            "Epoch 300, Weights: [0.32558696 0.35434902 0.32006399]\n",
            "Epoch 350, Weights: [0.32436748 0.35766573 0.31796676]\n",
            "Epoch 400, Weights: [0.32316635 0.36093489 0.31589873]\n",
            "Epoch 450, Weights: [0.32198327 0.36415724 0.31385946]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331971 0.3333427  0.33333756]\n",
            "Epoch 50, Weights: [0.33264004 0.33381058 0.33354936]\n",
            "Epoch 100, Weights: [0.33196217 0.33427632 0.33376147]\n",
            "Epoch 150, Weights: [0.33128612 0.33473993 0.33397391]\n",
            "Epoch 200, Weights: [0.3306119  0.3352014  0.33418667]\n",
            "Epoch 250, Weights: [0.32993951 0.33566073 0.33439974]\n",
            "Epoch 300, Weights: [0.32926895 0.3361179  0.33461313]\n",
            "Epoch 350, Weights: [0.32860023 0.33657291 0.33482683]\n",
            "Epoch 400, Weights: [0.32793336 0.33702577 0.33504084]\n",
            "Epoch 450, Weights: [0.32726835 0.33747646 0.33525516]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328766 0.33334758 0.33336473]\n",
            "Epoch 50, Weights: [0.33101108 0.33406312 0.33492577]\n",
            "Epoch 100, Weights: [0.32874756 0.33478362 0.33646879]\n",
            "Epoch 150, Weights: [0.32649708 0.33550901 0.33799388]\n",
            "Epoch 200, Weights: [0.32425961 0.33623919 0.33950117]\n",
            "Epoch 250, Weights: [0.32203511 0.3369741  0.34099077]\n",
            "Epoch 300, Weights: [0.31982354 0.33771364 0.34246278]\n",
            "Epoch 350, Weights: [0.3176249  0.33845775 0.34391733]\n",
            "Epoch 400, Weights: [0.31543913 0.33920633 0.34535451]\n",
            "Epoch 450, Weights: [0.31326622 0.33995931 0.34677444]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338276 0.33322954 0.33338767]\n",
            "Epoch 50, Weights: [0.33585841 0.32802801 0.33611355]\n",
            "Epoch 100, Weights: [0.33834144 0.3228025  0.33885603]\n",
            "Epoch 150, Weights: [0.3408318  0.31755307 0.3416151 ]\n",
            "Epoch 200, Weights: [0.34332943 0.31227981 0.34439073]\n",
            "Epoch 250, Weights: [0.34583431 0.30698278 0.34718288]\n",
            "Epoch 300, Weights: [0.34834637 0.30166206 0.34999154]\n",
            "Epoch 350, Weights: [0.35086556 0.29631774 0.35281666]\n",
            "Epoch 400, Weights: [0.35339185 0.2909499  0.35565822]\n",
            "Epoch 450, Weights: [0.35592517 0.28555862 0.35851618]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33325652 0.33332294 0.33342051]\n",
            "Epoch 50, Weights: [0.32942508 0.33280444 0.33777045]\n",
            "Epoch 100, Weights: [0.32560983 0.33228727 0.34210287]\n",
            "Epoch 150, Weights: [0.32181012 0.33177134 0.34641851]\n",
            "Epoch 200, Weights: [0.3180253  0.33125656 0.35071811]\n",
            "Epoch 250, Weights: [0.31425473 0.33074286 0.35500238]\n",
            "Epoch 300, Weights: [0.31049778 0.33023013 0.35927206]\n",
            "Epoch 350, Weights: [0.30675381 0.32971831 0.36352785]\n",
            "Epoch 400, Weights: [0.30302222 0.3292073  0.36777044]\n",
            "Epoch 450, Weights: [0.29930241 0.32869703 0.37200054]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333671  0.33324568 0.3333872 ]\n",
            "Epoch 50, Weights: [0.33506606 0.32887686 0.33605705]\n",
            "Epoch 100, Weights: [0.3367851  0.32453452 0.33868034]\n",
            "Epoch 150, Weights: [0.33852413 0.32021858 0.34125726]\n",
            "Epoch 200, Weights: [0.34028301 0.31592898 0.34378797]\n",
            "Epoch 250, Weights: [0.34206165 0.31166566 0.34627265]\n",
            "Epoch 300, Weights: [0.34385993 0.30742856 0.34871147]\n",
            "Epoch 350, Weights: [0.34567775 0.30321763 0.3511046 ]\n",
            "Epoch 400, Weights: [0.34751498 0.2990328  0.35345219]\n",
            "Epoch 450, Weights: [0.34937153 0.29487401 0.35575442]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329767 0.33330681 0.33339549]\n",
            "Epoch 50, Weights: [0.33151241 0.33198539 0.33650216]\n",
            "Epoch 100, Weights: [0.32972211 0.33067205 0.33960581]\n",
            "Epoch 150, Weights: [0.32792669 0.3293667  0.34270659]\n",
            "Epoch 200, Weights: [0.32612607 0.32806924 0.34580467]\n",
            "Epoch 250, Weights: [0.32432018 0.32677959 0.3489002 ]\n",
            "Epoch 300, Weights: [0.32250896 0.32549766 0.35199334]\n",
            "Epoch 350, Weights: [0.32069233 0.32422338 0.35508426]\n",
            "Epoch 400, Weights: [0.31887022 0.32295665 0.35817311]\n",
            "Epoch 450, Weights: [0.31704254 0.32169739 0.36126004]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338657 0.33332797 0.33328543]\n",
            "Epoch 50, Weights: [0.33605602 0.33304582 0.33089813]\n",
            "Epoch 100, Weights: [0.33873893 0.33273551 0.32852553]\n",
            "Epoch 150, Weights: [0.34143516 0.3323971  0.3261677 ]\n",
            "Epoch 200, Weights: [0.34414456 0.33203069 0.32382472]\n",
            "Epoch 250, Weights: [0.34686698 0.33163634 0.32149665]\n",
            "Epoch 300, Weights: [0.34960223 0.33121416 0.31918358]\n",
            "Epoch 350, Weights: [0.35235016 0.33076423 0.31688558]\n",
            "Epoch 400, Weights: [0.35511059 0.33028664 0.31460274]\n",
            "Epoch 450, Weights: [0.35788334 0.32978151 0.31233512]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333002 0.33329082 0.33337913]\n",
            "Epoch 50, Weights: [0.33316311 0.3311675  0.33566936]\n",
            "Epoch 100, Weights: [0.33299291 0.32904816 0.33795889]\n",
            "Epoch 150, Weights: [0.33281944 0.32693293 0.3402476 ]\n",
            "Epoch 200, Weights: [0.33264273 0.3248219  0.34253534]\n",
            "Epoch 250, Weights: [0.33246281 0.32271518 0.34482198]\n",
            "Epoch 300, Weights: [0.3322797  0.32061289 0.34710738]\n",
            "Epoch 350, Weights: [0.33209343 0.31851512 0.34939142]\n",
            "Epoch 400, Weights: [0.33190404 0.31642198 0.35167395]\n",
            "Epoch 450, Weights: [0.33171155 0.31433357 0.35395485]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3332831  0.33329818 0.33341869]\n",
            "Epoch 50, Weights: [0.33077382 0.33151723 0.33770891]\n",
            "Epoch 100, Weights: [0.32826848 0.32968944 0.34204205]\n",
            "Epoch 150, Weights: [0.32576765 0.32781426 0.34641806]\n",
            "Epoch 200, Weights: [0.32327192 0.32589117 0.35083687]\n",
            "Epoch 250, Weights: [0.32078188 0.32391966 0.35529842]\n",
            "Epoch 300, Weights: [0.31829813 0.32189921 0.35980263]\n",
            "Epoch 350, Weights: [0.31582126 0.31982931 0.3643494 ]\n",
            "Epoch 400, Weights: [0.31335189 0.31770945 0.36893863]\n",
            "Epoch 450, Weights: [0.31089063 0.31553913 0.37357021]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33324835 0.3333468  0.33340482]\n",
            "Epoch 50, Weights: [0.32900237 0.33400447 0.33699314]\n",
            "Epoch 100, Weights: [0.32476152 0.33463065 0.34060779]\n",
            "Epoch 150, Weights: [0.32052597 0.33522561 0.34424838]\n",
            "Epoch 200, Weights: [0.31629587 0.33578959 0.34791451]\n",
            "Epoch 250, Weights: [0.31207133 0.33632285 0.35160579]\n",
            "Epoch 300, Weights: [0.30785249 0.33682562 0.35532186]\n",
            "Epoch 350, Weights: [0.30363947 0.33729816 0.35906234]\n",
            "Epoch 400, Weights: [0.29943237 0.33774073 0.36282687]\n",
            "Epoch 450, Weights: [0.29523128 0.33815357 0.36661512]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328992 0.33330971 0.33340034]\n",
            "Epoch 50, Weights: [0.33112374 0.33212955 0.33674667]\n",
            "Epoch 100, Weights: [0.32896536 0.33095094 0.34008368]\n",
            "Epoch 150, Weights: [0.32681492 0.32977393 0.34341112]\n",
            "Epoch 200, Weights: [0.32467258 0.3285986  0.34672879]\n",
            "Epoch 250, Weights: [0.32253848 0.32742504 0.35003646]\n",
            "Epoch 300, Weights: [0.32041277 0.32625329 0.3533339 ]\n",
            "Epoch 350, Weights: [0.3182956  0.32508345 0.35662092]\n",
            "Epoch 400, Weights: [0.3161871  0.32391558 0.35989729]\n",
            "Epoch 450, Weights: [0.31408742 0.32274974 0.36316281]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33342051 0.33327194 0.33330752]\n",
            "Epoch 50, Weights: [0.33777521 0.3301964  0.33202837]\n",
            "Epoch 100, Weights: [0.34212118 0.32710809 0.3307707 ]\n",
            "Epoch 150, Weights: [0.34645829 0.32400738 0.3295343 ]\n",
            "Epoch 200, Weights: [0.35078642 0.32089461 0.32831893]\n",
            "Epoch 250, Weights: [0.35510545 0.31777015 0.32712436]\n",
            "Epoch 300, Weights: [0.35941526 0.31463434 0.32595037]\n",
            "Epoch 350, Weights: [0.3637157  0.31148753 0.32479673]\n",
            "Epoch 400, Weights: [0.36800668 0.30833007 0.32366322]\n",
            "Epoch 450, Weights: [0.37228804 0.3051623  0.32254962]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329308 0.33332165 0.33338524]\n",
            "Epoch 50, Weights: [0.33127494 0.33273547 0.33598956]\n",
            "Epoch 100, Weights: [0.32924466 0.33214415 0.33861116]\n",
            "Epoch 150, Weights: [0.32720193 0.3315476  0.34125044]\n",
            "Epoch 200, Weights: [0.32514643 0.33094572 0.34390782]\n",
            "Epoch 250, Weights: [0.32307786 0.3303384  0.34658371]\n",
            "Epoch 300, Weights: [0.32099588 0.32972553 0.34927855]\n",
            "Epoch 350, Weights: [0.31890017 0.32910702 0.35199278]\n",
            "Epoch 400, Weights: [0.31679038 0.32848275 0.35472683]\n",
            "Epoch 450, Weights: [0.31466618 0.32785261 0.35748118]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330312 0.333276   0.33342085]\n",
            "Epoch 50, Weights: [0.33179674 0.33039206 0.33781117]\n",
            "Epoch 100, Weights: [0.33029787 0.32747311 0.34222899]\n",
            "Epoch 150, Weights: [0.32880652 0.32451863 0.34667482]\n",
            "Epoch 200, Weights: [0.32732273 0.32152807 0.35114917]\n",
            "Epoch 250, Weights: [0.32584651 0.31850087 0.35565258]\n",
            "Epoch 300, Weights: [0.3243779  0.31543648 0.36018559]\n",
            "Epoch 350, Weights: [0.32291692 0.3123343  0.36474876]\n",
            "Epoch 400, Weights: [0.32146359 0.30919374 0.36934264]\n",
            "Epoch 450, Weights: [0.32001795 0.30601418 0.37396784]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336429 0.33337415 0.33326154]\n",
            "Epoch 50, Weights: [0.33491205 0.33541011 0.32967781]\n",
            "Epoch 100, Weights: [0.33645895 0.33743586 0.32610516]\n",
            "Epoch 150, Weights: [0.33800494 0.33945138 0.32254365]\n",
            "Epoch 200, Weights: [0.33954999 0.34145666 0.31899332]\n",
            "Epoch 250, Weights: [0.34109404 0.34345168 0.31545425]\n",
            "Epoch 300, Weights: [0.34263706 0.34543644 0.31192647]\n",
            "Epoch 350, Weights: [0.34417902 0.34741091 0.30841004]\n",
            "Epoch 400, Weights: [0.34571987 0.34937508 0.30490502]\n",
            "Epoch 450, Weights: [0.34725957 0.35132895 0.30141146]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329108 0.3334164  0.33329248]\n",
            "Epoch 50, Weights: [0.33118758 0.33757385 0.33123854]\n",
            "Epoch 100, Weights: [0.32910102 0.34173814 0.32916081]\n",
            "Epoch 150, Weights: [0.32703161 0.34590954 0.32705882]\n",
            "Epoch 200, Weights: [0.32497958 0.35008835 0.32493204]\n",
            "Epoch 250, Weights: [0.32294516 0.35427483 0.32277998]\n",
            "Epoch 300, Weights: [0.32092858 0.35846928 0.32060211]\n",
            "Epoch 350, Weights: [0.31893011 0.36267197 0.31839789]\n",
            "Epoch 400, Weights: [0.31694999 0.36688318 0.3161668 ]\n",
            "Epoch 450, Weights: [0.3149885  0.37110319 0.31390829]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331971 0.33329    0.33339025]\n",
            "Epoch 50, Weights: [0.33263621 0.33112227 0.33624149]\n",
            "Epoch 100, Weights: [0.33194682 0.32895113 0.33910202]\n",
            "Epoch 150, Weights: [0.33125158 0.32677669 0.3419717 ]\n",
            "Epoch 200, Weights: [0.33055054 0.3245991  0.34485033]\n",
            "Epoch 250, Weights: [0.32984376 0.32241846 0.34773775]\n",
            "Epoch 300, Weights: [0.32913126 0.32023492 0.35063378]\n",
            "Epoch 350, Weights: [0.32841311 0.31804861 0.35353825]\n",
            "Epoch 400, Weights: [0.32768936 0.31585964 0.35645097]\n",
            "Epoch 450, Weights: [0.32696005 0.31366816 0.35937175]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3332937  0.33326998 0.33343629]\n",
            "Epoch 50, Weights: [0.33129956 0.33010897 0.33859144]\n",
            "Epoch 100, Weights: [0.32927946 0.32695897 0.34376155]\n",
            "Epoch 150, Weights: [0.32723256 0.3238191  0.34894831]\n",
            "Epoch 200, Weights: [0.325158   0.32068851 0.35415345]\n",
            "Epoch 250, Weights: [0.32305489 0.31756636 0.35937872]\n",
            "Epoch 300, Weights: [0.32092232 0.3144518  0.36462586]\n",
            "Epoch 350, Weights: [0.31875933 0.31134398 0.36989666]\n",
            "Epoch 400, Weights: [0.31656497 0.30824206 0.37519293]\n",
            "Epoch 450, Weights: [0.31433824 0.30514521 0.38051652]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333268 0.3333373  0.33333   ]\n",
            "Epoch 50, Weights: [0.33330025 0.33353678 0.33316294]\n",
            "Epoch 100, Weights: [0.33326765 0.33373802 0.3329943 ]\n",
            "Epoch 150, Weights: [0.33323489 0.33394102 0.33282407]\n",
            "Epoch 200, Weights: [0.33320195 0.3341458  0.33265222]\n",
            "Epoch 250, Weights: [0.33316883 0.33435238 0.33247876]\n",
            "Epoch 300, Weights: [0.33313555 0.33456077 0.33230365]\n",
            "Epoch 350, Weights: [0.33310209 0.33477099 0.33212689]\n",
            "Epoch 400, Weights: [0.33306845 0.33498305 0.33194847]\n",
            "Epoch 450, Weights: [0.33303463 0.33519697 0.33176836]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336435 0.33328914 0.33334648]\n",
            "Epoch 50, Weights: [0.33491819 0.33107945 0.33400233]\n",
            "Epoch 100, Weights: [0.3364774  0.32886836 0.33465421]\n",
            "Epoch 150, Weights: [0.33804202 0.32665581 0.33530214]\n",
            "Epoch 200, Weights: [0.33961211 0.32444173 0.33594613]\n",
            "Epoch 250, Weights: [0.34118772 0.32222605 0.3365862 ]\n",
            "Epoch 300, Weights: [0.34276889 0.32000872 0.33722236]\n",
            "Epoch 350, Weights: [0.34435569 0.31778966 0.33785462]\n",
            "Epoch 400, Weights: [0.34594816 0.3155688  0.33848301]\n",
            "Epoch 450, Weights: [0.34754636 0.31334608 0.33910753]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336635 0.33332141 0.33331221]\n",
            "Epoch 50, Weights: [0.33502729 0.33272069 0.332252  ]\n",
            "Epoch 100, Weights: [0.33670762 0.33210968 0.33118267]\n",
            "Epoch 150, Weights: [0.33840766 0.3314882  0.33010411]\n",
            "Epoch 200, Weights: [0.34012774 0.33085605 0.32901618]\n",
            "Epoch 250, Weights: [0.34186818 0.33021305 0.32791874]\n",
            "Epoch 300, Weights: [0.34362931 0.32955899 0.32681167]\n",
            "Epoch 350, Weights: [0.3454115  0.32889366 0.32569481]\n",
            "Epoch 400, Weights: [0.34721507 0.32821686 0.32456804]\n",
            "Epoch 450, Weights: [0.3490404  0.32752837 0.3234312 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33340397 0.33333898 0.33325702]\n",
            "Epoch 50, Weights: [0.33692445 0.33361218 0.32946335]\n",
            "Epoch 100, Weights: [0.34042157 0.33386646 0.32571195]\n",
            "Epoch 150, Weights: [0.34389505 0.33410219 0.32200274]\n",
            "Epoch 200, Weights: [0.34734461 0.33431974 0.31833561]\n",
            "Epoch 250, Weights: [0.35077001 0.33451949 0.31471046]\n",
            "Epoch 300, Weights: [0.354171   0.3347018  0.31112717]\n",
            "Epoch 350, Weights: [0.35754733 0.33486702 0.30758562]\n",
            "Epoch 400, Weights: [0.36089879 0.33501553 0.30408565]\n",
            "Epoch 450, Weights: [0.36422516 0.33514767 0.30062714]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331621 0.33332094 0.33336282]\n",
            "Epoch 50, Weights: [0.33246213 0.33270178 0.33483607]\n",
            "Epoch 100, Weights: [0.3316113  0.33208278 0.33630589]\n",
            "Epoch 150, Weights: [0.33076369 0.33146395 0.33777233]\n",
            "Epoch 200, Weights: [0.32991926 0.33084528 0.33923544]\n",
            "Epoch 250, Weights: [0.32907796 0.33022677 0.34069524]\n",
            "Epoch 300, Weights: [0.32823977 0.32960843 0.34215178]\n",
            "Epoch 350, Weights: [0.32740464 0.32899024 0.34360509]\n",
            "Epoch 400, Weights: [0.32657253 0.32837222 0.34505522]\n",
            "Epoch 450, Weights: [0.32574342 0.32775435 0.34650219]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328806 0.3333558  0.3333561 ]\n",
            "Epoch 50, Weights: [0.33102775 0.3344728  0.33449942]\n",
            "Epoch 100, Weights: [0.32877269 0.33557625 0.33565102]\n",
            "Epoch 150, Weights: [0.32652304 0.33666614 0.33681079]\n",
            "Epoch 200, Weights: [0.32427896 0.33774245 0.33797856]\n",
            "Epoch 250, Weights: [0.32204059 0.33880517 0.33915421]\n",
            "Epoch 300, Weights: [0.3198081  0.33985429 0.34033759]\n",
            "Epoch 350, Weights: [0.31758163 0.3408898  0.34152854]\n",
            "Epoch 400, Weights: [0.31536134 0.3419117  0.34272693]\n",
            "Epoch 450, Weights: [0.31314738 0.34291998 0.34393261]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333764 0.33330672 0.33335561]\n",
            "Epoch 50, Weights: [0.3335501  0.33197956 0.3344703 ]\n",
            "Epoch 100, Weights: [0.33375644 0.33065794 0.33558559]\n",
            "Epoch 150, Weights: [0.33395662 0.32934186 0.33670149]\n",
            "Epoch 200, Weights: [0.33415061 0.32803134 0.33781802]\n",
            "Epoch 250, Weights: [0.33433837 0.3267264  0.3389352 ]\n",
            "Epoch 300, Weights: [0.33451988 0.32542704 0.34005304]\n",
            "Epoch 350, Weights: [0.33469511 0.32413329 0.34117157]\n",
            "Epoch 400, Weights: [0.33486402 0.32284514 0.34229081]\n",
            "Epoch 450, Weights: [0.33502658 0.32156262 0.34341076]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33323389 0.33341231 0.33335377]\n",
            "Epoch 50, Weights: [0.32828729 0.33734916 0.33436352]\n",
            "Epoch 100, Weights: [0.3233895  0.34126127 0.33534919]\n",
            "Epoch 150, Weights: [0.31854051 0.34514843 0.33631103]\n",
            "Epoch 200, Weights: [0.31374024 0.34901044 0.33724928]\n",
            "Epoch 250, Weights: [0.30898864 0.35284712 0.33816421]\n",
            "Epoch 300, Weights: [0.30428563 0.35665828 0.33905607]\n",
            "Epoch 350, Weights: [0.2996311  0.36044376 0.33992511]\n",
            "Epoch 400, Weights: [0.29502496 0.3642034  0.3407716 ]\n",
            "Epoch 450, Weights: [0.29046709 0.36793707 0.34159581]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332881 0.33328376 0.33338739]\n",
            "Epoch 50, Weights: [0.3331104  0.33080631 0.33608326]\n",
            "Epoch 100, Weights: [0.33290581 0.32833007 0.33876409]\n",
            "Epoch 150, Weights: [0.33271512 0.32585487 0.34142998]\n",
            "Epoch 200, Weights: [0.33253841 0.32338054 0.34408102]\n",
            "Epoch 250, Weights: [0.33237578 0.32090692 0.34671727]\n",
            "Epoch 300, Weights: [0.3322273  0.31843384 0.34933883]\n",
            "Epoch 350, Weights: [0.33209308 0.31596112 0.35194577]\n",
            "Epoch 400, Weights: [0.3319732  0.31348859 0.35453817]\n",
            "Epoch 450, Weights: [0.33186777 0.31101609 0.35711611]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333886 0.33328616 0.33337495]\n",
            "Epoch 50, Weights: [0.33361204 0.33091634 0.3354716 ]\n",
            "Epoch 100, Weights: [0.33387809 0.3285235  0.33759837]\n",
            "Epoch 150, Weights: [0.33413688 0.32610753 0.33975556]\n",
            "Epoch 200, Weights: [0.33438827 0.32366826 0.34194345]\n",
            "Epoch 250, Weights: [0.3346321  0.32120556 0.34416231]\n",
            "Epoch 300, Weights: [0.33486823 0.31871928 0.34641245]\n",
            "Epoch 350, Weights: [0.33509652 0.31620929 0.34869415]\n",
            "Epoch 400, Weights: [0.33531682 0.31367544 0.3510077 ]\n",
            "Epoch 450, Weights: [0.33552898 0.3111176  0.3533534 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333296 0.33327824 0.33338877]\n",
            "Epoch 50, Weights: [0.33330926 0.33053288 0.33615784]\n",
            "Epoch 100, Weights: [0.33327485 0.32780477 0.33892035]\n",
            "Epoch 150, Weights: [0.33322981 0.32509399 0.34167617]\n",
            "Epoch 200, Weights: [0.33317424 0.32240059 0.34442514]\n",
            "Epoch 250, Weights: [0.33310822 0.31972462 0.34716713]\n",
            "Epoch 300, Weights: [0.33303182 0.31706614 0.349902  ]\n",
            "Epoch 350, Weights: [0.33294515 0.31442521 0.35262962]\n",
            "Epoch 400, Weights: [0.33284827 0.31180186 0.35534984]\n",
            "Epoch 450, Weights: [0.33274129 0.30919614 0.35806254]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338359 0.33325912 0.33335727]\n",
            "Epoch 50, Weights: [0.33590473 0.32952208 0.33457316]\n",
            "Epoch 100, Weights: [0.33844197 0.32573171 0.33582629]\n",
            "Epoch 150, Weights: [0.34099577 0.32188679 0.33711741]\n",
            "Epoch 200, Weights: [0.34356657 0.31798609 0.33844731]\n",
            "Epoch 250, Weights: [0.34615487 0.31402834 0.33981676]\n",
            "Epoch 300, Weights: [0.34876113 0.31001225 0.34122659]\n",
            "Epoch 350, Weights: [0.35138586 0.30593648 0.34267763]\n",
            "Epoch 400, Weights: [0.35402954 0.30179969 0.34417074]\n",
            "Epoch 450, Weights: [0.3566927  0.29760046 0.34570681]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328614 0.33338076 0.33333307]\n",
            "Epoch 50, Weights: [0.33092068 0.33574874 0.33333055]\n",
            "Epoch 100, Weights: [0.32854297 0.33810928 0.33334772]\n",
            "Epoch 150, Weights: [0.32615287 0.34046251 0.33338459]\n",
            "Epoch 200, Weights: [0.32375025 0.34280854 0.33344118]\n",
            "Epoch 250, Weights: [0.32133496 0.3451475  0.33351751]\n",
            "Epoch 300, Weights: [0.31890687 0.34747949 0.33361361]\n",
            "Epoch 350, Weights: [0.31646584 0.34980463 0.3337295 ]\n",
            "Epoch 400, Weights: [0.31401174 0.35212301 0.33386521]\n",
            "Epoch 450, Weights: [0.31154445 0.35443474 0.33402078]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336275 0.3332709  0.33336632]\n",
            "Epoch 50, Weights: [0.33483706 0.3301523  0.3350106 ]\n",
            "Epoch 100, Weights: [0.33631723 0.32703921 0.33664353]\n",
            "Epoch 150, Weights: [0.3378032  0.32393168 0.33826509]\n",
            "Epoch 200, Weights: [0.33929493 0.32082975 0.33987529]\n",
            "Epoch 250, Weights: [0.34079237 0.31773348 0.34147412]\n",
            "Epoch 300, Weights: [0.34229547 0.31464293 0.34306158]\n",
            "Epoch 350, Weights: [0.34380417 0.31155814 0.34463766]\n",
            "Epoch 400, Weights: [0.34531842 0.30847918 0.34620237]\n",
            "Epoch 450, Weights: [0.34683817 0.3054061  0.34775569]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337348 0.33336535 0.33326114]\n",
            "Epoch 50, Weights: [0.33538886 0.33497223 0.32963888]\n",
            "Epoch 100, Weights: [0.33741877 0.33659012 0.32599108]\n",
            "Epoch 150, Weights: [0.33946329 0.33821907 0.32231761]\n",
            "Epoch 200, Weights: [0.3415225  0.33985914 0.31861833]\n",
            "Epoch 250, Weights: [0.34359647 0.34151038 0.31489312]\n",
            "Epoch 300, Weights: [0.34568528 0.34317285 0.31114184]\n",
            "Epoch 350, Weights: [0.34778899 0.3448466  0.30736437]\n",
            "Epoch 400, Weights: [0.3499077  0.34653169 0.30356058]\n",
            "Epoch 450, Weights: [0.35204146 0.34822817 0.29973034]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33339642 0.33332178 0.33328177]\n",
            "Epoch 50, Weights: [0.33656183 0.3327453  0.33069284]\n",
            "Epoch 100, Weights: [0.33974816 0.33217021 0.3280816 ]\n",
            "Epoch 150, Weights: [0.3429555  0.33159654 0.32544793]\n",
            "Epoch 200, Weights: [0.34618395 0.33102433 0.32279169]\n",
            "Epoch 250, Weights: [0.34943358 0.33045362 0.32011276]\n",
            "Epoch 300, Weights: [0.35270449 0.32988447 0.31741102]\n",
            "Epoch 350, Weights: [0.35599675 0.3293169  0.31468632]\n",
            "Epoch 400, Weights: [0.35931045 0.32875096 0.31193856]\n",
            "Epoch 450, Weights: [0.36264567 0.32818671 0.30916759]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333246 0.33336197 0.33330554]\n",
            "Epoch 50, Weights: [0.33328936 0.33479742 0.33191319]\n",
            "Epoch 100, Weights: [0.3332465  0.33623875 0.33051472]\n",
            "Epoch 150, Weights: [0.33320388 0.337686   0.32911009]\n",
            "Epoch 200, Weights: [0.3331615  0.33913924 0.32769923]\n",
            "Epoch 250, Weights: [0.33311937 0.34059851 0.32628209]\n",
            "Epoch 300, Weights: [0.33307747 0.34206387 0.32485863]\n",
            "Epoch 350, Weights: [0.33303583 0.34353537 0.32342877]\n",
            "Epoch 400, Weights: [0.33299442 0.34501307 0.32199247]\n",
            "Epoch 450, Weights: [0.33295327 0.34649703 0.32054968]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332354 0.33332798 0.33334845]\n",
            "Epoch 50, Weights: [0.33282967 0.33306046 0.33410984]\n",
            "Epoch 100, Weights: [0.33232652 0.3327918  0.33488165]\n",
            "Epoch 150, Weights: [0.3318139  0.33252201 0.33566406]\n",
            "Epoch 200, Weights: [0.3312916  0.33225113 0.33645724]\n",
            "Epoch 250, Weights: [0.3307594  0.3319792  0.33726136]\n",
            "Epoch 300, Weights: [0.3302171  0.33170625 0.33807662]\n",
            "Epoch 350, Weights: [0.32966446 0.33143232 0.33890319]\n",
            "Epoch 400, Weights: [0.32910126 0.33115744 0.33974127]\n",
            "Epoch 450, Weights: [0.32852726 0.33088166 0.34059106]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330306 0.33337289 0.33332402]\n",
            "Epoch 50, Weights: [0.33178218 0.33536416 0.33285362]\n",
            "Epoch 100, Weights: [0.33024563 0.33738101 0.33237333]\n",
            "Epoch 150, Weights: [0.32869329 0.33942375 0.33188293]\n",
            "Epoch 200, Weights: [0.32712501 0.34149271 0.33138224]\n",
            "Epoch 250, Weights: [0.32554067 0.34358823 0.33087106]\n",
            "Epoch 300, Weights: [0.32394013 0.34571065 0.33034919]\n",
            "Epoch 350, Weights: [0.32232326 0.3478603  0.32981641]\n",
            "Epoch 400, Weights: [0.32068992 0.35003754 0.32927252]\n",
            "Epoch 450, Weights: [0.31903997 0.3522427  0.3287173 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330967 0.33341122 0.33327908]\n",
            "Epoch 50, Weights: [0.33213564 0.33729236 0.33057197]\n",
            "Epoch 100, Weights: [0.33097818 0.34114716 0.32787464]\n",
            "Epoch 150, Weights: [0.32983718 0.34497563 0.32518715]\n",
            "Epoch 200, Weights: [0.32871255 0.34877783 0.32250959]\n",
            "Epoch 250, Weights: [0.32760418 0.35255377 0.31984202]\n",
            "Epoch 300, Weights: [0.32651194 0.3563035  0.31718453]\n",
            "Epoch 350, Weights: [0.32543574 0.36002706 0.31453717]\n",
            "Epoch 400, Weights: [0.32437547 0.36372447 0.31190003]\n",
            "Epoch 450, Weights: [0.32333102 0.36739578 0.30927317]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329387 0.33334347 0.33336263]\n",
            "Epoch 50, Weights: [0.33130876 0.33386065 0.33483055]\n",
            "Epoch 100, Weights: [0.32929928 0.33439766 0.33630303]\n",
            "Epoch 150, Weights: [0.32726535 0.33495469 0.33777993]\n",
            "Epoch 200, Weights: [0.32520691 0.33553197 0.33926109]\n",
            "Epoch 250, Weights: [0.32312389 0.3361297  0.34074637]\n",
            "Epoch 300, Weights: [0.32101625 0.3367481  0.34223562]\n",
            "Epoch 350, Weights: [0.31888392 0.33738738 0.34372867]\n",
            "Epoch 400, Weights: [0.31672685 0.33804774 0.34522538]\n",
            "Epoch 450, Weights: [0.31454501 0.33872939 0.34672557]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333036 0.33336106 0.33330855]\n",
            "Epoch 50, Weights: [0.33318351 0.33475551 0.33206095]\n",
            "Epoch 100, Weights: [0.33303913 0.33616474 0.3307961 ]\n",
            "Epoch 150, Weights: [0.33289731 0.33758889 0.32951377]\n",
            "Epoch 200, Weights: [0.33275816 0.33902808 0.32821373]\n",
            "Epoch 250, Weights: [0.33262177 0.34048246 0.32689574]\n",
            "Epoch 300, Weights: [0.33248826 0.34195215 0.32555956]\n",
            "Epoch 350, Weights: [0.33235773 0.34343729 0.32420495]\n",
            "Epoch 400, Weights: [0.33223029 0.34493802 0.32283166]\n",
            "Epoch 450, Weights: [0.33210606 0.34645446 0.32143945]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331245 0.33330618 0.33338134]\n",
            "Epoch 50, Weights: [0.3322664  0.33195243 0.33578114]\n",
            "Epoch 100, Weights: [0.33121589 0.3306052  0.33817888]\n",
            "Epoch 150, Weights: [0.33016095 0.32926449 0.34057453]\n",
            "Epoch 200, Weights: [0.32910158 0.32793032 0.34296806]\n",
            "Epoch 250, Weights: [0.32803779 0.32660271 0.34535947]\n",
            "Epoch 300, Weights: [0.3269696  0.32528164 0.34774873]\n",
            "Epoch 350, Weights: [0.32589701 0.32396715 0.35013581]\n",
            "Epoch 400, Weights: [0.32482003 0.32265923 0.35252071]\n",
            "Epoch 450, Weights: [0.32373867 0.3213579  0.3549034 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332426 0.33329698 0.33337874]\n",
            "Epoch 50, Weights: [0.33287264 0.33148023 0.3356471 ]\n",
            "Epoch 100, Weights: [0.3324244  0.32966461 0.33791096]\n",
            "Epoch 150, Weights: [0.33197957 0.32785013 0.34017028]\n",
            "Epoch 200, Weights: [0.33153815 0.32603684 0.34242499]\n",
            "Epoch 250, Weights: [0.33110016 0.32422476 0.34467505]\n",
            "Epoch 300, Weights: [0.33066562 0.32241394 0.34692041]\n",
            "Epoch 350, Weights: [0.33023454 0.32060441 0.34916102]\n",
            "Epoch 400, Weights: [0.32980694 0.31879619 0.35139684]\n",
            "Epoch 450, Weights: [0.32938284 0.31698933 0.3536278 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.333314   0.33334668 0.33333929]\n",
            "Epoch 50, Weights: [0.33234465 0.33401736 0.33363796]\n",
            "Epoch 100, Weights: [0.33136925 0.33469371 0.333937  ]\n",
            "Epoch 150, Weights: [0.33038781 0.33537576 0.3342364 ]\n",
            "Epoch 200, Weights: [0.32940031 0.33606352 0.33453614]\n",
            "Epoch 250, Weights: [0.32840673 0.33675701 0.33483623]\n",
            "Epoch 300, Weights: [0.32740708 0.33745625 0.33513665]\n",
            "Epoch 350, Weights: [0.32640133 0.33816125 0.33543739]\n",
            "Epoch 400, Weights: [0.32538949 0.33887204 0.33573844]\n",
            "Epoch 450, Weights: [0.32437153 0.33958863 0.33603981]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337509 0.33331433 0.33331055]\n",
            "Epoch 50, Weights: [0.33546421 0.33236728 0.33216848]\n",
            "Epoch 100, Weights: [0.33755506 0.33142546 0.33101945]\n",
            "Epoch 150, Weights: [0.33964776 0.33048884 0.32986338]\n",
            "Epoch 200, Weights: [0.34174244 0.32955737 0.32870015]\n",
            "Epoch 250, Weights: [0.34383926 0.32863104 0.32752967]\n",
            "Epoch 300, Weights: [0.34593833 0.3277098  0.32635183]\n",
            "Epoch 350, Weights: [0.34803981 0.32679364 0.32516652]\n",
            "Epoch 400, Weights: [0.35014382 0.32588252 0.32397363]\n",
            "Epoch 450, Weights: [0.35225051 0.32497641 0.32277305]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331584 0.3332634  0.33342073]\n",
            "Epoch 50, Weights: [0.33245109 0.32974975 0.33779914]\n",
            "Epoch 100, Weights: [0.33160511 0.32620156 0.34219329]\n",
            "Epoch 150, Weights: [0.33077827 0.32261854 0.34660315]\n",
            "Epoch 200, Weights: [0.32997092 0.31900037 0.35102868]\n",
            "Epoch 250, Weights: [0.32918341 0.31534675 0.35546981]\n",
            "Epoch 300, Weights: [0.32841611 0.31165735 0.35992651]\n",
            "Epoch 350, Weights: [0.32766939 0.30793187 0.36439871]\n",
            "Epoch 400, Weights: [0.32694362 0.30417    0.36888635]\n",
            "Epoch 450, Weights: [0.32623916 0.30037143 0.37338938]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336856 0.33323985 0.33339156]\n",
            "Epoch 50, Weights: [0.33512627 0.32855042 0.33632328]\n",
            "Epoch 100, Weights: [0.33687573 0.32383009 0.33929416]\n",
            "Epoch 150, Weights: [0.33861652 0.31907881 0.34230464]\n",
            "Epoch 200, Weights: [0.34034823 0.31429658 0.34535517]\n",
            "Epoch 250, Weights: [0.34207041 0.30948337 0.34844619]\n",
            "Epoch 300, Weights: [0.34378261 0.3046392  0.35157816]\n",
            "Epoch 350, Weights: [0.3454844  0.29976404 0.35475152]\n",
            "Epoch 400, Weights: [0.34717532 0.29485792 0.35796674]\n",
            "Epoch 450, Weights: [0.34885489 0.28992083 0.36122425]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329517 0.33337106 0.33333374]\n",
            "Epoch 50, Weights: [0.33138627 0.33524813 0.33336556]\n",
            "Epoch 100, Weights: [0.32947481 0.33710625 0.33341892]\n",
            "Epoch 150, Weights: [0.327561   0.33894535 0.33349362]\n",
            "Epoch 200, Weights: [0.32564507 0.3407654  0.33358951]\n",
            "Epoch 250, Weights: [0.32372725 0.34256634 0.33370638]\n",
            "Epoch 300, Weights: [0.32180776 0.34434813 0.33384408]\n",
            "Epoch 350, Weights: [0.31988683 0.34611075 0.33400239]\n",
            "Epoch 400, Weights: [0.31796467 0.34785414 0.33418115]\n",
            "Epoch 450, Weights: [0.31604152 0.34957829 0.33438016]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336517 0.33334534 0.33328946]\n",
            "Epoch 50, Weights: [0.3349503  0.33394972 0.33109994]\n",
            "Epoch 100, Weights: [0.33652088 0.33456108 0.32891801]\n",
            "Epoch 150, Weights: [0.33807685 0.33517942 0.3267437 ]\n",
            "Epoch 200, Weights: [0.33961817 0.33580473 0.32457707]\n",
            "Epoch 250, Weights: [0.3411448  0.33643701 0.32241816]\n",
            "Epoch 300, Weights: [0.3426567  0.33707626 0.32026701]\n",
            "Epoch 350, Weights: [0.34415382 0.33772247 0.31812368]\n",
            "Epoch 400, Weights: [0.34563612 0.33837563 0.31598821]\n",
            "Epoch 450, Weights: [0.34710357 0.33903576 0.31386065]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333567  0.33332911 0.33331417]\n",
            "Epoch 50, Weights: [0.33452621 0.33311799 0.33235577]\n",
            "Epoch 100, Weights: [0.3356973  0.33290631 0.33139635]\n",
            "Epoch 150, Weights: [0.33687001 0.33269408 0.33043589]\n",
            "Epoch 200, Weights: [0.33804436 0.33248128 0.32947433]\n",
            "Epoch 250, Weights: [0.33922041 0.33226791 0.32851165]\n",
            "Epoch 300, Weights: [0.34039819 0.33205396 0.32754782]\n",
            "Epoch 350, Weights: [0.34157774 0.33183944 0.32658279]\n",
            "Epoch 400, Weights: [0.3427591  0.33162433 0.32561654]\n",
            "Epoch 450, Weights: [0.3439423  0.33140864 0.32464903]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336779 0.33334031 0.33329187]\n",
            "Epoch 50, Weights: [0.33508764 0.33369743 0.3312149 ]\n",
            "Epoch 100, Weights: [0.33680053 0.33406975 0.32912969]\n",
            "Epoch 150, Weights: [0.33850642 0.33445727 0.32703629]\n",
            "Epoch 200, Weights: [0.34020526 0.33485997 0.32493474]\n",
            "Epoch 250, Weights: [0.341897   0.33527786 0.32282511]\n",
            "Epoch 300, Weights: [0.34358162 0.3357109  0.32070745]\n",
            "Epoch 350, Weights: [0.34525905 0.3361591  0.31858182]\n",
            "Epoch 400, Weights: [0.34692925 0.33662244 0.31644828]\n",
            "Epoch 450, Weights: [0.34859219 0.33710089 0.31430689]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332304 0.33339397 0.33328297]\n",
            "Epoch 50, Weights: [0.33281733 0.33641055 0.3307721 ]\n",
            "Epoch 100, Weights: [0.33232832 0.3393969  0.32827475]\n",
            "Epoch 150, Weights: [0.33185582 0.34235332 0.32579083]\n",
            "Epoch 200, Weights: [0.3313996  0.34528013 0.32332024]\n",
            "Epoch 250, Weights: [0.33095947 0.34817763 0.32086287]\n",
            "Epoch 300, Weights: [0.33053523 0.3510461  0.31841864]\n",
            "Epoch 350, Weights: [0.33012667 0.35388586 0.31598744]\n",
            "Epoch 400, Weights: [0.32973362 0.35669719 0.31356917]\n",
            "Epoch 450, Weights: [0.32935586 0.35948038 0.31116373]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338302 0.33326905 0.3333479 ]\n",
            "Epoch 50, Weights: [0.33587918 0.33006107 0.33405971]\n",
            "Epoch 100, Weights: [0.33839791 0.32686422 0.33473783]\n",
            "Epoch 150, Weights: [0.34093932 0.32367836 0.33538229]\n",
            "Epoch 200, Weights: [0.34350353 0.32050335 0.33599309]\n",
            "Epoch 250, Weights: [0.34609064 0.31733907 0.33657026]\n",
            "Epoch 300, Weights: [0.34870077 0.31418539 0.33711381]\n",
            "Epoch 350, Weights: [0.35133402 0.31104222 0.33762373]\n",
            "Epoch 400, Weights: [0.35399051 0.30790943 0.33810004]\n",
            "Epoch 450, Weights: [0.35667032 0.30478693 0.33854272]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331036 0.33327444 0.33341517]\n",
            "Epoch 50, Weights: [0.33215448 0.33033568 0.33750981]\n",
            "Epoch 100, Weights: [0.3309833  0.32740789 0.34160878]\n",
            "Epoch 150, Weights: [0.32979676 0.32449119 0.34571203]\n",
            "Epoch 200, Weights: [0.32859478 0.32158568 0.34981952]\n",
            "Epoch 250, Weights: [0.32737731 0.31869147 0.3539312 ]\n",
            "Epoch 300, Weights: [0.32614428 0.31580866 0.35804702]\n",
            "Epoch 350, Weights: [0.32489564 0.31293737 0.36216696]\n",
            "Epoch 400, Weights: [0.32363131 0.3100777  0.36629096]\n",
            "Epoch 450, Weights: [0.32235123 0.30722976 0.37041899]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329335 0.33333954 0.33336709]\n",
            "Epoch 50, Weights: [0.33130171 0.33364445 0.33505382]\n",
            "Epoch 100, Weights: [0.3293242  0.33393807 0.3367377 ]\n",
            "Epoch 150, Weights: [0.32736091 0.33422037 0.33841869]\n",
            "Epoch 200, Weights: [0.3254119  0.33449129 0.34009678]\n",
            "Epoch 250, Weights: [0.32347727 0.33475078 0.34177192]\n",
            "Epoch 300, Weights: [0.32155708 0.33499881 0.34344409]\n",
            "Epoch 350, Weights: [0.31965141 0.33523531 0.34511325]\n",
            "Epoch 400, Weights: [0.31776033 0.33546025 0.34677938]\n",
            "Epoch 450, Weights: [0.31588394 0.33567359 0.34844245]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328078 0.33334424 0.33337495]\n",
            "Epoch 50, Weights: [0.33065259 0.33389896 0.33544842]\n",
            "Epoch 100, Weights: [0.32802222 0.33447098 0.33750677]\n",
            "Epoch 150, Weights: [0.32538963 0.33506014 0.3395502 ]\n",
            "Epoch 200, Weights: [0.32275478 0.33566629 0.3415789 ]\n",
            "Epoch 250, Weights: [0.32011766 0.33628926 0.34359305]\n",
            "Epoch 300, Weights: [0.31747823 0.3369289  0.34559284]\n",
            "Epoch 350, Weights: [0.31483644 0.33758508 0.34757845]\n",
            "Epoch 400, Weights: [0.31219227 0.33825764 0.34955006]\n",
            "Epoch 450, Weights: [0.30954567 0.33894645 0.35150785]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33344188 0.33328172 0.33327637]\n",
            "Epoch 50, Weights: [0.33890171 0.33068607 0.3304122 ]\n",
            "Epoch 100, Weights: [0.34442485 0.32806009 0.32751504]\n",
            "Epoch 150, Weights: [0.35001242 0.32540324 0.32458431]\n",
            "Epoch 200, Weights: [0.35566557 0.32271498 0.32161942]\n",
            "Epoch 250, Weights: [0.36138549 0.31999473 0.31861975]\n",
            "Epoch 300, Weights: [0.36717337 0.31724192 0.31558468]\n",
            "Epoch 350, Weights: [0.37303045 0.31445597 0.31251356]\n",
            "Epoch 400, Weights: [0.37895799 0.31163625 0.30940573]\n",
            "Epoch 450, Weights: [0.38495729 0.30878216 0.30626052]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332564 0.33336151 0.33331282]\n",
            "Epoch 50, Weights: [0.33294574 0.33477264 0.33228159]\n",
            "Epoch 100, Weights: [0.33257384 0.33618767 0.33123846]\n",
            "Epoch 150, Weights: [0.33221005 0.33760658 0.33018334]\n",
            "Epoch 200, Weights: [0.33185452 0.33902932 0.32911614]\n",
            "Epoch 250, Weights: [0.33150736 0.34045586 0.32803675]\n",
            "Epoch 300, Weights: [0.3311687  0.34188617 0.32694509]\n",
            "Epoch 350, Weights: [0.33083868 0.34332022 0.32584107]\n",
            "Epoch 400, Weights: [0.33051743 0.34475796 0.32472458]\n",
            "Epoch 450, Weights: [0.33020508 0.34619936 0.32359553]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3332699  0.3332847  0.33344537]\n",
            "Epoch 50, Weights: [0.33007018 0.33084875 0.33908104]\n",
            "Epoch 100, Weights: [0.32681374 0.32840384 0.34478239]\n",
            "Epoch 150, Weights: [0.32349985 0.32595037 0.35054975]\n",
            "Epoch 200, Weights: [0.3201278  0.32348875 0.35638341]\n",
            "Epoch 250, Weights: [0.31669686 0.32101941 0.36228369]\n",
            "Epoch 300, Weights: [0.31320631 0.31854279 0.36825087]\n",
            "Epoch 350, Weights: [0.3096554  0.31605933 0.37428525]\n",
            "Epoch 400, Weights: [0.30604339 0.31356949 0.38038709]\n",
            "Epoch 450, Weights: [0.30236954 0.31107375 0.38655668]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334892 0.33329159 0.33335946]\n",
            "Epoch 50, Weights: [0.33413682 0.33119044 0.33467272]\n",
            "Epoch 100, Weights: [0.33494071 0.32906026 0.33599899]\n",
            "Epoch 150, Weights: [0.33576085 0.32690059 0.33733853]\n",
            "Epoch 200, Weights: [0.33659745 0.32471096 0.33869156]\n",
            "Epoch 250, Weights: [0.33745076 0.32249087 0.34005834]\n",
            "Epoch 300, Weights: [0.33832103 0.32023985 0.3414391 ]\n",
            "Epoch 350, Weights: [0.3392085  0.31795737 0.3428341 ]\n",
            "Epoch 400, Weights: [0.34011343 0.31564292 0.34424361]\n",
            "Epoch 450, Weights: [0.34103609 0.31329598 0.34566789]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329223 0.33340802 0.33329971]\n",
            "Epoch 50, Weights: [0.33124382 0.33716426 0.33159189]\n",
            "Epoch 100, Weights: [0.32920763 0.34096266 0.32982967]\n",
            "Epoch 150, Weights: [0.32718395 0.34480397 0.32801204]\n",
            "Epoch 200, Weights: [0.32517307 0.34868895 0.32613796]\n",
            "Epoch 250, Weights: [0.32317528 0.35261835 0.32420634]\n",
            "Epoch 300, Weights: [0.32119089 0.35659298 0.3222161 ]\n",
            "Epoch 350, Weights: [0.31922022 0.36061363 0.32016612]\n",
            "Epoch 400, Weights: [0.31726358 0.36468113 0.31805526]\n",
            "Epoch 450, Weights: [0.31532131 0.36879631 0.31588235]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335988 0.33325777 0.33338232]\n",
            "Epoch 50, Weights: [0.33470486 0.32946748 0.33582763]\n",
            "Epoch 100, Weights: [0.33608422 0.3256518  0.33826395]\n",
            "Epoch 150, Weights: [0.33749841 0.32181041 0.34069115]\n",
            "Epoch 200, Weights: [0.33894789 0.31794302 0.34310906]\n",
            "Epoch 250, Weights: [0.34043312 0.31404932 0.34551753]\n",
            "Epoch 300, Weights: [0.34195457 0.31012899 0.34791641]\n",
            "Epoch 350, Weights: [0.34351273 0.30618171 0.35030552]\n",
            "Epoch 400, Weights: [0.34510809 0.30220718 0.35268471]\n",
            "Epoch 450, Weights: [0.34674113 0.29820505 0.35505379]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329328 0.33343419 0.3332725 ]\n",
            "Epoch 50, Weights: [0.33129264 0.33849125 0.33021608]\n",
            "Epoch 100, Weights: [0.32929494 0.34357572 0.32712931]\n",
            "Epoch 150, Weights: [0.32730017 0.34868831 0.32401149]\n",
            "Epoch 200, Weights: [0.32530834 0.35382971 0.32086192]\n",
            "Epoch 250, Weights: [0.32331944 0.35900066 0.31767987]\n",
            "Epoch 300, Weights: [0.32133349 0.36420188 0.3144646 ]\n",
            "Epoch 350, Weights: [0.31935049 0.36943413 0.31121535]\n",
            "Epoch 400, Weights: [0.31737046 0.37469817 0.30793134]\n",
            "Epoch 450, Weights: [0.31539341 0.37999479 0.30461178]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33324073 0.33333313 0.33342611]\n",
            "Epoch 50, Weights: [0.32862663 0.33333274 0.3380406 ]\n",
            "Epoch 100, Weights: [0.32404286 0.33335064 0.34260647]\n",
            "Epoch 150, Weights: [0.31948893 0.33338668 0.34712436]\n",
            "Epoch 200, Weights: [0.31496433 0.33344073 0.3515949 ]\n",
            "Epoch 250, Weights: [0.31046859 0.33351266 0.35601872]\n",
            "Epoch 300, Weights: [0.30600122 0.33360232 0.36039643]\n",
            "Epoch 350, Weights: [0.30156177 0.3337096  0.3647286 ]\n",
            "Epoch 400, Weights: [0.29714976 0.33383437 0.36901584]\n",
            "Epoch 450, Weights: [0.29276475 0.33397651 0.3732587 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335023 0.33329078 0.33335896]\n",
            "Epoch 50, Weights: [0.33419793 0.33115315 0.33464889]\n",
            "Epoch 100, Weights: [0.33505027 0.32899478 0.33595492]\n",
            "Epoch 150, Weights: [0.33590734 0.32681535 0.33727728]\n",
            "Epoch 200, Weights: [0.33676922 0.32461453 0.33861622]\n",
            "Epoch 250, Weights: [0.337636   0.32239201 0.33997196]\n",
            "Epoch 300, Weights: [0.33850777 0.32014743 0.34134477]\n",
            "Epoch 350, Weights: [0.33938463 0.31788046 0.34273488]\n",
            "Epoch 400, Weights: [0.34026667 0.31559073 0.34414257]\n",
            "Epoch 450, Weights: [0.34115398 0.3132779  0.34556809]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330211 0.33335143 0.33334643]\n",
            "Epoch 50, Weights: [0.33172837 0.33426938 0.33400222]\n",
            "Epoch 100, Weights: [0.33012862 0.33521213 0.33465922]\n",
            "Epoch 150, Weights: [0.32850251 0.33618003 0.33531743]\n",
            "Epoch 200, Weights: [0.3268497  0.33717345 0.33597682]\n",
            "Epoch 250, Weights: [0.32516986 0.33819273 0.33663738]\n",
            "Epoch 300, Weights: [0.32346264 0.33923824 0.33729909]\n",
            "Epoch 350, Weights: [0.32172768 0.34031035 0.33796193]\n",
            "Epoch 400, Weights: [0.31996463 0.34140943 0.3386259 ]\n",
            "Epoch 450, Weights: [0.31817314 0.34253585 0.33929098]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33324791 0.33339316 0.3333589 ]\n",
            "Epoch 50, Weights: [0.32897627 0.33639615 0.33462755]\n",
            "Epoch 100, Weights: [0.32470291 0.33942039 0.33587667]\n",
            "Epoch 150, Weights: [0.32042815 0.34246553 0.33710629]\n",
            "Epoch 200, Weights: [0.31615231 0.34553125 0.33831641]\n",
            "Epoch 250, Weights: [0.31187571 0.34861718 0.33950708]\n",
            "Epoch 300, Weights: [0.30759867 0.35172299 0.34067831]\n",
            "Epoch 350, Weights: [0.30332152 0.35484831 0.34183014]\n",
            "Epoch 400, Weights: [0.29904459 0.35799278 0.3429626 ]\n",
            "Epoch 450, Weights: [0.29476821 0.36115603 0.34407573]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328855 0.33331207 0.33339935]\n",
            "Epoch 50, Weights: [0.33105017 0.33226156 0.33668824]\n",
            "Epoch 100, Weights: [0.32881213 0.33123518 0.33995266]\n",
            "Epoch 150, Weights: [0.3265744  0.33023267 0.3431929 ]\n",
            "Epoch 200, Weights: [0.3243369  0.32925377 0.34640929]\n",
            "Epoch 250, Weights: [0.32209959 0.32829825 0.34960213]\n",
            "Epoch 300, Weights: [0.31986239 0.32736585 0.35277173]\n",
            "Epoch 350, Weights: [0.31762525 0.32645632 0.3559184 ]\n",
            "Epoch 400, Weights: [0.31538809 0.32556942 0.35904247]\n",
            "Epoch 450, Weights: [0.31315085 0.32470488 0.36214424]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33340002 0.33331472 0.33328523]\n",
            "Epoch 50, Weights: [0.3367162  0.33240141 0.33088236]\n",
            "Epoch 100, Weights: [0.33999579 0.33152139 0.32848279]\n",
            "Epoch 150, Weights: [0.34323945 0.33067432 0.3260862 ]\n",
            "Epoch 200, Weights: [0.34644781 0.32985989 0.32369227]\n",
            "Epoch 250, Weights: [0.3496215  0.32907779 0.32130068]\n",
            "Epoch 300, Weights: [0.35276114 0.32832773 0.3189111 ]\n",
            "Epoch 350, Weights: [0.35586732 0.32760942 0.31652323]\n",
            "Epoch 400, Weights: [0.35894063 0.32692258 0.31413675]\n",
            "Epoch 450, Weights: [0.36198164 0.32626697 0.31175136]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331338 0.33338103 0.33330556]\n",
            "Epoch 50, Weights: [0.33231207 0.33577009 0.3319178 ]\n",
            "Epoch 100, Weights: [0.33130308 0.33816636 0.33053053]\n",
            "Epoch 150, Weights: [0.33028638 0.3405698  0.32914379]\n",
            "Epoch 200, Weights: [0.32926196 0.34298038 0.32775763]\n",
            "Epoch 250, Weights: [0.32822982 0.34539805 0.3263721 ]\n",
            "Epoch 300, Weights: [0.32718996 0.34782278 0.32498724]\n",
            "Epoch 350, Weights: [0.32614235 0.35025453 0.32360309]\n",
            "Epoch 400, Weights: [0.32508699 0.35269327 0.32221972]\n",
            "Epoch 450, Weights: [0.32402387 0.35513895 0.32083715]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338083 0.33334493 0.33327422]\n",
            "Epoch 50, Weights: [0.33574807 0.33393074 0.33032117]\n",
            "Epoch 100, Weights: [0.33810002 0.33452766 0.32737229]\n",
            "Epoch 150, Weights: [0.34043687 0.33513572 0.32442738]\n",
            "Epoch 200, Weights: [0.34275878 0.33575496 0.32148623]\n",
            "Epoch 250, Weights: [0.3450659  0.33638543 0.31854864]\n",
            "Epoch 300, Weights: [0.3473584  0.33702715 0.31561441]\n",
            "Epoch 350, Weights: [0.34963645 0.33768018 0.31268334]\n",
            "Epoch 400, Weights: [0.35190019 0.33834456 0.30975522]\n",
            "Epoch 450, Weights: [0.35414979 0.33902033 0.30682985]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333648  0.33337555 0.33325962]\n",
            "Epoch 50, Weights: [0.33493562 0.33550664 0.32955771]\n",
            "Epoch 100, Weights: [0.33650068 0.33767632 0.32582298]\n",
            "Epoch 150, Weights: [0.33806001 0.33988497 0.32205499]\n",
            "Epoch 200, Weights: [0.33961364 0.34213299 0.31825334]\n",
            "Epoch 250, Weights: [0.3411616  0.34442078 0.31441759]\n",
            "Epoch 300, Weights: [0.34270392 0.34674875 0.31054729]\n",
            "Epoch 350, Weights: [0.34424063 0.34911733 0.306642  ]\n",
            "Epoch 400, Weights: [0.34577177 0.35152695 0.30270125]\n",
            "Epoch 450, Weights: [0.34729735 0.35397805 0.29872457]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333783 0.33323417 0.33342797]\n",
            "Epoch 50, Weights: [0.33355979 0.32829532 0.33814486]\n",
            "Epoch 100, Weights: [0.33377469 0.32339316 0.34283212]\n",
            "Epoch 150, Weights: [0.33398266 0.31852732 0.34748999]\n",
            "Epoch 200, Weights: [0.33418378 0.31369744 0.35211875]\n",
            "Epoch 250, Weights: [0.33437815 0.30890315 0.35671867]\n",
            "Epoch 300, Weights: [0.33456588 0.3041441  0.36128999]\n",
            "Epoch 350, Weights: [0.33474705 0.29941994 0.36583298]\n",
            "Epoch 400, Weights: [0.33492177 0.29473032 0.37034788]\n",
            "Epoch 450, Weights: [0.33509012 0.29007489 0.37483496]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329247 0.33333796 0.33336954]\n",
            "Epoch 50, Weights: [0.33124508 0.33356862 0.33518626]\n",
            "Epoch 100, Weights: [0.32918863 0.33379715 0.33701418]\n",
            "Epoch 150, Weights: [0.32712307 0.33402354 0.33885336]\n",
            "Epoch 200, Weights: [0.32504836 0.33424777 0.34070384]\n",
            "Epoch 250, Weights: [0.32296444 0.33446986 0.34256567]\n",
            "Epoch 300, Weights: [0.32087126 0.3346898  0.34443891]\n",
            "Epoch 350, Weights: [0.31876878 0.33490758 0.3463236 ]\n",
            "Epoch 400, Weights: [0.31665695 0.33512321 0.34821981]\n",
            "Epoch 450, Weights: [0.31453572 0.33533667 0.35012758]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33342386 0.33325452 0.33332159]\n",
            "Epoch 50, Weights: [0.33793603 0.32932588 0.33273807]\n",
            "Epoch 100, Weights: [0.34242002 0.32541922 0.33216073]\n",
            "Epoch 150, Weights: [0.34687652 0.32153399 0.33158946]\n",
            "Epoch 200, Weights: [0.35130618 0.31766963 0.33102416]\n",
            "Epoch 250, Weights: [0.35570966 0.31382561 0.3304647 ]\n",
            "Epoch 300, Weights: [0.3600876  0.31000139 0.32991098]\n",
            "Epoch 350, Weights: [0.36444063 0.30619644 0.3293629 ]\n",
            "Epoch 400, Weights: [0.36876935 0.30241026 0.32882036]\n",
            "Epoch 450, Weights: [0.37307438 0.29864235 0.32828325]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328888 0.3333315  0.33337958]\n",
            "Epoch 50, Weights: [0.33105695 0.33324477 0.33569825]\n",
            "Epoch 100, Weights: [0.32880534 0.33316635 0.33802828]\n",
            "Epoch 150, Weights: [0.32653387 0.33309635 0.34036976]\n",
            "Epoch 200, Weights: [0.32424235 0.33303487 0.34272275]\n",
            "Epoch 250, Weights: [0.32193059 0.33298204 0.34508734]\n",
            "Epoch 300, Weights: [0.31959842 0.33293796 0.34746359]\n",
            "Epoch 350, Weights: [0.31724564 0.33290273 0.3498516 ]\n",
            "Epoch 400, Weights: [0.31487206 0.33287649 0.35225142]\n",
            "Epoch 450, Weights: [0.31247748 0.33285934 0.35466315]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328805 0.33341457 0.33329734]\n",
            "Epoch 50, Weights: [0.33102835 0.3374654  0.33150622]\n",
            "Epoch 100, Weights: [0.32877598 0.34149344 0.32973054]\n",
            "Epoch 150, Weights: [0.32653094 0.34549887 0.32797016]\n",
            "Epoch 200, Weights: [0.32429317 0.34948184 0.32622496]\n",
            "Epoch 250, Weights: [0.32206266 0.35344252 0.32449479]\n",
            "Epoch 300, Weights: [0.31983936 0.35738108 0.32277952]\n",
            "Epoch 350, Weights: [0.31762326 0.36129767 0.32107904]\n",
            "Epoch 400, Weights: [0.31541431 0.36519246 0.3193932 ]\n",
            "Epoch 450, Weights: [0.31321249 0.36906559 0.31772189]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333292 0.33332615 0.33334091]\n",
            "Epoch 50, Weights: [0.33331136 0.3329671  0.33372151]\n",
            "Epoch 100, Weights: [0.33328749 0.33260779 0.33410469]\n",
            "Epoch 150, Weights: [0.3332613  0.33224821 0.33449047]\n",
            "Epoch 200, Weights: [0.33323276 0.33188837 0.33487885]\n",
            "Epoch 250, Weights: [0.33320186 0.33152828 0.33526984]\n",
            "Epoch 300, Weights: [0.33316858 0.33116795 0.33566344]\n",
            "Epoch 350, Weights: [0.33313291 0.33080739 0.33605967]\n",
            "Epoch 400, Weights: [0.33309483 0.33044661 0.33645854]\n",
            "Epoch 450, Weights: [0.33305432 0.33008561 0.33686004]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328915 0.33334602 0.33336481]\n",
            "Epoch 50, Weights: [0.33109223 0.3339715  0.33493625]\n",
            "Epoch 100, Weights: [0.32891832 0.33457914 0.33650251]\n",
            "Epoch 150, Weights: [0.32676705 0.33516926 0.33806366]\n",
            "Epoch 200, Weights: [0.32463805 0.33574217 0.33961975]\n",
            "Epoch 250, Weights: [0.32253095 0.33629819 0.34117083]\n",
            "Epoch 300, Weights: [0.3204454  0.33683761 0.34271696]\n",
            "Epoch 350, Weights: [0.31838105 0.33736073 0.3442582 ]\n",
            "Epoch 400, Weights: [0.31633755 0.33786783 0.34579459]\n",
            "Epoch 450, Weights: [0.31431459 0.33835919 0.34732618]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333961 0.33326152 0.33339884]\n",
            "Epoch 50, Weights: [0.33365422 0.32965512 0.33669063]\n",
            "Epoch 100, Weights: [0.33396972 0.32601659 0.34001366]\n",
            "Epoch 150, Weights: [0.3342861  0.32234601 0.34336786]\n",
            "Epoch 200, Weights: [0.33460334 0.31864347 0.34675316]\n",
            "Epoch 250, Weights: [0.33492142 0.31490908 0.35016947]\n",
            "Epoch 300, Weights: [0.33524032 0.31114295 0.3536167 ]\n",
            "Epoch 350, Weights: [0.33556004 0.30734521 0.35709472]\n",
            "Epoch 400, Weights: [0.33588054 0.30351599 0.36060344]\n",
            "Epoch 450, Weights: [0.33620181 0.29965545 0.36414271]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333858 0.33336226 0.33329913]\n",
            "Epoch 50, Weights: [0.33359587 0.33479939 0.33160471]\n",
            "Epoch 100, Weights: [0.33384203 0.33621771 0.32994023]\n",
            "Epoch 150, Weights: [0.33407727 0.33761753 0.32830517]\n",
            "Epoch 200, Weights: [0.33430177 0.33899918 0.32669903]\n",
            "Epoch 250, Weights: [0.33451573 0.34036296 0.32512128]\n",
            "Epoch 300, Weights: [0.33471934 0.34170918 0.32357145]\n",
            "Epoch 350, Weights: [0.33491279 0.34303814 0.32204904]\n",
            "Epoch 400, Weights: [0.33509627 0.34435012 0.32055358]\n",
            "Epoch 450, Weights: [0.33526994 0.34564542 0.31908461]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328776 0.33340308 0.33330913]\n",
            "Epoch 50, Weights: [0.33102126 0.33687904 0.33209967]\n",
            "Epoch 100, Weights: [0.32877723 0.34033199 0.33089075]\n",
            "Epoch 150, Weights: [0.32655527 0.34376237 0.32968232]\n",
            "Epoch 200, Weights: [0.32435504 0.34717058 0.32847434]\n",
            "Epoch 250, Weights: [0.32217618 0.35055703 0.32726677]\n",
            "Epoch 300, Weights: [0.32001833 0.35392209 0.32605955]\n",
            "Epoch 350, Weights: [0.31788115 0.35726615 0.32485266]\n",
            "Epoch 400, Weights: [0.31576432 0.3605896  0.32364606]\n",
            "Epoch 450, Weights: [0.3136675  0.36389278 0.32243969]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338971 0.33327276 0.33333751]\n",
            "Epoch 50, Weights: [0.33622403 0.33023131 0.33354463]\n",
            "Epoch 100, Weights: [0.33908829 0.32716382 0.33374786]\n",
            "Epoch 150, Weights: [0.34198307 0.32406968 0.33394722]\n",
            "Epoch 200, Weights: [0.34490895 0.32094831 0.33414272]\n",
            "Epoch 250, Weights: [0.34786652 0.31779908 0.33433437]\n",
            "Epoch 300, Weights: [0.35085642 0.31462136 0.33452218]\n",
            "Epoch 350, Weights: [0.35387928 0.31141451 0.33470618]\n",
            "Epoch 400, Weights: [0.35693575 0.30817786 0.33488636]\n",
            "Epoch 450, Weights: [0.36002651 0.30491073 0.33506273]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333712 0.33341914 0.33324371]\n",
            "Epoch 50, Weights: [0.33353918 0.33770934 0.32875144]\n",
            "Epoch 100, Weights: [0.33376528 0.34199879 0.3242359 ]\n",
            "Epoch 150, Weights: [0.33401539 0.34628809 0.31969649]\n",
            "Epoch 200, Weights: [0.33428948 0.35057786 0.31513262]\n",
            "Epoch 250, Weights: [0.33458756 0.35486873 0.31054369]\n",
            "Epoch 300, Weights: [0.3349096  0.35916131 0.30592906]\n",
            "Epoch 350, Weights: [0.33525563 0.36345622 0.30128812]\n",
            "Epoch 400, Weights: [0.33562565 0.36775411 0.29662021]\n",
            "Epoch 450, Weights: [0.33601969 0.3720556  0.29192468]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3332687  0.33343666 0.33329462]\n",
            "Epoch 50, Weights: [0.33003628 0.33862063 0.33134307]\n",
            "Epoch 100, Weights: [0.32680225 0.3438381  0.32935962]\n",
            "Epoch 150, Weights: [0.32356721 0.3490885  0.32734426]\n",
            "Epoch 200, Weights: [0.32033179 0.35437123 0.32529696]\n",
            "Epoch 250, Weights: [0.31709659 0.35968566 0.32321772]\n",
            "Epoch 300, Weights: [0.31386226 0.36503116 0.32110655]\n",
            "Epoch 350, Weights: [0.31062941 0.37040707 0.31896349]\n",
            "Epoch 400, Weights: [0.3073987  0.37581269 0.31678858]\n",
            "Epoch 450, Weights: [0.30417075 0.38124733 0.31458188]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333673 0.33339516 0.33326808]\n",
            "Epoch 50, Weights: [0.33350623 0.33648665 0.33000709]\n",
            "Epoch 100, Weights: [0.33367427 0.33957786 0.32674784]\n",
            "Epoch 150, Weights: [0.33384083 0.34266888 0.32349026]\n",
            "Epoch 200, Weights: [0.33400593 0.34575977 0.32023427]\n",
            "Epoch 250, Weights: [0.33416956 0.34885061 0.31697979]\n",
            "Epoch 300, Weights: [0.33433173 0.35194149 0.31372674]\n",
            "Epoch 350, Weights: [0.33449243 0.35503249 0.31047504]\n",
            "Epoch 400, Weights: [0.33465167 0.35812368 0.30722462]\n",
            "Epoch 450, Weights: [0.33480944 0.36121514 0.30397538]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33321238 0.33341247 0.33337511]\n",
            "Epoch 50, Weights: [0.3271711  0.33740352 0.33542535]\n",
            "Epoch 100, Weights: [0.32114122 0.34146046 0.33739829]\n",
            "Epoch 150, Weights: [0.31512296 0.34558336 0.33929365]\n",
            "Epoch 200, Weights: [0.30911653 0.34977228 0.34111116]\n",
            "Epoch 250, Weights: [0.30312218 0.35402725 0.34285054]\n",
            "Epoch 300, Weights: [0.29714015 0.35834831 0.34451151]\n",
            "Epoch 350, Weights: [0.29117073 0.36273545 0.34609379]\n",
            "Epoch 400, Weights: [0.2852142  0.36718869 0.34759708]\n",
            "Epoch 450, Weights: [0.27927087 0.37170797 0.34902112]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334093 0.33332037 0.33333867]\n",
            "Epoch 50, Weights: [0.33372281 0.33267226 0.3336049 ]\n",
            "Epoch 100, Weights: [0.33410773 0.33202368 0.33386856]\n",
            "Epoch 150, Weights: [0.33449567 0.33137465 0.33412965]\n",
            "Epoch 200, Weights: [0.33488663 0.33072518 0.33438816]\n",
            "Epoch 250, Weights: [0.33528059 0.33007531 0.33464407]\n",
            "Epoch 300, Weights: [0.33567755 0.32942506 0.33489736]\n",
            "Epoch 350, Weights: [0.33607749 0.32877445 0.33514803]\n",
            "Epoch 400, Weights: [0.3364804  0.32812351 0.33539606]\n",
            "Epoch 450, Weights: [0.33688627 0.32747226 0.33564143]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331327 0.33330251 0.33338419]\n",
            "Epoch 50, Weights: [0.33230324 0.33176333 0.3359334 ]\n",
            "Epoch 100, Weights: [0.33127848 0.33022712 0.33849437]\n",
            "Epoch 150, Weights: [0.33023902 0.32869382 0.34106713]\n",
            "Epoch 200, Weights: [0.32918491 0.32716337 0.34365169]\n",
            "Epoch 250, Weights: [0.32811618 0.3256357  0.34624809]\n",
            "Epoch 300, Weights: [0.32703289 0.32411074 0.34885634]\n",
            "Epoch 350, Weights: [0.32593506 0.32258844 0.35147646]\n",
            "Epoch 400, Weights: [0.32482275 0.32106874 0.35410848]\n",
            "Epoch 450, Weights: [0.323696   0.31955156 0.35675241]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33324515 0.33337621 0.33337861]\n",
            "Epoch 50, Weights: [0.32882117 0.33552269 0.33565611]\n",
            "Epoch 100, Weights: [0.32436726 0.33767324 0.33795947]\n",
            "Epoch 150, Weights: [0.31988359 0.33982758 0.34028879]\n",
            "Epoch 200, Weights: [0.31537037 0.34198545 0.34264415]\n",
            "Epoch 250, Weights: [0.31082779 0.34414655 0.34502563]\n",
            "Epoch 300, Weights: [0.30625606 0.34631061 0.34743331]\n",
            "Epoch 350, Weights: [0.3016554  0.34847732 0.34986725]\n",
            "Epoch 400, Weights: [0.29702604 0.3506464  0.35232754]\n",
            "Epoch 450, Weights: [0.29236821 0.35281754 0.35481421]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337857 0.33331019 0.33331121]\n",
            "Epoch 50, Weights: [0.33564571 0.33214762 0.33220664]\n",
            "Epoch 100, Weights: [0.33792265 0.3309734  0.33110392]\n",
            "Epoch 150, Weights: [0.34020954 0.32978744 0.33000299]\n",
            "Epoch 200, Weights: [0.34250655 0.32858965 0.32890377]\n",
            "Epoch 250, Weights: [0.34481383 0.32737995 0.32780619]\n",
            "Epoch 300, Weights: [0.34713155 0.32615825 0.32671017]\n",
            "Epoch 350, Weights: [0.34945987 0.32492445 0.32561565]\n",
            "Epoch 400, Weights: [0.35179896 0.32367847 0.32452254]\n",
            "Epoch 450, Weights: [0.35414898 0.32242021 0.32343078]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33327383 0.33338348 0.33334266]\n",
            "Epoch 50, Weights: [0.33027877 0.33590458 0.33381663]\n",
            "Epoch 100, Weights: [0.32724324 0.33845222 0.33430451]\n",
            "Epoch 150, Weights: [0.32416676 0.34102663 0.33480657]\n",
            "Epoch 200, Weights: [0.32104883 0.34362803 0.3353231 ]\n",
            "Epoch 250, Weights: [0.31788894 0.34625666 0.33585437]\n",
            "Epoch 300, Weights: [0.31468658 0.34891273 0.33640066]\n",
            "Epoch 350, Weights: [0.31144124 0.35159647 0.33696226]\n",
            "Epoch 400, Weights: [0.30815241 0.3543081  0.33753945]\n",
            "Epoch 450, Weights: [0.30481957 0.35704787 0.33813253]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334835 0.33331481 0.33333681]\n",
            "Epoch 50, Weights: [0.3340994  0.33238716 0.33351341]\n",
            "Epoch 100, Weights: [0.33484987 0.33145527 0.33369484]\n",
            "Epoch 150, Weights: [0.33559976 0.33051914 0.33388107]\n",
            "Epoch 200, Weights: [0.33634909 0.32957877 0.33407211]\n",
            "Epoch 250, Weights: [0.33709786 0.32863417 0.33426794]\n",
            "Epoch 300, Weights: [0.33784608 0.32768534 0.33446855]\n",
            "Epoch 350, Weights: [0.33859375 0.32673228 0.33467394]\n",
            "Epoch 400, Weights: [0.33934089 0.32577499 0.3348841 ]\n",
            "Epoch 450, Weights: [0.34008749 0.32481347 0.33509901]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33325216 0.33337038 0.33337743]\n",
            "Epoch 50, Weights: [0.32919694 0.33523772 0.33556531]\n",
            "Epoch 100, Weights: [0.32514739 0.33713383 0.33771875]\n",
            "Epoch 150, Weights: [0.32110369 0.33905869 0.33983759]\n",
            "Epoch 200, Weights: [0.31706602 0.34101226 0.34192169]\n",
            "Epoch 250, Weights: [0.31303455 0.3429945  0.34397092]\n",
            "Epoch 300, Weights: [0.30900943 0.3450054  0.34598514]\n",
            "Epoch 350, Weights: [0.30499082 0.34704491 0.34796424]\n",
            "Epoch 400, Weights: [0.30097888 0.34911301 0.34990808]\n",
            "Epoch 450, Weights: [0.29697374 0.35120966 0.35181657]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33339058 0.33328316 0.33332624]\n",
            "Epoch 50, Weights: [0.33627578 0.33076539 0.33295881]\n",
            "Epoch 100, Weights: [0.33920547 0.32822884 0.33256566]\n",
            "Epoch 150, Weights: [0.34218007 0.32567335 0.33214655]\n",
            "Epoch 200, Weights: [0.34519999 0.32309877 0.33170121]\n",
            "Epoch 250, Weights: [0.34826564 0.32050494 0.33122939]\n",
            "Epoch 300, Weights: [0.35137742 0.31789173 0.33073082]\n",
            "Epoch 350, Weights: [0.35453574 0.31525897 0.33020526]\n",
            "Epoch 400, Weights: [0.357741   0.31260652 0.32965245]\n",
            "Epoch 450, Weights: [0.36099359 0.30993424 0.32907214]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336927 0.33332549 0.33330522]\n",
            "Epoch 50, Weights: [0.33516424 0.33292524 0.33191049]\n",
            "Epoch 100, Weights: [0.33695491 0.33250854 0.33053652]\n",
            "Epoch 150, Weights: [0.33874134 0.33207552 0.32918311]\n",
            "Epoch 200, Weights: [0.34052358 0.33162632 0.32785007]\n",
            "Epoch 250, Weights: [0.34230172 0.33116105 0.3265372 ]\n",
            "Epoch 300, Weights: [0.3440758  0.33067985 0.32524432]\n",
            "Epoch 350, Weights: [0.34584589 0.33018284 0.32397124]\n",
            "Epoch 400, Weights: [0.34761205 0.32967014 0.32271778]\n",
            "Epoch 450, Weights: [0.34937434 0.32914187 0.32148376]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33339664 0.33332095 0.33328238]\n",
            "Epoch 50, Weights: [0.33658625 0.33268829 0.33072543]\n",
            "Epoch 100, Weights: [0.33982271 0.3320285  0.32814876]\n",
            "Epoch 150, Weights: [0.34310664 0.3313413  0.32555203]\n",
            "Epoch 200, Weights: [0.34643863 0.33062641 0.32293493]\n",
            "Epoch 250, Weights: [0.34981929 0.32988356 0.32029713]\n",
            "Epoch 300, Weights: [0.35324922 0.32911245 0.31763831]\n",
            "Epoch 350, Weights: [0.35672903 0.3283128  0.31495814]\n",
            "Epoch 400, Weights: [0.36025933 0.32748433 0.31225631]\n",
            "Epoch 450, Weights: [0.36384075 0.32662673 0.30953249]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335921 0.33324092 0.33339984]\n",
            "Epoch 50, Weights: [0.33464663 0.32863367 0.33671968]\n",
            "Epoch 100, Weights: [0.33592103 0.32405121 0.34002772]\n",
            "Epoch 150, Weights: [0.33718258 0.31949327 0.34332412]\n",
            "Epoch 200, Weights: [0.33843143 0.31495953 0.34660901]\n",
            "Epoch 250, Weights: [0.33966773 0.31044971 0.34988254]\n",
            "Epoch 300, Weights: [0.34089163 0.30596351 0.35314483]\n",
            "Epoch 350, Weights: [0.34210329 0.30150065 0.35639603]\n",
            "Epoch 400, Weights: [0.34330285 0.29706084 0.35963628]\n",
            "Epoch 450, Weights: [0.34449045 0.29264382 0.3628657 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329145 0.33334298 0.33336554]\n",
            "Epoch 50, Weights: [0.33119278 0.33382859 0.3349786 ]\n",
            "Epoch 100, Weights: [0.32908399 0.33431972 0.33659626]\n",
            "Epoch 150, Weights: [0.32696504 0.3348164  0.33821853]\n",
            "Epoch 200, Weights: [0.32483587 0.33531865 0.33984545]\n",
            "Epoch 250, Weights: [0.32269645 0.33582649 0.34147702]\n",
            "Epoch 300, Weights: [0.32054674 0.33633994 0.34311329]\n",
            "Epoch 350, Weights: [0.31838668 0.33685903 0.34475426]\n",
            "Epoch 400, Weights: [0.31621624 0.33738378 0.34639996]\n",
            "Epoch 450, Weights: [0.31403536 0.3379142  0.34805041]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332368 0.3332984  0.33337789]\n",
            "Epoch 50, Weights: [0.33284522 0.33154513 0.33560962]\n",
            "Epoch 100, Weights: [0.33237462 0.32977779 0.33784756]\n",
            "Epoch 150, Weights: [0.33191199 0.32799632 0.34009166]\n",
            "Epoch 200, Weights: [0.33145746 0.32620065 0.34234187]\n",
            "Epoch 250, Weights: [0.33101115 0.32439069 0.34459813]\n",
            "Epoch 300, Weights: [0.3305732  0.32256639 0.34686038]\n",
            "Epoch 350, Weights: [0.33014374 0.32072767 0.34912856]\n",
            "Epoch 400, Weights: [0.32972289 0.31887447 0.35140261]\n",
            "Epoch 450, Weights: [0.32931078 0.31700672 0.35368248]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331648 0.33328669 0.3333968 ]\n",
            "Epoch 50, Weights: [0.33246061 0.33094772 0.33659165]\n",
            "Epoch 100, Weights: [0.331578   0.32859425 0.33982771]\n",
            "Epoch 150, Weights: [0.3306683  0.32622649 0.34310518]\n",
            "Epoch 200, Weights: [0.32973113 0.32384461 0.34642424]\n",
            "Epoch 250, Weights: [0.3287661  0.32144881 0.34978505]\n",
            "Epoch 300, Weights: [0.32777286 0.31903931 0.3531878 ]\n",
            "Epoch 350, Weights: [0.32675103 0.31661631 0.35663263]\n",
            "Epoch 400, Weights: [0.32570024 0.31418003 0.3601197 ]\n",
            "Epoch 450, Weights: [0.32462013 0.31173071 0.36364913]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33340206 0.33339235 0.33320556]\n",
            "Epoch 50, Weights: [0.33683961 0.3363511  0.32680926]\n",
            "Epoch 100, Weights: [0.34027905 0.33932465 0.32039627]\n",
            "Epoch 150, Weights: [0.34372069 0.34231337 0.31396592]\n",
            "Epoch 200, Weights: [0.3471648  0.34531764 0.30751753]\n",
            "Epoch 250, Weights: [0.35061168 0.34833785 0.30105044]\n",
            "Epoch 300, Weights: [0.35406163 0.35137439 0.29456395]\n",
            "Epoch 350, Weights: [0.35751492 0.35442766 0.28805739]\n",
            "Epoch 400, Weights: [0.36097187 0.35749806 0.28153004]\n",
            "Epoch 450, Weights: [0.36443277 0.360586   0.27498121]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337866 0.33331103 0.33331028]\n",
            "Epoch 50, Weights: [0.33564089 0.33219808 0.332161  ]\n",
            "Epoch 100, Weights: [0.33789451 0.3310886  0.33101686]\n",
            "Epoch 150, Weights: [0.34013954 0.32998257 0.32987786]\n",
            "Epoch 200, Weights: [0.34237599 0.32887999 0.32874399]\n",
            "Epoch 250, Weights: [0.34460388 0.32778087 0.32761522]\n",
            "Epoch 300, Weights: [0.34682323 0.32668519 0.32649156]\n",
            "Epoch 350, Weights: [0.34903405 0.32559295 0.32537297]\n",
            "Epoch 400, Weights: [0.35123638 0.32450415 0.32425944]\n",
            "Epoch 450, Weights: [0.35343021 0.32341879 0.32315097]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333493 0.33333382 0.33333121]\n",
            "Epoch 50, Weights: [0.33341544 0.33335889 0.33322564]\n",
            "Epoch 100, Weights: [0.33349604 0.33338416 0.33311977]\n",
            "Epoch 150, Weights: [0.33357675 0.33340962 0.3330136 ]\n",
            "Epoch 200, Weights: [0.33365756 0.33343527 0.33290714]\n",
            "Epoch 250, Weights: [0.33373846 0.33346112 0.33280039]\n",
            "Epoch 300, Weights: [0.33381946 0.33348717 0.33269333]\n",
            "Epoch 350, Weights: [0.33390057 0.33351342 0.33258598]\n",
            "Epoch 400, Weights: [0.33398177 0.33353987 0.33247833]\n",
            "Epoch 450, Weights: [0.33406308 0.33356652 0.33237038]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334606 0.33326884 0.33338507]\n",
            "Epoch 50, Weights: [0.33398629 0.33004267 0.33597101]\n",
            "Epoch 100, Weights: [0.33463319 0.32681279 0.338554  ]\n",
            "Epoch 150, Weights: [0.33528678 0.32357923 0.34113396]\n",
            "Epoch 200, Weights: [0.33594713 0.32034202 0.34371083]\n",
            "Epoch 250, Weights: [0.33661425 0.31710119 0.34628452]\n",
            "Epoch 300, Weights: [0.33728821 0.31385678 0.34885498]\n",
            "Epoch 350, Weights: [0.33796904 0.31060881 0.35142212]\n",
            "Epoch 400, Weights: [0.33865678 0.30735732 0.35398587]\n",
            "Epoch 450, Weights: [0.33935147 0.30410234 0.35654616]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338752 0.33332882 0.33328363]\n",
            "Epoch 50, Weights: [0.33609208 0.33311683 0.33079107]\n",
            "Epoch 100, Weights: [0.33878649 0.33293043 0.32828305]\n",
            "Epoch 150, Weights: [0.34147051 0.33276983 0.32575963]\n",
            "Epoch 200, Weights: [0.34414394 0.3326352  0.32322083]\n",
            "Epoch 250, Weights: [0.34680653 0.33252673 0.32066672]\n",
            "Epoch 300, Weights: [0.34945806 0.3324446  0.31809732]\n",
            "Epoch 350, Weights: [0.3520983  0.332389   0.31551267]\n",
            "Epoch 400, Weights: [0.35472702 0.33236012 0.31291284]\n",
            "Epoch 450, Weights: [0.35734399 0.33235814 0.31029784]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329757 0.3333753  0.3333271 ]\n",
            "Epoch 50, Weights: [0.33151754 0.33546677 0.33301566]\n",
            "Epoch 100, Weights: [0.32975233 0.33754374 0.33270389]\n",
            "Epoch 150, Weights: [0.32800176 0.33960639 0.33239183]\n",
            "Epoch 200, Weights: [0.32626562 0.34165487 0.33207948]\n",
            "Epoch 250, Weights: [0.32454375 0.34368934 0.33176688]\n",
            "Epoch 300, Weights: [0.32283595 0.34570996 0.33145406]\n",
            "Epoch 350, Weights: [0.32114206 0.34771689 0.33114103]\n",
            "Epoch 400, Weights: [0.31946189 0.34971027 0.33082781]\n",
            "Epoch 450, Weights: [0.31779527 0.35169026 0.33051444]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332385 0.3333282  0.33334792]\n",
            "Epoch 50, Weights: [0.33284806 0.33307254 0.33407937]\n",
            "Epoch 100, Weights: [0.33236802 0.33281764 0.33481431]\n",
            "Epoch 150, Weights: [0.33188371 0.33256354 0.33555272]\n",
            "Epoch 200, Weights: [0.3313951  0.33231028 0.33629459]\n",
            "Epoch 250, Weights: [0.33090216 0.33205789 0.33703992]\n",
            "Epoch 300, Weights: [0.33040487 0.33180641 0.33778868]\n",
            "Epoch 350, Weights: [0.32990321 0.33155588 0.33854088]\n",
            "Epoch 400, Weights: [0.32939715 0.33130632 0.3392965 ]\n",
            "Epoch 450, Weights: [0.32888666 0.33105779 0.34005553]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335505 0.33328112 0.3333638 ]\n",
            "Epoch 50, Weights: [0.3344513  0.33066627 0.3348824 ]\n",
            "Epoch 100, Weights: [0.33556735 0.32804199 0.33639063]\n",
            "Epoch 150, Weights: [0.33670323 0.32540828 0.33788846]\n",
            "Epoch 200, Weights: [0.33785895 0.32276515 0.33937587]\n",
            "Epoch 250, Weights: [0.33903452 0.3201126  0.34085285]\n",
            "Epoch 300, Weights: [0.34022996 0.31745063 0.34231938]\n",
            "Epoch 350, Weights: [0.34144529 0.31477926 0.34377543]\n",
            "Epoch 400, Weights: [0.34268051 0.31209849 0.34522098]\n",
            "Epoch 450, Weights: [0.34393563 0.30940833 0.34665601]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337156 0.33332516 0.33330324]\n",
            "Epoch 50, Weights: [0.33526987 0.33291725 0.33181285]\n",
            "Epoch 100, Weights: [0.33714167 0.33250942 0.33034889]\n",
            "Epoch 150, Weights: [0.33898743 0.33210172 0.32891081]\n",
            "Epoch 200, Weights: [0.34080763 0.33169422 0.32749812]\n",
            "Epoch 250, Weights: [0.34260271 0.33128695 0.3261103 ]\n",
            "Epoch 300, Weights: [0.34437312 0.33087997 0.32474688]\n",
            "Epoch 350, Weights: [0.34611929 0.33047331 0.32340736]\n",
            "Epoch 400, Weights: [0.34784165 0.33006703 0.3220913 ]\n",
            "Epoch 450, Weights: [0.34954059 0.32966116 0.32079822]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333426 0.33329289 0.33337282]\n",
            "Epoch 50, Weights: [0.33339102 0.33126269 0.33534626]\n",
            "Epoch 100, Weights: [0.33346711 0.32921582 0.33731704]\n",
            "Epoch 150, Weights: [0.33356243 0.32715236 0.33928518]\n",
            "Epoch 200, Weights: [0.33367685 0.32507239 0.34125073]\n",
            "Epoch 250, Weights: [0.33381027 0.32297598 0.34321372]\n",
            "Epoch 300, Weights: [0.33396255 0.32086321 0.3451742 ]\n",
            "Epoch 350, Weights: [0.3341336  0.31873416 0.34713221]\n",
            "Epoch 400, Weights: [0.3343233  0.3165889  0.34908778]\n",
            "Epoch 450, Weights: [0.33453153 0.31442749 0.35104096]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332867 0.33334852 0.33332278]\n",
            "Epoch 50, Weights: [0.33309622 0.33410995 0.33279381]\n",
            "Epoch 100, Weights: [0.33286441 0.33487476 0.33226081]\n",
            "Epoch 150, Weights: [0.33263325 0.33564296 0.33172376]\n",
            "Epoch 200, Weights: [0.33240276 0.33641459 0.33118262]\n",
            "Epoch 250, Weights: [0.33217295 0.33718966 0.33063736]\n",
            "Epoch 300, Weights: [0.33194383 0.33796819 0.33008794]\n",
            "Epoch 350, Weights: [0.33171543 0.33875021 0.32953433]\n",
            "Epoch 400, Weights: [0.33148775 0.33953573 0.32897649]\n",
            "Epoch 450, Weights: [0.33126081 0.34032477 0.32841439]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3332731  0.33330046 0.33342641]\n",
            "Epoch 50, Weights: [0.33024716 0.33166387 0.33808895]\n",
            "Epoch 100, Weights: [0.32719207 0.33004001 0.34276788]\n",
            "Epoch 150, Weights: [0.32410783 0.32842916 0.34746298]\n",
            "Epoch 200, Weights: [0.32099444 0.32683155 0.35217398]\n",
            "Epoch 250, Weights: [0.31785187 0.32524744 0.35690066]\n",
            "Epoch 300, Weights: [0.31468012 0.32367709 0.36164276]\n",
            "Epoch 350, Weights: [0.31147918 0.32212073 0.36640005]\n",
            "Epoch 400, Weights: [0.30824905 0.32057862 0.37117229]\n",
            "Epoch 450, Weights: [0.30498973 0.31905101 0.37595923]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335654 0.33327299 0.33337043]\n",
            "Epoch 50, Weights: [0.33453743 0.33025029 0.33521224]\n",
            "Epoch 100, Weights: [0.33575727 0.32721517 0.33702753]\n",
            "Epoch 150, Weights: [0.33701619 0.32416742 0.33881636]\n",
            "Epoch 200, Weights: [0.33831433 0.32110682 0.34057882]\n",
            "Epoch 250, Weights: [0.33965182 0.31803315 0.342315  ]\n",
            "Epoch 300, Weights: [0.34102881 0.31494621 0.34402495]\n",
            "Epoch 350, Weights: [0.34244545 0.31184576 0.34570876]\n",
            "Epoch 400, Weights: [0.34390189 0.3087316  0.34736648]\n",
            "Epoch 450, Weights: [0.34539828 0.3056035  0.34899819]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33339154 0.33325547 0.33335297]\n",
            "Epoch 50, Weights: [0.33631245 0.32935673 0.33433079]\n",
            "Epoch 100, Weights: [0.33925385 0.32544616 0.33529996]\n",
            "Epoch 150, Weights: [0.34221609 0.32152339 0.33626049]\n",
            "Epoch 200, Weights: [0.34519954 0.31758803 0.3372124 ]\n",
            "Epoch 250, Weights: [0.34820457 0.31363971 0.33815569]\n",
            "Epoch 300, Weights: [0.35123155 0.30967805 0.33909037]\n",
            "Epoch 350, Weights: [0.35428086 0.30570267 0.34001644]\n",
            "Epoch 400, Weights: [0.35735288 0.30171318 0.34093391]\n",
            "Epoch 450, Weights: [0.36044799 0.29770919 0.34184279]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331465 0.33327864 0.33340669]\n",
            "Epoch 50, Weights: [0.33237828 0.33053949 0.33708221]\n",
            "Epoch 100, Weights: [0.33143681 0.32779085 0.34077231]\n",
            "Epoch 150, Weights: [0.3304902  0.32503257 0.34447721]\n",
            "Epoch 200, Weights: [0.32953837 0.32226448 0.34819712]\n",
            "Epoch 250, Weights: [0.32858126 0.31948644 0.35193227]\n",
            "Epoch 300, Weights: [0.32761882 0.31669826 0.35568289]\n",
            "Epoch 350, Weights: [0.32665098 0.31389978 0.35944921]\n",
            "Epoch 400, Weights: [0.32567767 0.31109085 0.36323145]\n",
            "Epoch 450, Weights: [0.32469884 0.30827129 0.36702985]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330166 0.33332733 0.33337098]\n",
            "Epoch 50, Weights: [0.33171055 0.33303723 0.33525219]\n",
            "Epoch 100, Weights: [0.33010341 0.33276588 0.33713068]\n",
            "Epoch 150, Weights: [0.32848025 0.33251331 0.3390064 ]\n",
            "Epoch 200, Weights: [0.3268411  0.33227957 0.34087931]\n",
            "Epoch 250, Weights: [0.32518595 0.33206468 0.34274934]\n",
            "Epoch 300, Weights: [0.32351484 0.33186867 0.34461646]\n",
            "Epoch 350, Weights: [0.32182777 0.33169158 0.34648061]\n",
            "Epoch 400, Weights: [0.32012476 0.33153345 0.34834175]\n",
            "Epoch 450, Weights: [0.31840583 0.33139431 0.35019982]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.333297   0.33341619 0.33328678]\n",
            "Epoch 50, Weights: [0.33148496 0.33755022 0.33096479]\n",
            "Epoch 100, Weights: [0.32968114 0.34166619 0.32865264]\n",
            "Epoch 150, Weights: [0.32788538 0.34576447 0.32635012]\n",
            "Epoch 200, Weights: [0.3260975  0.34984543 0.32405704]\n",
            "Epoch 250, Weights: [0.32431734 0.35390943 0.3217732 ]\n",
            "Epoch 300, Weights: [0.32254474 0.35795683 0.3194984 ]\n",
            "Epoch 350, Weights: [0.32077955 0.36198798 0.31723245]\n",
            "Epoch 400, Weights: [0.3190216  0.36600322 0.31497515]\n",
            "Epoch 450, Weights: [0.31727074 0.3700029  0.31272632]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335465 0.33327435 0.33337096]\n",
            "Epoch 50, Weights: [0.33442495 0.33032523 0.33524979]\n",
            "Epoch 100, Weights: [0.33550254 0.32737491 0.33712252]\n",
            "Epoch 150, Weights: [0.33658742 0.32442348 0.33898907]\n",
            "Epoch 200, Weights: [0.33767958 0.32147103 0.34084936]\n",
            "Epoch 250, Weights: [0.33877902 0.31851763 0.34270331]\n",
            "Epoch 300, Weights: [0.33988574 0.31556339 0.34455084]\n",
            "Epoch 350, Weights: [0.34099972 0.31260837 0.34639188]\n",
            "Epoch 400, Weights: [0.34212096 0.30965267 0.34822634]\n",
            "Epoch 450, Weights: [0.34324945 0.30669636 0.35005416]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328576 0.33334555 0.33336865]\n",
            "Epoch 50, Weights: [0.33091323 0.33395109 0.33513565]\n",
            "Epoch 100, Weights: [0.32855173 0.33454489 0.33690334]\n",
            "Epoch 150, Weights: [0.32620142 0.33512704 0.33867151]\n",
            "Epoch 200, Weights: [0.32386243 0.3356976  0.34043994]\n",
            "Epoch 250, Weights: [0.32153491 0.33625667 0.3422084 ]\n",
            "Epoch 300, Weights: [0.31921899 0.33680431 0.34397668]\n",
            "Epoch 350, Weights: [0.3169148  0.3373406  0.34574456]\n",
            "Epoch 400, Weights: [0.31462248 0.33786564 0.34751184]\n",
            "Epoch 450, Weights: [0.31234216 0.33837951 0.34927831]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331443 0.33340976 0.33327578]\n",
            "Epoch 50, Weights: [0.33237958 0.33720188 0.33041852]\n",
            "Epoch 100, Weights: [0.33146385 0.34093665 0.32759946]\n",
            "Epoch 150, Weights: [0.3305668  0.34461536 0.32481781]\n",
            "Epoch 200, Weights: [0.32968796 0.34823924 0.32207277]\n",
            "Epoch 250, Weights: [0.32882692 0.35180948 0.31936358]\n",
            "Epoch 300, Weights: [0.32798324 0.35532724 0.31668949]\n",
            "Epoch 350, Weights: [0.32715652 0.35879366 0.31404978]\n",
            "Epoch 400, Weights: [0.32634637 0.36220984 0.31144376]\n",
            "Epoch 450, Weights: [0.32555239 0.36557686 0.30887072]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337946 0.33339625 0.33322426]\n",
            "Epoch 50, Weights: [0.33567976 0.33654202 0.32777819]\n",
            "Epoch 100, Weights: [0.33796769 0.33968696 0.32234531]\n",
            "Epoch 150, Weights: [0.34024357 0.34283155 0.31692485]\n",
            "Epoch 200, Weights: [0.34250772 0.34597623 0.31151602]\n",
            "Epoch 250, Weights: [0.34476044 0.34912147 0.30611806]\n",
            "Epoch 300, Weights: [0.34700205 0.35226772 0.3007302 ]\n",
            "Epoch 350, Weights: [0.34923285 0.35541545 0.29535167]\n",
            "Epoch 400, Weights: [0.35145314 0.3585651  0.28998173]\n",
            "Epoch 450, Weights: [0.35366322 0.36171713 0.28461962]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335096 0.3332913  0.33335771]\n",
            "Epoch 50, Weights: [0.33423853 0.33117991 0.33458153]\n",
            "Epoch 100, Weights: [0.33513748 0.3290482  0.33581429]\n",
            "Epoch 150, Weights: [0.33604794 0.32689598 0.33705605]\n",
            "Epoch 200, Weights: [0.33697    0.32472308 0.33830689]\n",
            "Epoch 250, Weights: [0.33790377 0.3225293  0.33956689]\n",
            "Epoch 300, Weights: [0.33884937 0.32031448 0.34083613]\n",
            "Epoch 350, Weights: [0.33980689 0.31807841 0.34211467]\n",
            "Epoch 400, Weights: [0.34077645 0.31582092 0.3434026 ]\n",
            "Epoch 450, Weights: [0.34175817 0.31354181 0.34469999]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336466 0.33329607 0.33333924]\n",
            "Epoch 50, Weights: [0.33492957 0.33143083 0.33363957]\n",
            "Epoch 100, Weights: [0.33649057 0.3295603  0.3339491 ]\n",
            "Epoch 150, Weights: [0.33804783 0.32768444 0.3342677 ]\n",
            "Epoch 200, Weights: [0.33960151 0.32580323 0.33459523]\n",
            "Epoch 250, Weights: [0.34115176 0.32391663 0.33493157]\n",
            "Epoch 300, Weights: [0.34269875 0.32202462 0.3352766 ]\n",
            "Epoch 350, Weights: [0.34424262 0.32012716 0.33563019]\n",
            "Epoch 400, Weights: [0.34578353 0.31822422 0.33599222]\n",
            "Epoch 450, Weights: [0.34732163 0.31631576 0.33636259]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33327818 0.33332415 0.33339764]\n",
            "Epoch 50, Weights: [0.33052595 0.33286193 0.33661209]\n",
            "Epoch 100, Weights: [0.32778303 0.33239261 0.33982433]\n",
            "Epoch 150, Weights: [0.32504937 0.33191613 0.34303447]\n",
            "Epoch 200, Weights: [0.32232489 0.33143244 0.34624264]\n",
            "Epoch 250, Weights: [0.31960954 0.33094146 0.34944897]\n",
            "Epoch 300, Weights: [0.31690325 0.33044314 0.35265359]\n",
            "Epoch 350, Weights: [0.31420595 0.3299374  0.35585661]\n",
            "Epoch 400, Weights: [0.31151759 0.3294242  0.35905818]\n",
            "Epoch 450, Weights: [0.30883809 0.32890346 0.36225842]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332754 0.33332304 0.33334939]\n",
            "Epoch 50, Weights: [0.33304337 0.33280949 0.33414712]\n",
            "Epoch 100, Weights: [0.33276917 0.33229702 0.33493378]\n",
            "Epoch 150, Weights: [0.33250482 0.33178569 0.33570946]\n",
            "Epoch 200, Weights: [0.33225017 0.33127555 0.33647425]\n",
            "Epoch 250, Weights: [0.33200509 0.33076666 0.33722822]\n",
            "Epoch 300, Weights: [0.33176944 0.33025906 0.33797147]\n",
            "Epoch 350, Weights: [0.33154309 0.32975279 0.33870408]\n",
            "Epoch 400, Weights: [0.33132592 0.32924791 0.33942614]\n",
            "Epoch 450, Weights: [0.33111777 0.32874447 0.34013773]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333564 0.33329955 0.33336478]\n",
            "Epoch 50, Weights: [0.33344779 0.3316073  0.33494488]\n",
            "Epoch 100, Weights: [0.33355227 0.32990812 0.33653957]\n",
            "Epoch 150, Weights: [0.33364904 0.32820205 0.33814888]\n",
            "Epoch 200, Weights: [0.33373803 0.3264891  0.33977284]\n",
            "Epoch 250, Weights: [0.33381916 0.32476933 0.34141148]\n",
            "Epoch 300, Weights: [0.33389237 0.32304275 0.34306485]\n",
            "Epoch 350, Weights: [0.3339576  0.32130941 0.34473296]\n",
            "Epoch 400, Weights: [0.33401478 0.31956934 0.34641585]\n",
            "Epoch 450, Weights: [0.33406384 0.31782258 0.34811355]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334536 0.33328966 0.33336495]\n",
            "Epoch 50, Weights: [0.33394426 0.33110376 0.33495194]\n",
            "Epoch 100, Weights: [0.33453744 0.32891277 0.33654975]\n",
            "Epoch 150, Weights: [0.33512485 0.32671665 0.33815846]\n",
            "Epoch 200, Weights: [0.33570646 0.32451537 0.33977815]\n",
            "Epoch 250, Weights: [0.33628222 0.32230888 0.34140887]\n",
            "Epoch 300, Weights: [0.3368521  0.32009714 0.34305073]\n",
            "Epoch 350, Weights: [0.33741606 0.31788013 0.34470378]\n",
            "Epoch 400, Weights: [0.33797406 0.31565781 0.3463681 ]\n",
            "Epoch 450, Weights: [0.33852606 0.31343014 0.34804377]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332956 0.33328547 0.33338494]\n",
            "Epoch 50, Weights: [0.33313593 0.33088758 0.33597646]\n",
            "Epoch 100, Weights: [0.33293142 0.32847969 0.33858886]\n",
            "Epoch 150, Weights: [0.33271579 0.32606169 0.34122249]\n",
            "Epoch 200, Weights: [0.33248881 0.32363347 0.3438777 ]\n",
            "Epoch 250, Weights: [0.33225022 0.32119491 0.34655485]\n",
            "Epoch 300, Weights: [0.33199977 0.3187459  0.3492543 ]\n",
            "Epoch 350, Weights: [0.3317372  0.31628634 0.35197643]\n",
            "Epoch 400, Weights: [0.33146226 0.31381609 0.35472162]\n",
            "Epoch 450, Weights: [0.33117466 0.31133505 0.35749026]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332001 0.33333261 0.33334735]\n",
            "Epoch 50, Weights: [0.33265118 0.3332976  0.33405119]\n",
            "Epoch 100, Weights: [0.33197599 0.33326417 0.33475981]\n",
            "Epoch 150, Weights: [0.33129438 0.33323233 0.33547326]\n",
            "Epoch 200, Weights: [0.33060634 0.33320207 0.33619156]\n",
            "Epoch 250, Weights: [0.32991181 0.33317339 0.33691476]\n",
            "Epoch 300, Weights: [0.32921076 0.33314631 0.3376429 ]\n",
            "Epoch 350, Weights: [0.32850315 0.33312081 0.33837601]\n",
            "Epoch 400, Weights: [0.32778894 0.3330969  0.33911413]\n",
            "Epoch 450, Weights: [0.32706809 0.33307458 0.33985729]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334492 0.33342517 0.33322988]\n",
            "Epoch 50, Weights: [0.33392647 0.33803972 0.32803378]\n",
            "Epoch 100, Weights: [0.33451185 0.34269818 0.32278993]\n",
            "Epoch 150, Weights: [0.33510113 0.34740108 0.31749776]\n",
            "Epoch 200, Weights: [0.33569436 0.35214895 0.31215666]\n",
            "Epoch 250, Weights: [0.33629159 0.35694232 0.30676605]\n",
            "Epoch 300, Weights: [0.3368929  0.36178174 0.30132533]\n",
            "Epoch 350, Weights: [0.33749833 0.36666777 0.29583387]\n",
            "Epoch 400, Weights: [0.33810796 0.37160096 0.29029105]\n",
            "Epoch 450, Weights: [0.33872183 0.37658189 0.28469624]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330356 0.33337876 0.33331765]\n",
            "Epoch 50, Weights: [0.33180063 0.3356546  0.33254473]\n",
            "Epoch 100, Weights: [0.33026871 0.33793783 0.33179343]\n",
            "Epoch 150, Weights: [0.32870765 0.34022836 0.33106396]\n",
            "Epoch 200, Weights: [0.32711729 0.34252611 0.33035657]\n",
            "Epoch 250, Weights: [0.32549749 0.344831   0.32967148]\n",
            "Epoch 300, Weights: [0.32384809 0.34714294 0.32900893]\n",
            "Epoch 350, Weights: [0.32216896 0.34946185 0.32836916]\n",
            "Epoch 400, Weights: [0.32045995 0.35178763 0.3277524 ]\n",
            "Epoch 450, Weights: [0.31872092 0.35412018 0.32715886]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338796 0.33327401 0.333338  ]\n",
            "Epoch 50, Weights: [0.3361408  0.33028966 0.33356951]\n",
            "Epoch 100, Weights: [0.33893503 0.32726863 0.33379632]\n",
            "Epoch 150, Weights: [0.34177138 0.32421031 0.33401828]\n",
            "Epoch 200, Weights: [0.34465059 0.3211141  0.33423528]\n",
            "Epoch 250, Weights: [0.34757343 0.31797938 0.33444716]\n",
            "Epoch 300, Weights: [0.35054068 0.3148055  0.33465379]\n",
            "Epoch 350, Weights: [0.35355315 0.3115918  0.33485503]\n",
            "Epoch 400, Weights: [0.35661165 0.30833761 0.33505072]\n",
            "Epoch 450, Weights: [0.35971702 0.30504223 0.33524071]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328576 0.33336726 0.33334695]\n",
            "Epoch 50, Weights: [0.33090606 0.33505993 0.33403397]\n",
            "Epoch 100, Weights: [0.328523   0.33674466 0.33473231]\n",
            "Epoch 150, Weights: [0.32613676 0.33842131 0.3354419 ]\n",
            "Epoch 200, Weights: [0.32374751 0.34008978 0.33616268]\n",
            "Epoch 250, Weights: [0.32135544 0.34174995 0.33689459]\n",
            "Epoch 300, Weights: [0.31896072 0.34340169 0.33763756]\n",
            "Epoch 350, Weights: [0.31656354 0.34504491 0.33839153]\n",
            "Epoch 400, Weights: [0.31416409 0.34667947 0.33915642]\n",
            "Epoch 450, Weights: [0.31176255 0.34830526 0.33993216]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33326661 0.33335509 0.33337827]\n",
            "Epoch 50, Weights: [0.32992477 0.33444031 0.33563489]\n",
            "Epoch 100, Weights: [0.32657022 0.33551965 0.3379101 ]\n",
            "Epoch 150, Weights: [0.32320282 0.336593   0.34020416]\n",
            "Epoch 200, Weights: [0.31982241 0.33766023 0.34251733]\n",
            "Epoch 250, Weights: [0.31642884 0.33872125 0.34484988]\n",
            "Epoch 300, Weights: [0.31302195 0.33977594 0.34720208]\n",
            "Epoch 350, Weights: [0.30960157 0.34082418 0.34957422]\n",
            "Epoch 400, Weights: [0.30616756 0.34186585 0.35196657]\n",
            "Epoch 450, Weights: [0.30271973 0.34290082 0.35437942]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334954 0.33331517 0.33333526]\n",
            "Epoch 50, Weights: [0.33416239 0.33240565 0.33343192]\n",
            "Epoch 100, Weights: [0.3349794  0.33149208 0.33352849]\n",
            "Epoch 150, Weights: [0.33580058 0.33057443 0.33362496]\n",
            "Epoch 200, Weights: [0.33662594 0.32965272 0.33372131]\n",
            "Epoch 250, Weights: [0.3374555  0.32872692 0.33381755]\n",
            "Epoch 300, Weights: [0.33828928 0.32779702 0.33391367]\n",
            "Epoch 350, Weights: [0.33912728 0.32686301 0.33400967]\n",
            "Epoch 400, Weights: [0.33996953 0.3259249  0.33410554]\n",
            "Epoch 450, Weights: [0.34081604 0.32498265 0.33420128]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.333266   0.33338944 0.33334453]\n",
            "Epoch 50, Weights: [0.3298807  0.33621166 0.33390761]\n",
            "Epoch 100, Weights: [0.32645782 0.339066   0.33447615]\n",
            "Epoch 150, Weights: [0.32299679 0.34195295 0.33505022]\n",
            "Epoch 200, Weights: [0.31949702 0.34487303 0.33562991]\n",
            "Epoch 250, Weights: [0.31595792 0.34782676 0.33621529]\n",
            "Epoch 300, Weights: [0.31237886 0.35081467 0.33680644]\n",
            "Epoch 350, Weights: [0.30875924 0.3538373  0.33740344]\n",
            "Epoch 400, Weights: [0.30509841 0.35689519 0.33800637]\n",
            "Epoch 450, Weights: [0.30139572 0.35998893 0.33861531]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334962 0.33336748 0.33328286]\n",
            "Epoch 50, Weights: [0.33416876 0.33506397 0.33076723]\n",
            "Epoch 100, Weights: [0.33499611 0.33673796 0.3282659 ]\n",
            "Epoch 150, Weights: [0.33583156 0.3383897  0.32577871]\n",
            "Epoch 200, Weights: [0.336675   0.34001945 0.32330552]\n",
            "Epoch 250, Weights: [0.33752634 0.34162744 0.32084619]\n",
            "Epoch 300, Weights: [0.33838548 0.34321391 0.31840058]\n",
            "Epoch 350, Weights: [0.3392523  0.34477909 0.31596858]\n",
            "Epoch 400, Weights: [0.34012672 0.3463232  0.31355005]\n",
            "Epoch 450, Weights: [0.34100863 0.34784646 0.31114488]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333594 0.33330938 0.33335465]\n",
            "Epoch 50, Weights: [0.33346379 0.33211213 0.33442405]\n",
            "Epoch 100, Weights: [0.33358566 0.33091515 0.33549916]\n",
            "Epoch 150, Weights: [0.33370155 0.32971837 0.33658005]\n",
            "Epoch 200, Weights: [0.33381146 0.32852174 0.33766677]\n",
            "Epoch 250, Weights: [0.33391541 0.32732517 0.33875939]\n",
            "Epoch 300, Weights: [0.3340134  0.32612859 0.33985798]\n",
            "Epoch 350, Weights: [0.33410542 0.32493195 0.3409626 ]\n",
            "Epoch 400, Weights: [0.33419149 0.32373516 0.34207332]\n",
            "Epoch 450, Weights: [0.33427161 0.32253816 0.3431902 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331841 0.33336707 0.33331449]\n",
            "Epoch 50, Weights: [0.33257796 0.3350554  0.33236661]\n",
            "Epoch 100, Weights: [0.33184769 0.33674588 0.3314064 ]\n",
            "Epoch 150, Weights: [0.33112765 0.33843854 0.33043378]\n",
            "Epoch 200, Weights: [0.3304179  0.34013338 0.32944869]\n",
            "Epoch 250, Weights: [0.32971849 0.34183042 0.32845106]\n",
            "Epoch 300, Weights: [0.32902949 0.34352966 0.32744081]\n",
            "Epoch 350, Weights: [0.32835095 0.34523114 0.32641788]\n",
            "Epoch 400, Weights: [0.32768293 0.34693486 0.32538218]\n",
            "Epoch 450, Weights: [0.32702549 0.34864083 0.32433365]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333769  0.33328373 0.33333934]\n",
            "Epoch 50, Weights: [0.3355737  0.33078394 0.33364233]\n",
            "Epoch 100, Weights: [0.3378058  0.32824457 0.3339496 ]\n",
            "Epoch 150, Weights: [0.34007382 0.32566492 0.33426123]\n",
            "Epoch 200, Weights: [0.3423784  0.32304426 0.33457731]\n",
            "Epoch 250, Weights: [0.3447202  0.32038186 0.33489792]\n",
            "Epoch 300, Weights: [0.34709988 0.31767695 0.33522314]\n",
            "Epoch 350, Weights: [0.34951813 0.31492876 0.33555308]\n",
            "Epoch 400, Weights: [0.35197564 0.31213651 0.33588782]\n",
            "Epoch 450, Weights: [0.35447313 0.3092994  0.33622744]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334102 0.33332727 0.33333168]\n",
            "Epoch 50, Weights: [0.33372508 0.3330261  0.33324879]\n",
            "Epoch 100, Weights: [0.33410785 0.33272768 0.33316444]\n",
            "Epoch 150, Weights: [0.33448933 0.33243199 0.33307865]\n",
            "Epoch 200, Weights: [0.33486953 0.33213904 0.33299141]\n",
            "Epoch 250, Weights: [0.33524844 0.3318488  0.33290273]\n",
            "Epoch 300, Weights: [0.33562606 0.33156128 0.33281263]\n",
            "Epoch 350, Weights: [0.33600242 0.33127646 0.3327211 ]\n",
            "Epoch 400, Weights: [0.3363775  0.33099433 0.33262814]\n",
            "Epoch 450, Weights: [0.33675131 0.33071489 0.33253377]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328602 0.33335609 0.33335786]\n",
            "Epoch 50, Weights: [0.33092098 0.33449257 0.33458642]\n",
            "Epoch 100, Weights: [0.32855612 0.3356252  0.33581865]\n",
            "Epoch 150, Weights: [0.32619167 0.33675384 0.33705446]\n",
            "Epoch 200, Weights: [0.32382788 0.33787838 0.33829372]\n",
            "Epoch 250, Weights: [0.32146498 0.33899868 0.33953631]\n",
            "Epoch 300, Weights: [0.31910322 0.34011462 0.34078214]\n",
            "Epoch 350, Weights: [0.31674283 0.34122607 0.34203107]\n",
            "Epoch 400, Weights: [0.31438407 0.3423329  0.343283  ]\n",
            "Epoch 450, Weights: [0.31202716 0.343435   0.34453781]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33323808 0.33335424 0.33340765]\n",
            "Epoch 50, Weights: [0.32848243 0.33439216 0.33712539]\n",
            "Epoch 100, Weights: [0.32373873 0.33541505 0.34084619]\n",
            "Epoch 150, Weights: [0.31900611 0.33642305 0.34457081]\n",
            "Epoch 200, Weights: [0.31428368 0.33741628 0.34830001]\n",
            "Epoch 250, Weights: [0.30957057 0.33839487 0.35203453]\n",
            "Epoch 300, Weights: [0.30486591 0.33935893 0.35577513]\n",
            "Epoch 350, Weights: [0.30016884 0.34030856 0.35952257]\n",
            "Epoch 400, Weights: [0.29547849 0.34124385 0.36327762]\n",
            "Epoch 450, Weights: [0.29079401 0.34216491 0.36704105]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330942 0.33336977 0.33332078]\n",
            "Epoch 50, Weights: [0.33210931 0.33520137 0.3326893 ]\n",
            "Epoch 100, Weights: [0.33089952 0.33705112 0.33204933]\n",
            "Epoch 150, Weights: [0.32967997 0.3389192  0.3314008 ]\n",
            "Epoch 200, Weights: [0.32845058 0.34080576 0.33074363]\n",
            "Epoch 250, Weights: [0.32721127 0.34271098 0.33007772]\n",
            "Epoch 300, Weights: [0.32596194 0.34463504 0.32940299]\n",
            "Epoch 350, Weights: [0.32470252 0.34657809 0.32871936]\n",
            "Epoch 400, Weights: [0.32343291 0.34854031 0.32802674]\n",
            "Epoch 450, Weights: [0.32215304 0.35052188 0.32732505]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329766 0.33332554 0.33337677]\n",
            "Epoch 50, Weights: [0.33151166 0.33293941 0.3355489 ]\n",
            "Epoch 100, Weights: [0.32972015 0.3325589  0.33772092]\n",
            "Epoch 150, Weights: [0.32792303 0.33218399 0.33989295]\n",
            "Epoch 200, Weights: [0.3261202  0.33181463 0.34206513]\n",
            "Epoch 250, Weights: [0.32431159 0.33145078 0.3442376 ]\n",
            "Epoch 300, Weights: [0.32249709 0.33109241 0.34641048]\n",
            "Epoch 350, Weights: [0.32067661 0.33073946 0.3485839 ]\n",
            "Epoch 400, Weights: [0.31885007 0.33039191 0.35075799]\n",
            "Epoch 450, Weights: [0.31701736 0.33004972 0.3529329 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336965 0.33326192 0.3333684 ]\n",
            "Epoch 50, Weights: [0.33519659 0.32968738 0.335116  ]\n",
            "Epoch 100, Weights: [0.33704452 0.3261036  0.33685184]\n",
            "Epoch 150, Weights: [0.33891355 0.32251025 0.33857617]\n",
            "Epoch 200, Weights: [0.34080377 0.31890698 0.34028922]\n",
            "Epoch 250, Weights: [0.3427153  0.31529342 0.34199125]\n",
            "Epoch 300, Weights: [0.34464826 0.31166924 0.34368247]\n",
            "Epoch 350, Weights: [0.34660276 0.30803406 0.34536315]\n",
            "Epoch 400, Weights: [0.34857894 0.30438752 0.34703351]\n",
            "Epoch 450, Weights: [0.35057694 0.30072923 0.3486938 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331515 0.33337957 0.33330525]\n",
            "Epoch 50, Weights: [0.33240747 0.33569308 0.33189942]\n",
            "Epoch 100, Weights: [0.33150168 0.33800887 0.33048942]\n",
            "Epoch 150, Weights: [0.33059777 0.34032695 0.32907525]\n",
            "Epoch 200, Weights: [0.32969575 0.3426473  0.32765692]\n",
            "Epoch 250, Weights: [0.32879561 0.34496995 0.32623441]\n",
            "Epoch 300, Weights: [0.32789736 0.34729487 0.32480774]\n",
            "Epoch 350, Weights: [0.32700099 0.34962209 0.32337689]\n",
            "Epoch 400, Weights: [0.32610651 0.3519516  0.32194186]\n",
            "Epoch 450, Weights: [0.32521391 0.3542834  0.32050266]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337411 0.33333405 0.33329181]\n",
            "Epoch 50, Weights: [0.33542035 0.33336083 0.33121879]\n",
            "Epoch 100, Weights: [0.33748016 0.33336865 0.32915115]\n",
            "Epoch 150, Weights: [0.33955363 0.33335739 0.32708894]\n",
            "Epoch 200, Weights: [0.34164082 0.33332695 0.3250322 ]\n",
            "Epoch 250, Weights: [0.34374181 0.33327719 0.32298096]\n",
            "Epoch 300, Weights: [0.34585668 0.33320801 0.32093528]\n",
            "Epoch 350, Weights: [0.3479855  0.33311929 0.31889518]\n",
            "Epoch 400, Weights: [0.35012836 0.33301089 0.31686072]\n",
            "Epoch 450, Weights: [0.35228534 0.3328827  0.31483193]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33340504 0.33324755 0.33334739]\n",
            "Epoch 50, Weights: [0.33700405 0.32893863 0.33405729]\n",
            "Epoch 100, Weights: [0.34062935 0.32459006 0.33478057]\n",
            "Epoch 150, Weights: [0.34428135 0.32020122 0.3355174 ]\n",
            "Epoch 200, Weights: [0.34796047 0.3157715  0.336268  ]\n",
            "Epoch 250, Weights: [0.35166713 0.31130027 0.33703257]\n",
            "Epoch 300, Weights: [0.35540178 0.30678687 0.33781132]\n",
            "Epoch 350, Weights: [0.35916485 0.30223067 0.33860445]\n",
            "Epoch 400, Weights: [0.36295679 0.29763098 0.3394122 ]\n",
            "Epoch 450, Weights: [0.36677807 0.29298713 0.34023477]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333081  0.33330891 0.33338295]\n",
            "Epoch 50, Weights: [0.33204352 0.33209177 0.33586469]\n",
            "Epoch 100, Weights: [0.33077204 0.33088122 0.33834671]\n",
            "Epoch 150, Weights: [0.32949383 0.32967741 0.34082873]\n",
            "Epoch 200, Weights: [0.32820901 0.32848049 0.34331046]\n",
            "Epoch 250, Weights: [0.32691775 0.32729059 0.34579163]\n",
            "Epoch 300, Weights: [0.32562018 0.32610785 0.34827194]\n",
            "Epoch 350, Weights: [0.32431646 0.32493239 0.35075112]\n",
            "Epoch 400, Weights: [0.32300674 0.32376436 0.35322887]\n",
            "Epoch 450, Weights: [0.32169117 0.32260388 0.35570492]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332378 0.33337742 0.33329877]\n",
            "Epoch 50, Weights: [0.33284925 0.33557133 0.33157939]\n",
            "Epoch 100, Weights: [0.33237955 0.33774406 0.32987636]\n",
            "Epoch 150, Weights: [0.33191462 0.33989582 0.32818952]\n",
            "Epoch 200, Weights: [0.33145443 0.34202682 0.32651872]\n",
            "Epoch 250, Weights: [0.33099893 0.34413724 0.3248638 ]\n",
            "Epoch 300, Weights: [0.33054807 0.3462273  0.32322461]\n",
            "Epoch 350, Weights: [0.3301018  0.34829718 0.32160099]\n",
            "Epoch 400, Weights: [0.3296601  0.35034707 0.3199928 ]\n",
            "Epoch 450, Weights: [0.3292229  0.35237718 0.31839989]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33326437 0.33333397 0.33340164]\n",
            "Epoch 50, Weights: [0.32977904 0.33338161 0.33683932]\n",
            "Epoch 100, Weights: [0.32621929 0.33345998 0.3403207 ]\n",
            "Epoch 150, Weights: [0.32258378 0.33356969 0.3438465 ]\n",
            "Epoch 200, Weights: [0.31887113 0.33371141 0.34741743]\n",
            "Epoch 250, Weights: [0.31507995 0.33388577 0.35103425]\n",
            "Epoch 300, Weights: [0.31120879 0.33409346 0.35469772]\n",
            "Epoch 350, Weights: [0.30725617 0.33433514 0.35840866]\n",
            "Epoch 400, Weights: [0.30322057 0.33461152 0.36216788]\n",
            "Epoch 450, Weights: [0.29910043 0.33492331 0.36597624]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331381 0.33337593 0.33331024]\n",
            "Epoch 50, Weights: [0.33233325 0.33550177 0.33216495]\n",
            "Epoch 100, Weights: [0.33134368 0.33761918 0.33103712]\n",
            "Epoch 150, Weights: [0.33034529 0.33972817 0.32992651]\n",
            "Epoch 200, Weights: [0.32933828 0.34182878 0.32883291]\n",
            "Epoch 250, Weights: [0.32832283 0.34392104 0.32775611]\n",
            "Epoch 300, Weights: [0.32729913 0.34600495 0.32669589]\n",
            "Epoch 350, Weights: [0.32626736 0.34808055 0.32565205]\n",
            "Epoch 400, Weights: [0.32522772 0.35014787 0.32462438]\n",
            "Epoch 450, Weights: [0.32418038 0.35220692 0.32361267]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3334063  0.33324924 0.33334443]\n",
            "Epoch 50, Weights: [0.33704479 0.32904286 0.33391231]\n",
            "Epoch 100, Weights: [0.34066227 0.32483317 0.33450453]\n",
            "Epoch 150, Weights: [0.34425811 0.32062096 0.3351209 ]\n",
            "Epoch 200, Weights: [0.34783171 0.31640704 0.33576123]\n",
            "Epoch 250, Weights: [0.35138247 0.3121922  0.3364253 ]\n",
            "Epoch 300, Weights: [0.3549098  0.30797726 0.33711292]\n",
            "Epoch 350, Weights: [0.35841312 0.30376301 0.33782384]\n",
            "Epoch 400, Weights: [0.36189185 0.29955026 0.33855786]\n",
            "Epoch 450, Weights: [0.36534545 0.29533981 0.33931471]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330236 0.33330772 0.33338989]\n",
            "Epoch 50, Weights: [0.33175571 0.33202853 0.33621573]\n",
            "Epoch 100, Weights: [0.33021214 0.33075113 0.3390367 ]\n",
            "Epoch 150, Weights: [0.32867155 0.32947544 0.34185298]\n",
            "Epoch 200, Weights: [0.32713385 0.32820138 0.34466475]\n",
            "Epoch 250, Weights: [0.32559892 0.32692886 0.34747218]\n",
            "Epoch 300, Weights: [0.32406669 0.3256578  0.35027547]\n",
            "Epoch 350, Weights: [0.32253706 0.32438812 0.35307479]\n",
            "Epoch 400, Weights: [0.32100993 0.32311973 0.35587031]\n",
            "Epoch 450, Weights: [0.31948521 0.32185255 0.3586622 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332617 0.33333292 0.33334088]\n",
            "Epoch 50, Weights: [0.33296701 0.33331335 0.33371961]\n",
            "Epoch 100, Weights: [0.3326047  0.33329514 0.33410013]\n",
            "Epoch 150, Weights: [0.33223923 0.33327829 0.33448245]\n",
            "Epoch 200, Weights: [0.33187059 0.3332628  0.33486657]\n",
            "Epoch 250, Weights: [0.33149877 0.3332487  0.33525249]\n",
            "Epoch 300, Weights: [0.33112376 0.333236   0.33564021]\n",
            "Epoch 350, Weights: [0.33074554 0.33322469 0.33602973]\n",
            "Epoch 400, Weights: [0.33036411 0.3332148  0.33642106]\n",
            "Epoch 450, Weights: [0.32997946 0.33320634 0.33681418]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331983 0.33327532 0.33340481]\n",
            "Epoch 50, Weights: [0.3326484  0.33035981 0.33699176]\n",
            "Epoch 100, Weights: [0.33198313 0.32741327 0.34060357]\n",
            "Epoch 150, Weights: [0.3313241  0.32443501 0.34424086]\n",
            "Epoch 200, Weights: [0.33067139 0.32142435 0.34790424]\n",
            "Epoch 250, Weights: [0.33002509 0.31838055 0.35159433]\n",
            "Epoch 300, Weights: [0.32938529 0.31530288 0.35531179]\n",
            "Epoch 350, Weights: [0.3287521  0.31219059 0.35905728]\n",
            "Epoch 400, Weights: [0.3281256  0.30904289 0.36283148]\n",
            "Epoch 450, Weights: [0.32750591 0.30585899 0.36663507]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33339996 0.33333544 0.33326457]\n",
            "Epoch 50, Weights: [0.336727   0.33344255 0.32983042]\n",
            "Epoch 100, Weights: [0.34004478 0.33355249 0.32640269]\n",
            "Epoch 150, Weights: [0.34335337 0.33366525 0.32298135]\n",
            "Epoch 200, Weights: [0.34665282 0.33378079 0.31956636]\n",
            "Epoch 250, Weights: [0.34994317 0.33389911 0.31615769]\n",
            "Epoch 300, Weights: [0.35322449 0.33402018 0.3127553 ]\n",
            "Epoch 350, Weights: [0.35649682 0.33414399 0.30935917]\n",
            "Epoch 400, Weights: [0.35976022 0.3342705  0.30596925]\n",
            "Epoch 450, Weights: [0.36301474 0.33439971 0.30258552]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333233 0.33324828 0.33341935]\n",
            "Epoch 50, Weights: [0.33329731 0.32898    0.33772266]\n",
            "Epoch 100, Weights: [0.33329066 0.32467903 0.34203028]\n",
            "Epoch 150, Weights: [0.33331226 0.32034442 0.34634329]\n",
            "Epoch 200, Weights: [0.33336202 0.3159752  0.35066275]\n",
            "Epoch 250, Weights: [0.33343987 0.31157035 0.35498975]\n",
            "Epoch 300, Weights: [0.33354575 0.30712884 0.35932538]\n",
            "Epoch 350, Weights: [0.33367962 0.3026496  0.36367075]\n",
            "Epoch 400, Weights: [0.33384145 0.29813152 0.368027  ]\n",
            "Epoch 450, Weights: [0.33403124 0.29357348 0.37239525]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333531 0.33331851 0.33334615]\n",
            "Epoch 50, Weights: [0.33343467 0.33257484 0.33399047]\n",
            "Epoch 100, Weights: [0.33353439 0.33182542 0.33464016]\n",
            "Epoch 150, Weights: [0.33363448 0.33107023 0.33529526]\n",
            "Epoch 200, Weights: [0.33373494 0.33030921 0.33595582]\n",
            "Epoch 250, Weights: [0.33383576 0.32954233 0.33662187]\n",
            "Epoch 300, Weights: [0.33393696 0.32876956 0.33729345]\n",
            "Epoch 350, Weights: [0.33403852 0.32799085 0.3379706 ]\n",
            "Epoch 400, Weights: [0.33414046 0.32720616 0.33865335]\n",
            "Epoch 450, Weights: [0.33424277 0.32641546 0.33934174]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329014 0.33331234 0.3333975 ]\n",
            "Epoch 50, Weights: [0.3311382  0.33226316 0.33659861]\n",
            "Epoch 100, Weights: [0.32900079 0.33121441 0.33978476]\n",
            "Epoch 150, Weights: [0.32687781 0.33016607 0.34295609]\n",
            "Epoch 200, Weights: [0.32476913 0.32911813 0.34611271]\n",
            "Epoch 250, Weights: [0.32267467 0.32807056 0.34925474]\n",
            "Epoch 300, Weights: [0.32059432 0.32702335 0.3523823 ]\n",
            "Epoch 350, Weights: [0.31852797 0.3259765  0.3554955 ]\n",
            "Epoch 400, Weights: [0.31647553 0.32492998 0.35859446]\n",
            "Epoch 450, Weights: [0.31443691 0.32388378 0.36167928]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333125  0.33335757 0.3333299 ]\n",
            "Epoch 50, Weights: [0.33227499 0.33457049 0.33315449]\n",
            "Epoch 100, Weights: [0.33124506 0.33578422 0.33297069]\n",
            "Epoch 150, Weights: [0.33022269 0.33699875 0.33277853]\n",
            "Epoch 200, Weights: [0.32920784 0.33821406 0.33257807]\n",
            "Epoch 250, Weights: [0.3282005  0.33943012 0.33236935]\n",
            "Epoch 300, Weights: [0.32720063 0.34064692 0.33215243]\n",
            "Epoch 350, Weights: [0.3262082  0.34186443 0.33192734]\n",
            "Epoch 400, Weights: [0.32522319 0.34308264 0.33169415]\n",
            "Epoch 450, Weights: [0.32424557 0.34430151 0.33145289]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338362 0.33331812 0.33329823]\n",
            "Epoch 50, Weights: [0.33590639 0.33255624 0.33153734]\n",
            "Epoch 100, Weights: [0.33844477 0.33179127 0.32976393]\n",
            "Epoch 150, Weights: [0.34099874 0.33102322 0.32797801]\n",
            "Epoch 200, Weights: [0.34356832 0.3302521  0.32617955]\n",
            "Epoch 250, Weights: [0.3461535  0.32947792 0.32436856]\n",
            "Epoch 300, Weights: [0.34875427 0.32870069 0.32254501]\n",
            "Epoch 350, Weights: [0.35137062 0.32792044 0.32070891]\n",
            "Epoch 400, Weights: [0.35400254 0.32713717 0.31886025]\n",
            "Epoch 450, Weights: [0.35665003 0.32635091 0.31699903]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329588 0.33334793 0.33335616]\n",
            "Epoch 50, Weights: [0.3314241  0.33407943 0.33449644]\n",
            "Epoch 100, Weights: [0.32955344 0.33481313 0.3356334 ]\n",
            "Epoch 150, Weights: [0.32768392 0.33554901 0.33676705]\n",
            "Epoch 200, Weights: [0.32581556 0.33628704 0.33789737]\n",
            "Epoch 250, Weights: [0.32394838 0.33702723 0.33902436]\n",
            "Epoch 300, Weights: [0.32208241 0.33776953 0.34014803]\n",
            "Epoch 350, Weights: [0.32021766 0.33851395 0.34126836]\n",
            "Epoch 400, Weights: [0.31835416 0.33926045 0.34238536]\n",
            "Epoch 450, Weights: [0.31649192 0.34000903 0.34349902]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329855 0.33331029 0.33339114]\n",
            "Epoch 50, Weights: [0.33156702 0.33216856 0.3362644 ]\n",
            "Epoch 100, Weights: [0.32984982 0.33104638 0.33910377]\n",
            "Epoch 150, Weights: [0.32814693 0.32994353 0.3419095 ]\n",
            "Epoch 200, Weights: [0.32645835 0.32885976 0.34468186]\n",
            "Epoch 250, Weights: [0.32478405 0.32779483 0.34742108]\n",
            "Epoch 300, Weights: [0.32312401 0.32674851 0.35012745]\n",
            "Epoch 350, Weights: [0.3214782  0.32572056 0.35280121]\n",
            "Epoch 400, Weights: [0.3198466  0.32471074 0.35544264]\n",
            "Epoch 450, Weights: [0.31822916 0.32371882 0.35805199]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33340048 0.33334857 0.33325092]\n",
            "Epoch 50, Weights: [0.33677527 0.33409775 0.32912695]\n",
            "Epoch 100, Weights: [0.34018336 0.33482111 0.3249955 ]\n",
            "Epoch 150, Weights: [0.34362502 0.33551834 0.32085661]\n",
            "Epoch 200, Weights: [0.34710052 0.33618915 0.31671031]\n",
            "Epoch 250, Weights: [0.35061012 0.33683322 0.31255662]\n",
            "Epoch 300, Weights: [0.35415412 0.33745026 0.30839559]\n",
            "Epoch 350, Weights: [0.35773279 0.33803995 0.30422723]\n",
            "Epoch 400, Weights: [0.36134642 0.33860198 0.30005158]\n",
            "Epoch 450, Weights: [0.3649953  0.33913603 0.29586864]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33341184 0.33326344 0.33332469]\n",
            "Epoch 50, Weights: [0.33734323 0.32978213 0.3328746 ]\n",
            "Epoch 100, Weights: [0.34128605 0.32632565 0.33238827]\n",
            "Epoch 150, Weights: [0.34524035 0.3228939  0.33186571]\n",
            "Epoch 200, Weights: [0.34920623 0.3194868  0.33130694]\n",
            "Epoch 250, Weights: [0.35318377 0.31610423 0.33071197]\n",
            "Epoch 300, Weights: [0.35717308 0.3127461  0.33008079]\n",
            "Epoch 350, Weights: [0.36117429 0.30941226 0.32941341]\n",
            "Epoch 400, Weights: [0.36518754 0.3061026  0.32870983]\n",
            "Epoch 450, Weights: [0.36921297 0.30281698 0.32797002]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33338647 0.33328946 0.33332404]\n",
            "Epoch 50, Weights: [0.33604789 0.33109031 0.33286177]\n",
            "Epoch 100, Weights: [0.33871754 0.32887946 0.33240297]\n",
            "Epoch 150, Weights: [0.34139547 0.32665683 0.33194766]\n",
            "Epoch 200, Weights: [0.34408177 0.32442234 0.33149586]\n",
            "Epoch 250, Weights: [0.3467765  0.32217589 0.33104758]\n",
            "Epoch 300, Weights: [0.34947974 0.31991739 0.33060284]\n",
            "Epoch 350, Weights: [0.35219157 0.31764674 0.33016166]\n",
            "Epoch 400, Weights: [0.35491206 0.31536386 0.32972406]\n",
            "Epoch 450, Weights: [0.35764129 0.31306864 0.32929004]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333787 0.33338048 0.33328161]\n",
            "Epoch 50, Weights: [0.33355822 0.33575089 0.33069086]\n",
            "Epoch 100, Weights: [0.33376435 0.33814614 0.32808948]\n",
            "Epoch 150, Weights: [0.33395622 0.34056654 0.32547721]\n",
            "Epoch 200, Weights: [0.33413379 0.34301241 0.32285377]\n",
            "Epoch 250, Weights: [0.334297   0.3454841  0.32021888]\n",
            "Epoch 300, Weights: [0.33444578 0.34798193 0.31757226]\n",
            "Epoch 350, Weights: [0.3345801  0.35050624 0.31491363]\n",
            "Epoch 400, Weights: [0.33469989 0.35305738 0.3122427 ]\n",
            "Epoch 450, Weights: [0.33480508 0.3556357  0.30955918]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336169 0.33332094 0.33331734]\n",
            "Epoch 50, Weights: [0.33477977 0.33270133 0.33251887]\n",
            "Epoch 100, Weights: [0.33619713 0.33208069 0.33172215]\n",
            "Epoch 150, Weights: [0.3376138  0.33145901 0.33092716]\n",
            "Epoch 200, Weights: [0.33902983 0.33083627 0.33013388]\n",
            "Epoch 250, Weights: [0.34044524 0.33021246 0.32934227]\n",
            "Epoch 300, Weights: [0.34186008 0.32958756 0.32855233]\n",
            "Epoch 350, Weights: [0.34327438 0.32896156 0.32776403]\n",
            "Epoch 400, Weights: [0.34468818 0.32833445 0.32697734]\n",
            "Epoch 450, Weights: [0.34610152 0.32770621 0.32619224]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33331788 0.33332259 0.3333595 ]\n",
            "Epoch 50, Weights: [0.33254576 0.33278699 0.33466721]\n",
            "Epoch 100, Weights: [0.33177387 0.33225342 0.33597268]\n",
            "Epoch 150, Weights: [0.33100216 0.33172186 0.33727596]\n",
            "Epoch 200, Weights: [0.3302306  0.3311923  0.33857707]\n",
            "Epoch 250, Weights: [0.32945918 0.33066473 0.33987606]\n",
            "Epoch 300, Weights: [0.32868785 0.33013915 0.34117297]\n",
            "Epoch 350, Weights: [0.3279166  0.32961553 0.34246784]\n",
            "Epoch 400, Weights: [0.32714539 0.32909388 0.34376071]\n",
            "Epoch 450, Weights: [0.32637419 0.32857418 0.34505161]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335677 0.33328881 0.33335439]\n",
            "Epoch 50, Weights: [0.33452895 0.33105645 0.33441457]\n",
            "Epoch 100, Weights: [0.33570128 0.32881059 0.3354881 ]\n",
            "Epoch 150, Weights: [0.33687382 0.32655114 0.33657502]\n",
            "Epoch 200, Weights: [0.33804662 0.32427797 0.33767538]\n",
            "Epoch 250, Weights: [0.33921976 0.32199096 0.33878924]\n",
            "Epoch 300, Weights: [0.34039329 0.31969002 0.33991666]\n",
            "Epoch 350, Weights: [0.34156728 0.31737501 0.34105769]\n",
            "Epoch 400, Weights: [0.34274178 0.31504582 0.34221237]\n",
            "Epoch 450, Weights: [0.34391686 0.31270232 0.34338079]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33333216 0.33337249 0.33329532]\n",
            "Epoch 50, Weights: [0.33325916 0.33533641 0.33140441]\n",
            "Epoch 100, Weights: [0.33315695 0.33731163 0.32953139]\n",
            "Epoch 150, Weights: [0.33302577 0.33929838 0.32767582]\n",
            "Epoch 200, Weights: [0.33286582 0.34129691 0.32583725]\n",
            "Epoch 250, Weights: [0.3326773  0.34330744 0.32401523]\n",
            "Epoch 300, Weights: [0.3324604  0.34533023 0.32220934]\n",
            "Epoch 350, Weights: [0.3322153  0.34736552 0.32041916]\n",
            "Epoch 400, Weights: [0.33194217 0.34941354 0.31864426]\n",
            "Epoch 450, Weights: [0.33164118 0.35147454 0.31688425]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3333009  0.33333787 0.3333612 ]\n",
            "Epoch 50, Weights: [0.33167676 0.33356592 0.33475728]\n",
            "Epoch 100, Weights: [0.33004707 0.33379532 0.33615758]\n",
            "Epoch 150, Weights: [0.32841192 0.33402605 0.33756201]\n",
            "Epoch 200, Weights: [0.32677137 0.33425811 0.33897049]\n",
            "Epoch 250, Weights: [0.32512552 0.33449149 0.34038296]\n",
            "Epoch 300, Weights: [0.32347444 0.33472619 0.34179934]\n",
            "Epoch 350, Weights: [0.32181824 0.3349622  0.34321954]\n",
            "Epoch 400, Weights: [0.32015698 0.33519951 0.34464348]\n",
            "Epoch 450, Weights: [0.31849077 0.33543811 0.34607109]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33327729 0.33337142 0.33335126]\n",
            "Epoch 50, Weights: [0.33047207 0.33527374 0.33425416]\n",
            "Epoch 100, Weights: [0.32765993 0.33717089 0.33516915]\n",
            "Epoch 150, Weights: [0.32484076 0.33906294 0.33609627]\n",
            "Epoch 200, Weights: [0.32201443 0.34094994 0.3370356 ]\n",
            "Epoch 250, Weights: [0.31918082 0.34283196 0.33798719]\n",
            "Epoch 300, Weights: [0.31633981 0.34470906 0.3389511 ]\n",
            "Epoch 350, Weights: [0.31349127 0.34658131 0.3399274 ]\n",
            "Epoch 400, Weights: [0.31063509 0.34844875 0.34091614]\n",
            "Epoch 450, Weights: [0.30777113 0.35031145 0.34191739]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33328366 0.33333784 0.33337847]\n",
            "Epoch 50, Weights: [0.33080133 0.33356233 0.3356363 ]\n",
            "Epoch 100, Weights: [0.32832068 0.33378418 0.33789511]\n",
            "Epoch 150, Weights: [0.32584158 0.33400338 0.34015501]\n",
            "Epoch 200, Weights: [0.32336391 0.33421994 0.34241612]\n",
            "Epoch 250, Weights: [0.32088756 0.33443386 0.34467856]\n",
            "Epoch 300, Weights: [0.31841241 0.33464512 0.34694244]\n",
            "Epoch 350, Weights: [0.31593835 0.33485374 0.34920788]\n",
            "Epoch 400, Weights: [0.31346525 0.33505971 0.35147501]\n",
            "Epoch 450, Weights: [0.31099301 0.33526302 0.35374394]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335921 0.33326254 0.33337822]\n",
            "Epoch 50, Weights: [0.33465168 0.32971006 0.33563822]\n",
            "Epoch 100, Weights: [0.33594039 0.32613153 0.33792805]\n",
            "Epoch 150, Weights: [0.33722494 0.32252707 0.34024796]\n",
            "Epoch 200, Weights: [0.33850497 0.31889682 0.34259819]\n",
            "Epoch 250, Weights: [0.33978008 0.3152409  0.34497899]\n",
            "Epoch 300, Weights: [0.34104988 0.31155948 0.3473906 ]\n",
            "Epoch 350, Weights: [0.34231398 0.30785272 0.34983327]\n",
            "Epoch 400, Weights: [0.34357196 0.30412077 0.35230724]\n",
            "Epoch 450, Weights: [0.34482341 0.30036383 0.35481273]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33334822 0.33334626 0.33330549]\n",
            "Epoch 50, Weights: [0.33409678 0.33399162 0.33191157]\n",
            "Epoch 100, Weights: [0.33485272 0.3346338  0.33051344]\n",
            "Epoch 150, Weights: [0.33561608 0.33527278 0.32911111]\n",
            "Epoch 200, Weights: [0.33638688 0.33590854 0.32770455]\n",
            "Epoch 250, Weights: [0.33716517 0.33654104 0.32629376]\n",
            "Epoch 300, Weights: [0.33795098 0.33717029 0.3248787 ]\n",
            "Epoch 350, Weights: [0.33874435 0.33779623 0.32345939]\n",
            "Epoch 400, Weights: [0.33954531 0.33841887 0.32203579]\n",
            "Epoch 450, Weights: [0.3403539  0.33903817 0.32060791]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33332952 0.33327506 0.33339539]\n",
            "Epoch 50, Weights: [0.33314601 0.33036834 0.33648562]\n",
            "Epoch 100, Weights: [0.33297557 0.32747434 0.33955006]\n",
            "Epoch 150, Weights: [0.33281823 0.32459313 0.34258861]\n",
            "Epoch 200, Weights: [0.33267399 0.32172479 0.34560119]\n",
            "Epoch 250, Weights: [0.33254289 0.31886937 0.34858771]\n",
            "Epoch 300, Weights: [0.33242493 0.31602696 0.35154808]\n",
            "Epoch 350, Weights: [0.33232013 0.3131976  0.35448224]\n",
            "Epoch 400, Weights: [0.33222851 0.31038136 0.3573901 ]\n",
            "Epoch 450, Weights: [0.33215006 0.3075783  0.36027161]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33337894 0.33328697 0.33333406]\n",
            "Epoch 50, Weights: [0.33567738 0.330956   0.33336659]\n",
            "Epoch 100, Weights: [0.33801065 0.32859835 0.33339097]\n",
            "Epoch 150, Weights: [0.34037949 0.32621345 0.33340702]\n",
            "Epoch 200, Weights: [0.34278466 0.32380072 0.33341459]\n",
            "Epoch 250, Weights: [0.34522692 0.32135957 0.33341348]\n",
            "Epoch 300, Weights: [0.34770708 0.31888937 0.33340352]\n",
            "Epoch 350, Weights: [0.35022594 0.31638951 0.33338452]\n",
            "Epoch 400, Weights: [0.35278436 0.31385933 0.33335628]\n",
            "Epoch 450, Weights: [0.35538319 0.31129817 0.3333186 ]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33330233 0.33331297 0.33338467]\n",
            "Epoch 50, Weights: [0.3317521  0.33230129 0.33594658]\n",
            "Epoch 100, Weights: [0.33020137 0.33130113 0.33849747]\n",
            "Epoch 150, Weights: [0.32865029 0.33031248 0.3410372 ]\n",
            "Epoch 200, Weights: [0.327099   0.32933533 0.34356564]\n",
            "Epoch 250, Weights: [0.32554765 0.32836967 0.34608264]\n",
            "Epoch 300, Weights: [0.3239964  0.32741548 0.34858809]\n",
            "Epoch 350, Weights: [0.32244539 0.32647274 0.35108184]\n",
            "Epoch 400, Weights: [0.32089476 0.32554143 0.35356378]\n",
            "Epoch 450, Weights: [0.31934467 0.32462152 0.35603378]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33329387 0.33330605 0.33340005]\n",
            "Epoch 50, Weights: [0.33132445 0.33194378 0.33673174]\n",
            "Epoch 100, Weights: [0.32936117 0.33058449 0.34005431]\n",
            "Epoch 150, Weights: [0.3274041  0.32922823 0.34336765]\n",
            "Epoch 200, Weights: [0.32545329 0.32787505 0.34667162]\n",
            "Epoch 250, Weights: [0.32350882 0.32652502 0.34996613]\n",
            "Epoch 300, Weights: [0.32157074 0.32517818 0.35325104]\n",
            "Epoch 350, Weights: [0.31963912 0.32383459 0.35652626]\n",
            "Epoch 400, Weights: [0.317714   0.3224943  0.35979167]\n",
            "Epoch 450, Weights: [0.31579545 0.32115735 0.36304717]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33336145 0.33335036 0.33328816]\n",
            "Epoch 50, Weights: [0.33476765 0.33419878 0.33103355]\n",
            "Epoch 100, Weights: [0.33617379 0.33504071 0.32878547]\n",
            "Epoch 150, Weights: [0.33757985 0.33587619 0.32654393]\n",
            "Epoch 200, Weights: [0.33898578 0.33670527 0.32430893]\n",
            "Epoch 250, Weights: [0.34039154 0.33752798 0.32208045]\n",
            "Epoch 300, Weights: [0.34179711 0.33834437 0.31985848]\n",
            "Epoch 350, Weights: [0.34320245 0.33915449 0.31764303]\n",
            "Epoch 400, Weights: [0.34460752 0.33995837 0.31543408]\n",
            "Epoch 450, Weights: [0.34601229 0.34075606 0.31323162]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33340774 0.33324976 0.33334247]\n",
            "Epoch 50, Weights: [0.33714899 0.32904848 0.33380249]\n",
            "Epoch 100, Weights: [0.3409309  0.32480131 0.33426776]\n",
            "Epoch 150, Weights: [0.34475393 0.32050771 0.33473833]\n",
            "Epoch 200, Weights: [0.34861856 0.31616714 0.33521427]\n",
            "Epoch 250, Weights: [0.35252529 0.31177904 0.33569563]\n",
            "Epoch 300, Weights: [0.35647461 0.30734287 0.33618249]\n",
            "Epoch 350, Weights: [0.36046702 0.30285805 0.3366749 ]\n",
            "Epoch 400, Weights: [0.36450303 0.29832401 0.33717292]\n",
            "Epoch 450, Weights: [0.36858316 0.29374018 0.33767663]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3332957  0.33332225 0.33338202]\n",
            "Epoch 50, Weights: [0.33142015 0.33276556 0.33581427]\n",
            "Epoch 100, Weights: [0.32955559 0.33220292 0.33824146]\n",
            "Epoch 150, Weights: [0.3277021  0.33163432 0.34066355]\n",
            "Epoch 200, Weights: [0.32585973 0.33105972 0.34308052]\n",
            "Epoch 250, Weights: [0.32402855 0.3304791  0.34549232]\n",
            "Epoch 300, Weights: [0.32220861 0.32989243 0.34789892]\n",
            "Epoch 350, Weights: [0.32039998 0.32929968 0.3503003 ]\n",
            "Epoch 400, Weights: [0.31860272 0.32870082 0.35269643]\n",
            "Epoch 450, Weights: [0.31681687 0.32809583 0.35508727]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.33335992 0.33329817 0.33334187]\n",
            "Epoch 50, Weights: [0.33469159 0.33153836 0.33377002]\n",
            "Epoch 100, Weights: [0.33602647 0.32977412 0.33419939]\n",
            "Epoch 150, Weights: [0.33736454 0.32800545 0.33462997]\n",
            "Epoch 200, Weights: [0.33870579 0.32623241 0.33506177]\n",
            "Epoch 250, Weights: [0.34005017 0.32445502 0.33549478]\n",
            "Epoch 300, Weights: [0.34139768 0.3226733  0.33592899]\n",
            "Epoch 350, Weights: [0.34274828 0.32088729 0.3363644 ]\n",
            "Epoch 400, Weights: [0.34410195 0.31909702 0.336801  ]\n",
            "Epoch 450, Weights: [0.34545867 0.31730252 0.33723878]\n",
            "Initial weights: [0.33333333 0.33333333 0.33333333]\n",
            "Epoch 0, Weights: [0.3332064  0.33334299 0.33345058]\n",
            "Epoch 50, Weights: [0.32684671 0.33382196 0.33933131]\n",
            "Epoch 100, Weights: [0.32045982 0.33429251 0.34524765]\n",
            "Epoch 150, Weights: [0.31404532 0.33475459 0.35120007]\n",
            "Epoch 200, Weights: [0.30760278 0.33520813 0.35718906]\n",
            "Epoch 250, Weights: [0.30113177 0.33565308 0.36321512]\n",
            "Epoch 300, Weights: [0.29463185 0.33608937 0.36927875]\n",
            "Epoch 350, Weights: [0.28810257 0.33651694 0.37538046]\n",
            "Epoch 400, Weights: [0.28154347 0.33693572 0.38152078]\n",
            "Epoch 450, Weights: [0.27495409 0.33734563 0.38770024]\n",
            "Monte Carlo Simulation Results (Summary): [array([0.30439693, 0.3992975 , 0.29630555]), array([0.35680079, 0.35323697, 0.2899622 ]), array([0.34977879, 0.30988523, 0.34033594]), array([0.31483085, 0.32683369, 0.35833543]), array([0.27649859, 0.30733178, 0.4161696 ])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe9GHaz-2P6M",
        "outputId": "996cc368-d5e8-42b6-956c-82e5caaa53bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "\n",
        "# Simulating returns and covariance matrix (for demonstration)\n",
        "np.random.seed(42)\n",
        "n_assets = 3\n",
        "mu = np.random.randn(n_assets)  # expected returns\n",
        "Sigma = np.random.randn(n_assets, n_assets)\n",
        "Sigma = np.dot(Sigma, Sigma.T)  # Make it positive semi-definite\n",
        "\n",
        "# Portfolio optimization using cvxpy\n",
        "def optimize_portfolio(Sigma, mu, risk_aversion=1.0):\n",
        "    # Define variables\n",
        "    w = cp.Variable(len(mu))  # portfolio weights\n",
        "    ret = mu.T @ w  # expected return\n",
        "    risk = cp.quad_form(w, Sigma)  # portfolio risk (variance)\n",
        "\n",
        "    # Define objective function (maximize return - risk_aversion * risk)\n",
        "    objective = cp.Maximize(ret - risk_aversion * risk)\n",
        "\n",
        "    # Constraints: sum of weights equals 1 (fully invested portfolio), weights >= 0 (no shorting)\n",
        "    constraints = [cp.sum(w) == 1, w >= 0]\n",
        "\n",
        "    # Define problem\n",
        "    prob = cp.Problem(objective, constraints)\n",
        "\n",
        "    # Solve problem\n",
        "    prob.solve()\n",
        "\n",
        "    # Extract the optimal weights and return them\n",
        "    optimal_weights = w.value\n",
        "    return optimal_weights, ret.value, risk.value\n",
        "\n",
        "# Run the optimization with a range of risk aversion values\n",
        "risk_aversion_values = [0.1, 1.0, 10.0]\n",
        "optimal_weights = []\n",
        "\n",
        "# Verbose output for each risk aversion value\n",
        "for ra in risk_aversion_values:\n",
        "    weights, expected_return, portfolio_risk = optimize_portfolio(Sigma, mu, ra)\n",
        "    optimal_weights.append(weights)\n",
        "\n",
        "    # Output the results in a verbose format\n",
        "    print(f\"Risk Aversion: {ra}\")\n",
        "    print(f\"Optimal Weights: {weights}\")\n",
        "    print(f\"Expected Return: {expected_return:.4f}\")\n",
        "    print(f\"Portfolio Risk (Variance): {portfolio_risk:.4f}\")\n",
        "    print('-' * 50)\n",
        "\n",
        "# Optionally print the final optimized weights for each risk aversion value\n",
        "print(\"\\nFinal Optimal Weights for Each Risk Aversion:\")\n",
        "for i, w in enumerate(optimal_weights):\n",
        "    print(f\"Risk Aversion = {risk_aversion_values[i]}: {w}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvghLukbsjxc",
        "outputId": "10798db0-28f4-4476-9c5d-e6f37a966160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Risk Aversion: 0.1\n",
            "Optimal Weights: [9.66123437e-25 1.53173358e-24 1.00000000e+00]\n",
            "Expected Return: 0.6477\n",
            "Portfolio Risk (Variance): 0.7260\n",
            "--------------------------------------------------\n",
            "Risk Aversion: 1.0\n",
            "Optimal Weights: [-2.60272977e-24 -1.97363759e-23  1.00000000e+00]\n",
            "Expected Return: 0.6477\n",
            "Portfolio Risk (Variance): 0.7260\n",
            "--------------------------------------------------\n",
            "Risk Aversion: 10.0\n",
            "Optimal Weights: [9.24014485e-23 1.85162727e-23 1.00000000e+00]\n",
            "Expected Return: 0.6477\n",
            "Portfolio Risk (Variance): 0.7260\n",
            "--------------------------------------------------\n",
            "\n",
            "Final Optimal Weights for Each Risk Aversion:\n",
            "Risk Aversion = 0.1: [9.66123437e-25 1.53173358e-24 1.00000000e+00]\n",
            "Risk Aversion = 1.0: [-2.60272977e-24 -1.97363759e-23  1.00000000e+00]\n",
            "Risk Aversion = 10.0: [9.24014485e-23 1.85162727e-23 1.00000000e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.covariance import EmpiricalCovariance  # Use Empirical Covariance for shrinkage-like behavior\n",
        "\n",
        "# Simulating returns and covariance matrix (for demonstration)\n",
        "np.random.seed(42)\n",
        "n_assets = 3\n",
        "mu = np.random.randn(n_assets)  # expected returns\n",
        "Sigma = np.random.randn(n_assets, n_assets)\n",
        "Sigma = np.dot(Sigma, Sigma.T)  # Make it positive semi-definite\n",
        "\n",
        "# Use Empirical Covariance as a substitute for LedoitShrinkage\n",
        "cov_estimator = EmpiricalCovariance()\n",
        "cov_estimator.fit(Sigma)\n",
        "Sigma = cov_estimator.covariance_\n",
        "\n",
        "# Portfolio optimization using cvxpy\n",
        "def optimize_portfolio(Sigma, mu, risk_aversion=1.0):\n",
        "    # Define variables\n",
        "    w = cp.Variable(len(mu))  # portfolio weights\n",
        "    ret = mu.T @ w  # expected return\n",
        "    risk = cp.quad_form(w, Sigma)  # portfolio risk (variance)\n",
        "\n",
        "    # Define objective function (maximize return - risk_aversion * risk)\n",
        "    objective = cp.Maximize(ret - risk_aversion * risk)\n",
        "\n",
        "    # Constraints: sum of weights equals 1 (fully invested portfolio), weights >= 0 (no shorting)\n",
        "    constraints = [cp.sum(w) == 1, w >= 0]\n",
        "\n",
        "    # Define problem\n",
        "    prob = cp.Problem(objective, constraints)\n",
        "\n",
        "    # Solve problem\n",
        "    prob.solve()\n",
        "\n",
        "    # Extract the optimal weights\n",
        "    return w.value\n",
        "\n",
        "# Run the optimization with a range of risk aversion values\n",
        "risk_aversion_values = [0.1, 1.0, 10.0]\n",
        "optimal_weights = [optimize_portfolio(Sigma, mu, ra) for ra in risk_aversion_values]\n",
        "\n",
        "# Printing results instead of plotting\n",
        "for i, w in enumerate(optimal_weights):\n",
        "    print(f\"Risk Aversion = {risk_aversion_values[i]}: Weights: {w}\")\n",
        "    expected_return = mu.T @ w\n",
        "    portfolio_risk = np.sqrt(w.T @ Sigma @ w)  # Standard deviation\n",
        "    sharpe_ratio = expected_return / portfolio_risk if portfolio_risk != 0 else np.nan  # Avoid division by 0\n",
        "    print(f\"Expected Return: {expected_return:.4f}, Risk: {portfolio_risk:.4f}, Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Stress tests and Monte Carlo results can be analyzed here in similar fashion (printed output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT8ZqYnH4Wl9",
        "outputId": "cfdd4441-6db1-4a02-c27c-bb85b01f053d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Risk Aversion = 0.1: Weights: [3.08004961e-22 2.23292509e-22 1.00000000e+00]\n",
            "Expected Return: 0.6477, Risk: 0.1513, Sharpe Ratio: 4.2801\n",
            "--------------------------------------------------\n",
            "Risk Aversion = 1.0: Weights: [-5.14741172e-23 -9.82483555e-25  1.00000000e+00]\n",
            "Expected Return: 0.6477, Risk: 0.1513, Sharpe Ratio: 4.2801\n",
            "--------------------------------------------------\n",
            "Risk Aversion = 10.0: Weights: [4.24110228e-24 1.39939270e-24 1.00000000e+00]\n",
            "Expected Return: 0.6477, Risk: 0.1513, Sharpe Ratio: 4.2801\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulating returns and covariance matrix (for demonstration)\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Adding more assets to analyze the portfolio's behavior with more options\n",
        "mu = np.random.randn(n_assets)  # Expected returns for 5 assets\n",
        "Sigma = np.random.randn(n_assets, n_assets)\n",
        "Sigma = np.dot(Sigma, Sigma.T)  # Make the covariance matrix positive semi-definite\n",
        "\n",
        "# Visualizing Covariance Matrix\n",
        "plt.imshow(Sigma, cmap='hot', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.title(\"Covariance Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Portfolio optimization using cvxpy\n",
        "def optimize_portfolio(Sigma, mu, risk_aversion=1.0):\n",
        "    # Define variables\n",
        "    w = cp.Variable(len(mu))  # Portfolio weights\n",
        "    ret = mu.T @ w  # Expected return\n",
        "    risk = cp.quad_form(w, Sigma)  # Portfolio risk (variance)\n",
        "\n",
        "    # Define objective function (maximize return - risk_aversion * risk)\n",
        "    objective = cp.Maximize(ret - risk_aversion * risk)\n",
        "\n",
        "    # Constraints: sum of weights equals 1 (fully invested portfolio), weights >= 0 (no shorting)\n",
        "    constraints = [cp.sum(w) == 1, w >= 0]\n",
        "\n",
        "    # Define problem\n",
        "    prob = cp.Problem(objective, constraints)\n",
        "\n",
        "    # Solve problem\n",
        "    prob.solve()\n",
        "\n",
        "    # Extract the optimal weights\n",
        "    return w.value\n",
        "\n",
        "# Running optimization for different risk aversion levels\n",
        "risk_aversion_values = [0.1, 1.0, 10.0]\n",
        "optimal_weights = [optimize_portfolio(Sigma, mu, ra) for ra in risk_aversion_values]\n",
        "\n",
        "# Plotting results\n",
        "for i, w in enumerate(optimal_weights):\n",
        "    print(f\"Risk Aversion = {risk_aversion_values[i]}: Weights: {w}\")\n",
        "    plt.plot(mu, w, label=f'Risk Aversion = {risk_aversion_values[i]}')\n",
        "\n",
        "plt.title('Portfolio Weights for Different Risk Aversion Values')\n",
        "plt.xlabel('Asset Index')\n",
        "plt.ylabel('Weight')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the portfolio analysis for each risk aversion level\n",
        "for i, ra in enumerate(risk_aversion_values):\n",
        "    weights = optimal_weights[i]\n",
        "    expected_return = np.dot(weights, mu)\n",
        "    risk = np.sqrt(np.dot(weights.T, np.dot(Sigma, weights)))  # Portfolio risk (standard deviation)\n",
        "    sharpe_ratio = expected_return / risk if risk != 0 else 0\n",
        "    print(f\"Risk Aversion = {ra}:\")\n",
        "    print(f\"  Weights: {weights}\")\n",
        "    print(f\"  Expected Return: {expected_return:.4f}\")\n",
        "    print(f\"  Risk (Standard Deviation): {risk:.4f}\")\n",
        "    print(f\"  Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Stress Test Scenarios: Simulate different market conditions\n",
        "def stress_test(Sigma, mu):\n",
        "    # Scenarios to simulate extreme market conditions\n",
        "    scenarios = {\n",
        "        \"Market Bubble\": {\"Sigma_change\": 0.2 * np.identity(len(mu)), \"mu_change\": 0.05 * np.ones(len(mu))},\n",
        "        \"Liquidity Crisis\": {\"Sigma_change\": 0.15 * np.identity(len(mu)), \"mu_change\": -0.05 * np.ones(len(mu))},\n",
        "        \"Interest Rate Shock\": {\"Sigma_change\": 0.1 * np.identity(len(mu)), \"mu_change\": 0.03 * np.ones(len(mu))}\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for scenario, changes in scenarios.items():\n",
        "        Sigma_stressed = Sigma + changes[\"Sigma_change\"]\n",
        "        mu_stressed = mu + changes[\"mu_change\"]\n",
        "        weights = optimize_portfolio(Sigma_stressed, mu_stressed)\n",
        "        expected_return = np.dot(weights, mu_stressed)\n",
        "        risk = np.sqrt(np.dot(weights.T, np.dot(Sigma_stressed, weights)))  # Portfolio risk (standard deviation)\n",
        "        sharpe_ratio = expected_return / risk if risk != 0 else 0\n",
        "        results[scenario] = {\n",
        "            \"weights\": weights,\n",
        "            \"expected_return\": expected_return,\n",
        "            \"risk\": risk,\n",
        "            \"sharpe_ratio\": sharpe_ratio\n",
        "        }\n",
        "    return results\n",
        "\n",
        "# Running the stress tests\n",
        "stress_test_results = stress_test(Sigma, mu)\n",
        "\n",
        "# Output the results of the stress tests\n",
        "for scenario, result in stress_test_results.items():\n",
        "    print(f\"Stress Test: {scenario}\")\n",
        "    print(f\"  Weights: {result['weights']}\")\n",
        "    print(f\"  Expected Return: {result['expected_return']:.4f}\")\n",
        "    print(f\"  Risk (Standard Deviation): {result['risk']:.4f}\")\n",
        "    print(f\"  Sharpe Ratio: {result['sharpe_ratio']:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Monte Carlo Simulation\n",
        "def monte_carlo_testing(Sigma, mu, n_samples=1000, n_assets=5):\n",
        "    results = []\n",
        "    for _ in range(n_samples):\n",
        "        random_weights = np.random.rand(n_assets)\n",
        "        random_weights /= np.sum(random_weights)  # Normalize to sum to 1\n",
        "        expected_return = np.dot(random_weights, mu)\n",
        "        risk = np.sqrt(np.dot(random_weights.T, np.dot(Sigma, random_weights)))  # Portfolio risk (standard deviation)\n",
        "        sharpe_ratio = expected_return / risk if risk != 0 else 0\n",
        "        results.append((random_weights, expected_return, risk, sharpe_ratio))\n",
        "    return results\n",
        "\n",
        "# Running Monte Carlo simulation\n",
        "monte_carlo_results = monte_carlo_testing(Sigma, mu)\n",
        "\n",
        "# Output the first 5 Monte Carlo simulation results\n",
        "print(\"Monte Carlo Simulation Metrics:\")\n",
        "for i in range(5):\n",
        "    weights, expected_return, risk, sharpe_ratio = monte_carlo_results[i]\n",
        "    print(f\"Sample {i + 1}:\")\n",
        "    print(f\"  Weights: {weights}\")\n",
        "    print(f\"  Expected Return: {expected_return:.4f}\")\n",
        "    print(f\"  Risk (Standard Deviation): {risk:.4f}\")\n",
        "    print(f\"  Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6fQxm4Yl6Yb8",
        "outputId": "3577125b-14d7-4301-ee7d-ce153119a63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Risk Aversion = 0.1: Weights: [-6.52123531e-23 -1.95225336e-24 -2.71488788e-24  1.00000000e+00\n",
            "  1.09127170e-22]\n",
            "Risk Aversion = 1.0: Weights: [ 4.11538260e-01 -6.35646461e-24  2.15453259e-01  2.04955680e-01\n",
            "  1.68052800e-01]\n",
            "Risk Aversion = 10.0: Weights: [ 4.01934765e-01 -2.40957472e-23 -2.10408101e-23  5.49397522e-24\n",
            "  5.98065235e-01]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Risk Aversion = 0.1:\n",
            "  Weights: [-6.52123531e-23 -1.95225336e-24 -2.71488788e-24  1.00000000e+00\n",
            "  1.09127170e-22]\n",
            "  Expected Return: 1.5230\n",
            "  Risk (Standard Deviation): 2.1284\n",
            "  Sharpe Ratio: 0.7156\n",
            "--------------------------------------------------\n",
            "Risk Aversion = 1.0:\n",
            "  Weights: [ 4.11538260e-01 -6.35646461e-24  2.15453259e-01  2.04955680e-01\n",
            "  1.68052800e-01]\n",
            "  Expected Return: 0.6168\n",
            "  Risk (Standard Deviation): 0.9637\n",
            "  Sharpe Ratio: 0.6400\n",
            "--------------------------------------------------\n",
            "Risk Aversion = 10.0:\n",
            "  Weights: [ 4.01934765e-01 -2.40957472e-23 -2.10408101e-23  5.49397522e-24\n",
            "  5.98065235e-01]\n",
            "  Expected Return: 0.0596\n",
            "  Risk (Standard Deviation): 0.7681\n",
            "  Sharpe Ratio: 0.0776\n",
            "--------------------------------------------------\n",
            "Stress Test: Market Bubble\n",
            "  Weights: [4.05772009e-01 3.98852341e-23 2.10713391e-01 2.05711955e-01\n",
            " 1.77802645e-01]\n",
            "  Expected Return: 0.6597\n",
            "  Risk (Standard Deviation): 0.9893\n",
            "  Sharpe Ratio: 0.6669\n",
            "--------------------------------------------------\n",
            "Stress Test: Liquidity Crisis\n",
            "  Weights: [ 4.07152997e-01 -7.25929829e-23  2.11633559e-01  2.05457349e-01\n",
            "  1.75756095e-01]\n",
            "  Expected Return: 0.5611\n",
            "  Risk (Standard Deviation): 0.9828\n",
            "  Sharpe Ratio: 0.5709\n",
            "--------------------------------------------------\n",
            "Stress Test: Interest Rate Shock\n",
            "  Weights: [4.08571661e-01 1.97168869e-23 2.12707812e-01 2.05238747e-01\n",
            " 1.73481780e-01]\n",
            "  Expected Return: 0.6427\n",
            "  Risk (Standard Deviation): 0.9763\n",
            "  Sharpe Ratio: 0.6583\n",
            "--------------------------------------------------\n",
            "Monte Carlo Simulation Metrics:\n",
            "Sample 1:\n",
            "  Weights: [0.38526869 0.24783512 0.06871564 0.27881753 0.01936302]\n",
            "  Expected Return: 0.6217\n",
            "  Risk (Standard Deviation): 1.3406\n",
            "  Sharpe Ratio: 0.4638\n",
            "--------------------------------------------------\n",
            "Sample 2:\n",
            "  Weights: [0.34154139 0.09719795 0.24884384 0.1170789  0.19533792]\n",
            "  Expected Return: 0.4500\n",
            "  Risk (Standard Deviation): 1.0693\n",
            "  Sharpe Ratio: 0.4208\n",
            "--------------------------------------------------\n",
            "Sample 3:\n",
            "  Weights: [0.16005425 0.05411777 0.28385444 0.22692696 0.27504659]\n",
            "  Expected Return: 0.5371\n",
            "  Risk (Standard Deviation): 1.2450\n",
            "  Sharpe Ratio: 0.4314\n",
            "--------------------------------------------------\n",
            "Sample 4:\n",
            "  Weights: [0.33153088 0.22152017 0.34155167 0.03278621 0.07261107]\n",
            "  Expected Return: 0.3882\n",
            "  Risk (Standard Deviation): 1.3349\n",
            "  Sharpe Ratio: 0.2908\n",
            "--------------------------------------------------\n",
            "Sample 5:\n",
            "  Weights: [0.02432462 0.17497261 0.20904255 0.14593982 0.4457204 ]\n",
            "  Expected Return: 0.2412\n",
            "  Risk (Standard Deviation): 1.5459\n",
            "  Sharpe Ratio: 0.1560\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulating returns and covariance matrix (for demonstration)\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Increasing the number of assets\n",
        "mu = np.random.randn(n_assets)  # Expected returns\n",
        "Sigma = np.random.randn(n_assets, n_assets)\n",
        "Sigma = np.dot(Sigma, Sigma.T)  # Making the covariance matrix positive semi-definite\n",
        "\n",
        "# Regularizing the covariance matrix manually (ridge shrinkage)\n",
        "def regularize_cov_matrix(Sigma, alpha=0.1):\n",
        "    # Adding alpha to the diagonal elements of Sigma to stabilize the covariance matrix\n",
        "    return Sigma + alpha * np.eye(Sigma.shape[0])\n",
        "\n",
        "# Apply regularization\n",
        "Sigma_shrinked = regularize_cov_matrix(Sigma)\n",
        "\n",
        "# Portfolio optimization using cvxpy with L2 regularization\n",
        "def optimize_portfolio(Sigma, mu, risk_aversion=1.0, l2_regularization=0.1):\n",
        "    w = cp.Variable(len(mu))  # Portfolio weights\n",
        "    ret = mu.T @ w  # Expected return\n",
        "    risk = cp.quad_form(w, Sigma)  # Portfolio risk (variance)\n",
        "\n",
        "    # Adding L2 regularization to penalize large portfolio weights\n",
        "    # We compute the L2 norm correctly by using cp.norm with 'fro' for vector\n",
        "    l2_penalty = cp.norm(w, 'fro')  # L2 norm of the weight vector (Frobenius norm)\n",
        "\n",
        "    objective = cp.Maximize(ret - risk_aversion * risk - l2_regularization * l2_penalty)\n",
        "\n",
        "    # Constraints: Sum of weights equals 1 (fully invested portfolio), weights >= 0 (no shorting)\n",
        "    constraints = [cp.sum(w) == 1, w >= 0]\n",
        "\n",
        "    # Define the problem and solve it\n",
        "    prob = cp.Problem(objective, constraints)\n",
        "    prob.solve()\n",
        "\n",
        "    return w.value\n",
        "\n",
        "# Running the optimization with a range of risk aversion values and L2 regularization\n",
        "risk_aversion_values = [0.1, 1.0, 10.0]\n",
        "l2_regularization_values = [0.01, 0.1, 1.0]\n",
        "optimal_weights = []\n",
        "\n",
        "# Looping over risk aversion and L2 regularization values\n",
        "for ra in risk_aversion_values:\n",
        "    for l2 in l2_regularization_values:\n",
        "        optimal_weights.append(optimize_portfolio(Sigma_shrinked, mu, ra, l2))\n",
        "        print(f\"Risk Aversion: {ra}, L2 Regularization: {l2}\")\n",
        "        print(f\"Optimal Weights: {optimal_weights[-1]}\")\n",
        "\n",
        "# Plot portfolio weights for different risk aversion values and L2 regularization\n",
        "for i, w in enumerate(optimal_weights):\n",
        "    plt.plot(mu, w, label=f'Risk Aversion = {risk_aversion_values[i//3]}, L2 = {l2_regularization_values[i%3]}')\n",
        "\n",
        "plt.title('Portfolio Weights for Different Risk Aversion and L2 Regularization Values')\n",
        "plt.xlabel('Asset Index')\n",
        "plt.ylabel('Weight')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Define market stress test scenarios\n",
        "def extreme_market_scenarios(Sigma, mu):\n",
        "    scenarios = [\n",
        "        {'name': 'Financial Crisis', 'sigma_change': 0.2 * np.identity(n_assets), 'mu_change': -0.05 * np.ones(n_assets)},\n",
        "        {'name': 'Inflation Shock', 'sigma_change': 0.1 * np.identity(n_assets), 'mu_change': 0.02 * np.ones(n_assets)},\n",
        "        {'name': 'Market Bubble', 'sigma_change': 0.15 * np.identity(n_assets), 'mu_change': 0.05 * np.ones(n_assets)},\n",
        "        {'name': 'Liquidity Crisis', 'sigma_change': 0.1 * np.identity(n_assets), 'mu_change': 0.0 * np.ones(n_assets)},\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "    for scenario in scenarios:\n",
        "        stressed_Sigma = Sigma + scenario['sigma_change']\n",
        "        stressed_mu = mu + scenario['mu_change']\n",
        "        optimized_weights = optimize_portfolio(stressed_Sigma, stressed_mu)\n",
        "        results[scenario['name']] = optimized_weights\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the stress test scenarios\n",
        "stress_test_results = extreme_market_scenarios(Sigma_shrinked, mu)\n",
        "\n",
        "# Display results from the stress tests\n",
        "for scenario, weights in stress_test_results.items():\n",
        "    print(f\"{scenario}: Weights: {weights}\")\n",
        "    expected_return = mu.T @ weights\n",
        "    portfolio_risk = np.sqrt(weights.T @ Sigma_shrinked @ weights)\n",
        "    sharpe_ratio = expected_return / portfolio_risk\n",
        "    print(f\"Expected Return: {expected_return:.4f}, Risk (Standard Deviation): {portfolio_risk:.4f}, Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Expanding Monte Carlo Simulations with 5000 samples\n",
        "def monte_carlo_simulation(Sigma, mu, n_samples=5000):\n",
        "    portfolio_returns = []\n",
        "    portfolio_risks = []\n",
        "    portfolio_weights = []\n",
        "\n",
        "    for _ in range(n_samples):\n",
        "        # Random weights\n",
        "        weights = np.random.rand(len(mu))\n",
        "        weights /= np.sum(weights)  # Ensure the sum of weights is 1\n",
        "        portfolio_weights.append(weights)\n",
        "\n",
        "        # Calculate portfolio return and risk\n",
        "        portfolio_return = np.dot(mu, weights)\n",
        "        portfolio_risk = np.sqrt(np.dot(weights.T, np.dot(Sigma, weights)))\n",
        "\n",
        "        portfolio_returns.append(portfolio_return)\n",
        "        portfolio_risks.append(portfolio_risk)\n",
        "\n",
        "    return np.array(portfolio_weights), np.array(portfolio_returns), np.array(portfolio_risks)\n",
        "\n",
        "# Run the Monte Carlo simulation\n",
        "weights, returns, risks = monte_carlo_simulation(Sigma_shrinked, mu)\n",
        "\n",
        "# Plot Monte Carlo simulation results\n",
        "plt.scatter(risks, returns, alpha=0.5, color='blue')\n",
        "plt.title('Monte Carlo Simulation: Portfolio Return vs Risk')\n",
        "plt.xlabel('Risk (Standard Deviation)')\n",
        "plt.ylabel('Return')\n",
        "plt.show()\n",
        "\n",
        "# Calculate performance metrics: Maximum Drawdown, VaR, and CVaR\n",
        "def calculate_risk_metrics(returns, confidence_level=0.05):\n",
        "    # Maximum Drawdown\n",
        "    cumulative_returns = np.cumsum(returns)\n",
        "    max_drawdown = np.min(cumulative_returns)\n",
        "\n",
        "    # Value at Risk (VaR)\n",
        "    var = np.percentile(returns, 100 * confidence_level)\n",
        "\n",
        "    # Conditional VaR (CVaR)\n",
        "    cvar = np.mean(returns[returns <= var])\n",
        "\n",
        "    return max_drawdown, var, cvar\n",
        "\n",
        "# Calculate and display risk metrics for Monte Carlo simulation\n",
        "max_drawdown, var, cvar = calculate_risk_metrics(returns)\n",
        "print(f\"Maximum Drawdown: {max_drawdown:.4f}\")\n",
        "print(f\"Value at Risk (VaR) at 5%: {var:.4f}\")\n",
        "print(f\"Conditional VaR (CVaR): {cvar:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rvM6vq5v7vDr",
        "outputId": "95847a97-0e41-4ccb-dc0d-aec64fcb8a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Risk Aversion: 0.1, L2 Regularization: 0.01\n",
            "Optimal Weights: [1.34980835e-07 6.37585926e-10 5.69391691e-09 9.99999858e-01\n",
            " 7.41732048e-10]\n",
            "Risk Aversion: 0.1, L2 Regularization: 0.1\n",
            "Optimal Weights: [2.88487294e-02 1.44904695e-10 2.34372970e-09 9.71151268e-01\n",
            " 1.65594452e-10]\n",
            "Risk Aversion: 0.1, L2 Regularization: 1.0\n",
            "Optimal Weights: [2.28930342e-01 1.26099536e-11 2.00363584e-01 5.70706074e-01\n",
            " 9.67243871e-12]\n",
            "Risk Aversion: 1.0, L2 Regularization: 0.01\n",
            "Optimal Weights: [4.08302532e-01 9.40673777e-12 2.12493213e-01 2.05276544e-01\n",
            " 1.73927711e-01]\n",
            "Risk Aversion: 1.0, L2 Regularization: 0.1\n",
            "Optimal Weights: [4.05936426e-01 3.42661822e-10 2.10817323e-01 2.05679989e-01\n",
            " 1.77566261e-01]\n",
            "Risk Aversion: 1.0, L2 Regularization: 1.0\n",
            "Optimal Weights: [3.86167121e-01 5.35236793e-11 2.05670327e-01 2.11522247e-01\n",
            " 1.96640305e-01]\n",
            "Risk Aversion: 10.0, L2 Regularization: 0.01\n",
            "Optimal Weights: [4.04334941e-01 2.17856855e-11 3.42642444e-03 7.59662210e-10\n",
            " 5.92238634e-01]\n",
            "Risk Aversion: 10.0, L2 Regularization: 0.1\n",
            "Optimal Weights: [4.04531740e-01 1.13840112e-11 5.26905165e-03 2.74281942e-10\n",
            " 5.90199208e-01]\n",
            "Risk Aversion: 10.0, L2 Regularization: 1.0\n",
            "Optimal Weights: [4.06327755e-01 2.33439482e-10 2.27988479e-02 1.81319618e-07\n",
            " 5.70873216e-01]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Financial Crisis: Weights: [4.00709553e-01 2.45726256e-10 2.08199789e-01 2.06912396e-01\n",
            " 1.84178262e-01]\n",
            "Expected Return: 0.6059, Risk (Standard Deviation): 0.9731, Sharpe Ratio: 0.6226\n",
            "------------------------------------------------------------\n",
            "Inflation Shock: Weights: [4.03262287e-01 3.21571404e-10 2.09315646e-01 2.06262030e-01\n",
            " 1.81160037e-01]\n",
            "Expected Return: 0.6076, Risk (Standard Deviation): 0.9739, Sharpe Ratio: 0.6239\n",
            "------------------------------------------------------------\n",
            "Market Bubble: Weights: [4.01971769e-01 3.22552265e-10 2.08716838e-01 2.06580980e-01\n",
            " 1.82730413e-01]\n",
            "Expected Return: 0.6067, Risk (Standard Deviation): 0.9735, Sharpe Ratio: 0.6232\n",
            "------------------------------------------------------------\n",
            "Liquidity Crisis: Weights: [4.03262277e-01 3.10552565e-10 2.09315536e-01 2.06261986e-01\n",
            " 1.81160201e-01]\n",
            "Expected Return: 0.6076, Risk (Standard Deviation): 0.9739, Sharpe Ratio: 0.6239\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Drawdown: 0.6217\n",
            "Value at Risk (VaR) at 5%: 0.1693\n",
            "Conditional VaR (CVaR): 0.1014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cvxpy as cp\n",
        "from scipy.stats import t\n",
        "\n",
        "# Simulating returns and covariance matrix (for demonstration)\n",
        "np.random.seed(42)\n",
        "n_assets = 5\n",
        "mu = np.random.randn(n_assets)  # Expected returns\n",
        "Sigma = np.random.randn(n_assets, n_assets)\n",
        "Sigma = np.dot(Sigma, Sigma.T)  # Make it positive semi-definite\n",
        "\n",
        "# Portfolio Optimization using cvxpy (for comparison)\n",
        "def optimize_portfolio(Sigma, mu, risk_aversion=1.0, l2_regularization=0.01):\n",
        "    w = cp.Variable(len(mu))  # Portfolio weights\n",
        "    ret = mu.T @ w  # Expected return\n",
        "    risk = cp.quad_form(w, Sigma)  # Portfolio risk (variance)\n",
        "    l2_penalty = cp.sum_squares(w)  # L2 regularization for vector w\n",
        "    objective = cp.Maximize(ret - risk_aversion * risk - l2_regularization * l2_penalty)\n",
        "    constraints = [cp.sum(w) == 1, w >= 0]\n",
        "    prob = cp.Problem(objective, constraints)\n",
        "    prob.solve()\n",
        "    return w.value\n",
        "\n",
        "# Simulated Annealing Optimization (Alternative optimization)\n",
        "def simulated_annealing(Sigma, mu, max_iter=1000, initial_temp=100, cooling_rate=0.99):\n",
        "    n_assets = len(mu)\n",
        "    w = np.ones(n_assets) / n_assets  # Start with equal weights\n",
        "    best_w = w\n",
        "    best_score = -np.inf\n",
        "    temperature = initial_temp\n",
        "    for _ in range(max_iter):\n",
        "        # Generate a neighboring solution\n",
        "        new_w = np.clip(w + np.random.randn(n_assets) * 0.05, 0, 1)\n",
        "        new_w /= np.sum(new_w)  # Ensure it sums to 1\n",
        "\n",
        "        # Calculate the score (return - risk)\n",
        "        ret = np.dot(mu, new_w)\n",
        "        risk = np.dot(new_w.T, np.dot(Sigma, new_w))\n",
        "        score = ret - 0.5 * risk\n",
        "\n",
        "        # Accept new solution with probability based on temperature\n",
        "        if score > best_score or np.random.rand() < np.exp((score - best_score) / temperature):\n",
        "            best_w = new_w\n",
        "            best_score = score\n",
        "\n",
        "        temperature *= cooling_rate  # Reduce temperature\n",
        "    return best_w\n",
        "\n",
        "# Particle Swarm Optimization (PSO) (Alternative optimization)\n",
        "def particle_swarm_optimization(Sigma, mu, n_particles=50, max_iter=1000, w_range=(0, 1)):\n",
        "    n_assets = len(mu)\n",
        "    particles = np.random.rand(n_particles, n_assets) * (w_range[1] - w_range[0]) + w_range[0]\n",
        "    particles /= np.sum(particles, axis=1)[:, np.newaxis]  # Normalize particles\n",
        "    velocities = np.zeros_like(particles)\n",
        "    best_particles = particles.copy()\n",
        "    best_scores = np.array([np.dot(mu, p) - 0.5 * np.dot(p.T, np.dot(Sigma, p)) for p in particles])\n",
        "    global_best_particle = best_particles[np.argmin(best_scores)]\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        for i in range(n_particles):\n",
        "            # Calculate the score (return - risk)\n",
        "            score = np.dot(mu, particles[i]) - 0.5 * np.dot(particles[i].T, np.dot(Sigma, particles[i]))\n",
        "            if score > best_scores[i]:\n",
        "                best_particles[i] = particles[i]\n",
        "                best_scores[i] = score\n",
        "\n",
        "        # Update velocity and particle position\n",
        "        r1, r2 = np.random.rand(2, n_particles, n_assets)\n",
        "        velocities = 0.7 * velocities + 1.5 * r1 * (best_particles - particles) + 1.5 * r2 * (global_best_particle - particles)\n",
        "        particles += velocities\n",
        "        particles = np.clip(particles, 0, 1)  # Ensure the weights are within the bounds\n",
        "        particles /= np.sum(particles, axis=1)[:, np.newaxis]  # Normalize particles\n",
        "\n",
        "        global_best_particle = best_particles[np.argmin(best_scores)]\n",
        "\n",
        "    return global_best_particle\n",
        "\n",
        "# Monte Carlo Simulation: Run more simulations\n",
        "def monte_carlo_simulation(Sigma, mu, n_samples=1000):\n",
        "    portfolios = []\n",
        "    for _ in range(n_samples):\n",
        "        weights = np.random.rand(len(mu))\n",
        "        weights /= np.sum(weights)  # Normalize weights\n",
        "        expected_return = np.dot(mu, weights)\n",
        "        risk = np.sqrt(np.dot(weights.T, np.dot(Sigma, weights)))\n",
        "        portfolios.append((weights, expected_return, risk))\n",
        "    return portfolios\n",
        "\n",
        "# Adding Performance Metrics (Sortino, Treynor, Omega, Max Drawdown)\n",
        "def performance_metrics(portfolios, risk_free_rate=0.02):\n",
        "    results = []\n",
        "    for weights, expected_return, risk in portfolios:\n",
        "        # Sortino Ratio (compares return against downside risk)\n",
        "        downside_risk = np.sqrt(np.mean(np.minimum(0, expected_return - risk)**2))\n",
        "        sortino_ratio = (expected_return - risk_free_rate) / downside_risk if downside_risk != 0 else np.nan\n",
        "\n",
        "        # Treynor Ratio (compares return against systematic risk)\n",
        "        beta = np.dot(weights.T, np.dot(Sigma, weights))  # Assuming market risk is defined by Sigma\n",
        "        treynor_ratio = (expected_return - risk_free_rate) / beta if beta != 0 else np.nan\n",
        "\n",
        "        # Omega Ratio (compares the probability of positive returns vs negative returns)\n",
        "        omega_ratio = expected_return / risk if risk != 0 else np.nan\n",
        "\n",
        "        # Maximum Drawdown\n",
        "        drawdowns = np.maximum(0, expected_return - np.minimum(expected_return))\n",
        "        max_drawdown = np.max(drawdowns)\n",
        "\n",
        "        results.append((weights, expected_return, risk, sortino_ratio, treynor_ratio, omega_ratio, max_drawdown))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Stress Testing (Adding new scenarios)\n",
        "def stress_test(Sigma, mu):\n",
        "    # Define new scenarios\n",
        "    scenarios = {\n",
        "        'Hyperinflation': (Sigma * 1.5, mu * 1.2),\n",
        "        'Currency Devaluation': (Sigma * 2, mu * 0.5),\n",
        "        'Market Crash': (Sigma * 3, mu * -0.5)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for name, (sigma_stress, mu_stress) in scenarios.items():\n",
        "        optimal_weights = optimize_portfolio(sigma_stress, mu_stress, risk_aversion=1.0, l2_regularization=0.1)\n",
        "        results[name] = optimal_weights\n",
        "    return results\n",
        "\n",
        "# Sensitivity Analysis: Risk Aversion vs. L2 Regularization\n",
        "def sensitivity_analysis(Sigma, mu, risk_aversion_values, l2_regularization_values):\n",
        "    optimal_weights = []\n",
        "    for ra in risk_aversion_values:\n",
        "        for l2 in l2_regularization_values:\n",
        "            w = optimize_portfolio(Sigma, mu, risk_aversion=ra, l2_regularization=l2)\n",
        "            optimal_weights.append((ra, l2, w))\n",
        "    return optimal_weights\n",
        "\n",
        "# Dynamic Rebalancing (Hypothetical model based on changing risk aversion)\n",
        "def dynamic_rebalancing(Sigma, mu, risk_aversion_values, initial_weights):\n",
        "    weights = initial_weights.copy()\n",
        "    rebalanced_weights = []\n",
        "\n",
        "    for ra in risk_aversion_values:\n",
        "        # Rebalance portfolio based on changing risk aversion\n",
        "        optimized_weights = optimize_portfolio(Sigma, mu, risk_aversion=ra)\n",
        "        rebalanced_weights.append(optimized_weights)\n",
        "        weights = optimized_weights  # Update portfolio for the next iteration\n",
        "\n",
        "    return rebalanced_weights\n",
        "\n",
        "# Example Use of Functions\n",
        "risk_aversion_values = [0.1, 1.0, 10.0]\n",
        "l2_regularization_values = [0.01, 0.1, 1.0]\n",
        "risk_free_rate = 0.02\n",
        "\n",
        "# Running Monte Carlo Simulations\n",
        "monte_carlo_results = monte_carlo_simulation(Sigma, mu, n_samples=1000)\n",
        "\n",
        "# Running Stress Tests\n",
        "stress_test_results = stress_test(Sigma, mu)\n",
        "\n",
        "# Running Sensitivity Analysis\n",
        "sensitivity_results = sensitivity_analysis(Sigma, mu, risk_aversion_values, l2_regularization_values)\n",
        "\n",
        "# Dynamic Rebalancing Simulation\n",
        "initial_weights = np.ones(len(mu)) / len(mu)  # Starting with equal weights\n",
        "dynamic_weights = dynamic_rebalancing(Sigma, mu, risk_aversion_values, initial_weights)\n",
        "\n",
        "# Show Results\n",
        "print(\"Monte Carlo Results: \")\n",
        "for portfolio in monte_carlo_results[:5]:\n",
        "    print(f\"Expected Return: {portfolio[1]}, Risk: {portfolio[2]}\")\n",
        "\n",
        "print(\"\\nStress Test Results: \")\n",
        "for test, result in stress_test_results.items():\n",
        "    print(f\"{test}: Weights: {result}, Expected Return: {np.dot(mu, result)}, Risk: {np.sqrt(np.dot(result.T, np.dot(Sigma, result)))}\")\n",
        "\n",
        "print(\"\\nSensitivity Analysis: \")\n",
        "for ra, l2, w in sensitivity_results:\n",
        "    print(f\"Risk Aversion: {ra}, L2 Regularization: {l2}, Weights: {w}\")\n",
        "\n",
        "print(\"\\nDynamic Rebalancing Results: \")\n",
        "for i, weights in enumerate(dynamic_weights):\n",
        "    print(f\"Risk Aversion: {risk_aversion_values[i]}, Weights: {weights}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbg62svB8kt7",
        "outputId": "e03dd128-bf8c-4197-fd46-63ca8ad12116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monte Carlo Results: \n",
            "Expected Return: 0.6217214922958851, Risk: 1.3406080401336482\n",
            "Expected Return: 0.44995837167029723, Risk: 1.0692802207593959\n",
            "Expected Return: 0.5370813666894423, Risk: 1.2450195254819487\n",
            "Expected Return: 0.38819910067365576, Risk: 1.3349476769610618\n",
            "Expected Return: 0.24118813964359254, Risk: 1.5459351751645432\n",
            "\n",
            "Stress Test Results: \n",
            "Hyperinflation: Weights: [4.08969762e-01 9.93854752e-23 1.64304138e-01 1.55226230e-01\n",
            " 2.71499869e-01], Expected Return: 0.4824005487447595, Risk: 0.8984149790528918\n",
            "Currency Devaluation: Weights: [4.07468870e-01 2.33505350e-23 2.42398350e-02 1.53374090e-02\n",
            " 5.52953886e-01], Expected Return: 0.11197873115737861, Risk: 0.7766160703844905\n",
            "Market Crash: Weights: [ 3.91247364e-01  2.50201548e-23 -1.84979144e-23 -3.05174423e-23\n",
            "  6.08752636e-01], Expected Return: 0.051796618589244225, Risk: 0.7682434036389787\n",
            "\n",
            "Sensitivity Analysis: \n",
            "Risk Aversion: 0.1, L2 Regularization: 0.01, Weights: [1.05196622e-22 1.35906998e-22 8.84233130e-23 1.00000000e+00\n",
            " 2.01172570e-22]\n",
            "Risk Aversion: 0.1, L2 Regularization: 0.1, Weights: [6.26876055e-02 6.27211509e-23 5.20535434e-25 9.37312394e-01\n",
            " 1.69368211e-22]\n",
            "Risk Aversion: 0.1, L2 Regularization: 1.0, Weights: [2.44992959e-01 2.97863990e-23 2.24164528e-01 5.30842514e-01\n",
            " 2.26094196e-23]\n",
            "Risk Aversion: 1.0, L2 Regularization: 0.01, Weights: [ 4.11233001e-01 -6.77963471e-23  2.15135707e-01  2.04972234e-01\n",
            "  1.68659058e-01]\n",
            "Risk Aversion: 1.0, L2 Regularization: 0.1, Weights: [4.08571661e-01 5.03206152e-23 2.12707812e-01 2.05238747e-01\n",
            " 1.73481780e-01]\n",
            "Risk Aversion: 1.0, L2 Regularization: 1.0, Weights: [ 3.87230297e-01 -5.83544755e-23  2.05692211e-01  2.11156583e-01\n",
            "  1.95920910e-01]\n",
            "Risk Aversion: 10.0, L2 Regularization: 0.01, Weights: [ 4.01957999e-01  1.15480505e-23 -6.54677770e-23 -3.58550730e-23\n",
            "  5.98042001e-01]\n",
            "Risk Aversion: 10.0, L2 Regularization: 0.1, Weights: [ 4.02166605e-01 -1.34927768e-23  1.83968511e-23 -2.46429944e-23\n",
            "  5.97833395e-01]\n",
            "Risk Aversion: 10.0, L2 Regularization: 1.0, Weights: [ 4.04312876e-01  1.99774330e-23  3.22065021e-03 -7.53076986e-24\n",
            "  5.92466474e-01]\n",
            "\n",
            "Dynamic Rebalancing Results: \n",
            "Risk Aversion: 0.1, Weights: [1.05196622e-22 1.35906998e-22 8.84233130e-23 1.00000000e+00\n",
            " 2.01172570e-22]\n",
            "Risk Aversion: 1.0, Weights: [ 4.11233001e-01 -6.77963471e-23  2.15135707e-01  2.04972234e-01\n",
            "  1.68659058e-01]\n",
            "Risk Aversion: 10.0, Weights: [ 4.01957999e-01  1.15480505e-23 -6.54677770e-23 -3.58550730e-23\n",
            "  5.98042001e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cvxpy as cp\n",
        "from scipy.stats import t\n",
        "from scipy.optimize import differential_evolution\n",
        "from pyswarm import pso  # Particle Swarm Optimization\n",
        "import random\n",
        "from sklearn.covariance import EmpiricalCovariance  # Use Empirical Covariance\n",
        "\n",
        "# Simulating returns and covariance matrix (for demonstration)\n",
        "np.random.seed(42)\n",
        "n_assets = 5\n",
        "mu = np.random.randn(n_assets)  # expected returns\n",
        "Sigma = np.random.randn(n_assets, n_assets)\n",
        "Sigma = np.dot(Sigma, Sigma.T)  # Make it positive semi-definite\n",
        "\n",
        "# Use Empirical Covariance as a robust estimate\n",
        "def empirical_covariance_matrix(Sigma):\n",
        "    cov_estimator = EmpiricalCovariance()\n",
        "    cov_estimator.fit(Sigma)\n",
        "    return cov_estimator.covariance_\n",
        "\n",
        "Sigma_robust = empirical_covariance_matrix(Sigma)\n",
        "\n",
        "# Portfolio optimization using cvxpy (for comparison)\n",
        "def optimize_portfolio(Sigma, mu, risk_aversion=1.0, l2_regularization=0.1):\n",
        "    w = cp.Variable(len(mu))  # portfolio weights\n",
        "    ret = mu.T @ w  # expected return\n",
        "    risk = cp.quad_form(w, Sigma)  # portfolio risk (variance)\n",
        "\n",
        "    # Adding L2 regularization to penalize large portfolio weights\n",
        "    l2_penalty = cp.sum_squares(w)  # Sum of squares for L2 regularization\n",
        "    objective = cp.Maximize(ret - risk_aversion * risk - l2_regularization * l2_penalty)\n",
        "\n",
        "    # Constraints: sum of weights equals 1 (fully invested portfolio), weights >= 0 (no shorting)\n",
        "    constraints = [cp.sum(w) == 1, w >= 0]\n",
        "\n",
        "    # Define problem\n",
        "    prob = cp.Problem(objective, constraints)\n",
        "\n",
        "    # Solve problem\n",
        "    prob.solve()\n",
        "\n",
        "    return w.value\n",
        "\n",
        "# Alternative optimization technique: Genetic Algorithm\n",
        "def genetic_algorithm(Sigma, mu, risk_aversion=1.0, l2_regularization=0.1):\n",
        "    def fitness_function(weights):\n",
        "        weights = np.array(weights)\n",
        "        ret = np.dot(mu, weights)\n",
        "        risk = np.dot(weights.T, np.dot(Sigma, weights))\n",
        "        l2_penalty = np.linalg.norm(weights, 2)\n",
        "        return -(ret - risk_aversion * risk - l2_regularization * l2_penalty)  # Negative for maximization\n",
        "\n",
        "    bounds = [(0, 1) for _ in range(len(mu))]\n",
        "    result = differential_evolution(fitness_function, bounds)\n",
        "    return result.x\n",
        "\n",
        "# Alternative optimization technique: Particle Swarm Optimization (PSO)\n",
        "def pso_optimization(Sigma, mu, risk_aversion=1.0, l2_regularization=0.1):\n",
        "    def fitness_function(weights):\n",
        "        weights = np.array(weights)\n",
        "        ret = np.dot(mu, weights)\n",
        "        risk = np.dot(weights.T, np.dot(Sigma, weights))\n",
        "        l2_penalty = np.linalg.norm(weights, 2)\n",
        "        return -(ret - risk_aversion * risk - l2_regularization * l2_penalty)\n",
        "\n",
        "    lb = [0] * len(mu)\n",
        "    ub = [1] * len(mu)\n",
        "    optimal_weights, _ = pso(fitness_function, lb, ub)\n",
        "    return optimal_weights\n",
        "\n",
        "# Stress test scenarios\n",
        "def stress_test(Sigma, mu):\n",
        "    scenarios = {\n",
        "        'Hyperinflation': (Sigma * 1.5, mu * 1.2),  # Increase Sigma and mu for hyperinflation\n",
        "        'Currency Devaluation': (Sigma * 1.2, mu * 0.8),  # Decrease mu for currency devaluation\n",
        "        'Market Crash': (Sigma * 3, mu * 0.5),  # Increase Sigma and decrease mu for a crash\n",
        "        'Geopolitical Risk': (Sigma * 1.3, mu * 0.9)  # Moderate change for geopolitical risk\n",
        "    }\n",
        "    results = {}\n",
        "    for name, (sigma_stress, mu_stress) in scenarios.items():\n",
        "        optimal_weights = optimize_portfolio(sigma_stress, mu_stress)\n",
        "        results[name] = {\n",
        "            'Weights': optimal_weights,\n",
        "            'Expected Return': np.dot(mu_stress, optimal_weights),\n",
        "            'Risk': np.sqrt(np.dot(optimal_weights.T, np.dot(sigma_stress, optimal_weights)))\n",
        "        }\n",
        "    return results\n",
        "\n",
        "# Monte Carlo Simulation: Generate random portfolio weights and calculate risk/return\n",
        "def monte_carlo_simulation(Sigma, mu, n_simulations=1000):\n",
        "    portfolios = []\n",
        "    for _ in range(n_simulations):\n",
        "        weights = np.random.random(len(mu))\n",
        "        weights /= np.sum(weights)  # Normalize to sum to 1\n",
        "        return_ = np.dot(mu, weights)\n",
        "        risk_ = np.sqrt(np.dot(weights.T, np.dot(Sigma, weights)))\n",
        "        portfolios.append((return_, risk_))\n",
        "    return portfolios\n",
        "\n",
        "# Expand Monte Carlo Simulations with correlated returns and fat-tailed distributions\n",
        "def expanded_monte_carlo_simulation(Sigma, mu, n_simulations=1000):\n",
        "    portfolios = []\n",
        "    for _ in range(n_simulations):\n",
        "        # Generate correlated returns using Student's t-distribution for fat tails\n",
        "        t_returns = np.random.standard_t(3, size=len(mu))  # t-distribution with 3 degrees of freedom\n",
        "        correlated_returns = np.dot(t_returns, Sigma) + mu\n",
        "        weights = np.random.random(len(mu))\n",
        "        weights /= np.sum(weights)  # Normalize to sum to 1\n",
        "        return_ = np.dot(mu, weights)\n",
        "        risk_ = np.sqrt(np.dot(weights.T, np.dot(Sigma, weights)))\n",
        "        portfolios.append((return_, risk_))\n",
        "    return portfolios\n",
        "\n",
        "# Additional performance metrics: Sortino ratio, Treynor ratio, Omega ratio, VaR, CVaR\n",
        "def performance_metrics(returns, risks, risk_free_rate=0.03):\n",
        "    sortino_ratio = np.mean(returns) / np.std(returns)\n",
        "    treynor_ratio = np.mean(returns - risk_free_rate) / np.mean(risks)  # Treynor ratio\n",
        "    omega_ratio = np.sum(returns[returns > 0]) / np.abs(np.sum(returns[returns < 0]))  # Omega ratio\n",
        "    var_5 = np.percentile(risks, 5)  # 5% Value at Risk (VaR)\n",
        "    cvar_5 = np.mean(risks[risks <= var_5])  # Conditional VaR (CVaR)\n",
        "\n",
        "    return {\n",
        "        'Sortino Ratio': sortino_ratio,\n",
        "        'Treynor Ratio': treynor_ratio,\n",
        "        'Omega Ratio': omega_ratio,\n",
        "        'VaR at 5%': var_5,\n",
        "        'CVaR at 5%': cvar_5\n",
        "    }\n",
        "\n",
        "# Portfolio Sensitivity Analysis\n",
        "def sensitivity_analysis(Sigma, mu, risk_aversion_values, l2_regularization_values):\n",
        "    results = []\n",
        "    for ra in risk_aversion_values:\n",
        "        for l2 in l2_regularization_values:\n",
        "            optimal_weights = optimize_portfolio(Sigma, mu, risk_aversion=ra, l2_regularization=l2)\n",
        "            results.append((ra, l2, optimal_weights))\n",
        "    return results\n",
        "\n",
        "# Dynamic Portfolio Rebalancing\n",
        "def dynamic_rebalancing(Sigma, mu, initial_weights, risk_aversion_values, periods=10):\n",
        "    # Simulate dynamic rebalancing with periodic changes in risk aversion\n",
        "    rebalanced_weights = []\n",
        "    for period in range(periods):\n",
        "        risk_aversion = random.choice(risk_aversion_values)\n",
        "        new_weights = optimize_portfolio(Sigma, mu, risk_aversion=risk_aversion)\n",
        "        rebalanced_weights.append(new_weights)\n",
        "    return rebalanced_weights\n",
        "\n",
        "# Run tests\n",
        "print(\"Running Stress Tests...\")\n",
        "stress_test_results = stress_test(Sigma, mu)\n",
        "\n",
        "print(\"Running Monte Carlo Simulation...\")\n",
        "monte_carlo_results = monte_carlo_simulation(Sigma, mu, n_simulations=1000)\n",
        "\n",
        "print(\"Expanding Monte Carlo Simulation with Fat-Tailed Distributions...\")\n",
        "expanded_monte_carlo_results = expanded_monte_carlo_simulation(Sigma, mu, n_simulations=1000)\n",
        "\n",
        "print(\"Calculating Performance Metrics...\")\n",
        "metrics = performance_metrics(np.array([x[0] for x in monte_carlo_results]), np.array([x[1] for x in monte_carlo_results]))\n",
        "\n",
        "print(\"Performing Sensitivity Analysis...\")\n",
        "sensitivity_results = sensitivity_analysis(Sigma, mu, risk_aversion_values=[0.1, 1.0, 10.0], l2_regularization_values=[0.01, 0.1, 1.0])\n",
        "\n",
        "print(\"Performing Dynamic Portfolio Rebalancing...\")\n",
        "rebalanced_weights = dynamic_rebalancing(Sigma, mu, initial_weights=np.ones(len(mu)) / len(mu), risk_aversion_values=[0.1, 1.0, 10.0])\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nStress Test Results:\", stress_test_results)\n",
        "print(\"\\nMonte Carlo Simulation Results:\", monte_carlo_results[:5])  # Print only first 5\n",
        "print(\"\\nExpanded Monte Carlo Simulation Results:\", expanded_monte_carlo_results[:5])\n",
        "print(\"\\nPerformance Metrics:\", metrics)\n",
        "print(\"\\nSensitivity Analysis Results:\", sensitivity_results[:5])  # Print only first 5\n",
        "print(\"\\nDynamic Rebalancing Results:\", rebalanced_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55_OsBDa-Prv",
        "outputId": "ddaa3ca7-6883-4af3-88c0-32b3c539dcd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Stress Tests...\n",
            "Running Monte Carlo Simulation...\n",
            "Expanding Monte Carlo Simulation with Fat-Tailed Distributions...\n",
            "Calculating Performance Metrics...\n",
            "Performing Sensitivity Analysis...\n",
            "Performing Dynamic Portfolio Rebalancing...\n",
            "\n",
            "Stress Test Results: {'Hyperinflation': {'Weights': array([4.08969762e-01, 9.93854752e-23, 1.64304138e-01, 1.55226230e-01,\n",
            "       2.71499869e-01]), 'Expected Return': np.float64(0.5788806584937115), 'Risk': np.float64(1.100329137976411)}, 'Currency Devaluation': {'Weights': array([ 4.08281049e-01, -2.21730914e-23,  1.33256075e-01,  1.23150456e-01,\n",
            "        3.35312421e-01]), 'Expected Return': np.float64(0.3185237548588027), 'Risk': np.float64(0.9447661731300475)}, 'Market Crash': {'Weights': array([ 4.05567316e-01, -3.45032403e-23, -3.61437558e-24, -2.47024725e-23,\n",
            "        5.94432684e-01]), 'Expected Return': np.float64(0.031131303387827394), 'Risk': np.float64(1.3308552522821657)}, 'Geopolitical Risk': {'Weights': array([4.08464851e-01, 1.94844944e-23, 1.38792150e-01, 1.29041328e-01,\n",
            "       3.23701670e-01]), 'Expected Return': np.float64(0.3721700833434828), 'Risk': np.float64(0.990459685087477)}}\n",
            "\n",
            "Monte Carlo Simulation Results: [(np.float64(0.6217214922958851), np.float64(1.3406080401336482)), (np.float64(0.44995837167029723), np.float64(1.0692802207593959)), (np.float64(0.5370813666894423), np.float64(1.2450195254819487)), (np.float64(0.38819910067365576), np.float64(1.3349476769610618)), (np.float64(0.24118813964359254), np.float64(1.5459351751645432))]\n",
            "\n",
            "Expanded Monte Carlo Simulation Results: [(np.float64(0.5333277307958268), np.float64(1.7139091713028805)), (np.float64(0.4214539181843213), np.float64(1.3661868872422065)), (np.float64(0.46181492596182955), np.float64(1.4038424545724706)), (np.float64(0.45778175262092397), np.float64(1.5837613897510745)), (np.float64(0.46429920184267953), np.float64(1.2140342518238652))]\n",
            "\n",
            "Performance Metrics: {'Sortino Ratio': np.float64(2.552006080398693), 'Treynor Ratio': np.float64(0.3093181439966217), 'Omega Ratio': np.float64(2312.897623924176), 'VaR at 5%': np.float64(1.0624435943246908), 'CVaR at 5%': np.float64(0.9752024676026595)}\n",
            "\n",
            "Sensitivity Analysis Results: [(0.1, 0.01, array([1.05196622e-22, 1.35906998e-22, 8.84233130e-23, 1.00000000e+00,\n",
            "       2.01172570e-22])), (0.1, 0.1, array([6.26876055e-02, 6.27211509e-23, 5.20535434e-25, 9.37312394e-01,\n",
            "       1.69368211e-22])), (0.1, 1.0, array([2.44992959e-01, 2.97863990e-23, 2.24164528e-01, 5.30842514e-01,\n",
            "       2.26094196e-23])), (1.0, 0.01, array([ 4.11233001e-01, -6.77963471e-23,  2.15135707e-01,  2.04972234e-01,\n",
            "        1.68659058e-01])), (1.0, 0.1, array([4.08571661e-01, 5.03206152e-23, 2.12707812e-01, 2.05238747e-01,\n",
            "       1.73481780e-01]))]\n",
            "\n",
            "Dynamic Rebalancing Results: [array([4.08571661e-01, 5.03206152e-23, 2.12707812e-01, 2.05238747e-01,\n",
            "       1.73481780e-01]), array([4.08571661e-01, 5.03206152e-23, 2.12707812e-01, 2.05238747e-01,\n",
            "       1.73481780e-01]), array([ 4.02166605e-01, -1.34927768e-23,  1.83968511e-23, -2.46429944e-23,\n",
            "        5.97833395e-01]), array([6.26876055e-02, 6.27211509e-23, 5.20535434e-25, 9.37312394e-01,\n",
            "       1.69368211e-22]), array([4.08571661e-01, 5.03206152e-23, 2.12707812e-01, 2.05238747e-01,\n",
            "       1.73481780e-01]), array([6.26876055e-02, 6.27211509e-23, 5.20535434e-25, 9.37312394e-01,\n",
            "       1.69368211e-22]), array([6.26876055e-02, 6.27211509e-23, 5.20535434e-25, 9.37312394e-01,\n",
            "       1.69368211e-22]), array([6.26876055e-02, 6.27211509e-23, 5.20535434e-25, 9.37312394e-01,\n",
            "       1.69368211e-22]), array([ 4.02166605e-01, -1.34927768e-23,  1.83968511e-23, -2.46429944e-23,\n",
            "        5.97833395e-01]), array([4.08571661e-01, 5.03206152e-23, 2.12707812e-01, 2.05238747e-01,\n",
            "       1.73481780e-01])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "from pyswarm import pso  # Particle Swarm Optimization\n",
        "from scipy.stats import t\n",
        "\n",
        "# Portfolio Optimization using cvxpy\n",
        "def optimize_portfolio(Sigma, mu, risk_aversion=1.0, l2_regularization=0.1):\n",
        "    w = cp.Variable(len(mu))\n",
        "    ret = mu.T @ w\n",
        "    risk = cp.quad_form(w, Sigma)\n",
        "    l2_penalty = cp.sum_squares(w)\n",
        "    objective = cp.Maximize(ret - risk_aversion * risk - l2_regularization * l2_penalty)\n",
        "    constraints = [cp.sum(w) == 1, w >= 0]\n",
        "    prob = cp.Problem(objective, constraints)\n",
        "    prob.solve()\n",
        "    return w.value\n",
        "\n",
        "# Stress Test Scenarios\n",
        "def stress_test(Sigma, mu):\n",
        "    scenarios = {\n",
        "        'Hyperinflation': (Sigma * 1.2, mu * 1.1),\n",
        "        'Currency Devaluation': (Sigma * 0.8, mu * 0.9),\n",
        "        'Market Crash': (Sigma * 2.0, mu * -0.5),\n",
        "    }\n",
        "    results = {}\n",
        "    for name, (sigma_stress, mu_stress) in scenarios.items():\n",
        "        optimal_weights = optimize_portfolio(sigma_stress, mu_stress)\n",
        "        results[name] = {\n",
        "            'Weights': optimal_weights,\n",
        "            'Expected Return': mu_stress.T @ optimal_weights,\n",
        "            'Risk': np.sqrt(optimal_weights.T @ sigma_stress @ optimal_weights)\n",
        "        }\n",
        "    return results\n",
        "\n",
        "# Monte Carlo Simulation\n",
        "def monte_carlo_simulation(Sigma, mu, n_samples=1000):\n",
        "    simulations = []\n",
        "    for _ in range(n_samples):\n",
        "        random_weights = np.random.random(len(mu))\n",
        "        random_weights /= np.sum(random_weights)\n",
        "        expected_return = mu.T @ random_weights\n",
        "        risk = np.sqrt(np.dot(random_weights.T, np.dot(Sigma, random_weights)))\n",
        "        simulations.append((expected_return, risk))\n",
        "    return simulations\n",
        "\n",
        "# Performance Metrics\n",
        "def performance_metrics(weights, Sigma, mu):\n",
        "    expected_return = mu.T @ weights\n",
        "    risk = np.sqrt(weights.T @ Sigma @ weights)\n",
        "    sortino_ratio = expected_return / np.std(weights)  # Proxy for downside risk\n",
        "    return {'Sortino Ratio': sortino_ratio, 'Expected Return': expected_return, 'Risk': risk}\n",
        "\n",
        "# Example Data\n",
        "np.random.seed(42)\n",
        "n_assets = 5\n",
        "mu = np.random.randn(n_assets)\n",
        "Sigma = np.random.randn(n_assets, n_assets)\n",
        "Sigma = np.dot(Sigma, Sigma.T)  # Make it positive semi-definite\n",
        "\n",
        "# Running Stress Tests\n",
        "stress_test_results = stress_test(Sigma, mu)\n",
        "\n",
        "# Running Monte Carlo Simulation\n",
        "monte_carlo_results = monte_carlo_simulation(Sigma, mu, n_samples=1000)\n",
        "\n",
        "# Performance Metrics Calculation\n",
        "weights = np.random.random(n_assets)\n",
        "performance = performance_metrics(weights, Sigma, mu)\n",
        "\n",
        "# Print results\n",
        "print(\"Stress Test Results:\", stress_test_results)\n",
        "print(\"Monte Carlo Simulation Results:\", monte_carlo_results[:5])  # First 5 results\n",
        "print(\"Performance Metrics:\", performance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBzDowi8_xdi",
        "outputId": "3685c854-561b-40de-df5d-a323cc17e7c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stress Test Results: {'Hyperinflation': {'Weights': array([ 4.08860430e-01, -2.49631601e-23,  1.93143016e-01,  1.84669346e-01,\n",
            "        2.13327207e-01]), 'Expected Return': np.float64(0.6154378148207278), 'Risk': np.float64(1.024034916575328)}, 'Currency Devaluation': {'Weights': array([ 4.07994897e-01, -2.74248657e-23,  2.40199209e-01,  2.35109055e-01,\n",
            "        1.16696838e-01]), 'Expected Return': np.float64(0.6200858399731686), 'Risk': np.float64(0.8965883277779333)}, 'Market Crash': {'Weights': array([3.88105846e-01, 5.67377791e-24, 5.29554259e-24, 8.98235788e-25,\n",
            "       6.11894154e-01]), 'Expected Return': np.float64(-0.024750292667115356), 'Risk': np.float64(1.0868471274238065)}}\n",
            "Monte Carlo Simulation Results: [(np.float64(0.6217214922958851), np.float64(1.3406080401336482)), (np.float64(0.44995837167029723), np.float64(1.0692802207593959)), (np.float64(0.5370813666894423), np.float64(1.2450195254819487)), (np.float64(0.38819910067365576), np.float64(1.3349476769610618)), (np.float64(0.24118813964359254), np.float64(1.5459351751645432))]\n",
            "Performance Metrics: {'Sortino Ratio': np.float64(6.407368140928439), 'Expected Return': np.float64(1.5837772245641886), 'Risk': np.float64(4.755573494324765)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.stats.correlation_tools import cov_nearest\n",
        "from sklearn.linear_model import ElasticNet, Ridge\n",
        "from scipy.stats import t\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Simulating returns and covariance matrix (for demonstration)\n",
        "np.random.seed(42)\n",
        "n_assets = 5\n",
        "mu = np.array([0.05, 0.08, 0.06, 0.09, 0.07])  # Expected returns adjusted to realistic values\n",
        "Sigma = np.random.randn(n_assets, n_assets)\n",
        "Sigma = np.dot(Sigma, Sigma.T)  # Make it positive semi-definite\n",
        "\n",
        "# Using statsmodels' covariance shrinkage to ensure positive semi-definiteness\n",
        "Sigma_shrinked = cov_nearest(Sigma)  # Nearest positive-definite matrix\n",
        "\n",
        "# Handling NaN and Inf values in metrics\n",
        "def calculate_sortino_ratio(returns):\n",
        "    downside_returns = returns[returns < 0]\n",
        "    if len(downside_returns) == 0 or np.std(downside_returns) == 0:\n",
        "        return np.nan  # Avoid division by zero if there are no downside returns or if std is zero\n",
        "    return np.mean(returns) / np.std(downside_returns)\n",
        "\n",
        "def calculate_omega_ratio(returns):\n",
        "    if np.sum(returns < 0) == 0 or np.sum(returns[returns < 0]) == 0:\n",
        "        return np.nan  # Avoid division by zero if there are no negative returns\n",
        "    return np.sum(returns[returns > 0]) / np.abs(np.sum(returns[returns < 0]))\n",
        "\n",
        "# Portfolio optimization using cvxpy with L2 regularization\n",
        "def optimize_portfolio(Sigma, mu, risk_aversion=1.0, l2_regularization=0.1):\n",
        "    w = cp.Variable(len(mu))  # portfolio weights\n",
        "    ret = mu.T @ w  # expected return\n",
        "    risk = cp.quad_form(w, Sigma)  # portfolio risk (variance)\n",
        "\n",
        "    # L2 regularization (penalizing large weights) using sum of squares for L2 norm\n",
        "    l2_penalty = cp.sum_squares(w)  # L2 norm of the weight vector\n",
        "\n",
        "    # Objective function: maximize return - risk_aversion * risk - l2_regularization * l2_penalty\n",
        "    objective = cp.Maximize(ret - risk_aversion * risk - l2_regularization * l2_penalty)\n",
        "\n",
        "    # Constraints: Sum of weights equals 1, weights >= 0 (no short selling)\n",
        "    constraints = [cp.sum(w) == 1, w >= 0]\n",
        "\n",
        "    # Define and solve problem\n",
        "    prob = cp.Problem(objective, constraints)\n",
        "    prob.solve()\n",
        "\n",
        "    return w.value\n",
        "\n",
        "# Running Monte Carlo Simulation\n",
        "def monte_carlo_simulation(Sigma, mu, n_samples=1000):\n",
        "    simulations = []\n",
        "    for _ in range(n_samples):\n",
        "        random_weights = np.random.rand(len(mu))\n",
        "        random_weights /= np.sum(random_weights)  # Normalize to sum to 1\n",
        "        expected_return = mu.T @ random_weights\n",
        "\n",
        "        # Correct calculation of portfolio risk (quadratic form)\n",
        "        risk = np.sqrt(np.dot(random_weights.T, np.dot(Sigma, random_weights)))  # Portfolio risk (standard deviation)\n",
        "\n",
        "        simulations.append((expected_return, risk))\n",
        "    return simulations\n",
        "\n",
        "# Expanding Monte Carlo Simulations with Fat-Tailed Distributions\n",
        "def monte_carlo_simulation_fat_tails(Sigma, mu, n_samples=1000):\n",
        "    simulations = []\n",
        "    for _ in range(n_samples):\n",
        "        random_weights = np.random.rand(len(mu))\n",
        "        random_weights /= np.sum(random_weights)  # Normalize to sum to 1\n",
        "        expected_return = mu.T @ random_weights\n",
        "\n",
        "        # Correct calculation of portfolio risk (quadratic form)\n",
        "        risk = np.sqrt(np.dot(random_weights.T, np.dot(Sigma, random_weights)))  # Portfolio risk (standard deviation)\n",
        "\n",
        "        fat_tail_risk = np.random.standard_t(df=3, size=len(mu))  # Fat-tailed distribution (Student's t)\n",
        "        simulations.append((expected_return, risk * fat_tail_risk))\n",
        "    return simulations\n",
        "\n",
        "# Performance Metrics: Sortino Ratio, Treynor Ratio, Omega Ratio\n",
        "def calculate_performance_metrics(returns, risk, risk_free_rate=0.03):\n",
        "    sortino_ratio = calculate_sortino_ratio(returns)\n",
        "    treynor_ratio = np.mean(returns - risk_free_rate) / risk if risk > 0 else np.nan  # Treynor ratio (assuming beta = 1)\n",
        "    omega_ratio = calculate_omega_ratio(returns)\n",
        "    VaR_5 = np.percentile(returns, 5)  # 5% VaR\n",
        "    CVaR_5 = np.mean(returns[returns <= VaR_5])  # Conditional VaR\n",
        "    return sortino_ratio, treynor_ratio, omega_ratio, VaR_5, CVaR_5\n",
        "\n",
        "# Dynamic Portfolio Rebalancing (Simple Example)\n",
        "def dynamic_rebalancing(Sigma, mu, risk_aversion=1.0):\n",
        "    # Rebalance the portfolio based on new data or shocks\n",
        "    new_mu = mu + np.random.normal(0, 0.02, len(mu))  # Introducing random market shocks\n",
        "    return optimize_portfolio(Sigma, new_mu, risk_aversion)\n",
        "\n",
        "# Incorporating Robust Estimators for covariance matrix (Ridge Regression for shrinkage)\n",
        "def estimate_covariance_ridge(X_train, y_train, alpha=1.0):\n",
        "    ridge = Ridge(alpha=alpha)\n",
        "    ridge.fit(X_train, y_train)  # Assuming X_train is your feature matrix\n",
        "    return ridge.coef_\n",
        "\n",
        "# Regularization with ElasticNet\n",
        "def optimize_portfolio_elasticnet(Sigma, mu, risk_aversion=1.0, alpha=1.0, l1_ratio=0.5):\n",
        "    elastic_net = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
        "    # Assuming we already have the covariance matrix and expected returns (Sigma, mu)\n",
        "    elastic_net.fit(Sigma, mu)\n",
        "    return elastic_net.coef_\n",
        "\n",
        "# Stress Testing with Scenario Generation\n",
        "def generate_scenarios(mu, Sigma, n_scenarios=1000):\n",
        "    scenarios = []\n",
        "    for _ in range(n_scenarios):\n",
        "        random_weights = np.random.rand(len(mu))\n",
        "        random_weights /= np.sum(random_weights)  # Normalize to sum to 1\n",
        "        returns = mu.T @ random_weights\n",
        "        risk = np.sqrt(np.dot(random_weights.T, np.dot(Sigma, random_weights)))  # Portfolio risk\n",
        "        scenarios.append((returns, risk))\n",
        "    return scenarios\n",
        "\n",
        "# Advanced Stress Testing with generated scenarios\n",
        "def advanced_stress_test(mu, Sigma):\n",
        "    # Generate scenarios under different market conditions\n",
        "    scenarios = generate_scenarios(mu, Sigma)\n",
        "    results = []\n",
        "    for scenario in scenarios:\n",
        "        # Apply advanced model or perform analysis per scenario\n",
        "        results.append(scenario)\n",
        "    return results\n",
        "\n",
        "# Example of running advanced models with ElasticNet and Stress Testing\n",
        "elastic_net_weights = optimize_portfolio_elasticnet(Sigma_shrinked, mu)\n",
        "stress_test_results = advanced_stress_test(mu, Sigma_shrinked)\n",
        "\n",
        "print(\"ElasticNet Portfolio Weights:\", elastic_net_weights)\n",
        "print(\"Advanced Stress Test Results:\", stress_test_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzw-8_C-Bj5Q",
        "outputId": "9a0cf86f-e13f-4472-b48e-b68bf2f31ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet Portfolio Weights: [-0.  0.  0.  0.  0.]\n",
            "Advanced Stress Test Results: [(np.float64(0.07362710072328668), np.float64(0.8418146199545673)), (np.float64(0.06676396768835047), np.float64(1.2700220893781369)), (np.float64(0.07101357564220812), np.float64(0.7307797658413125)), (np.float64(0.06920447730488608), np.float64(0.7760517185597083)), (np.float64(0.07066849463470351), np.float64(0.9748329208099877)), (np.float64(0.07409139769690538), np.float64(1.2436311358544025)), (np.float64(0.07112828028221696), np.float64(1.047975490705823)), (np.float64(0.0666876896829506), np.float64(0.9586851081467271)), (np.float64(0.06930133009050796), np.float64(0.9659488687899319)), (np.float64(0.06846073788042842), np.float64(1.114698623595678)), (np.float64(0.06751741434487918), np.float64(0.7601362349874995)), (np.float64(0.07043300998854672), np.float64(0.7934801032017781)), (np.float64(0.06556991101573136), np.float64(0.7891656743559955)), (np.float64(0.0792983227332068), np.float64(1.0692075357517066)), (np.float64(0.06823634866918764), np.float64(0.9120729690474304)), (np.float64(0.07350861425903739), np.float64(0.9332294865899541)), (np.float64(0.06791524798822034), np.float64(0.8470137188972566)), (np.float64(0.07285076311739999), np.float64(1.2468796566082387)), (np.float64(0.06891350092385015), np.float64(1.5999021561921123)), (np.float64(0.07277203435526168), np.float64(1.4570095828322684)), (np.float64(0.06968594825872985), np.float64(0.7806684834805698)), (np.float64(0.06897647825974214), np.float64(1.0585819947385344)), (np.float64(0.06923768880885674), np.float64(1.292091896481103)), (np.float64(0.07048272928154817), np.float64(1.0640379914581055)), (np.float64(0.069215343971272), np.float64(0.9213527939504335)), (np.float64(0.06526036528683331), np.float64(1.153307697761217)), (np.float64(0.07368255383172254), np.float64(0.9842648796338168)), (np.float64(0.07483515496251772), np.float64(1.4454640913038581)), (np.float64(0.07072514932658797), np.float64(1.2891475915240591)), (np.float64(0.06903300155721964), np.float64(0.9821456902123091)), (np.float64(0.06829366047519596), np.float64(1.0950545171725288)), (np.float64(0.07217439541141295), np.float64(1.2103071706162043)), (np.float64(0.06346619807129321), np.float64(0.8837583753040608)), (np.float64(0.0683727701633604), np.float64(1.0391377423250854)), (np.float64(0.06944110524298436), np.float64(0.9327040284073566)), (np.float64(0.06926877461936778), np.float64(1.172732251184935)), (np.float64(0.06450924602492644), np.float64(1.2378224266339262)), (np.float64(0.07124948322135212), np.float64(1.217637797041)), (np.float64(0.07307304280971416), np.float64(1.1519710892476807)), (np.float64(0.07331156334748554), np.float64(0.9355106322168676)), (np.float64(0.07364101021596674), np.float64(1.2179565540045663)), (np.float64(0.0661509198733217), np.float64(1.176416211521337)), (np.float64(0.06985486437979532), np.float64(1.4019057433460638)), (np.float64(0.07509513200371395), np.float64(1.2624974309785424)), (np.float64(0.0717350170678041), np.float64(1.1651918979571232)), (np.float64(0.06700050044286433), np.float64(1.1384981546111346)), (np.float64(0.06659678029157134), np.float64(0.7609640044752097)), (np.float64(0.06542372893316362), np.float64(1.1231368430043271)), (np.float64(0.0624929439167284), np.float64(1.0942498846416333)), (np.float64(0.06747223215830508), np.float64(0.8182802042970136)), (np.float64(0.06444025699042678), np.float64(1.12350005962047)), (np.float64(0.07061548489883869), np.float64(1.6014696631939564)), (np.float64(0.07212371732384193), np.float64(1.4369694771043366)), (np.float64(0.0657344319876608), np.float64(1.1938846812821025)), (np.float64(0.06746113779630293), np.float64(1.358105101552447)), (np.float64(0.0681704914186092), np.float64(0.6807564809869829)), (np.float64(0.06253853128020212), np.float64(0.7827356124561773)), (np.float64(0.06424364962839103), np.float64(1.3395513334195657)), (np.float64(0.07142434914952038), np.float64(1.790917424098204)), (np.float64(0.0637898252637728), np.float64(1.0030968498086357)), (np.float64(0.06559398518148699), np.float64(1.3190366315315547)), (np.float64(0.07095993323274997), np.float64(1.0574534139452894)), (np.float64(0.06813169387471263), np.float64(0.9410119652705093)), (np.float64(0.06592214188396184), np.float64(0.8256005622790323)), (np.float64(0.07025893128060981), np.float64(1.1873532046438442)), (np.float64(0.0698578459751765), np.float64(0.8931322792509883)), (np.float64(0.06936425283481497), np.float64(1.392248228253745)), (np.float64(0.06700928800198361), np.float64(0.6892134357520201)), (np.float64(0.06674010348099554), np.float64(1.4559198036783585)), (np.float64(0.07042314875775621), np.float64(0.9294018442119042)), (np.float64(0.06221472774868213), np.float64(1.0868926103193692)), (np.float64(0.06781538338317378), np.float64(0.8193280620010883)), (np.float64(0.06833972706520047), np.float64(1.0704647107772882)), (np.float64(0.06965358794727215), np.float64(1.2603020621793475)), (np.float64(0.07029364731168931), np.float64(1.0336345320183855)), (np.float64(0.07254643487991952), np.float64(1.9503345786997268)), (np.float64(0.06727640392416519), np.float64(0.7627318950292395)), (np.float64(0.07120819729768782), np.float64(1.3901754071456711)), (np.float64(0.0688630794060299), np.float64(1.3583532109058887)), (np.float64(0.07494818810242523), np.float64(1.3611051138792654)), (np.float64(0.07074115002403329), np.float64(1.0605045664202997)), (np.float64(0.06701901506538767), np.float64(1.4530242621480383)), (np.float64(0.07362265884039595), np.float64(1.42342862075551)), (np.float64(0.06948232662436016), np.float64(0.8086937299409291)), (np.float64(0.0717213928195924), np.float64(0.8762909612301694)), (np.float64(0.07295163323513174), np.float64(1.3519580240145317)), (np.float64(0.06616256588606821), np.float64(0.8412204194202784)), (np.float64(0.06773037699636465), np.float64(1.32651713934554)), (np.float64(0.06420923262534876), np.float64(0.83833856136882)), (np.float64(0.059902253798468384), np.float64(0.8728204970711384)), (np.float64(0.07100026403359838), np.float64(1.6249629974823823)), (np.float64(0.06542370589994158), np.float64(0.9503079964459306)), (np.float64(0.077565293125943), np.float64(1.2414656472438605)), (np.float64(0.06978776464400196), np.float64(1.2114894596840722)), (np.float64(0.07632105448141557), np.float64(1.559515095287698)), (np.float64(0.06861516529396819), np.float64(1.8349876239679712)), (np.float64(0.06410757085932141), np.float64(1.2021525755260922)), (np.float64(0.07476797650046867), np.float64(1.0720461366865017)), (np.float64(0.06693905963661448), np.float64(1.0993672771739715)), (np.float64(0.07207326587607121), np.float64(1.6025540712056914)), (np.float64(0.06982429786389206), np.float64(1.1581844073164476)), (np.float64(0.06472139845536418), np.float64(0.7132644833735178)), (np.float64(0.07135642967569711), np.float64(1.6605385500817758)), (np.float64(0.0697095112013049), np.float64(1.5573582929887453)), (np.float64(0.06801395017327538), np.float64(1.2704752645645594)), (np.float64(0.07154031415761773), np.float64(1.0331215008761145)), (np.float64(0.07162678035984155), np.float64(0.9460015990149412)), (np.float64(0.07251659660626078), np.float64(0.7972147554155109)), (np.float64(0.07263104681370058), np.float64(0.9188224940718039)), (np.float64(0.06850718321210894), np.float64(1.1069655374082825)), (np.float64(0.07332766251747427), np.float64(1.2774440253304)), (np.float64(0.07242490123253957), np.float64(1.0979237584589967)), (np.float64(0.06619452781514593), np.float64(1.2022454025460585)), (np.float64(0.06932524132372912), np.float64(1.0026279384652175)), (np.float64(0.07400012038354567), np.float64(0.9226841815393758)), (np.float64(0.07351458222680111), np.float64(0.8505781473847033)), (np.float64(0.07327534142547083), np.float64(1.488031222528746)), (np.float64(0.06248051216432621), np.float64(1.5785824548623806)), (np.float64(0.06451712682616129), np.float64(0.9769319303763977)), (np.float64(0.06931736648750238), np.float64(0.8212553829827796)), (np.float64(0.06616672872284014), np.float64(0.9857276311505498)), (np.float64(0.07596221840127505), np.float64(0.9326005366569454)), (np.float64(0.07306858632986293), np.float64(0.7188731093513541)), (np.float64(0.07317621139037381), np.float64(1.087887461917238)), (np.float64(0.07060649100360872), np.float64(0.9711196595130904)), (np.float64(0.06957599310862635), np.float64(1.4134231640889021)), (np.float64(0.062056908451466566), np.float64(1.2056343380954015)), (np.float64(0.0664663619484427), np.float64(1.2927852125120063)), (np.float64(0.07429993725787187), np.float64(1.2748224518430376)), (np.float64(0.07047424956264965), np.float64(0.8452157787838797)), (np.float64(0.06855001546811716), np.float64(1.0143960598232067)), (np.float64(0.06661999172215075), np.float64(1.2125714394600269)), (np.float64(0.0668826810863697), np.float64(1.1954266249487095)), (np.float64(0.06965436126602297), np.float64(1.2119371416780496)), (np.float64(0.06930177348725625), np.float64(1.0221407771442224)), (np.float64(0.074220575542967), np.float64(1.0632956700302665)), (np.float64(0.07582870088750518), np.float64(1.473075748151731)), (np.float64(0.07127733832764489), np.float64(0.9353799026681441)), (np.float64(0.07213109463298342), np.float64(1.608655788469307)), (np.float64(0.0702848052223285), np.float64(1.0123665177284829)), (np.float64(0.0745828642320324), np.float64(1.5801026821346484)), (np.float64(0.07144450164332297), np.float64(1.017198670247472)), (np.float64(0.07003078648727856), np.float64(0.9093863743743125)), (np.float64(0.07079647104226652), np.float64(0.8315695719334067)), (np.float64(0.07053769519717554), np.float64(1.3130070703618377)), (np.float64(0.06767522844501452), np.float64(1.0528382250275228)), (np.float64(0.06501033521695739), np.float64(0.8775888937869549)), (np.float64(0.06430090193703861), np.float64(0.969301586759309)), (np.float64(0.07040281541277721), np.float64(1.1858371280992057)), (np.float64(0.06424351409665133), np.float64(1.1483543622667804)), (np.float64(0.07174475629817159), np.float64(1.0971965814551392)), (np.float64(0.0683893540668935), np.float64(1.21694629348722)), (np.float64(0.06403267498662506), np.float64(0.9015955770482174)), (np.float64(0.0682600387541081), np.float64(1.4900142763798667)), (np.float64(0.06604753118874868), np.float64(0.7920290608095797)), (np.float64(0.0698504173702047), np.float64(1.3940101884322416)), (np.float64(0.06826551289942223), np.float64(1.6786229566730315)), (np.float64(0.07583201846350077), np.float64(1.1370804907196692)), (np.float64(0.06907638392177784), np.float64(0.8044939458374983)), (np.float64(0.06992845118702677), np.float64(0.9563076668710025)), (np.float64(0.07905587354056191), np.float64(1.1445113494251142)), (np.float64(0.07011329301504472), np.float64(1.7343590405068599)), (np.float64(0.07011857517740416), np.float64(1.4585028948956873)), (np.float64(0.06647684036876914), np.float64(0.6880602586633272)), (np.float64(0.06807964842036308), np.float64(1.2522271704493577)), (np.float64(0.06803774443821387), np.float64(1.1801265095380422)), (np.float64(0.06828356728369234), np.float64(0.9212965871315452)), (np.float64(0.062069275430953946), np.float64(1.0050407090125788)), (np.float64(0.07410745422323756), np.float64(1.172916096338045)), (np.float64(0.06959894488743089), np.float64(0.7551339936796904)), (np.float64(0.07089224882096036), np.float64(1.2914022598475192)), (np.float64(0.06917054985045329), np.float64(1.0549846456980025)), (np.float64(0.0715228362645028), np.float64(1.1626402519056391)), (np.float64(0.0731592814998753), np.float64(1.0257118627236441)), (np.float64(0.06635569255392929), np.float64(1.240038789321337)), (np.float64(0.07038524347495181), np.float64(0.8083062313323265)), (np.float64(0.07837219356488857), np.float64(1.2172725458484115)), (np.float64(0.0668883787395063), np.float64(1.2495542391320247)), (np.float64(0.0690590250139526), np.float64(0.8797366459133712)), (np.float64(0.06931233821770666), np.float64(1.0045346956508063)), (np.float64(0.07286819486868752), np.float64(0.9356431950827129)), (np.float64(0.07166238895303863), np.float64(0.9314290043053797)), (np.float64(0.07675604181307177), np.float64(1.5054385455700354)), (np.float64(0.07714721476089824), np.float64(1.3604664367680397)), (np.float64(0.07112463427511084), np.float64(1.0644357441409757)), (np.float64(0.07164916906294061), np.float64(0.9593436428762213)), (np.float64(0.07996540274760924), np.float64(1.0439684627353378)), (np.float64(0.07049733975296696), np.float64(0.9236336439197862)), (np.float64(0.06279313715446305), np.float64(0.938940801688492)), (np.float64(0.07534338369231451), np.float64(1.3748620736031623)), (np.float64(0.06812476279834635), np.float64(0.8277218855585448)), (np.float64(0.06415953000930774), np.float64(0.7550661400491404)), (np.float64(0.07835584040726905), np.float64(1.3500044169404317)), (np.float64(0.06983652024549593), np.float64(0.9431498840429486)), (np.float64(0.07407423737300785), np.float64(1.3231961789580593)), (np.float64(0.07003899275069943), np.float64(1.1641790399200778)), (np.float64(0.060638832214718066), np.float64(1.1517065361334888)), (np.float64(0.06475819208782738), np.float64(0.7398046945817984)), (np.float64(0.06760105139395836), np.float64(0.9418571967424714)), (np.float64(0.06650910052021311), np.float64(1.1358040156826588)), (np.float64(0.06818401377371633), np.float64(1.0997464284552958)), (np.float64(0.07043918507156391), np.float64(1.0622377809252683)), (np.float64(0.0696094580519149), np.float64(1.0670304440607419)), (np.float64(0.07066594662913843), np.float64(0.9978631832804186)), (np.float64(0.06507118684211186), np.float64(1.5671407800431747)), (np.float64(0.07730328358180219), np.float64(1.4864334750089099)), (np.float64(0.0726901449768168), np.float64(0.9435604385069872)), (np.float64(0.06432791327269041), np.float64(1.1847219584469784)), (np.float64(0.0727643395160806), np.float64(0.8675043223463653)), (np.float64(0.07824007546592421), np.float64(1.303299848047131)), (np.float64(0.07258428748504261), np.float64(1.431268880465661)), (np.float64(0.0653860063499574), np.float64(0.8174890770549855)), (np.float64(0.06015135252936229), np.float64(0.9969414588021221)), (np.float64(0.06987917692845105), np.float64(1.630450018037243)), (np.float64(0.06852435443093383), np.float64(1.123726829386725)), (np.float64(0.0747343616993029), np.float64(1.305671012640572)), (np.float64(0.06644727277842236), np.float64(1.1547309274310267)), (np.float64(0.065492644078649), np.float64(0.780578849793452)), (np.float64(0.0763217159630893), np.float64(1.1857817521335516)), (np.float64(0.06273358652598188), np.float64(0.8841384466479014)), (np.float64(0.06879036657813475), np.float64(1.603159898827538)), (np.float64(0.06472327995862984), np.float64(1.2067425183622673)), (np.float64(0.07208756468828406), np.float64(1.0633115208191106)), (np.float64(0.06946239764291355), np.float64(0.8261988372462127)), (np.float64(0.06502624857072047), np.float64(1.470236310940934)), (np.float64(0.07140023282859688), np.float64(1.3451980070213705)), (np.float64(0.08221485509311834), np.float64(1.2354452485406506)), (np.float64(0.06876591276677131), np.float64(0.7803505380198656)), (np.float64(0.07392339607509477), np.float64(0.7918649499084717)), (np.float64(0.0595582874576831), np.float64(0.8571221286791663)), (np.float64(0.07013474892871759), np.float64(0.8905643324261516)), (np.float64(0.06932422237735897), np.float64(1.3748682997104464)), (np.float64(0.06606814596640281), np.float64(0.8767881419046559)), (np.float64(0.06864329730742728), np.float64(0.811138754304365)), (np.float64(0.06683329162653137), np.float64(1.5304865238090237)), (np.float64(0.07150194265188187), np.float64(1.481390535530312)), (np.float64(0.07680532473488116), np.float64(1.24963840112208)), (np.float64(0.06692333067242204), np.float64(0.81363382971037)), (np.float64(0.06768833994023685), np.float64(1.2433003200345007)), (np.float64(0.06647328614589078), np.float64(0.7214422647167558)), (np.float64(0.07255981478020862), np.float64(0.8396319455255269)), (np.float64(0.06547164954851359), np.float64(0.8822215353875237)), (np.float64(0.07463887990152143), np.float64(1.7209550720162232)), (np.float64(0.06912464781247578), np.float64(1.253760705527008)), (np.float64(0.06839980867351309), np.float64(1.4578630271632285)), (np.float64(0.07130540646821823), np.float64(0.9430709054310695)), (np.float64(0.0645368090426927), np.float64(1.1689398617543623)), (np.float64(0.0685529021893772), np.float64(1.2419585559722661)), (np.float64(0.06994935695895889), np.float64(1.0312035898343725)), (np.float64(0.06452247656735487), np.float64(1.6269997811532597)), (np.float64(0.07439925326983017), np.float64(1.5862022186306857)), (np.float64(0.0751483026057743), np.float64(0.768929981916447)), (np.float64(0.07503740678235113), np.float64(0.9784582234845617)), (np.float64(0.06957112356482721), np.float64(1.5397651234364966)), (np.float64(0.06807667998642358), np.float64(1.0538803639802774)), (np.float64(0.06760102298202447), np.float64(0.9413760648368452)), (np.float64(0.07065522836255643), np.float64(1.0340727160646257)), (np.float64(0.07557510130437735), np.float64(1.6644868809752655)), (np.float64(0.06693844784414626), np.float64(0.9669151243829058)), (np.float64(0.06572942760295433), np.float64(0.7874912467038688)), (np.float64(0.0766799883301566), np.float64(1.2691554439666246)), (np.float64(0.07645241757041564), np.float64(1.1777393053700576)), (np.float64(0.06494783812483848), np.float64(1.2247691572074375)), (np.float64(0.06911882127117185), np.float64(1.0247989749137605)), (np.float64(0.07150810842212857), np.float64(0.7769466642456935)), (np.float64(0.07279005544960462), np.float64(1.050608416797782)), (np.float64(0.07382236242621294), np.float64(1.2244157384047254)), (np.float64(0.06826625417870874), np.float64(0.9925083606556249)), (np.float64(0.07126197585046463), np.float64(0.6981111249053332)), (np.float64(0.07226402924555896), np.float64(1.110099659693029)), (np.float64(0.07145641029986317), np.float64(1.0720299024951019)), (np.float64(0.06756400777214627), np.float64(0.7901056147976139)), (np.float64(0.06726563522626434), np.float64(0.8330325900182055)), (np.float64(0.07615389525725474), np.float64(1.4555520402682691)), (np.float64(0.06896519047721161), np.float64(1.211418422676484)), (np.float64(0.07126305467158782), np.float64(0.902615224242257)), (np.float64(0.060671125464285464), np.float64(1.4445580240971994)), (np.float64(0.0756704447361733), np.float64(1.1780458642036815)), (np.float64(0.07215048025917437), np.float64(1.389583891383796)), (np.float64(0.07104085897415355), np.float64(1.239183351776934)), (np.float64(0.07413778906030243), np.float64(1.0602369680674326)), (np.float64(0.07191389225197027), np.float64(1.8721738102595835)), (np.float64(0.07293452059302619), np.float64(1.1419399855688217)), (np.float64(0.06965454646225425), np.float64(1.3759876540739617)), (np.float64(0.07359772443406026), np.float64(1.3058525631481919)), (np.float64(0.07149123439439976), np.float64(0.9589265481143328)), (np.float64(0.06666200258425257), np.float64(0.8252081327736556)), (np.float64(0.07245542564545822), np.float64(1.2956614485387925)), (np.float64(0.07150267466334002), np.float64(1.5500333480753075)), (np.float64(0.06935811226615293), np.float64(1.1550230981396947)), (np.float64(0.06804058317278135), np.float64(1.2780502946460153)), (np.float64(0.07022364694247489), np.float64(0.917324133556118)), (np.float64(0.07291148102728456), np.float64(0.7028208885860937)), (np.float64(0.07017217559635062), np.float64(0.9840452671542599)), (np.float64(0.06514299760371425), np.float64(0.8414583377660383)), (np.float64(0.06774234052402194), np.float64(1.098608879719745)), (np.float64(0.07106675166146642), np.float64(0.9209765661331824)), (np.float64(0.06537630377595159), np.float64(1.267227486168705)), (np.float64(0.07134765316507052), np.float64(1.0906749515894167)), (np.float64(0.06925475760964689), np.float64(1.173471398475355)), (np.float64(0.07695670786863021), np.float64(1.2918293766954583)), (np.float64(0.069270560066485), np.float64(1.4742341514697959)), (np.float64(0.07708329457398115), np.float64(0.975233965266573)), (np.float64(0.07689864159887629), np.float64(1.1698081473561601)), (np.float64(0.07513057565355298), np.float64(1.4138100398040196)), (np.float64(0.07116365549343444), np.float64(1.1713640751425354)), (np.float64(0.07112781963920811), np.float64(1.1278866219636134)), (np.float64(0.06645347214315331), np.float64(0.9259460331851238)), (np.float64(0.07002556479420702), np.float64(1.6497673472120673)), (np.float64(0.07087857044619397), np.float64(1.2149655330108897)), (np.float64(0.07251778155526581), np.float64(0.8848365640604898)), (np.float64(0.06975956403100182), np.float64(0.9377258116866066)), (np.float64(0.07329469887603644), np.float64(1.7318014842314924)), (np.float64(0.06995998022895983), np.float64(0.7522270148024497)), (np.float64(0.0720970736193071), np.float64(1.0762915669532458)), (np.float64(0.0708711487472645), np.float64(0.673985802679366)), (np.float64(0.0708247309442971), np.float64(0.7399177298374566)), (np.float64(0.07850698620374202), np.float64(1.4723979493500465)), (np.float64(0.07672414639582543), np.float64(1.0060887216987724)), (np.float64(0.06649788267643174), np.float64(1.0578420449833752)), (np.float64(0.07124684262407661), np.float64(1.8408013084736723)), (np.float64(0.0783549916215229), np.float64(1.0228065528568346)), (np.float64(0.0729487716104357), np.float64(1.0277886937417051)), (np.float64(0.06405602051027406), np.float64(0.7431769612777931)), (np.float64(0.07291534236896366), np.float64(1.3844540462535297)), (np.float64(0.06916218755882417), np.float64(1.3983443031918388)), (np.float64(0.0658751258048706), np.float64(1.1496972595016042)), (np.float64(0.07163347410604247), np.float64(1.072612557729345)), (np.float64(0.06449918821781804), np.float64(0.9122914670174287)), (np.float64(0.06331682219340472), np.float64(0.8136265909176147)), (np.float64(0.06680462735687129), np.float64(1.0023644517832067)), (np.float64(0.07587120548470516), np.float64(1.0627341153193992)), (np.float64(0.07286152050882969), np.float64(1.1876309070006943)), (np.float64(0.06747788575175995), np.float64(0.956349464639812)), (np.float64(0.0741543797983402), np.float64(1.5322633847859555)), (np.float64(0.06719544562559128), np.float64(0.9904543288658438)), (np.float64(0.07012641180914324), np.float64(0.8599372111455998)), (np.float64(0.06856657346236875), np.float64(0.724116936709299)), (np.float64(0.0736322604849706), np.float64(1.205729643597334)), (np.float64(0.06810446695695233), np.float64(0.7149865671370611)), (np.float64(0.07284553870996112), np.float64(1.4027839846912031)), (np.float64(0.07223634730125794), np.float64(0.959787435041629)), (np.float64(0.06773131817285283), np.float64(1.5801002938028421)), (np.float64(0.07275375861833741), np.float64(1.0455405794273291)), (np.float64(0.06811854420157148), np.float64(1.2221387286724394)), (np.float64(0.07052994081878378), np.float64(1.7397303011230294)), (np.float64(0.06670861391420459), np.float64(1.0401266038473018)), (np.float64(0.07183119214978786), np.float64(1.3395198272825997)), (np.float64(0.07108172889646391), np.float64(0.9523071243060942)), (np.float64(0.06804313411543741), np.float64(0.7760521729456281)), (np.float64(0.07160817039851244), np.float64(1.2540073774329723)), (np.float64(0.06771884020575379), np.float64(1.4105032974416825)), (np.float64(0.0726448051095342), np.float64(1.2854505648142955)), (np.float64(0.06393277118978898), np.float64(0.8847454180140985)), (np.float64(0.07268537064407199), np.float64(1.2151417969958185)), (np.float64(0.06792500747952879), np.float64(0.7377163574411206)), (np.float64(0.06036131343053637), np.float64(0.9541802340574477)), (np.float64(0.07199523915130496), np.float64(0.8057746990606154)), (np.float64(0.07072223481903578), np.float64(1.6454619590811579)), (np.float64(0.06942311535358239), np.float64(0.8803221406585274)), (np.float64(0.07142741684956332), np.float64(1.5418918041972165)), (np.float64(0.07812872873169215), np.float64(1.4843446426317999)), (np.float64(0.06849801893677136), np.float64(0.9082464867782861)), (np.float64(0.06468271589107559), np.float64(0.8809644656666172)), (np.float64(0.06480021051478328), np.float64(1.1485930859598186)), (np.float64(0.06830308635557172), np.float64(1.351250047743405)), (np.float64(0.07557366150950717), np.float64(1.5892553376853258)), (np.float64(0.07423432254836769), np.float64(0.7459311311067096)), (np.float64(0.06615349930194546), np.float64(0.7037560245550297)), (np.float64(0.06505271664559986), np.float64(0.7975865975379248)), (np.float64(0.07177401661002783), np.float64(1.5190793506735454)), (np.float64(0.07219164739020198), np.float64(1.0793457402414957)), (np.float64(0.07163831446489698), np.float64(1.221483697445354)), (np.float64(0.0668580381204712), np.float64(0.9133049811060242)), (np.float64(0.07204564900823457), np.float64(1.1290301257133377)), (np.float64(0.07024766177680232), np.float64(1.0110870894558388)), (np.float64(0.07097585584064167), np.float64(0.8626898202365721)), (np.float64(0.06913722871164481), np.float64(0.9095069820863136)), (np.float64(0.06860153423473099), np.float64(1.3462809070698103)), (np.float64(0.07384933092982009), np.float64(1.0121159212133686)), (np.float64(0.0756718463322184), np.float64(0.9394592956241239)), (np.float64(0.06866977400084515), np.float64(1.1925886371395296)), (np.float64(0.06650927529711062), np.float64(1.0770110260547767)), (np.float64(0.07342895192056156), np.float64(0.8708002434759573)), (np.float64(0.0701882934997602), np.float64(1.5684814810366405)), (np.float64(0.0696043049296837), np.float64(1.4549226023800748)), (np.float64(0.07085563798023091), np.float64(1.0252602619002742)), (np.float64(0.06812761536042461), np.float64(1.4353899980460896)), (np.float64(0.07160914705957885), np.float64(1.15642453422671)), (np.float64(0.07643035528246479), np.float64(1.129775841489804)), (np.float64(0.06983718923298415), np.float64(1.3186783435888318)), (np.float64(0.07549076327641482), np.float64(1.472736673429594)), (np.float64(0.07447749828393752), np.float64(1.47673231831374)), (np.float64(0.05675909010379673), np.float64(0.97825252678996)), (np.float64(0.07330309240728465), np.float64(1.5380735412729885)), (np.float64(0.07012401689897653), np.float64(0.9744998750212331)), (np.float64(0.07347507186256991), np.float64(1.3629369059255665)), (np.float64(0.06903538084733621), np.float64(1.048516638224626)), (np.float64(0.07625489355333424), np.float64(1.1928419787273539)), (np.float64(0.06513575068200608), np.float64(1.1595735862829912)), (np.float64(0.07559938200356013), np.float64(1.372828784159418)), (np.float64(0.06840753069367546), np.float64(1.8712363706476929)), (np.float64(0.0652544435413058), np.float64(1.0784393260650804)), (np.float64(0.07140907970087867), np.float64(0.9454583181843536)), (np.float64(0.06966130643127266), np.float64(0.7524313281185195)), (np.float64(0.07693315525269254), np.float64(1.209113555713689)), (np.float64(0.0704563893947504), np.float64(1.1577212969405153)), (np.float64(0.068548770105774), np.float64(1.3133751297232705)), (np.float64(0.06827127074164124), np.float64(1.052672223313335)), (np.float64(0.07448361233527943), np.float64(1.1323533085125015)), (np.float64(0.06525288596426784), np.float64(1.0349637424986202)), (np.float64(0.06582684034600982), np.float64(0.8343472638904104)), (np.float64(0.06462224486637551), np.float64(0.8420345969529782)), (np.float64(0.0687846791194021), np.float64(0.9088279682508617)), (np.float64(0.06766017726293681), np.float64(0.8065354002732102)), (np.float64(0.06441760712021471), np.float64(1.1495020988515665)), (np.float64(0.06975957828324238), np.float64(1.2634830867978357)), (np.float64(0.07300672081760706), np.float64(1.3235597222666573)), (np.float64(0.06556601454017931), np.float64(1.1931906062781634)), (np.float64(0.06955212016423502), np.float64(1.29898332948969)), (np.float64(0.07222983480204576), np.float64(1.2163967469707113)), (np.float64(0.06845026346436575), np.float64(1.5382248748683092)), (np.float64(0.06861573514541325), np.float64(1.2454726116354908)), (np.float64(0.0713528465516976), np.float64(1.0544473832734698)), (np.float64(0.06558836473464376), np.float64(0.9597374725355939)), (np.float64(0.06898254723101273), np.float64(1.0741950314996453)), (np.float64(0.07170715335605168), np.float64(1.2404951572811203)), (np.float64(0.06305888385137683), np.float64(1.6481320693582293)), (np.float64(0.0772709084694283), np.float64(1.2861576807427288)), (np.float64(0.07842297072892108), np.float64(1.7631359863389766)), (np.float64(0.06746230520124921), np.float64(1.5256890664080394)), (np.float64(0.0621988685363741), np.float64(1.0756111035251101)), (np.float64(0.06957132631729626), np.float64(0.80632405644205)), (np.float64(0.06570964879148425), np.float64(1.049888089305937)), (np.float64(0.0682795016582084), np.float64(0.7497308954319638)), (np.float64(0.06842461981609085), np.float64(1.1950125324121448)), (np.float64(0.06919407961305428), np.float64(1.0725529652193733)), (np.float64(0.06813656142432406), np.float64(0.9684117750881251)), (np.float64(0.06677785651950317), np.float64(1.5566760471249488)), (np.float64(0.0729699864631002), np.float64(0.9761296869851535)), (np.float64(0.0690690618055686), np.float64(1.465918887191611)), (np.float64(0.07223956640987148), np.float64(1.1138689212953805)), (np.float64(0.0749862119550034), np.float64(1.1501779956476954)), (np.float64(0.06868994529360942), np.float64(0.967450692790928)), (np.float64(0.06281958044061128), np.float64(1.4505773618820623)), (np.float64(0.08094814855880271), np.float64(1.1476887285156345)), (np.float64(0.07137416179475672), np.float64(1.3483643150749345)), (np.float64(0.07228042058182849), np.float64(1.0584132373672157)), (np.float64(0.06949708540823757), np.float64(1.312554797578749)), (np.float64(0.07192791774611818), np.float64(0.9360671177901153)), (np.float64(0.07694152635766224), np.float64(2.0276707388780415)), (np.float64(0.07113968142495072), np.float64(1.0732628494634588)), (np.float64(0.07021523649346653), np.float64(1.1823264994099434)), (np.float64(0.07185983055681314), np.float64(1.4098404160643685)), (np.float64(0.06297565551538514), np.float64(0.9323285248781631)), (np.float64(0.0666861647444304), np.float64(1.1573639367424444)), (np.float64(0.060392687915414335), np.float64(1.2268380557679213)), (np.float64(0.06798554290860105), np.float64(1.0505809284083187)), (np.float64(0.06555204939184142), np.float64(1.1201137930008203)), (np.float64(0.06372204953328013), np.float64(0.8962040355345405)), (np.float64(0.06257873213262785), np.float64(1.2218628868772299)), (np.float64(0.07060089011640229), np.float64(0.7143628362945502)), (np.float64(0.06921890517270558), np.float64(0.860027186561662)), (np.float64(0.06897484821539522), np.float64(1.1024906524279126)), (np.float64(0.07022459455032316), np.float64(1.4051670415166928)), (np.float64(0.07051527491269254), np.float64(1.165351554347992)), (np.float64(0.06473450922958557), np.float64(0.7922916408520955)), (np.float64(0.06908289415447601), np.float64(0.764677158928441)), (np.float64(0.07116645805500126), np.float64(1.693791866359366)), (np.float64(0.06861404098377605), np.float64(0.9159615241415742)), (np.float64(0.0668221119812509), np.float64(1.1804980845175375)), (np.float64(0.07830939850724167), np.float64(1.2189515547237806)), (np.float64(0.06336106279066506), np.float64(1.239760664723636)), (np.float64(0.06400285537807106), np.float64(0.849909835393347)), (np.float64(0.06862784683704753), np.float64(1.1041158846685177)), (np.float64(0.07121738250077853), np.float64(0.9469811774784801)), (np.float64(0.07821525526111808), np.float64(1.0631607719497533)), (np.float64(0.07824185323275464), np.float64(1.5410021170368635)), (np.float64(0.0723817721575758), np.float64(0.8553185537094806)), (np.float64(0.06708920341412815), np.float64(0.8437967606491613)), (np.float64(0.06866445380760472), np.float64(0.7083291322985337)), (np.float64(0.07648132495425458), np.float64(1.3786091235843854)), (np.float64(0.07249278185680838), np.float64(1.1263867223134771)), (np.float64(0.06963112296372378), np.float64(0.750371583333916)), (np.float64(0.07062011095976088), np.float64(1.1345584523713113)), (np.float64(0.06238660334205334), np.float64(1.0719097208814505)), (np.float64(0.07034630266315242), np.float64(0.979545673239974)), (np.float64(0.06777725116236985), np.float64(1.3033817886679266)), (np.float64(0.07204111388343928), np.float64(1.3059185442588481)), (np.float64(0.07113945540994804), np.float64(1.3686238690623647)), (np.float64(0.07010063080989654), np.float64(1.6062591578371697)), (np.float64(0.0731874949497199), np.float64(1.2370493809792018)), (np.float64(0.07094435469642682), np.float64(1.0691023170593539)), (np.float64(0.07006484077813502), np.float64(1.2210969056470709)), (np.float64(0.06865681695071482), np.float64(1.1544346971040478)), (np.float64(0.0687186152986398), np.float64(2.0340004222482255)), (np.float64(0.07862374265795782), np.float64(1.119428251515041)), (np.float64(0.06527225347405963), np.float64(1.033253567878989)), (np.float64(0.07193821675944918), np.float64(1.0938431058600628)), (np.float64(0.07077066173327842), np.float64(1.1346498833216705)), (np.float64(0.0576862166701645), np.float64(1.0849191518623953)), (np.float64(0.07272329462502723), np.float64(1.1209984854964643)), (np.float64(0.06577112073638737), np.float64(0.83404157469564)), (np.float64(0.06107419894674125), np.float64(0.9336592159688497)), (np.float64(0.07143089316807626), np.float64(1.0200881763519862)), (np.float64(0.07081014823417274), np.float64(1.329493815913117)), (np.float64(0.07565768115722624), np.float64(1.0909200552580014)), (np.float64(0.07325356231524094), np.float64(1.0525409673167876)), (np.float64(0.07261150128849983), np.float64(1.6309314377578832)), (np.float64(0.07129456283296268), np.float64(1.4616958957756312)), (np.float64(0.07126253349608486), np.float64(1.3616006270680863)), (np.float64(0.0697013191958306), np.float64(0.9229289732072974)), (np.float64(0.06708280145685923), np.float64(1.3507211647674275)), (np.float64(0.07189350894971334), np.float64(0.9509051552259541)), (np.float64(0.061888333362407746), np.float64(0.7798670711666876)), (np.float64(0.07097176808239289), np.float64(0.9682764685158751)), (np.float64(0.06504386271446991), np.float64(1.337507447404663)), (np.float64(0.06879935993279013), np.float64(1.166278395678878)), (np.float64(0.06849653511166343), np.float64(0.8298257873398286)), (np.float64(0.06936737548380842), np.float64(1.047696784665255)), (np.float64(0.06769339139230512), np.float64(0.7960991612235527)), (np.float64(0.07513556161409916), np.float64(1.310327222425036)), (np.float64(0.07524393568678164), np.float64(1.384733000609835)), (np.float64(0.06774912899824706), np.float64(1.2526854832951577)), (np.float64(0.07496099410035235), np.float64(0.7068259280798872)), (np.float64(0.06772687652305434), np.float64(1.1781161711957615)), (np.float64(0.06965769237836883), np.float64(1.044492104652814)), (np.float64(0.07471017690648984), np.float64(0.9566871891960322)), (np.float64(0.07679137676162481), np.float64(1.3131191493819494)), (np.float64(0.07462249772406254), np.float64(1.4908442987231099)), (np.float64(0.0774431789421191), np.float64(1.2245802057391635)), (np.float64(0.07352331621597985), np.float64(0.691050343405062)), (np.float64(0.0681716824340284), np.float64(1.1345020904069432)), (np.float64(0.07022170179678223), np.float64(0.9273190185470233)), (np.float64(0.07084780120340349), np.float64(1.2248308520449462)), (np.float64(0.06804062793941755), np.float64(1.181179655626235)), (np.float64(0.06724615321857938), np.float64(1.1192160656614338)), (np.float64(0.06907724040485698), np.float64(1.1396160062085243)), (np.float64(0.06554112387052476), np.float64(0.7017800742283014)), (np.float64(0.06773828278655192), np.float64(0.994177634925908)), (np.float64(0.07766994900122456), np.float64(1.4121774678612282)), (np.float64(0.07021323551117631), np.float64(1.0870597686740968)), (np.float64(0.07451807982539159), np.float64(1.2908203020496882)), (np.float64(0.06906483025728707), np.float64(1.5204020525980797)), (np.float64(0.07482538635264162), np.float64(1.4259821907260728)), (np.float64(0.07150940391722038), np.float64(0.7924288632084526)), (np.float64(0.0628953863330574), np.float64(0.7361865525537753)), (np.float64(0.07511056254766256), np.float64(1.1418780350933155)), (np.float64(0.07006488050999911), np.float64(0.9707679832518346)), (np.float64(0.06921035761426295), np.float64(1.2491992778873593)), (np.float64(0.06577234045764066), np.float64(1.343589449964692)), (np.float64(0.06632207352791217), np.float64(1.1978493011800986)), (np.float64(0.06777193431441593), np.float64(0.8273819982124689)), (np.float64(0.06901430219230013), np.float64(1.3765400915165187)), (np.float64(0.06675417809854825), np.float64(1.1177004110179762)), (np.float64(0.06742397671153087), np.float64(1.0639683746380566)), (np.float64(0.07354619859329549), np.float64(1.4465261900134812)), (np.float64(0.06891778140852119), np.float64(1.0296559206782157)), (np.float64(0.07532020932905953), np.float64(0.7345816202482935)), (np.float64(0.07007505332178594), np.float64(0.8056690040062952)), (np.float64(0.061896875965624064), np.float64(1.255538069950852)), (np.float64(0.0668672730944316), np.float64(0.8644942565570354)), (np.float64(0.07757198369623942), np.float64(1.4609309456141908)), (np.float64(0.0717046148982957), np.float64(0.8308519206486046)), (np.float64(0.07076908450674266), np.float64(1.0106726383838243)), (np.float64(0.07258510188134348), np.float64(0.7798172216500796)), (np.float64(0.06951392035605668), np.float64(1.0871820001431969)), (np.float64(0.06501389850408151), np.float64(1.2608371199312947)), (np.float64(0.0704378307109942), np.float64(0.8452420613396876)), (np.float64(0.06576440370203572), np.float64(0.6911319674522879)), (np.float64(0.06447455331096244), np.float64(1.2601894424771591)), (np.float64(0.07112367467518667), np.float64(0.786415721937628)), (np.float64(0.07193311786580385), np.float64(0.8538463584819411)), (np.float64(0.07325612616506122), np.float64(1.3307831057509978)), (np.float64(0.06508210461868538), np.float64(0.9205475036791088)), (np.float64(0.060925465616529596), np.float64(0.8195129670508196)), (np.float64(0.06407391034238383), np.float64(0.9774881824666263)), (np.float64(0.06878020134548271), np.float64(1.1803477485275042)), (np.float64(0.06222007417878066), np.float64(0.768416130583005)), (np.float64(0.06901438002060742), np.float64(0.8200665221942318)), (np.float64(0.07683244627817971), np.float64(1.3986463678766472)), (np.float64(0.07446760710327023), np.float64(1.0670491978468863)), (np.float64(0.0694436671586222), np.float64(1.060703866189836)), (np.float64(0.07298382320943478), np.float64(0.7978686357297373)), (np.float64(0.08005827287710227), np.float64(1.3397163648437351)), (np.float64(0.07226189018610542), np.float64(1.2348419380528983)), (np.float64(0.06675042242589505), np.float64(1.5954125879616066)), (np.float64(0.06928601882060625), np.float64(0.8102613788580049)), (np.float64(0.07420861359582136), np.float64(1.0664795187754452)), (np.float64(0.06682076964335913), np.float64(1.2511385532043613)), (np.float64(0.07531782762768796), np.float64(1.3470734546840204)), (np.float64(0.07002032588856708), np.float64(0.8492384681460643)), (np.float64(0.06542737520470879), np.float64(0.9747503813230438)), (np.float64(0.06712584171728442), np.float64(0.9249843268225778)), (np.float64(0.0723419457602866), np.float64(1.3151815318896771)), (np.float64(0.06430168108970391), np.float64(0.7178999960551478)), (np.float64(0.0750446298006178), np.float64(1.421979835764238)), (np.float64(0.06962520068483954), np.float64(0.9847106086616197)), (np.float64(0.06732085611861523), np.float64(1.5626593646946547)), (np.float64(0.07300623773887464), np.float64(1.1325359178380028)), (np.float64(0.07173098139528217), np.float64(0.9552615249995998)), (np.float64(0.06976003499934741), np.float64(0.7590206863111016)), (np.float64(0.07199385498803222), np.float64(1.578199026460568)), (np.float64(0.06329484234680688), np.float64(1.0573050640805834)), (np.float64(0.0705599344113449), np.float64(1.0495632939302482)), (np.float64(0.08042587780701224), np.float64(1.1392962093412675)), (np.float64(0.06803206761040048), np.float64(0.9633980197167806)), (np.float64(0.0706388096371128), np.float64(0.8433364810789034)), (np.float64(0.06612741450009815), np.float64(0.8197617408148586)), (np.float64(0.07347689527063017), np.float64(1.5765784050534253)), (np.float64(0.06487145304852013), np.float64(1.2114959522110165)), (np.float64(0.07792112998249076), np.float64(1.3292151216102688)), (np.float64(0.06645322842298616), np.float64(1.096172307335078)), (np.float64(0.07499788422554603), np.float64(1.1963512295434637)), (np.float64(0.06774469413030962), np.float64(1.260763673288304)), (np.float64(0.06586921759849731), np.float64(1.2462910327816463)), (np.float64(0.07152063819740234), np.float64(0.8580394071444584)), (np.float64(0.08096192608815472), np.float64(1.1733231243334152)), (np.float64(0.07631052470564594), np.float64(1.1504639546988171)), (np.float64(0.06890183986050036), np.float64(0.9592971826777337)), (np.float64(0.07433887913651813), np.float64(1.034276024697693)), (np.float64(0.06581615526003724), np.float64(1.3622976205854829)), (np.float64(0.06703187600999616), np.float64(1.392852002682601)), (np.float64(0.07189127861747042), np.float64(0.6709354312257978)), (np.float64(0.06720611446601896), np.float64(0.9681061400941194)), (np.float64(0.0740866169539196), np.float64(1.1986571634897625)), (np.float64(0.06659706681025868), np.float64(1.1310848374237128)), (np.float64(0.08043782105790218), np.float64(1.6704875160622186)), (np.float64(0.07373936789129301), np.float64(0.8943091991423539)), (np.float64(0.06944094546500269), np.float64(0.9289928633024324)), (np.float64(0.06399766482807771), np.float64(1.3030809371442122)), (np.float64(0.07685238969372221), np.float64(1.024155676995126)), (np.float64(0.07303562021753975), np.float64(1.2706015581899257)), (np.float64(0.07652931169357527), np.float64(1.3681932695846792)), (np.float64(0.06928518451467904), np.float64(1.0632817522268647)), (np.float64(0.06597647964968131), np.float64(0.950866910168878)), (np.float64(0.06813652158190939), np.float64(1.1815741476555262)), (np.float64(0.07024508578712224), np.float64(1.1998564550683095)), (np.float64(0.07184964786233863), np.float64(1.1053992744665766)), (np.float64(0.06524374720525966), np.float64(1.1004736959334622)), (np.float64(0.07172951183279964), np.float64(0.9494106204458729)), (np.float64(0.07483532948702984), np.float64(1.16793685228082)), (np.float64(0.06858344920287983), np.float64(0.9311080046562699)), (np.float64(0.07312960525594883), np.float64(1.4184620534034962)), (np.float64(0.07050417763672451), np.float64(0.8762623890944958)), (np.float64(0.06989094255061756), np.float64(0.7769075612595218)), (np.float64(0.06962628760073156), np.float64(0.9821933421858494)), (np.float64(0.06524649618755826), np.float64(1.132651366359918)), (np.float64(0.06936559295722651), np.float64(0.9304304402570119)), (np.float64(0.0645537571164567), np.float64(0.8205754812185977)), (np.float64(0.07162796022414888), np.float64(0.8663221331775316)), (np.float64(0.07239811077788635), np.float64(0.7390101684550617)), (np.float64(0.07740952547790303), np.float64(1.3243278190205614)), (np.float64(0.07134122682817189), np.float64(0.9796214568586215)), (np.float64(0.07766957921053579), np.float64(0.9322195822320797)), (np.float64(0.067103068841104), np.float64(0.9330587368891023)), (np.float64(0.07034897802157973), np.float64(1.689904178597065)), (np.float64(0.06143628164801865), np.float64(0.8186658039677991)), (np.float64(0.07542953098658106), np.float64(1.326126712903435)), (np.float64(0.06625418509010272), np.float64(1.112689768829853)), (np.float64(0.06596645249115013), np.float64(0.9841012123014675)), (np.float64(0.07090496400385997), np.float64(1.5130245098136392)), (np.float64(0.06544950803853003), np.float64(0.9145858934686103)), (np.float64(0.06491594652505273), np.float64(0.9550864273412384)), (np.float64(0.06976790506090534), np.float64(0.9277378351158845)), (np.float64(0.06775227294086798), np.float64(1.2190374901618803)), (np.float64(0.07147460783752689), np.float64(1.1346378263179924)), (np.float64(0.07077092904382776), np.float64(1.3758854119602144)), (np.float64(0.07319991166151739), np.float64(0.8089258080601152)), (np.float64(0.0721872322546019), np.float64(1.117317331369344)), (np.float64(0.06986786387913677), np.float64(1.4258492545785946)), (np.float64(0.07033527941542779), np.float64(0.93853287859929)), (np.float64(0.07130066927692938), np.float64(1.2920783482086207)), (np.float64(0.06880543071302175), np.float64(1.3341723273689228)), (np.float64(0.06967547608172261), np.float64(1.0147634176173013)), (np.float64(0.06670310905205105), np.float64(1.204242506952665)), (np.float64(0.07330281084243026), np.float64(1.2278875294102476)), (np.float64(0.0648091052763156), np.float64(1.9744244853047748)), (np.float64(0.065455295790462), np.float64(0.7278632727273792)), (np.float64(0.07612397869678185), np.float64(1.2451307425241382)), (np.float64(0.0749614458688548), np.float64(1.466065009517624)), (np.float64(0.06747646100132783), np.float64(1.1027013265213284)), (np.float64(0.06929593118471007), np.float64(0.8967383578191263)), (np.float64(0.06678940487348085), np.float64(1.1197689604213787)), (np.float64(0.06370802252818411), np.float64(1.087727971999949)), (np.float64(0.0704015529842564), np.float64(1.4199383714575304)), (np.float64(0.06873958927775661), np.float64(1.0360809872381305)), (np.float64(0.0793057261944824), np.float64(1.4360274492544378)), (np.float64(0.06659208713328577), np.float64(1.0128744405147214)), (np.float64(0.07198238540745751), np.float64(1.0213265093573471)), (np.float64(0.06715044481376208), np.float64(0.9756449596823795)), (np.float64(0.06902117451534223), np.float64(1.3930829900613237)), (np.float64(0.06973919269930437), np.float64(0.7771893954055262)), (np.float64(0.0701212684425342), np.float64(1.0361143428522739)), (np.float64(0.06333247763202765), np.float64(0.9138468047788462)), (np.float64(0.07007813802696328), np.float64(1.1841387742529266)), (np.float64(0.07249277061952314), np.float64(1.2040674498190218)), (np.float64(0.0699229318742981), np.float64(1.2672826651286486)), (np.float64(0.06150271511997474), np.float64(1.4542023557492376)), (np.float64(0.06993561387099038), np.float64(0.9694783499090898)), (np.float64(0.06760842583444486), np.float64(1.5931042037067715)), (np.float64(0.06632979253646744), np.float64(0.9512478344486316)), (np.float64(0.06977363167242272), np.float64(1.3167441304777803)), (np.float64(0.07055441578718914), np.float64(1.6359840709862992)), (np.float64(0.06996663741470252), np.float64(1.428870350670788)), (np.float64(0.07071022677416604), np.float64(1.8393126718540491)), (np.float64(0.07189730115113352), np.float64(0.8443224180402873)), (np.float64(0.07069009608369024), np.float64(1.3791024092961914)), (np.float64(0.07411649330538676), np.float64(1.7292005039067777)), (np.float64(0.0759878915053279), np.float64(1.4266389506312933)), (np.float64(0.07321342310033352), np.float64(0.7441029911332062)), (np.float64(0.07091132664950359), np.float64(1.1071678276938062)), (np.float64(0.06757449625367021), np.float64(1.0403853625411712)), (np.float64(0.07215120367320536), np.float64(1.1792927777688509)), (np.float64(0.07854979887786466), np.float64(1.0579234692160173)), (np.float64(0.0720972737052091), np.float64(1.049834796153664)), (np.float64(0.07514510427295763), np.float64(1.0461929221145583)), (np.float64(0.07052785122721994), np.float64(1.1707043146257576)), (np.float64(0.07380336721519344), np.float64(1.628151217291937)), (np.float64(0.06429167182350268), np.float64(0.8180424856248856)), (np.float64(0.0677086192919508), np.float64(1.3135623316371565)), (np.float64(0.07204531707931067), np.float64(1.3002472227893722)), (np.float64(0.07194918059956386), np.float64(0.7627086592880555)), (np.float64(0.07120515532582714), np.float64(1.0144440911900916)), (np.float64(0.07276826443794361), np.float64(1.0987911464661888)), (np.float64(0.07811323323367567), np.float64(1.1951266813827794)), (np.float64(0.06437575071224312), np.float64(1.1426351876616598)), (np.float64(0.06959645796907471), np.float64(0.8925175527035121)), (np.float64(0.07317343552183957), np.float64(1.3758284048816467)), (np.float64(0.06452444534467768), np.float64(0.764586938885129)), (np.float64(0.07146539769637664), np.float64(0.9098384626916463)), (np.float64(0.06852129056752336), np.float64(0.7964266280283313)), (np.float64(0.06854344572704325), np.float64(1.1202629388031806)), (np.float64(0.07453267579582096), np.float64(1.0187279194542245)), (np.float64(0.0692761145094234), np.float64(1.0337577119230743)), (np.float64(0.06650832142263075), np.float64(1.2627626931141223)), (np.float64(0.068967878948688), np.float64(0.9107559603426583)), (np.float64(0.07581256329051371), np.float64(1.4302933448564474)), (np.float64(0.06859505681032732), np.float64(1.087842028932174)), (np.float64(0.07597673944600483), np.float64(1.3623538817641903)), (np.float64(0.06595060275131079), np.float64(1.3300360937291387)), (np.float64(0.07331200527797642), np.float64(1.3112821168868631)), (np.float64(0.07058964750734419), np.float64(1.6654612258600239)), (np.float64(0.06840587377731375), np.float64(1.1359586028952826)), (np.float64(0.06312829587191579), np.float64(0.7860084547890772)), (np.float64(0.07152365809808421), np.float64(0.9680367798889656)), (np.float64(0.0757576217570857), np.float64(1.1985317718494484)), (np.float64(0.07401575225680881), np.float64(1.282591209709678)), (np.float64(0.06298229807974695), np.float64(1.2070314609896378)), (np.float64(0.0758852166427818), np.float64(1.2254443341731112)), (np.float64(0.07094141435022744), np.float64(0.8484504155739508)), (np.float64(0.06753526034339194), np.float64(1.6387553819655953)), (np.float64(0.07328683277436929), np.float64(1.2512923133202103)), (np.float64(0.06771825127472576), np.float64(1.2108704425708925)), (np.float64(0.07139723123855167), np.float64(1.1540697837886467)), (np.float64(0.06055327380364617), np.float64(0.8746542711809582)), (np.float64(0.06726030728849212), np.float64(1.4218985806035087)), (np.float64(0.06675521138368598), np.float64(0.984274637245785)), (np.float64(0.06786463861875937), np.float64(0.8660401747477585)), (np.float64(0.0661727575667509), np.float64(1.1080657258744757)), (np.float64(0.0743325757386816), np.float64(1.4417358736534924)), (np.float64(0.06343270807945026), np.float64(1.7033506530530074)), (np.float64(0.06971496653999179), np.float64(0.8442611207489206)), (np.float64(0.06950905375617815), np.float64(1.3084606376572492)), (np.float64(0.06712040835076737), np.float64(1.3386162611364842)), (np.float64(0.07052597986436038), np.float64(1.2855092967803952)), (np.float64(0.07269020510665543), np.float64(1.4446566660606288)), (np.float64(0.0658955793530115), np.float64(0.8154116918330417)), (np.float64(0.07911533660230845), np.float64(0.9218430460745143)), (np.float64(0.0676338308379413), np.float64(1.2747201188914201)), (np.float64(0.06870387560668477), np.float64(0.9449788258243577)), (np.float64(0.06808310692033298), np.float64(0.8283735831281177)), (np.float64(0.06711703630706203), np.float64(1.6007863085482854)), (np.float64(0.07273411937311981), np.float64(0.707807097641261)), (np.float64(0.07446694152388583), np.float64(1.2148808028189713)), (np.float64(0.06542425433505165), np.float64(1.3015382663612076)), (np.float64(0.07124999663693887), np.float64(1.0284004247531782)), (np.float64(0.07205524562038003), np.float64(1.6211756849970347)), (np.float64(0.07171792251362465), np.float64(1.4642420938110168)), (np.float64(0.06411969017413921), np.float64(0.8980654691826552)), (np.float64(0.06817222148057125), np.float64(1.7580236576749515)), (np.float64(0.0697977861631259), np.float64(0.6812099304005389)), (np.float64(0.06738036374851147), np.float64(0.9438638239892343)), (np.float64(0.0685203632048083), np.float64(1.5310332847798631)), (np.float64(0.06814410176763816), np.float64(0.7318866236382001)), (np.float64(0.06456306063322906), np.float64(1.7069869702985523)), (np.float64(0.07601392640195012), np.float64(1.4141311508261034)), (np.float64(0.06840598007770277), np.float64(1.386114381425427)), (np.float64(0.07336352994235887), np.float64(1.8450116900764533)), (np.float64(0.07179557210461986), np.float64(1.0876136289206733)), (np.float64(0.07091774055248612), np.float64(1.2327452938233867)), (np.float64(0.0797455984854043), np.float64(1.2739416724236516)), (np.float64(0.07293010078924875), np.float64(1.425184250082746)), (np.float64(0.06791027041097383), np.float64(0.9965101397595768)), (np.float64(0.07023448223896786), np.float64(1.5672092585632702)), (np.float64(0.06963599831285132), np.float64(1.051587527720182)), (np.float64(0.06612604004794546), np.float64(1.2303263076003184)), (np.float64(0.06657771649299163), np.float64(0.8541330628973637)), (np.float64(0.06690245623811904), np.float64(0.9583699237766565)), (np.float64(0.0717412383450894), np.float64(1.9520999363858205)), (np.float64(0.07887741311383306), np.float64(1.0784841429836578)), (np.float64(0.0758002265123556), np.float64(1.5312841988354813)), (np.float64(0.06941956512374445), np.float64(0.9591584248203535)), (np.float64(0.07088386135753229), np.float64(0.917588080147384)), (np.float64(0.06668680516346503), np.float64(0.9330696259073353)), (np.float64(0.06855272358945788), np.float64(1.0723060665493689)), (np.float64(0.06967114610681065), np.float64(1.025774650428794)), (np.float64(0.0829797194037645), np.float64(1.4026279942066486)), (np.float64(0.07425811541936093), np.float64(1.4039386310323245)), (np.float64(0.07267369356636696), np.float64(1.3846582638762053)), (np.float64(0.06675427547167302), np.float64(0.9903250753548419)), (np.float64(0.0710682575208792), np.float64(0.9161168638015339)), (np.float64(0.06315926542070548), np.float64(0.9494989319476582)), (np.float64(0.07519153694356095), np.float64(1.5260711981872306)), (np.float64(0.06811814105417), np.float64(0.9658185024431869)), (np.float64(0.06698221805534987), np.float64(1.0613318789533852)), (np.float64(0.07760649598253588), np.float64(1.6394741504039354)), (np.float64(0.07222889930447722), np.float64(0.8360386890856465)), (np.float64(0.06867707234374495), np.float64(1.060515706886362)), (np.float64(0.06627199340895729), np.float64(0.8903785479595032)), (np.float64(0.07363148150005677), np.float64(1.1876575850993374)), (np.float64(0.07528634310835995), np.float64(1.5606437692488846)), (np.float64(0.07197612065102679), np.float64(1.3835539447668728)), (np.float64(0.07077457162298045), np.float64(1.0756930724984972)), (np.float64(0.06490823477256615), np.float64(1.0500654703583776)), (np.float64(0.07392684175149361), np.float64(1.4234537176206656)), (np.float64(0.07192252654924894), np.float64(0.99119642343238)), (np.float64(0.06798002390011863), np.float64(1.1438815384459973)), (np.float64(0.06614295577848699), np.float64(0.8015117471683013)), (np.float64(0.0672228836965064), np.float64(0.7594934701521282)), (np.float64(0.07226147240832172), np.float64(1.0059891039183846)), (np.float64(0.07091470906327625), np.float64(0.6867686825387789)), (np.float64(0.06356934760888928), np.float64(1.0963229908983076)), (np.float64(0.07031829176521456), np.float64(0.861028215518527)), (np.float64(0.07141577307260036), np.float64(1.4236711586563733)), (np.float64(0.07075565802562889), np.float64(1.4642327228753536)), (np.float64(0.06221923592550598), np.float64(0.8370559023262366)), (np.float64(0.06700830280288483), np.float64(0.7582473899133946)), (np.float64(0.06578200245462973), np.float64(1.3578186911948211)), (np.float64(0.06824451429520542), np.float64(1.3131878845051157)), (np.float64(0.06672123297736525), np.float64(1.1839735000947644)), (np.float64(0.06725090177705784), np.float64(0.9334455323603937)), (np.float64(0.06867030792172797), np.float64(0.9107266005519311)), (np.float64(0.07205335500887294), np.float64(0.8314702016351325)), (np.float64(0.06426726304954827), np.float64(0.816453870969259)), (np.float64(0.07345645803721171), np.float64(1.2171431048266717)), (np.float64(0.07490691902676137), np.float64(0.7997816976540042)), (np.float64(0.06941593833246376), np.float64(1.104095805839148)), (np.float64(0.07208532552653214), np.float64(1.282792745384135)), (np.float64(0.07318438982355589), np.float64(1.1704026806455037)), (np.float64(0.0717074255001713), np.float64(1.4875706787822942)), (np.float64(0.06790285268750168), np.float64(0.9393074842639367)), (np.float64(0.06680615762468443), np.float64(0.8993723908119395)), (np.float64(0.06597318975914063), np.float64(0.9890815980557021)), (np.float64(0.0690419623154846), np.float64(1.03496774665929)), (np.float64(0.0698705415632327), np.float64(1.2294325931874746)), (np.float64(0.07773037574101643), np.float64(1.1616032689281854)), (np.float64(0.06982319197804282), np.float64(1.1979797861879036)), (np.float64(0.060912105876498854), np.float64(1.1656460377338151)), (np.float64(0.06159737165639542), np.float64(1.068726052838909)), (np.float64(0.06194671619633474), np.float64(0.906648478859732)), (np.float64(0.07402739227376484), np.float64(1.5615086343427018)), (np.float64(0.0621737007550843), np.float64(0.8507668810532163)), (np.float64(0.07325646411725074), np.float64(1.2466155177455618)), (np.float64(0.07350335519828087), np.float64(1.364294061127667)), (np.float64(0.06283804338524511), np.float64(1.3902006508723113)), (np.float64(0.07161512046820065), np.float64(0.7997187655159865)), (np.float64(0.07079765791391501), np.float64(0.9362716788153476)), (np.float64(0.07472630579978659), np.float64(1.2310060986697136)), (np.float64(0.06594669252751571), np.float64(0.8346956479721993)), (np.float64(0.07858192236481165), np.float64(1.4862568353745684)), (np.float64(0.07081476195722675), np.float64(1.660229696841064)), (np.float64(0.06617276407586434), np.float64(0.8117934676670735)), (np.float64(0.07158459387537339), np.float64(1.3685584115433125)), (np.float64(0.06625695848063047), np.float64(0.7994377779697458)), (np.float64(0.059637082392258706), np.float64(1.2328506419704566)), (np.float64(0.06715244145822313), np.float64(0.9406336913687464)), (np.float64(0.0673294739177091), np.float64(1.147060641090704)), (np.float64(0.07857563098765487), np.float64(1.304802875800529)), (np.float64(0.07242657466963709), np.float64(0.9194976879168568)), (np.float64(0.07074753051981786), np.float64(1.167511015544918)), (np.float64(0.06743884677692555), np.float64(1.0726005534689786)), (np.float64(0.06881360233952802), np.float64(0.8449091419188491)), (np.float64(0.07114475660071931), np.float64(1.1793000243837337)), (np.float64(0.06212199214447621), np.float64(0.7925490309993812)), (np.float64(0.06791681305958779), np.float64(0.8436285311744618)), (np.float64(0.07203076729956101), np.float64(0.8316752622406366)), (np.float64(0.0642955867790099), np.float64(1.156082045725051)), (np.float64(0.07210535952647146), np.float64(1.3150703041421354)), (np.float64(0.06627440776349651), np.float64(1.0481909817862973)), (np.float64(0.06331186219649176), np.float64(0.8261238346845449)), (np.float64(0.06222336616569754), np.float64(0.9220464693864028)), (np.float64(0.07028554118499776), np.float64(0.9990807415793026)), (np.float64(0.0790592615342956), np.float64(1.3440129495762576)), (np.float64(0.06724808942814901), np.float64(1.0673508706185089)), (np.float64(0.06431973937784687), np.float64(1.1685731589961572)), (np.float64(0.07178612901758798), np.float64(1.0337102478956015)), (np.float64(0.06950784596029655), np.float64(1.5471443564985687)), (np.float64(0.07521106210318776), np.float64(1.248765812891249)), (np.float64(0.06680731201508099), np.float64(0.784056481592558)), (np.float64(0.07383637715594185), np.float64(1.1173678783267784)), (np.float64(0.06334935877765827), np.float64(1.3363349195574001)), (np.float64(0.0643469849160522), np.float64(0.7963254358041334)), (np.float64(0.06565477577639182), np.float64(1.1334274267371005)), (np.float64(0.07260281797609064), np.float64(1.3633324858167073)), (np.float64(0.06538160329378119), np.float64(0.9235887603917942)), (np.float64(0.06791495286959491), np.float64(1.611530912386071)), (np.float64(0.06115639036740046), np.float64(2.0408386958623415)), (np.float64(0.06415706286011993), np.float64(1.2886835066514586)), (np.float64(0.07178805483274052), np.float64(0.9317610858783415)), (np.float64(0.06496531472821078), np.float64(1.2877151569518368)), (np.float64(0.07584677688342176), np.float64(1.384859507308874)), (np.float64(0.06706366623770173), np.float64(0.7637213299288705)), (np.float64(0.06800240585109867), np.float64(1.0289859479175218)), (np.float64(0.0692765794425565), np.float64(0.9476433720357205)), (np.float64(0.0665887425527982), np.float64(1.1830376308088895)), (np.float64(0.0723994769686136), np.float64(0.7602451622157953)), (np.float64(0.07172699877733064), np.float64(1.1384772514434125)), (np.float64(0.0631086254784551), np.float64(1.1471114399642508)), (np.float64(0.06469678576915149), np.float64(1.493238876329648)), (np.float64(0.06282883943713424), np.float64(1.1262657484756922)), (np.float64(0.07768687670980717), np.float64(1.243451662013133)), (np.float64(0.07397137028192156), np.float64(1.2840500210437527)), (np.float64(0.07493470864979962), np.float64(1.3726506616847909)), (np.float64(0.06654547358197227), np.float64(0.8037166364128847)), (np.float64(0.0674399125605742), np.float64(0.8883603623350517)), (np.float64(0.0729522080101592), np.float64(1.400179952370823)), (np.float64(0.06847634168456891), np.float64(0.9789075995311216)), (np.float64(0.07734824495002404), np.float64(1.1741542711385098)), (np.float64(0.07430906352703014), np.float64(1.4530570954672042)), (np.float64(0.06587755674335088), np.float64(1.0850618072680391)), (np.float64(0.06769354945979218), np.float64(1.007826098329962)), (np.float64(0.07113328271586304), np.float64(0.9455886067407829)), (np.float64(0.06772772485381395), np.float64(1.7271638704571277)), (np.float64(0.06890493504577766), np.float64(1.2511269692866585)), (np.float64(0.0722125856905171), np.float64(1.2255982106124719)), (np.float64(0.07885869761708254), np.float64(1.2882658732625185)), (np.float64(0.06303595491720453), np.float64(1.0585558921722629)), (np.float64(0.0744430331006078), np.float64(1.136021241717237)), (np.float64(0.07262854626359898), np.float64(1.0031309208752053)), (np.float64(0.0674042045135664), np.float64(0.9297221293644857)), (np.float64(0.07501379891847977), np.float64(1.1533271258286781)), (np.float64(0.0681207379119158), np.float64(1.291766378099819)), (np.float64(0.06226520123472361), np.float64(0.9386218347448445)), (np.float64(0.0703807538690026), np.float64(1.0914601315346635)), (np.float64(0.06150197627943897), np.float64(1.1847857518196245)), (np.float64(0.07426096379040581), np.float64(1.1321637516743464)), (np.float64(0.07297010867699857), np.float64(0.9764291189986335)), (np.float64(0.07093343496432647), np.float64(1.0723283089324571)), (np.float64(0.07065972144321506), np.float64(1.3922348532372353)), (np.float64(0.07352492141610564), np.float64(0.9058306767857583)), (np.float64(0.0721928372919207), np.float64(1.1169213554765784)), (np.float64(0.06980603386295003), np.float64(1.1328697964107868)), (np.float64(0.07004755271176681), np.float64(1.0581522764543545)), (np.float64(0.06351361443996062), np.float64(1.5967503955396376)), (np.float64(0.07568017148356446), np.float64(1.6525970213637444)), (np.float64(0.06818674746777), np.float64(1.000388600001439)), (np.float64(0.07057819872617004), np.float64(1.53622583373296)), (np.float64(0.07120962031072918), np.float64(1.1583453216402972)), (np.float64(0.0707835947603383), np.float64(0.868364716159044)), (np.float64(0.06410779330684102), np.float64(0.9504972108347952)), (np.float64(0.07148254578467379), np.float64(0.9838507676373897)), (np.float64(0.07316184553942623), np.float64(1.2119851875088103)), (np.float64(0.07611742165166491), np.float64(1.2824426928334556)), (np.float64(0.07099250700117148), np.float64(1.04053170648073)), (np.float64(0.07045218998258325), np.float64(1.0937415615498434)), (np.float64(0.06954916703979566), np.float64(1.0016204259157084)), (np.float64(0.06811512386997456), np.float64(0.9879006688262546)), (np.float64(0.0710151698232629), np.float64(1.4955665312421442)), (np.float64(0.0691844774877723), np.float64(0.9161227538407704)), (np.float64(0.07181511209173262), np.float64(1.2387235503825578)), (np.float64(0.06465185351228198), np.float64(1.1173309861171579)), (np.float64(0.0647577108378176), np.float64(0.725882476370014)), (np.float64(0.06594160271776703), np.float64(1.1882797954438102)), (np.float64(0.06646164210294464), np.float64(1.366314545656092)), (np.float64(0.06688899523031962), np.float64(1.009268196006973)), (np.float64(0.07042877609670008), np.float64(1.7013452033184895)), (np.float64(0.060838358437571405), np.float64(1.204908267848789)), (np.float64(0.06561741108211425), np.float64(0.923782885117186)), (np.float64(0.06315628297619416), np.float64(1.2801585041421075)), (np.float64(0.06966674193843979), np.float64(1.1683294716112225)), (np.float64(0.06714134268047268), np.float64(0.718986379463062)), (np.float64(0.07117315496971986), np.float64(1.091636086434767)), (np.float64(0.0641613711657106), np.float64(1.0372643317314272)), (np.float64(0.06893647678089414), np.float64(1.1138510180580776)), (np.float64(0.07218409804621072), np.float64(1.0038832190034042)), (np.float64(0.06962401786322679), np.float64(1.2561871324125002)), (np.float64(0.0746885201185282), np.float64(1.321955053877825)), (np.float64(0.07343535288822393), np.float64(1.0931283052972571)), (np.float64(0.0632017517002078), np.float64(0.7560096108868215)), (np.float64(0.06747823547020945), np.float64(1.1453829060073226)), (np.float64(0.07034895658738653), np.float64(0.8733702104372945)), (np.float64(0.06838444938742053), np.float64(1.3929009420404423)), (np.float64(0.07066460377286204), np.float64(1.7613757580435414)), (np.float64(0.07000875795336954), np.float64(1.3121772133096403)), (np.float64(0.07024935763123712), np.float64(1.5581329210145136)), (np.float64(0.06745194947707772), np.float64(0.7581006519962209)), (np.float64(0.07076026557320489), np.float64(1.1748410232356603)), (np.float64(0.06980030738176182), np.float64(0.8934562417788053)), (np.float64(0.06883715373820942), np.float64(1.5048257367105022))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.stats.correlation_tools import cov_nearest\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from scipy.stats import t\n",
        "\n",
        "# Simulating returns and covariance matrix (for demonstration)\n",
        "np.random.seed(42)\n",
        "n_assets = 5\n",
        "mu = np.array([0.05, 0.08, 0.06, 0.09, 0.07])  # Expected returns adjusted to realistic values\n",
        "Sigma = np.random.randn(n_assets, n_assets)\n",
        "Sigma = np.dot(Sigma, Sigma.T)  # Make it positive semi-definite\n",
        "\n",
        "# Using statsmodels' covariance shrinkage to ensure positive semi-definiteness\n",
        "Sigma_shrinked = cov_nearest(Sigma)  # Nearest positive-definite matrix\n",
        "\n",
        "# Fixing ElasticNet Regularization (Tuning the regularization parameters)\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.3)  # Try smaller regularization parameter\n",
        "elastic_net.fit(Sigma_shrinked, mu)  # Fit the ElasticNet model\n",
        "elastic_net_weights = elastic_net.coef_  # Portfolio weights from ElasticNet\n",
        "\n",
        "# Improved Sortino Ratio Calculation to handle edge cases\n",
        "def calculate_sortino_ratio(returns):\n",
        "    downside_returns = returns[returns < 0]\n",
        "    if len(downside_returns) == 0 or np.std(downside_returns) == 0:\n",
        "        return np.nan  # Return NaN if there are no downside returns or std is zero\n",
        "    return np.mean(returns) / np.std(downside_returns)\n",
        "\n",
        "# Improved Omega Ratio Calculation to handle edge cases\n",
        "def calculate_omega_ratio(returns):\n",
        "    if np.sum(returns < 0) == 0:\n",
        "        return np.nan  # Return NaN if there are no negative returns\n",
        "    return np.sum(returns[returns > 0]) / np.abs(np.sum(returns[returns < 0]))\n",
        "\n",
        "# Portfolio optimization using cvxpy with L2 regularization\n",
        "def optimize_portfolio(Sigma, mu, risk_aversion=1.0, l2_regularization=0.1):\n",
        "    w = cp.Variable(len(mu))  # portfolio weights\n",
        "    ret = mu.T @ w  # expected return\n",
        "    risk = cp.quad_form(w, Sigma)  # portfolio risk (variance)\n",
        "\n",
        "    # L2 regularization (penalizing large weights) using sum of squares for L2 norm\n",
        "    l2_penalty = cp.sum_squares(w)  # L2 norm of the weight vector\n",
        "\n",
        "    # Objective function: maximize return - risk_aversion * risk - l2_regularization * l2_penalty\n",
        "    objective = cp.Maximize(ret - risk_aversion * risk - l2_regularization * l2_penalty)\n",
        "\n",
        "    # Constraints: Sum of weights equals 1, weights >= 0 (no short selling)\n",
        "    constraints = [cp.sum(w) == 1, w >= 0]\n",
        "\n",
        "    # Define and solve problem\n",
        "    prob = cp.Problem(objective, constraints)\n",
        "    prob.solve()\n",
        "\n",
        "    return w.value\n",
        "\n",
        "# Running Monte Carlo Simulation with Larger Sample Sizes and Fat-Tailed Distributions\n",
        "def monte_carlo_simulation(Sigma, mu, n_samples=10000):  # Increased sample size for more precision\n",
        "    simulations = []\n",
        "    for _ in range(n_samples):\n",
        "        random_weights = np.random.rand(len(mu))\n",
        "        random_weights /= np.sum(random_weights)  # Normalize to sum to 1\n",
        "        expected_return = mu.T @ random_weights\n",
        "\n",
        "        # Correct calculation of portfolio risk (quadratic form)\n",
        "        risk = np.sqrt(np.dot(random_weights.T, np.dot(Sigma, random_weights)))  # Portfolio risk (standard deviation)\n",
        "\n",
        "        simulations.append((expected_return, risk))\n",
        "    return simulations\n",
        "\n",
        "# Expanding Monte Carlo Simulations with Fat-Tailed Distributions (Student's t-distribution)\n",
        "def monte_carlo_simulation_fat_tails(Sigma, mu, n_samples=10000):\n",
        "    simulations = []\n",
        "    for _ in range(n_samples):\n",
        "        random_weights = np.random.rand(len(mu))\n",
        "        random_weights /= np.sum(random_weights)  # Normalize to sum to 1\n",
        "        expected_return = mu.T @ random_weights\n",
        "\n",
        "        # Correct calculation of portfolio risk (quadratic form)\n",
        "        risk = np.sqrt(np.dot(random_weights.T, np.dot(Sigma, random_weights)))  # Portfolio risk (standard deviation)\n",
        "\n",
        "        # Simulating fat-tailed risk using Student's t-distribution\n",
        "        fat_tail_risk = np.random.standard_t(df=3, size=len(mu))  # Fat-tailed distribution (Student's t)\n",
        "        simulations.append((expected_return, risk * fat_tail_risk))\n",
        "    return simulations\n",
        "\n",
        "# Stress Testing Enhancements: Adding More Realistic and Complex Market Scenarios\n",
        "def generate_scenarios(mu, Sigma, n_scenarios=1000):\n",
        "    scenarios = []\n",
        "    for _ in range(n_scenarios):\n",
        "        random_weights = np.random.rand(len(mu))\n",
        "        random_weights /= np.sum(random_weights)  # Normalize to sum to 1\n",
        "        returns = mu.T @ random_weights\n",
        "        risk = np.sqrt(np.dot(random_weights.T, np.dot(Sigma, random_weights)))  # Portfolio risk\n",
        "        scenarios.append((returns, risk))\n",
        "    return scenarios\n",
        "\n",
        "# Advanced Stress Testing with generated scenarios\n",
        "def advanced_stress_test(mu, Sigma):\n",
        "    # Generate scenarios under different market conditions\n",
        "    scenarios = generate_scenarios(mu, Sigma)\n",
        "    results = []\n",
        "    for scenario in scenarios:\n",
        "        # Apply advanced model or perform analysis per scenario\n",
        "        results.append(scenario)\n",
        "    return results\n",
        "\n",
        "# Example of running advanced models with ElasticNet and Stress Testing\n",
        "elastic_net_weights = optimize_portfolio(Sigma_shrinked, mu)\n",
        "stress_test_results = advanced_stress_test(mu, Sigma_shrinked)\n",
        "\n",
        "print(\"ElasticNet Portfolio Weights:\", elastic_net_weights)\n",
        "print(\"Advanced Stress Test Results:\", stress_test_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdPnsbpFE3f2",
        "outputId": "3c14f24f-bbe2-4250-b30a-ba0ff9812445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet Portfolio Weights: [ 3.32085482e-01  2.95610671e-01 -5.17332891e-24  2.45813008e-01\n",
            "  1.26490839e-01]\n",
            "Advanced Stress Test Results: [(np.float64(0.07362710072328668), np.float64(0.8418146199545673)), (np.float64(0.06676396768835047), np.float64(1.2700220893781369)), (np.float64(0.07101357564220812), np.float64(0.7307797658413125)), (np.float64(0.06920447730488608), np.float64(0.7760517185597083)), (np.float64(0.07066849463470351), np.float64(0.9748329208099877)), (np.float64(0.07409139769690538), np.float64(1.2436311358544025)), (np.float64(0.07112828028221696), np.float64(1.047975490705823)), (np.float64(0.0666876896829506), np.float64(0.9586851081467271)), (np.float64(0.06930133009050796), np.float64(0.9659488687899319)), (np.float64(0.06846073788042842), np.float64(1.114698623595678)), (np.float64(0.06751741434487918), np.float64(0.7601362349874995)), (np.float64(0.07043300998854672), np.float64(0.7934801032017781)), (np.float64(0.06556991101573136), np.float64(0.7891656743559955)), (np.float64(0.0792983227332068), np.float64(1.0692075357517066)), (np.float64(0.06823634866918764), np.float64(0.9120729690474304)), (np.float64(0.07350861425903739), np.float64(0.9332294865899541)), (np.float64(0.06791524798822034), np.float64(0.8470137188972566)), (np.float64(0.07285076311739999), np.float64(1.2468796566082387)), (np.float64(0.06891350092385015), np.float64(1.5999021561921123)), (np.float64(0.07277203435526168), np.float64(1.4570095828322684)), (np.float64(0.06968594825872985), np.float64(0.7806684834805698)), (np.float64(0.06897647825974214), np.float64(1.0585819947385344)), (np.float64(0.06923768880885674), np.float64(1.292091896481103)), (np.float64(0.07048272928154817), np.float64(1.0640379914581055)), (np.float64(0.069215343971272), np.float64(0.9213527939504335)), (np.float64(0.06526036528683331), np.float64(1.153307697761217)), (np.float64(0.07368255383172254), np.float64(0.9842648796338168)), (np.float64(0.07483515496251772), np.float64(1.4454640913038581)), (np.float64(0.07072514932658797), np.float64(1.2891475915240591)), (np.float64(0.06903300155721964), np.float64(0.9821456902123091)), (np.float64(0.06829366047519596), np.float64(1.0950545171725288)), (np.float64(0.07217439541141295), np.float64(1.2103071706162043)), (np.float64(0.06346619807129321), np.float64(0.8837583753040608)), (np.float64(0.0683727701633604), np.float64(1.0391377423250854)), (np.float64(0.06944110524298436), np.float64(0.9327040284073566)), (np.float64(0.06926877461936778), np.float64(1.172732251184935)), (np.float64(0.06450924602492644), np.float64(1.2378224266339262)), (np.float64(0.07124948322135212), np.float64(1.217637797041)), (np.float64(0.07307304280971416), np.float64(1.1519710892476807)), (np.float64(0.07331156334748554), np.float64(0.9355106322168676)), (np.float64(0.07364101021596674), np.float64(1.2179565540045663)), (np.float64(0.0661509198733217), np.float64(1.176416211521337)), (np.float64(0.06985486437979532), np.float64(1.4019057433460638)), (np.float64(0.07509513200371395), np.float64(1.2624974309785424)), (np.float64(0.0717350170678041), np.float64(1.1651918979571232)), (np.float64(0.06700050044286433), np.float64(1.1384981546111346)), (np.float64(0.06659678029157134), np.float64(0.7609640044752097)), (np.float64(0.06542372893316362), np.float64(1.1231368430043271)), (np.float64(0.0624929439167284), np.float64(1.0942498846416333)), (np.float64(0.06747223215830508), np.float64(0.8182802042970136)), (np.float64(0.06444025699042678), np.float64(1.12350005962047)), (np.float64(0.07061548489883869), np.float64(1.6014696631939564)), (np.float64(0.07212371732384193), np.float64(1.4369694771043366)), (np.float64(0.0657344319876608), np.float64(1.1938846812821025)), (np.float64(0.06746113779630293), np.float64(1.358105101552447)), (np.float64(0.0681704914186092), np.float64(0.6807564809869829)), (np.float64(0.06253853128020212), np.float64(0.7827356124561773)), (np.float64(0.06424364962839103), np.float64(1.3395513334195657)), (np.float64(0.07142434914952038), np.float64(1.790917424098204)), (np.float64(0.0637898252637728), np.float64(1.0030968498086357)), (np.float64(0.06559398518148699), np.float64(1.3190366315315547)), (np.float64(0.07095993323274997), np.float64(1.0574534139452894)), (np.float64(0.06813169387471263), np.float64(0.9410119652705093)), (np.float64(0.06592214188396184), np.float64(0.8256005622790323)), (np.float64(0.07025893128060981), np.float64(1.1873532046438442)), (np.float64(0.0698578459751765), np.float64(0.8931322792509883)), (np.float64(0.06936425283481497), np.float64(1.392248228253745)), (np.float64(0.06700928800198361), np.float64(0.6892134357520201)), (np.float64(0.06674010348099554), np.float64(1.4559198036783585)), (np.float64(0.07042314875775621), np.float64(0.9294018442119042)), (np.float64(0.06221472774868213), np.float64(1.0868926103193692)), (np.float64(0.06781538338317378), np.float64(0.8193280620010883)), (np.float64(0.06833972706520047), np.float64(1.0704647107772882)), (np.float64(0.06965358794727215), np.float64(1.2603020621793475)), (np.float64(0.07029364731168931), np.float64(1.0336345320183855)), (np.float64(0.07254643487991952), np.float64(1.9503345786997268)), (np.float64(0.06727640392416519), np.float64(0.7627318950292395)), (np.float64(0.07120819729768782), np.float64(1.3901754071456711)), (np.float64(0.0688630794060299), np.float64(1.3583532109058887)), (np.float64(0.07494818810242523), np.float64(1.3611051138792654)), (np.float64(0.07074115002403329), np.float64(1.0605045664202997)), (np.float64(0.06701901506538767), np.float64(1.4530242621480383)), (np.float64(0.07362265884039595), np.float64(1.42342862075551)), (np.float64(0.06948232662436016), np.float64(0.8086937299409291)), (np.float64(0.0717213928195924), np.float64(0.8762909612301694)), (np.float64(0.07295163323513174), np.float64(1.3519580240145317)), (np.float64(0.06616256588606821), np.float64(0.8412204194202784)), (np.float64(0.06773037699636465), np.float64(1.32651713934554)), (np.float64(0.06420923262534876), np.float64(0.83833856136882)), (np.float64(0.059902253798468384), np.float64(0.8728204970711384)), (np.float64(0.07100026403359838), np.float64(1.6249629974823823)), (np.float64(0.06542370589994158), np.float64(0.9503079964459306)), (np.float64(0.077565293125943), np.float64(1.2414656472438605)), (np.float64(0.06978776464400196), np.float64(1.2114894596840722)), (np.float64(0.07632105448141557), np.float64(1.559515095287698)), (np.float64(0.06861516529396819), np.float64(1.8349876239679712)), (np.float64(0.06410757085932141), np.float64(1.2021525755260922)), (np.float64(0.07476797650046867), np.float64(1.0720461366865017)), (np.float64(0.06693905963661448), np.float64(1.0993672771739715)), (np.float64(0.07207326587607121), np.float64(1.6025540712056914)), (np.float64(0.06982429786389206), np.float64(1.1581844073164476)), (np.float64(0.06472139845536418), np.float64(0.7132644833735178)), (np.float64(0.07135642967569711), np.float64(1.6605385500817758)), (np.float64(0.0697095112013049), np.float64(1.5573582929887453)), (np.float64(0.06801395017327538), np.float64(1.2704752645645594)), (np.float64(0.07154031415761773), np.float64(1.0331215008761145)), (np.float64(0.07162678035984155), np.float64(0.9460015990149412)), (np.float64(0.07251659660626078), np.float64(0.7972147554155109)), (np.float64(0.07263104681370058), np.float64(0.9188224940718039)), (np.float64(0.06850718321210894), np.float64(1.1069655374082825)), (np.float64(0.07332766251747427), np.float64(1.2774440253304)), (np.float64(0.07242490123253957), np.float64(1.0979237584589967)), (np.float64(0.06619452781514593), np.float64(1.2022454025460585)), (np.float64(0.06932524132372912), np.float64(1.0026279384652175)), (np.float64(0.07400012038354567), np.float64(0.9226841815393758)), (np.float64(0.07351458222680111), np.float64(0.8505781473847033)), (np.float64(0.07327534142547083), np.float64(1.488031222528746)), (np.float64(0.06248051216432621), np.float64(1.5785824548623806)), (np.float64(0.06451712682616129), np.float64(0.9769319303763977)), (np.float64(0.06931736648750238), np.float64(0.8212553829827796)), (np.float64(0.06616672872284014), np.float64(0.9857276311505498)), (np.float64(0.07596221840127505), np.float64(0.9326005366569454)), (np.float64(0.07306858632986293), np.float64(0.7188731093513541)), (np.float64(0.07317621139037381), np.float64(1.087887461917238)), (np.float64(0.07060649100360872), np.float64(0.9711196595130904)), (np.float64(0.06957599310862635), np.float64(1.4134231640889021)), (np.float64(0.062056908451466566), np.float64(1.2056343380954015)), (np.float64(0.0664663619484427), np.float64(1.2927852125120063)), (np.float64(0.07429993725787187), np.float64(1.2748224518430376)), (np.float64(0.07047424956264965), np.float64(0.8452157787838797)), (np.float64(0.06855001546811716), np.float64(1.0143960598232067)), (np.float64(0.06661999172215075), np.float64(1.2125714394600269)), (np.float64(0.0668826810863697), np.float64(1.1954266249487095)), (np.float64(0.06965436126602297), np.float64(1.2119371416780496)), (np.float64(0.06930177348725625), np.float64(1.0221407771442224)), (np.float64(0.074220575542967), np.float64(1.0632956700302665)), (np.float64(0.07582870088750518), np.float64(1.473075748151731)), (np.float64(0.07127733832764489), np.float64(0.9353799026681441)), (np.float64(0.07213109463298342), np.float64(1.608655788469307)), (np.float64(0.0702848052223285), np.float64(1.0123665177284829)), (np.float64(0.0745828642320324), np.float64(1.5801026821346484)), (np.float64(0.07144450164332297), np.float64(1.017198670247472)), (np.float64(0.07003078648727856), np.float64(0.9093863743743125)), (np.float64(0.07079647104226652), np.float64(0.8315695719334067)), (np.float64(0.07053769519717554), np.float64(1.3130070703618377)), (np.float64(0.06767522844501452), np.float64(1.0528382250275228)), (np.float64(0.06501033521695739), np.float64(0.8775888937869549)), (np.float64(0.06430090193703861), np.float64(0.969301586759309)), (np.float64(0.07040281541277721), np.float64(1.1858371280992057)), (np.float64(0.06424351409665133), np.float64(1.1483543622667804)), (np.float64(0.07174475629817159), np.float64(1.0971965814551392)), (np.float64(0.0683893540668935), np.float64(1.21694629348722)), (np.float64(0.06403267498662506), np.float64(0.9015955770482174)), (np.float64(0.0682600387541081), np.float64(1.4900142763798667)), (np.float64(0.06604753118874868), np.float64(0.7920290608095797)), (np.float64(0.0698504173702047), np.float64(1.3940101884322416)), (np.float64(0.06826551289942223), np.float64(1.6786229566730315)), (np.float64(0.07583201846350077), np.float64(1.1370804907196692)), (np.float64(0.06907638392177784), np.float64(0.8044939458374983)), (np.float64(0.06992845118702677), np.float64(0.9563076668710025)), (np.float64(0.07905587354056191), np.float64(1.1445113494251142)), (np.float64(0.07011329301504472), np.float64(1.7343590405068599)), (np.float64(0.07011857517740416), np.float64(1.4585028948956873)), (np.float64(0.06647684036876914), np.float64(0.6880602586633272)), (np.float64(0.06807964842036308), np.float64(1.2522271704493577)), (np.float64(0.06803774443821387), np.float64(1.1801265095380422)), (np.float64(0.06828356728369234), np.float64(0.9212965871315452)), (np.float64(0.062069275430953946), np.float64(1.0050407090125788)), (np.float64(0.07410745422323756), np.float64(1.172916096338045)), (np.float64(0.06959894488743089), np.float64(0.7551339936796904)), (np.float64(0.07089224882096036), np.float64(1.2914022598475192)), (np.float64(0.06917054985045329), np.float64(1.0549846456980025)), (np.float64(0.0715228362645028), np.float64(1.1626402519056391)), (np.float64(0.0731592814998753), np.float64(1.0257118627236441)), (np.float64(0.06635569255392929), np.float64(1.240038789321337)), (np.float64(0.07038524347495181), np.float64(0.8083062313323265)), (np.float64(0.07837219356488857), np.float64(1.2172725458484115)), (np.float64(0.0668883787395063), np.float64(1.2495542391320247)), (np.float64(0.0690590250139526), np.float64(0.8797366459133712)), (np.float64(0.06931233821770666), np.float64(1.0045346956508063)), (np.float64(0.07286819486868752), np.float64(0.9356431950827129)), (np.float64(0.07166238895303863), np.float64(0.9314290043053797)), (np.float64(0.07675604181307177), np.float64(1.5054385455700354)), (np.float64(0.07714721476089824), np.float64(1.3604664367680397)), (np.float64(0.07112463427511084), np.float64(1.0644357441409757)), (np.float64(0.07164916906294061), np.float64(0.9593436428762213)), (np.float64(0.07996540274760924), np.float64(1.0439684627353378)), (np.float64(0.07049733975296696), np.float64(0.9236336439197862)), (np.float64(0.06279313715446305), np.float64(0.938940801688492)), (np.float64(0.07534338369231451), np.float64(1.3748620736031623)), (np.float64(0.06812476279834635), np.float64(0.8277218855585448)), (np.float64(0.06415953000930774), np.float64(0.7550661400491404)), (np.float64(0.07835584040726905), np.float64(1.3500044169404317)), (np.float64(0.06983652024549593), np.float64(0.9431498840429486)), (np.float64(0.07407423737300785), np.float64(1.3231961789580593)), (np.float64(0.07003899275069943), np.float64(1.1641790399200778)), (np.float64(0.060638832214718066), np.float64(1.1517065361334888)), (np.float64(0.06475819208782738), np.float64(0.7398046945817984)), (np.float64(0.06760105139395836), np.float64(0.9418571967424714)), (np.float64(0.06650910052021311), np.float64(1.1358040156826588)), (np.float64(0.06818401377371633), np.float64(1.0997464284552958)), (np.float64(0.07043918507156391), np.float64(1.0622377809252683)), (np.float64(0.0696094580519149), np.float64(1.0670304440607419)), (np.float64(0.07066594662913843), np.float64(0.9978631832804186)), (np.float64(0.06507118684211186), np.float64(1.5671407800431747)), (np.float64(0.07730328358180219), np.float64(1.4864334750089099)), (np.float64(0.0726901449768168), np.float64(0.9435604385069872)), (np.float64(0.06432791327269041), np.float64(1.1847219584469784)), (np.float64(0.0727643395160806), np.float64(0.8675043223463653)), (np.float64(0.07824007546592421), np.float64(1.303299848047131)), (np.float64(0.07258428748504261), np.float64(1.431268880465661)), (np.float64(0.0653860063499574), np.float64(0.8174890770549855)), (np.float64(0.06015135252936229), np.float64(0.9969414588021221)), (np.float64(0.06987917692845105), np.float64(1.630450018037243)), (np.float64(0.06852435443093383), np.float64(1.123726829386725)), (np.float64(0.0747343616993029), np.float64(1.305671012640572)), (np.float64(0.06644727277842236), np.float64(1.1547309274310267)), (np.float64(0.065492644078649), np.float64(0.780578849793452)), (np.float64(0.0763217159630893), np.float64(1.1857817521335516)), (np.float64(0.06273358652598188), np.float64(0.8841384466479014)), (np.float64(0.06879036657813475), np.float64(1.603159898827538)), (np.float64(0.06472327995862984), np.float64(1.2067425183622673)), (np.float64(0.07208756468828406), np.float64(1.0633115208191106)), (np.float64(0.06946239764291355), np.float64(0.8261988372462127)), (np.float64(0.06502624857072047), np.float64(1.470236310940934)), (np.float64(0.07140023282859688), np.float64(1.3451980070213705)), (np.float64(0.08221485509311834), np.float64(1.2354452485406506)), (np.float64(0.06876591276677131), np.float64(0.7803505380198656)), (np.float64(0.07392339607509477), np.float64(0.7918649499084717)), (np.float64(0.0595582874576831), np.float64(0.8571221286791663)), (np.float64(0.07013474892871759), np.float64(0.8905643324261516)), (np.float64(0.06932422237735897), np.float64(1.3748682997104464)), (np.float64(0.06606814596640281), np.float64(0.8767881419046559)), (np.float64(0.06864329730742728), np.float64(0.811138754304365)), (np.float64(0.06683329162653137), np.float64(1.5304865238090237)), (np.float64(0.07150194265188187), np.float64(1.481390535530312)), (np.float64(0.07680532473488116), np.float64(1.24963840112208)), (np.float64(0.06692333067242204), np.float64(0.81363382971037)), (np.float64(0.06768833994023685), np.float64(1.2433003200345007)), (np.float64(0.06647328614589078), np.float64(0.7214422647167558)), (np.float64(0.07255981478020862), np.float64(0.8396319455255269)), (np.float64(0.06547164954851359), np.float64(0.8822215353875237)), (np.float64(0.07463887990152143), np.float64(1.7209550720162232)), (np.float64(0.06912464781247578), np.float64(1.253760705527008)), (np.float64(0.06839980867351309), np.float64(1.4578630271632285)), (np.float64(0.07130540646821823), np.float64(0.9430709054310695)), (np.float64(0.0645368090426927), np.float64(1.1689398617543623)), (np.float64(0.0685529021893772), np.float64(1.2419585559722661)), (np.float64(0.06994935695895889), np.float64(1.0312035898343725)), (np.float64(0.06452247656735487), np.float64(1.6269997811532597)), (np.float64(0.07439925326983017), np.float64(1.5862022186306857)), (np.float64(0.0751483026057743), np.float64(0.768929981916447)), (np.float64(0.07503740678235113), np.float64(0.9784582234845617)), (np.float64(0.06957112356482721), np.float64(1.5397651234364966)), (np.float64(0.06807667998642358), np.float64(1.0538803639802774)), (np.float64(0.06760102298202447), np.float64(0.9413760648368452)), (np.float64(0.07065522836255643), np.float64(1.0340727160646257)), (np.float64(0.07557510130437735), np.float64(1.6644868809752655)), (np.float64(0.06693844784414626), np.float64(0.9669151243829058)), (np.float64(0.06572942760295433), np.float64(0.7874912467038688)), (np.float64(0.0766799883301566), np.float64(1.2691554439666246)), (np.float64(0.07645241757041564), np.float64(1.1777393053700576)), (np.float64(0.06494783812483848), np.float64(1.2247691572074375)), (np.float64(0.06911882127117185), np.float64(1.0247989749137605)), (np.float64(0.07150810842212857), np.float64(0.7769466642456935)), (np.float64(0.07279005544960462), np.float64(1.050608416797782)), (np.float64(0.07382236242621294), np.float64(1.2244157384047254)), (np.float64(0.06826625417870874), np.float64(0.9925083606556249)), (np.float64(0.07126197585046463), np.float64(0.6981111249053332)), (np.float64(0.07226402924555896), np.float64(1.110099659693029)), (np.float64(0.07145641029986317), np.float64(1.0720299024951019)), (np.float64(0.06756400777214627), np.float64(0.7901056147976139)), (np.float64(0.06726563522626434), np.float64(0.8330325900182055)), (np.float64(0.07615389525725474), np.float64(1.4555520402682691)), (np.float64(0.06896519047721161), np.float64(1.211418422676484)), (np.float64(0.07126305467158782), np.float64(0.902615224242257)), (np.float64(0.060671125464285464), np.float64(1.4445580240971994)), (np.float64(0.0756704447361733), np.float64(1.1780458642036815)), (np.float64(0.07215048025917437), np.float64(1.389583891383796)), (np.float64(0.07104085897415355), np.float64(1.239183351776934)), (np.float64(0.07413778906030243), np.float64(1.0602369680674326)), (np.float64(0.07191389225197027), np.float64(1.8721738102595835)), (np.float64(0.07293452059302619), np.float64(1.1419399855688217)), (np.float64(0.06965454646225425), np.float64(1.3759876540739617)), (np.float64(0.07359772443406026), np.float64(1.3058525631481919)), (np.float64(0.07149123439439976), np.float64(0.9589265481143328)), (np.float64(0.06666200258425257), np.float64(0.8252081327736556)), (np.float64(0.07245542564545822), np.float64(1.2956614485387925)), (np.float64(0.07150267466334002), np.float64(1.5500333480753075)), (np.float64(0.06935811226615293), np.float64(1.1550230981396947)), (np.float64(0.06804058317278135), np.float64(1.2780502946460153)), (np.float64(0.07022364694247489), np.float64(0.917324133556118)), (np.float64(0.07291148102728456), np.float64(0.7028208885860937)), (np.float64(0.07017217559635062), np.float64(0.9840452671542599)), (np.float64(0.06514299760371425), np.float64(0.8414583377660383)), (np.float64(0.06774234052402194), np.float64(1.098608879719745)), (np.float64(0.07106675166146642), np.float64(0.9209765661331824)), (np.float64(0.06537630377595159), np.float64(1.267227486168705)), (np.float64(0.07134765316507052), np.float64(1.0906749515894167)), (np.float64(0.06925475760964689), np.float64(1.173471398475355)), (np.float64(0.07695670786863021), np.float64(1.2918293766954583)), (np.float64(0.069270560066485), np.float64(1.4742341514697959)), (np.float64(0.07708329457398115), np.float64(0.975233965266573)), (np.float64(0.07689864159887629), np.float64(1.1698081473561601)), (np.float64(0.07513057565355298), np.float64(1.4138100398040196)), (np.float64(0.07116365549343444), np.float64(1.1713640751425354)), (np.float64(0.07112781963920811), np.float64(1.1278866219636134)), (np.float64(0.06645347214315331), np.float64(0.9259460331851238)), (np.float64(0.07002556479420702), np.float64(1.6497673472120673)), (np.float64(0.07087857044619397), np.float64(1.2149655330108897)), (np.float64(0.07251778155526581), np.float64(0.8848365640604898)), (np.float64(0.06975956403100182), np.float64(0.9377258116866066)), (np.float64(0.07329469887603644), np.float64(1.7318014842314924)), (np.float64(0.06995998022895983), np.float64(0.7522270148024497)), (np.float64(0.0720970736193071), np.float64(1.0762915669532458)), (np.float64(0.0708711487472645), np.float64(0.673985802679366)), (np.float64(0.0708247309442971), np.float64(0.7399177298374566)), (np.float64(0.07850698620374202), np.float64(1.4723979493500465)), (np.float64(0.07672414639582543), np.float64(1.0060887216987724)), (np.float64(0.06649788267643174), np.float64(1.0578420449833752)), (np.float64(0.07124684262407661), np.float64(1.8408013084736723)), (np.float64(0.0783549916215229), np.float64(1.0228065528568346)), (np.float64(0.0729487716104357), np.float64(1.0277886937417051)), (np.float64(0.06405602051027406), np.float64(0.7431769612777931)), (np.float64(0.07291534236896366), np.float64(1.3844540462535297)), (np.float64(0.06916218755882417), np.float64(1.3983443031918388)), (np.float64(0.0658751258048706), np.float64(1.1496972595016042)), (np.float64(0.07163347410604247), np.float64(1.072612557729345)), (np.float64(0.06449918821781804), np.float64(0.9122914670174287)), (np.float64(0.06331682219340472), np.float64(0.8136265909176147)), (np.float64(0.06680462735687129), np.float64(1.0023644517832067)), (np.float64(0.07587120548470516), np.float64(1.0627341153193992)), (np.float64(0.07286152050882969), np.float64(1.1876309070006943)), (np.float64(0.06747788575175995), np.float64(0.956349464639812)), (np.float64(0.0741543797983402), np.float64(1.5322633847859555)), (np.float64(0.06719544562559128), np.float64(0.9904543288658438)), (np.float64(0.07012641180914324), np.float64(0.8599372111455998)), (np.float64(0.06856657346236875), np.float64(0.724116936709299)), (np.float64(0.0736322604849706), np.float64(1.205729643597334)), (np.float64(0.06810446695695233), np.float64(0.7149865671370611)), (np.float64(0.07284553870996112), np.float64(1.4027839846912031)), (np.float64(0.07223634730125794), np.float64(0.959787435041629)), (np.float64(0.06773131817285283), np.float64(1.5801002938028421)), (np.float64(0.07275375861833741), np.float64(1.0455405794273291)), (np.float64(0.06811854420157148), np.float64(1.2221387286724394)), (np.float64(0.07052994081878378), np.float64(1.7397303011230294)), (np.float64(0.06670861391420459), np.float64(1.0401266038473018)), (np.float64(0.07183119214978786), np.float64(1.3395198272825997)), (np.float64(0.07108172889646391), np.float64(0.9523071243060942)), (np.float64(0.06804313411543741), np.float64(0.7760521729456281)), (np.float64(0.07160817039851244), np.float64(1.2540073774329723)), (np.float64(0.06771884020575379), np.float64(1.4105032974416825)), (np.float64(0.0726448051095342), np.float64(1.2854505648142955)), (np.float64(0.06393277118978898), np.float64(0.8847454180140985)), (np.float64(0.07268537064407199), np.float64(1.2151417969958185)), (np.float64(0.06792500747952879), np.float64(0.7377163574411206)), (np.float64(0.06036131343053637), np.float64(0.9541802340574477)), (np.float64(0.07199523915130496), np.float64(0.8057746990606154)), (np.float64(0.07072223481903578), np.float64(1.6454619590811579)), (np.float64(0.06942311535358239), np.float64(0.8803221406585274)), (np.float64(0.07142741684956332), np.float64(1.5418918041972165)), (np.float64(0.07812872873169215), np.float64(1.4843446426317999)), (np.float64(0.06849801893677136), np.float64(0.9082464867782861)), (np.float64(0.06468271589107559), np.float64(0.8809644656666172)), (np.float64(0.06480021051478328), np.float64(1.1485930859598186)), (np.float64(0.06830308635557172), np.float64(1.351250047743405)), (np.float64(0.07557366150950717), np.float64(1.5892553376853258)), (np.float64(0.07423432254836769), np.float64(0.7459311311067096)), (np.float64(0.06615349930194546), np.float64(0.7037560245550297)), (np.float64(0.06505271664559986), np.float64(0.7975865975379248)), (np.float64(0.07177401661002783), np.float64(1.5190793506735454)), (np.float64(0.07219164739020198), np.float64(1.0793457402414957)), (np.float64(0.07163831446489698), np.float64(1.221483697445354)), (np.float64(0.0668580381204712), np.float64(0.9133049811060242)), (np.float64(0.07204564900823457), np.float64(1.1290301257133377)), (np.float64(0.07024766177680232), np.float64(1.0110870894558388)), (np.float64(0.07097585584064167), np.float64(0.8626898202365721)), (np.float64(0.06913722871164481), np.float64(0.9095069820863136)), (np.float64(0.06860153423473099), np.float64(1.3462809070698103)), (np.float64(0.07384933092982009), np.float64(1.0121159212133686)), (np.float64(0.0756718463322184), np.float64(0.9394592956241239)), (np.float64(0.06866977400084515), np.float64(1.1925886371395296)), (np.float64(0.06650927529711062), np.float64(1.0770110260547767)), (np.float64(0.07342895192056156), np.float64(0.8708002434759573)), (np.float64(0.0701882934997602), np.float64(1.5684814810366405)), (np.float64(0.0696043049296837), np.float64(1.4549226023800748)), (np.float64(0.07085563798023091), np.float64(1.0252602619002742)), (np.float64(0.06812761536042461), np.float64(1.4353899980460896)), (np.float64(0.07160914705957885), np.float64(1.15642453422671)), (np.float64(0.07643035528246479), np.float64(1.129775841489804)), (np.float64(0.06983718923298415), np.float64(1.3186783435888318)), (np.float64(0.07549076327641482), np.float64(1.472736673429594)), (np.float64(0.07447749828393752), np.float64(1.47673231831374)), (np.float64(0.05675909010379673), np.float64(0.97825252678996)), (np.float64(0.07330309240728465), np.float64(1.5380735412729885)), (np.float64(0.07012401689897653), np.float64(0.9744998750212331)), (np.float64(0.07347507186256991), np.float64(1.3629369059255665)), (np.float64(0.06903538084733621), np.float64(1.048516638224626)), (np.float64(0.07625489355333424), np.float64(1.1928419787273539)), (np.float64(0.06513575068200608), np.float64(1.1595735862829912)), (np.float64(0.07559938200356013), np.float64(1.372828784159418)), (np.float64(0.06840753069367546), np.float64(1.8712363706476929)), (np.float64(0.0652544435413058), np.float64(1.0784393260650804)), (np.float64(0.07140907970087867), np.float64(0.9454583181843536)), (np.float64(0.06966130643127266), np.float64(0.7524313281185195)), (np.float64(0.07693315525269254), np.float64(1.209113555713689)), (np.float64(0.0704563893947504), np.float64(1.1577212969405153)), (np.float64(0.068548770105774), np.float64(1.3133751297232705)), (np.float64(0.06827127074164124), np.float64(1.052672223313335)), (np.float64(0.07448361233527943), np.float64(1.1323533085125015)), (np.float64(0.06525288596426784), np.float64(1.0349637424986202)), (np.float64(0.06582684034600982), np.float64(0.8343472638904104)), (np.float64(0.06462224486637551), np.float64(0.8420345969529782)), (np.float64(0.0687846791194021), np.float64(0.9088279682508617)), (np.float64(0.06766017726293681), np.float64(0.8065354002732102)), (np.float64(0.06441760712021471), np.float64(1.1495020988515665)), (np.float64(0.06975957828324238), np.float64(1.2634830867978357)), (np.float64(0.07300672081760706), np.float64(1.3235597222666573)), (np.float64(0.06556601454017931), np.float64(1.1931906062781634)), (np.float64(0.06955212016423502), np.float64(1.29898332948969)), (np.float64(0.07222983480204576), np.float64(1.2163967469707113)), (np.float64(0.06845026346436575), np.float64(1.5382248748683092)), (np.float64(0.06861573514541325), np.float64(1.2454726116354908)), (np.float64(0.0713528465516976), np.float64(1.0544473832734698)), (np.float64(0.06558836473464376), np.float64(0.9597374725355939)), (np.float64(0.06898254723101273), np.float64(1.0741950314996453)), (np.float64(0.07170715335605168), np.float64(1.2404951572811203)), (np.float64(0.06305888385137683), np.float64(1.6481320693582293)), (np.float64(0.0772709084694283), np.float64(1.2861576807427288)), (np.float64(0.07842297072892108), np.float64(1.7631359863389766)), (np.float64(0.06746230520124921), np.float64(1.5256890664080394)), (np.float64(0.0621988685363741), np.float64(1.0756111035251101)), (np.float64(0.06957132631729626), np.float64(0.80632405644205)), (np.float64(0.06570964879148425), np.float64(1.049888089305937)), (np.float64(0.0682795016582084), np.float64(0.7497308954319638)), (np.float64(0.06842461981609085), np.float64(1.1950125324121448)), (np.float64(0.06919407961305428), np.float64(1.0725529652193733)), (np.float64(0.06813656142432406), np.float64(0.9684117750881251)), (np.float64(0.06677785651950317), np.float64(1.5566760471249488)), (np.float64(0.0729699864631002), np.float64(0.9761296869851535)), (np.float64(0.0690690618055686), np.float64(1.465918887191611)), (np.float64(0.07223956640987148), np.float64(1.1138689212953805)), (np.float64(0.0749862119550034), np.float64(1.1501779956476954)), (np.float64(0.06868994529360942), np.float64(0.967450692790928)), (np.float64(0.06281958044061128), np.float64(1.4505773618820623)), (np.float64(0.08094814855880271), np.float64(1.1476887285156345)), (np.float64(0.07137416179475672), np.float64(1.3483643150749345)), (np.float64(0.07228042058182849), np.float64(1.0584132373672157)), (np.float64(0.06949708540823757), np.float64(1.312554797578749)), (np.float64(0.07192791774611818), np.float64(0.9360671177901153)), (np.float64(0.07694152635766224), np.float64(2.0276707388780415)), (np.float64(0.07113968142495072), np.float64(1.0732628494634588)), (np.float64(0.07021523649346653), np.float64(1.1823264994099434)), (np.float64(0.07185983055681314), np.float64(1.4098404160643685)), (np.float64(0.06297565551538514), np.float64(0.9323285248781631)), (np.float64(0.0666861647444304), np.float64(1.1573639367424444)), (np.float64(0.060392687915414335), np.float64(1.2268380557679213)), (np.float64(0.06798554290860105), np.float64(1.0505809284083187)), (np.float64(0.06555204939184142), np.float64(1.1201137930008203)), (np.float64(0.06372204953328013), np.float64(0.8962040355345405)), (np.float64(0.06257873213262785), np.float64(1.2218628868772299)), (np.float64(0.07060089011640229), np.float64(0.7143628362945502)), (np.float64(0.06921890517270558), np.float64(0.860027186561662)), (np.float64(0.06897484821539522), np.float64(1.1024906524279126)), (np.float64(0.07022459455032316), np.float64(1.4051670415166928)), (np.float64(0.07051527491269254), np.float64(1.165351554347992)), (np.float64(0.06473450922958557), np.float64(0.7922916408520955)), (np.float64(0.06908289415447601), np.float64(0.764677158928441)), (np.float64(0.07116645805500126), np.float64(1.693791866359366)), (np.float64(0.06861404098377605), np.float64(0.9159615241415742)), (np.float64(0.0668221119812509), np.float64(1.1804980845175375)), (np.float64(0.07830939850724167), np.float64(1.2189515547237806)), (np.float64(0.06336106279066506), np.float64(1.239760664723636)), (np.float64(0.06400285537807106), np.float64(0.849909835393347)), (np.float64(0.06862784683704753), np.float64(1.1041158846685177)), (np.float64(0.07121738250077853), np.float64(0.9469811774784801)), (np.float64(0.07821525526111808), np.float64(1.0631607719497533)), (np.float64(0.07824185323275464), np.float64(1.5410021170368635)), (np.float64(0.0723817721575758), np.float64(0.8553185537094806)), (np.float64(0.06708920341412815), np.float64(0.8437967606491613)), (np.float64(0.06866445380760472), np.float64(0.7083291322985337)), (np.float64(0.07648132495425458), np.float64(1.3786091235843854)), (np.float64(0.07249278185680838), np.float64(1.1263867223134771)), (np.float64(0.06963112296372378), np.float64(0.750371583333916)), (np.float64(0.07062011095976088), np.float64(1.1345584523713113)), (np.float64(0.06238660334205334), np.float64(1.0719097208814505)), (np.float64(0.07034630266315242), np.float64(0.979545673239974)), (np.float64(0.06777725116236985), np.float64(1.3033817886679266)), (np.float64(0.07204111388343928), np.float64(1.3059185442588481)), (np.float64(0.07113945540994804), np.float64(1.3686238690623647)), (np.float64(0.07010063080989654), np.float64(1.6062591578371697)), (np.float64(0.0731874949497199), np.float64(1.2370493809792018)), (np.float64(0.07094435469642682), np.float64(1.0691023170593539)), (np.float64(0.07006484077813502), np.float64(1.2210969056470709)), (np.float64(0.06865681695071482), np.float64(1.1544346971040478)), (np.float64(0.0687186152986398), np.float64(2.0340004222482255)), (np.float64(0.07862374265795782), np.float64(1.119428251515041)), (np.float64(0.06527225347405963), np.float64(1.033253567878989)), (np.float64(0.07193821675944918), np.float64(1.0938431058600628)), (np.float64(0.07077066173327842), np.float64(1.1346498833216705)), (np.float64(0.0576862166701645), np.float64(1.0849191518623953)), (np.float64(0.07272329462502723), np.float64(1.1209984854964643)), (np.float64(0.06577112073638737), np.float64(0.83404157469564)), (np.float64(0.06107419894674125), np.float64(0.9336592159688497)), (np.float64(0.07143089316807626), np.float64(1.0200881763519862)), (np.float64(0.07081014823417274), np.float64(1.329493815913117)), (np.float64(0.07565768115722624), np.float64(1.0909200552580014)), (np.float64(0.07325356231524094), np.float64(1.0525409673167876)), (np.float64(0.07261150128849983), np.float64(1.6309314377578832)), (np.float64(0.07129456283296268), np.float64(1.4616958957756312)), (np.float64(0.07126253349608486), np.float64(1.3616006270680863)), (np.float64(0.0697013191958306), np.float64(0.9229289732072974)), (np.float64(0.06708280145685923), np.float64(1.3507211647674275)), (np.float64(0.07189350894971334), np.float64(0.9509051552259541)), (np.float64(0.061888333362407746), np.float64(0.7798670711666876)), (np.float64(0.07097176808239289), np.float64(0.9682764685158751)), (np.float64(0.06504386271446991), np.float64(1.337507447404663)), (np.float64(0.06879935993279013), np.float64(1.166278395678878)), (np.float64(0.06849653511166343), np.float64(0.8298257873398286)), (np.float64(0.06936737548380842), np.float64(1.047696784665255)), (np.float64(0.06769339139230512), np.float64(0.7960991612235527)), (np.float64(0.07513556161409916), np.float64(1.310327222425036)), (np.float64(0.07524393568678164), np.float64(1.384733000609835)), (np.float64(0.06774912899824706), np.float64(1.2526854832951577)), (np.float64(0.07496099410035235), np.float64(0.7068259280798872)), (np.float64(0.06772687652305434), np.float64(1.1781161711957615)), (np.float64(0.06965769237836883), np.float64(1.044492104652814)), (np.float64(0.07471017690648984), np.float64(0.9566871891960322)), (np.float64(0.07679137676162481), np.float64(1.3131191493819494)), (np.float64(0.07462249772406254), np.float64(1.4908442987231099)), (np.float64(0.0774431789421191), np.float64(1.2245802057391635)), (np.float64(0.07352331621597985), np.float64(0.691050343405062)), (np.float64(0.0681716824340284), np.float64(1.1345020904069432)), (np.float64(0.07022170179678223), np.float64(0.9273190185470233)), (np.float64(0.07084780120340349), np.float64(1.2248308520449462)), (np.float64(0.06804062793941755), np.float64(1.181179655626235)), (np.float64(0.06724615321857938), np.float64(1.1192160656614338)), (np.float64(0.06907724040485698), np.float64(1.1396160062085243)), (np.float64(0.06554112387052476), np.float64(0.7017800742283014)), (np.float64(0.06773828278655192), np.float64(0.994177634925908)), (np.float64(0.07766994900122456), np.float64(1.4121774678612282)), (np.float64(0.07021323551117631), np.float64(1.0870597686740968)), (np.float64(0.07451807982539159), np.float64(1.2908203020496882)), (np.float64(0.06906483025728707), np.float64(1.5204020525980797)), (np.float64(0.07482538635264162), np.float64(1.4259821907260728)), (np.float64(0.07150940391722038), np.float64(0.7924288632084526)), (np.float64(0.0628953863330574), np.float64(0.7361865525537753)), (np.float64(0.07511056254766256), np.float64(1.1418780350933155)), (np.float64(0.07006488050999911), np.float64(0.9707679832518346)), (np.float64(0.06921035761426295), np.float64(1.2491992778873593)), (np.float64(0.06577234045764066), np.float64(1.343589449964692)), (np.float64(0.06632207352791217), np.float64(1.1978493011800986)), (np.float64(0.06777193431441593), np.float64(0.8273819982124689)), (np.float64(0.06901430219230013), np.float64(1.3765400915165187)), (np.float64(0.06675417809854825), np.float64(1.1177004110179762)), (np.float64(0.06742397671153087), np.float64(1.0639683746380566)), (np.float64(0.07354619859329549), np.float64(1.4465261900134812)), (np.float64(0.06891778140852119), np.float64(1.0296559206782157)), (np.float64(0.07532020932905953), np.float64(0.7345816202482935)), (np.float64(0.07007505332178594), np.float64(0.8056690040062952)), (np.float64(0.061896875965624064), np.float64(1.255538069950852)), (np.float64(0.0668672730944316), np.float64(0.8644942565570354)), (np.float64(0.07757198369623942), np.float64(1.4609309456141908)), (np.float64(0.0717046148982957), np.float64(0.8308519206486046)), (np.float64(0.07076908450674266), np.float64(1.0106726383838243)), (np.float64(0.07258510188134348), np.float64(0.7798172216500796)), (np.float64(0.06951392035605668), np.float64(1.0871820001431969)), (np.float64(0.06501389850408151), np.float64(1.2608371199312947)), (np.float64(0.0704378307109942), np.float64(0.8452420613396876)), (np.float64(0.06576440370203572), np.float64(0.6911319674522879)), (np.float64(0.06447455331096244), np.float64(1.2601894424771591)), (np.float64(0.07112367467518667), np.float64(0.786415721937628)), (np.float64(0.07193311786580385), np.float64(0.8538463584819411)), (np.float64(0.07325612616506122), np.float64(1.3307831057509978)), (np.float64(0.06508210461868538), np.float64(0.9205475036791088)), (np.float64(0.060925465616529596), np.float64(0.8195129670508196)), (np.float64(0.06407391034238383), np.float64(0.9774881824666263)), (np.float64(0.06878020134548271), np.float64(1.1803477485275042)), (np.float64(0.06222007417878066), np.float64(0.768416130583005)), (np.float64(0.06901438002060742), np.float64(0.8200665221942318)), (np.float64(0.07683244627817971), np.float64(1.3986463678766472)), (np.float64(0.07446760710327023), np.float64(1.0670491978468863)), (np.float64(0.0694436671586222), np.float64(1.060703866189836)), (np.float64(0.07298382320943478), np.float64(0.7978686357297373)), (np.float64(0.08005827287710227), np.float64(1.3397163648437351)), (np.float64(0.07226189018610542), np.float64(1.2348419380528983)), (np.float64(0.06675042242589505), np.float64(1.5954125879616066)), (np.float64(0.06928601882060625), np.float64(0.8102613788580049)), (np.float64(0.07420861359582136), np.float64(1.0664795187754452)), (np.float64(0.06682076964335913), np.float64(1.2511385532043613)), (np.float64(0.07531782762768796), np.float64(1.3470734546840204)), (np.float64(0.07002032588856708), np.float64(0.8492384681460643)), (np.float64(0.06542737520470879), np.float64(0.9747503813230438)), (np.float64(0.06712584171728442), np.float64(0.9249843268225778)), (np.float64(0.0723419457602866), np.float64(1.3151815318896771)), (np.float64(0.06430168108970391), np.float64(0.7178999960551478)), (np.float64(0.0750446298006178), np.float64(1.421979835764238)), (np.float64(0.06962520068483954), np.float64(0.9847106086616197)), (np.float64(0.06732085611861523), np.float64(1.5626593646946547)), (np.float64(0.07300623773887464), np.float64(1.1325359178380028)), (np.float64(0.07173098139528217), np.float64(0.9552615249995998)), (np.float64(0.06976003499934741), np.float64(0.7590206863111016)), (np.float64(0.07199385498803222), np.float64(1.578199026460568)), (np.float64(0.06329484234680688), np.float64(1.0573050640805834)), (np.float64(0.0705599344113449), np.float64(1.0495632939302482)), (np.float64(0.08042587780701224), np.float64(1.1392962093412675)), (np.float64(0.06803206761040048), np.float64(0.9633980197167806)), (np.float64(0.0706388096371128), np.float64(0.8433364810789034)), (np.float64(0.06612741450009815), np.float64(0.8197617408148586)), (np.float64(0.07347689527063017), np.float64(1.5765784050534253)), (np.float64(0.06487145304852013), np.float64(1.2114959522110165)), (np.float64(0.07792112998249076), np.float64(1.3292151216102688)), (np.float64(0.06645322842298616), np.float64(1.096172307335078)), (np.float64(0.07499788422554603), np.float64(1.1963512295434637)), (np.float64(0.06774469413030962), np.float64(1.260763673288304)), (np.float64(0.06586921759849731), np.float64(1.2462910327816463)), (np.float64(0.07152063819740234), np.float64(0.8580394071444584)), (np.float64(0.08096192608815472), np.float64(1.1733231243334152)), (np.float64(0.07631052470564594), np.float64(1.1504639546988171)), (np.float64(0.06890183986050036), np.float64(0.9592971826777337)), (np.float64(0.07433887913651813), np.float64(1.034276024697693)), (np.float64(0.06581615526003724), np.float64(1.3622976205854829)), (np.float64(0.06703187600999616), np.float64(1.392852002682601)), (np.float64(0.07189127861747042), np.float64(0.6709354312257978)), (np.float64(0.06720611446601896), np.float64(0.9681061400941194)), (np.float64(0.0740866169539196), np.float64(1.1986571634897625)), (np.float64(0.06659706681025868), np.float64(1.1310848374237128)), (np.float64(0.08043782105790218), np.float64(1.6704875160622186)), (np.float64(0.07373936789129301), np.float64(0.8943091991423539)), (np.float64(0.06944094546500269), np.float64(0.9289928633024324)), (np.float64(0.06399766482807771), np.float64(1.3030809371442122)), (np.float64(0.07685238969372221), np.float64(1.024155676995126)), (np.float64(0.07303562021753975), np.float64(1.2706015581899257)), (np.float64(0.07652931169357527), np.float64(1.3681932695846792)), (np.float64(0.06928518451467904), np.float64(1.0632817522268647)), (np.float64(0.06597647964968131), np.float64(0.950866910168878)), (np.float64(0.06813652158190939), np.float64(1.1815741476555262)), (np.float64(0.07024508578712224), np.float64(1.1998564550683095)), (np.float64(0.07184964786233863), np.float64(1.1053992744665766)), (np.float64(0.06524374720525966), np.float64(1.1004736959334622)), (np.float64(0.07172951183279964), np.float64(0.9494106204458729)), (np.float64(0.07483532948702984), np.float64(1.16793685228082)), (np.float64(0.06858344920287983), np.float64(0.9311080046562699)), (np.float64(0.07312960525594883), np.float64(1.4184620534034962)), (np.float64(0.07050417763672451), np.float64(0.8762623890944958)), (np.float64(0.06989094255061756), np.float64(0.7769075612595218)), (np.float64(0.06962628760073156), np.float64(0.9821933421858494)), (np.float64(0.06524649618755826), np.float64(1.132651366359918)), (np.float64(0.06936559295722651), np.float64(0.9304304402570119)), (np.float64(0.0645537571164567), np.float64(0.8205754812185977)), (np.float64(0.07162796022414888), np.float64(0.8663221331775316)), (np.float64(0.07239811077788635), np.float64(0.7390101684550617)), (np.float64(0.07740952547790303), np.float64(1.3243278190205614)), (np.float64(0.07134122682817189), np.float64(0.9796214568586215)), (np.float64(0.07766957921053579), np.float64(0.9322195822320797)), (np.float64(0.067103068841104), np.float64(0.9330587368891023)), (np.float64(0.07034897802157973), np.float64(1.689904178597065)), (np.float64(0.06143628164801865), np.float64(0.8186658039677991)), (np.float64(0.07542953098658106), np.float64(1.326126712903435)), (np.float64(0.06625418509010272), np.float64(1.112689768829853)), (np.float64(0.06596645249115013), np.float64(0.9841012123014675)), (np.float64(0.07090496400385997), np.float64(1.5130245098136392)), (np.float64(0.06544950803853003), np.float64(0.9145858934686103)), (np.float64(0.06491594652505273), np.float64(0.9550864273412384)), (np.float64(0.06976790506090534), np.float64(0.9277378351158845)), (np.float64(0.06775227294086798), np.float64(1.2190374901618803)), (np.float64(0.07147460783752689), np.float64(1.1346378263179924)), (np.float64(0.07077092904382776), np.float64(1.3758854119602144)), (np.float64(0.07319991166151739), np.float64(0.8089258080601152)), (np.float64(0.0721872322546019), np.float64(1.117317331369344)), (np.float64(0.06986786387913677), np.float64(1.4258492545785946)), (np.float64(0.07033527941542779), np.float64(0.93853287859929)), (np.float64(0.07130066927692938), np.float64(1.2920783482086207)), (np.float64(0.06880543071302175), np.float64(1.3341723273689228)), (np.float64(0.06967547608172261), np.float64(1.0147634176173013)), (np.float64(0.06670310905205105), np.float64(1.204242506952665)), (np.float64(0.07330281084243026), np.float64(1.2278875294102476)), (np.float64(0.0648091052763156), np.float64(1.9744244853047748)), (np.float64(0.065455295790462), np.float64(0.7278632727273792)), (np.float64(0.07612397869678185), np.float64(1.2451307425241382)), (np.float64(0.0749614458688548), np.float64(1.466065009517624)), (np.float64(0.06747646100132783), np.float64(1.1027013265213284)), (np.float64(0.06929593118471007), np.float64(0.8967383578191263)), (np.float64(0.06678940487348085), np.float64(1.1197689604213787)), (np.float64(0.06370802252818411), np.float64(1.087727971999949)), (np.float64(0.0704015529842564), np.float64(1.4199383714575304)), (np.float64(0.06873958927775661), np.float64(1.0360809872381305)), (np.float64(0.0793057261944824), np.float64(1.4360274492544378)), (np.float64(0.06659208713328577), np.float64(1.0128744405147214)), (np.float64(0.07198238540745751), np.float64(1.0213265093573471)), (np.float64(0.06715044481376208), np.float64(0.9756449596823795)), (np.float64(0.06902117451534223), np.float64(1.3930829900613237)), (np.float64(0.06973919269930437), np.float64(0.7771893954055262)), (np.float64(0.0701212684425342), np.float64(1.0361143428522739)), (np.float64(0.06333247763202765), np.float64(0.9138468047788462)), (np.float64(0.07007813802696328), np.float64(1.1841387742529266)), (np.float64(0.07249277061952314), np.float64(1.2040674498190218)), (np.float64(0.0699229318742981), np.float64(1.2672826651286486)), (np.float64(0.06150271511997474), np.float64(1.4542023557492376)), (np.float64(0.06993561387099038), np.float64(0.9694783499090898)), (np.float64(0.06760842583444486), np.float64(1.5931042037067715)), (np.float64(0.06632979253646744), np.float64(0.9512478344486316)), (np.float64(0.06977363167242272), np.float64(1.3167441304777803)), (np.float64(0.07055441578718914), np.float64(1.6359840709862992)), (np.float64(0.06996663741470252), np.float64(1.428870350670788)), (np.float64(0.07071022677416604), np.float64(1.8393126718540491)), (np.float64(0.07189730115113352), np.float64(0.8443224180402873)), (np.float64(0.07069009608369024), np.float64(1.3791024092961914)), (np.float64(0.07411649330538676), np.float64(1.7292005039067777)), (np.float64(0.0759878915053279), np.float64(1.4266389506312933)), (np.float64(0.07321342310033352), np.float64(0.7441029911332062)), (np.float64(0.07091132664950359), np.float64(1.1071678276938062)), (np.float64(0.06757449625367021), np.float64(1.0403853625411712)), (np.float64(0.07215120367320536), np.float64(1.1792927777688509)), (np.float64(0.07854979887786466), np.float64(1.0579234692160173)), (np.float64(0.0720972737052091), np.float64(1.049834796153664)), (np.float64(0.07514510427295763), np.float64(1.0461929221145583)), (np.float64(0.07052785122721994), np.float64(1.1707043146257576)), (np.float64(0.07380336721519344), np.float64(1.628151217291937)), (np.float64(0.06429167182350268), np.float64(0.8180424856248856)), (np.float64(0.0677086192919508), np.float64(1.3135623316371565)), (np.float64(0.07204531707931067), np.float64(1.3002472227893722)), (np.float64(0.07194918059956386), np.float64(0.7627086592880555)), (np.float64(0.07120515532582714), np.float64(1.0144440911900916)), (np.float64(0.07276826443794361), np.float64(1.0987911464661888)), (np.float64(0.07811323323367567), np.float64(1.1951266813827794)), (np.float64(0.06437575071224312), np.float64(1.1426351876616598)), (np.float64(0.06959645796907471), np.float64(0.8925175527035121)), (np.float64(0.07317343552183957), np.float64(1.3758284048816467)), (np.float64(0.06452444534467768), np.float64(0.764586938885129)), (np.float64(0.07146539769637664), np.float64(0.9098384626916463)), (np.float64(0.06852129056752336), np.float64(0.7964266280283313)), (np.float64(0.06854344572704325), np.float64(1.1202629388031806)), (np.float64(0.07453267579582096), np.float64(1.0187279194542245)), (np.float64(0.0692761145094234), np.float64(1.0337577119230743)), (np.float64(0.06650832142263075), np.float64(1.2627626931141223)), (np.float64(0.068967878948688), np.float64(0.9107559603426583)), (np.float64(0.07581256329051371), np.float64(1.4302933448564474)), (np.float64(0.06859505681032732), np.float64(1.087842028932174)), (np.float64(0.07597673944600483), np.float64(1.3623538817641903)), (np.float64(0.06595060275131079), np.float64(1.3300360937291387)), (np.float64(0.07331200527797642), np.float64(1.3112821168868631)), (np.float64(0.07058964750734419), np.float64(1.6654612258600239)), (np.float64(0.06840587377731375), np.float64(1.1359586028952826)), (np.float64(0.06312829587191579), np.float64(0.7860084547890772)), (np.float64(0.07152365809808421), np.float64(0.9680367798889656)), (np.float64(0.0757576217570857), np.float64(1.1985317718494484)), (np.float64(0.07401575225680881), np.float64(1.282591209709678)), (np.float64(0.06298229807974695), np.float64(1.2070314609896378)), (np.float64(0.0758852166427818), np.float64(1.2254443341731112)), (np.float64(0.07094141435022744), np.float64(0.8484504155739508)), (np.float64(0.06753526034339194), np.float64(1.6387553819655953)), (np.float64(0.07328683277436929), np.float64(1.2512923133202103)), (np.float64(0.06771825127472576), np.float64(1.2108704425708925)), (np.float64(0.07139723123855167), np.float64(1.1540697837886467)), (np.float64(0.06055327380364617), np.float64(0.8746542711809582)), (np.float64(0.06726030728849212), np.float64(1.4218985806035087)), (np.float64(0.06675521138368598), np.float64(0.984274637245785)), (np.float64(0.06786463861875937), np.float64(0.8660401747477585)), (np.float64(0.0661727575667509), np.float64(1.1080657258744757)), (np.float64(0.0743325757386816), np.float64(1.4417358736534924)), (np.float64(0.06343270807945026), np.float64(1.7033506530530074)), (np.float64(0.06971496653999179), np.float64(0.8442611207489206)), (np.float64(0.06950905375617815), np.float64(1.3084606376572492)), (np.float64(0.06712040835076737), np.float64(1.3386162611364842)), (np.float64(0.07052597986436038), np.float64(1.2855092967803952)), (np.float64(0.07269020510665543), np.float64(1.4446566660606288)), (np.float64(0.0658955793530115), np.float64(0.8154116918330417)), (np.float64(0.07911533660230845), np.float64(0.9218430460745143)), (np.float64(0.0676338308379413), np.float64(1.2747201188914201)), (np.float64(0.06870387560668477), np.float64(0.9449788258243577)), (np.float64(0.06808310692033298), np.float64(0.8283735831281177)), (np.float64(0.06711703630706203), np.float64(1.6007863085482854)), (np.float64(0.07273411937311981), np.float64(0.707807097641261)), (np.float64(0.07446694152388583), np.float64(1.2148808028189713)), (np.float64(0.06542425433505165), np.float64(1.3015382663612076)), (np.float64(0.07124999663693887), np.float64(1.0284004247531782)), (np.float64(0.07205524562038003), np.float64(1.6211756849970347)), (np.float64(0.07171792251362465), np.float64(1.4642420938110168)), (np.float64(0.06411969017413921), np.float64(0.8980654691826552)), (np.float64(0.06817222148057125), np.float64(1.7580236576749515)), (np.float64(0.0697977861631259), np.float64(0.6812099304005389)), (np.float64(0.06738036374851147), np.float64(0.9438638239892343)), (np.float64(0.0685203632048083), np.float64(1.5310332847798631)), (np.float64(0.06814410176763816), np.float64(0.7318866236382001)), (np.float64(0.06456306063322906), np.float64(1.7069869702985523)), (np.float64(0.07601392640195012), np.float64(1.4141311508261034)), (np.float64(0.06840598007770277), np.float64(1.386114381425427)), (np.float64(0.07336352994235887), np.float64(1.8450116900764533)), (np.float64(0.07179557210461986), np.float64(1.0876136289206733)), (np.float64(0.07091774055248612), np.float64(1.2327452938233867)), (np.float64(0.0797455984854043), np.float64(1.2739416724236516)), (np.float64(0.07293010078924875), np.float64(1.425184250082746)), (np.float64(0.06791027041097383), np.float64(0.9965101397595768)), (np.float64(0.07023448223896786), np.float64(1.5672092585632702)), (np.float64(0.06963599831285132), np.float64(1.051587527720182)), (np.float64(0.06612604004794546), np.float64(1.2303263076003184)), (np.float64(0.06657771649299163), np.float64(0.8541330628973637)), (np.float64(0.06690245623811904), np.float64(0.9583699237766565)), (np.float64(0.0717412383450894), np.float64(1.9520999363858205)), (np.float64(0.07887741311383306), np.float64(1.0784841429836578)), (np.float64(0.0758002265123556), np.float64(1.5312841988354813)), (np.float64(0.06941956512374445), np.float64(0.9591584248203535)), (np.float64(0.07088386135753229), np.float64(0.917588080147384)), (np.float64(0.06668680516346503), np.float64(0.9330696259073353)), (np.float64(0.06855272358945788), np.float64(1.0723060665493689)), (np.float64(0.06967114610681065), np.float64(1.025774650428794)), (np.float64(0.0829797194037645), np.float64(1.4026279942066486)), (np.float64(0.07425811541936093), np.float64(1.4039386310323245)), (np.float64(0.07267369356636696), np.float64(1.3846582638762053)), (np.float64(0.06675427547167302), np.float64(0.9903250753548419)), (np.float64(0.0710682575208792), np.float64(0.9161168638015339)), (np.float64(0.06315926542070548), np.float64(0.9494989319476582)), (np.float64(0.07519153694356095), np.float64(1.5260711981872306)), (np.float64(0.06811814105417), np.float64(0.9658185024431869)), (np.float64(0.06698221805534987), np.float64(1.0613318789533852)), (np.float64(0.07760649598253588), np.float64(1.6394741504039354)), (np.float64(0.07222889930447722), np.float64(0.8360386890856465)), (np.float64(0.06867707234374495), np.float64(1.060515706886362)), (np.float64(0.06627199340895729), np.float64(0.8903785479595032)), (np.float64(0.07363148150005677), np.float64(1.1876575850993374)), (np.float64(0.07528634310835995), np.float64(1.5606437692488846)), (np.float64(0.07197612065102679), np.float64(1.3835539447668728)), (np.float64(0.07077457162298045), np.float64(1.0756930724984972)), (np.float64(0.06490823477256615), np.float64(1.0500654703583776)), (np.float64(0.07392684175149361), np.float64(1.4234537176206656)), (np.float64(0.07192252654924894), np.float64(0.99119642343238)), (np.float64(0.06798002390011863), np.float64(1.1438815384459973)), (np.float64(0.06614295577848699), np.float64(0.8015117471683013)), (np.float64(0.0672228836965064), np.float64(0.7594934701521282)), (np.float64(0.07226147240832172), np.float64(1.0059891039183846)), (np.float64(0.07091470906327625), np.float64(0.6867686825387789)), (np.float64(0.06356934760888928), np.float64(1.0963229908983076)), (np.float64(0.07031829176521456), np.float64(0.861028215518527)), (np.float64(0.07141577307260036), np.float64(1.4236711586563733)), (np.float64(0.07075565802562889), np.float64(1.4642327228753536)), (np.float64(0.06221923592550598), np.float64(0.8370559023262366)), (np.float64(0.06700830280288483), np.float64(0.7582473899133946)), (np.float64(0.06578200245462973), np.float64(1.3578186911948211)), (np.float64(0.06824451429520542), np.float64(1.3131878845051157)), (np.float64(0.06672123297736525), np.float64(1.1839735000947644)), (np.float64(0.06725090177705784), np.float64(0.9334455323603937)), (np.float64(0.06867030792172797), np.float64(0.9107266005519311)), (np.float64(0.07205335500887294), np.float64(0.8314702016351325)), (np.float64(0.06426726304954827), np.float64(0.816453870969259)), (np.float64(0.07345645803721171), np.float64(1.2171431048266717)), (np.float64(0.07490691902676137), np.float64(0.7997816976540042)), (np.float64(0.06941593833246376), np.float64(1.104095805839148)), (np.float64(0.07208532552653214), np.float64(1.282792745384135)), (np.float64(0.07318438982355589), np.float64(1.1704026806455037)), (np.float64(0.0717074255001713), np.float64(1.4875706787822942)), (np.float64(0.06790285268750168), np.float64(0.9393074842639367)), (np.float64(0.06680615762468443), np.float64(0.8993723908119395)), (np.float64(0.06597318975914063), np.float64(0.9890815980557021)), (np.float64(0.0690419623154846), np.float64(1.03496774665929)), (np.float64(0.0698705415632327), np.float64(1.2294325931874746)), (np.float64(0.07773037574101643), np.float64(1.1616032689281854)), (np.float64(0.06982319197804282), np.float64(1.1979797861879036)), (np.float64(0.060912105876498854), np.float64(1.1656460377338151)), (np.float64(0.06159737165639542), np.float64(1.068726052838909)), (np.float64(0.06194671619633474), np.float64(0.906648478859732)), (np.float64(0.07402739227376484), np.float64(1.5615086343427018)), (np.float64(0.0621737007550843), np.float64(0.8507668810532163)), (np.float64(0.07325646411725074), np.float64(1.2466155177455618)), (np.float64(0.07350335519828087), np.float64(1.364294061127667)), (np.float64(0.06283804338524511), np.float64(1.3902006508723113)), (np.float64(0.07161512046820065), np.float64(0.7997187655159865)), (np.float64(0.07079765791391501), np.float64(0.9362716788153476)), (np.float64(0.07472630579978659), np.float64(1.2310060986697136)), (np.float64(0.06594669252751571), np.float64(0.8346956479721993)), (np.float64(0.07858192236481165), np.float64(1.4862568353745684)), (np.float64(0.07081476195722675), np.float64(1.660229696841064)), (np.float64(0.06617276407586434), np.float64(0.8117934676670735)), (np.float64(0.07158459387537339), np.float64(1.3685584115433125)), (np.float64(0.06625695848063047), np.float64(0.7994377779697458)), (np.float64(0.059637082392258706), np.float64(1.2328506419704566)), (np.float64(0.06715244145822313), np.float64(0.9406336913687464)), (np.float64(0.0673294739177091), np.float64(1.147060641090704)), (np.float64(0.07857563098765487), np.float64(1.304802875800529)), (np.float64(0.07242657466963709), np.float64(0.9194976879168568)), (np.float64(0.07074753051981786), np.float64(1.167511015544918)), (np.float64(0.06743884677692555), np.float64(1.0726005534689786)), (np.float64(0.06881360233952802), np.float64(0.8449091419188491)), (np.float64(0.07114475660071931), np.float64(1.1793000243837337)), (np.float64(0.06212199214447621), np.float64(0.7925490309993812)), (np.float64(0.06791681305958779), np.float64(0.8436285311744618)), (np.float64(0.07203076729956101), np.float64(0.8316752622406366)), (np.float64(0.0642955867790099), np.float64(1.156082045725051)), (np.float64(0.07210535952647146), np.float64(1.3150703041421354)), (np.float64(0.06627440776349651), np.float64(1.0481909817862973)), (np.float64(0.06331186219649176), np.float64(0.8261238346845449)), (np.float64(0.06222336616569754), np.float64(0.9220464693864028)), (np.float64(0.07028554118499776), np.float64(0.9990807415793026)), (np.float64(0.0790592615342956), np.float64(1.3440129495762576)), (np.float64(0.06724808942814901), np.float64(1.0673508706185089)), (np.float64(0.06431973937784687), np.float64(1.1685731589961572)), (np.float64(0.07178612901758798), np.float64(1.0337102478956015)), (np.float64(0.06950784596029655), np.float64(1.5471443564985687)), (np.float64(0.07521106210318776), np.float64(1.248765812891249)), (np.float64(0.06680731201508099), np.float64(0.784056481592558)), (np.float64(0.07383637715594185), np.float64(1.1173678783267784)), (np.float64(0.06334935877765827), np.float64(1.3363349195574001)), (np.float64(0.0643469849160522), np.float64(0.7963254358041334)), (np.float64(0.06565477577639182), np.float64(1.1334274267371005)), (np.float64(0.07260281797609064), np.float64(1.3633324858167073)), (np.float64(0.06538160329378119), np.float64(0.9235887603917942)), (np.float64(0.06791495286959491), np.float64(1.611530912386071)), (np.float64(0.06115639036740046), np.float64(2.0408386958623415)), (np.float64(0.06415706286011993), np.float64(1.2886835066514586)), (np.float64(0.07178805483274052), np.float64(0.9317610858783415)), (np.float64(0.06496531472821078), np.float64(1.2877151569518368)), (np.float64(0.07584677688342176), np.float64(1.384859507308874)), (np.float64(0.06706366623770173), np.float64(0.7637213299288705)), (np.float64(0.06800240585109867), np.float64(1.0289859479175218)), (np.float64(0.0692765794425565), np.float64(0.9476433720357205)), (np.float64(0.0665887425527982), np.float64(1.1830376308088895)), (np.float64(0.0723994769686136), np.float64(0.7602451622157953)), (np.float64(0.07172699877733064), np.float64(1.1384772514434125)), (np.float64(0.0631086254784551), np.float64(1.1471114399642508)), (np.float64(0.06469678576915149), np.float64(1.493238876329648)), (np.float64(0.06282883943713424), np.float64(1.1262657484756922)), (np.float64(0.07768687670980717), np.float64(1.243451662013133)), (np.float64(0.07397137028192156), np.float64(1.2840500210437527)), (np.float64(0.07493470864979962), np.float64(1.3726506616847909)), (np.float64(0.06654547358197227), np.float64(0.8037166364128847)), (np.float64(0.0674399125605742), np.float64(0.8883603623350517)), (np.float64(0.0729522080101592), np.float64(1.400179952370823)), (np.float64(0.06847634168456891), np.float64(0.9789075995311216)), (np.float64(0.07734824495002404), np.float64(1.1741542711385098)), (np.float64(0.07430906352703014), np.float64(1.4530570954672042)), (np.float64(0.06587755674335088), np.float64(1.0850618072680391)), (np.float64(0.06769354945979218), np.float64(1.007826098329962)), (np.float64(0.07113328271586304), np.float64(0.9455886067407829)), (np.float64(0.06772772485381395), np.float64(1.7271638704571277)), (np.float64(0.06890493504577766), np.float64(1.2511269692866585)), (np.float64(0.0722125856905171), np.float64(1.2255982106124719)), (np.float64(0.07885869761708254), np.float64(1.2882658732625185)), (np.float64(0.06303595491720453), np.float64(1.0585558921722629)), (np.float64(0.0744430331006078), np.float64(1.136021241717237)), (np.float64(0.07262854626359898), np.float64(1.0031309208752053)), (np.float64(0.0674042045135664), np.float64(0.9297221293644857)), (np.float64(0.07501379891847977), np.float64(1.1533271258286781)), (np.float64(0.0681207379119158), np.float64(1.291766378099819)), (np.float64(0.06226520123472361), np.float64(0.9386218347448445)), (np.float64(0.0703807538690026), np.float64(1.0914601315346635)), (np.float64(0.06150197627943897), np.float64(1.1847857518196245)), (np.float64(0.07426096379040581), np.float64(1.1321637516743464)), (np.float64(0.07297010867699857), np.float64(0.9764291189986335)), (np.float64(0.07093343496432647), np.float64(1.0723283089324571)), (np.float64(0.07065972144321506), np.float64(1.3922348532372353)), (np.float64(0.07352492141610564), np.float64(0.9058306767857583)), (np.float64(0.0721928372919207), np.float64(1.1169213554765784)), (np.float64(0.06980603386295003), np.float64(1.1328697964107868)), (np.float64(0.07004755271176681), np.float64(1.0581522764543545)), (np.float64(0.06351361443996062), np.float64(1.5967503955396376)), (np.float64(0.07568017148356446), np.float64(1.6525970213637444)), (np.float64(0.06818674746777), np.float64(1.000388600001439)), (np.float64(0.07057819872617004), np.float64(1.53622583373296)), (np.float64(0.07120962031072918), np.float64(1.1583453216402972)), (np.float64(0.0707835947603383), np.float64(0.868364716159044)), (np.float64(0.06410779330684102), np.float64(0.9504972108347952)), (np.float64(0.07148254578467379), np.float64(0.9838507676373897)), (np.float64(0.07316184553942623), np.float64(1.2119851875088103)), (np.float64(0.07611742165166491), np.float64(1.2824426928334556)), (np.float64(0.07099250700117148), np.float64(1.04053170648073)), (np.float64(0.07045218998258325), np.float64(1.0937415615498434)), (np.float64(0.06954916703979566), np.float64(1.0016204259157084)), (np.float64(0.06811512386997456), np.float64(0.9879006688262546)), (np.float64(0.0710151698232629), np.float64(1.4955665312421442)), (np.float64(0.0691844774877723), np.float64(0.9161227538407704)), (np.float64(0.07181511209173262), np.float64(1.2387235503825578)), (np.float64(0.06465185351228198), np.float64(1.1173309861171579)), (np.float64(0.0647577108378176), np.float64(0.725882476370014)), (np.float64(0.06594160271776703), np.float64(1.1882797954438102)), (np.float64(0.06646164210294464), np.float64(1.366314545656092)), (np.float64(0.06688899523031962), np.float64(1.009268196006973)), (np.float64(0.07042877609670008), np.float64(1.7013452033184895)), (np.float64(0.060838358437571405), np.float64(1.204908267848789)), (np.float64(0.06561741108211425), np.float64(0.923782885117186)), (np.float64(0.06315628297619416), np.float64(1.2801585041421075)), (np.float64(0.06966674193843979), np.float64(1.1683294716112225)), (np.float64(0.06714134268047268), np.float64(0.718986379463062)), (np.float64(0.07117315496971986), np.float64(1.091636086434767)), (np.float64(0.0641613711657106), np.float64(1.0372643317314272)), (np.float64(0.06893647678089414), np.float64(1.1138510180580776)), (np.float64(0.07218409804621072), np.float64(1.0038832190034042)), (np.float64(0.06962401786322679), np.float64(1.2561871324125002)), (np.float64(0.0746885201185282), np.float64(1.321955053877825)), (np.float64(0.07343535288822393), np.float64(1.0931283052972571)), (np.float64(0.0632017517002078), np.float64(0.7560096108868215)), (np.float64(0.06747823547020945), np.float64(1.1453829060073226)), (np.float64(0.07034895658738653), np.float64(0.8733702104372945)), (np.float64(0.06838444938742053), np.float64(1.3929009420404423)), (np.float64(0.07066460377286204), np.float64(1.7613757580435414)), (np.float64(0.07000875795336954), np.float64(1.3121772133096403)), (np.float64(0.07024935763123712), np.float64(1.5581329210145136)), (np.float64(0.06745194947707772), np.float64(0.7581006519962209)), (np.float64(0.07076026557320489), np.float64(1.1748410232356603)), (np.float64(0.06980030738176182), np.float64(0.8934562417788053)), (np.float64(0.06883715373820942), np.float64(1.5048257367105022))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cvxpy as cp\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "# Function for Stochastic Gradient Descent (SGD)\n",
        "def stochastic_gradient_descent(X, y, lr=0.01, epochs=100):\n",
        "    weights = np.random.randn(X.shape[1])\n",
        "    for epoch in range(epochs):\n",
        "        predictions = np.dot(X, weights)\n",
        "        error = predictions - y\n",
        "        gradient = np.dot(X.T, error) / len(y)\n",
        "        weights -= lr * gradient  # Update weights\n",
        "    return weights\n",
        "\n",
        "# Generate synthetic classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_redundant=1, n_classes=2)\n",
        "\n",
        "# Use SGD to optimize portfolio weights\n",
        "portfolio_weights_sgd = stochastic_gradient_descent(X, y)\n",
        "print(\"Optimized Portfolio Weights using SGD:\", portfolio_weights_sgd)\n",
        "\n",
        "# ElasticNet Regularization Adjustment\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.3)\n",
        "elastic_net.fit(X, y)\n",
        "optimized_weights_elasticnet = elastic_net.coef_\n",
        "print(\"ElasticNet Optimized Portfolio Weights:\", optimized_weights_elasticnet)\n",
        "\n",
        "# Stress Test Calibration\n",
        "def stress_test_simulation():\n",
        "    shocks = []\n",
        "    for _ in range(1000):  # Simulate 1000 stress tests\n",
        "        market_shock = np.random.normal(0, 0.05)  # Simulated market shock (returns)\n",
        "        risk_factor = np.random.normal(0, 0.1)  # Simulated risk factor\n",
        "        shocks.append((market_shock, risk_factor))\n",
        "    return np.array(shocks)\n",
        "\n",
        "# Run stress test simulation\n",
        "stress_test_results = stress_test_simulation()\n",
        "returns, risks = stress_test_results[:, 0], stress_test_results[:, 1]\n",
        "\n",
        "# Plotting stress test results\n",
        "plt.scatter(risks, returns, alpha=0.5)\n",
        "plt.title('Stress Test Results: Portfolio Returns vs. Risk')\n",
        "plt.xlabel('Risk')\n",
        "plt.ylabel('Return')\n",
        "plt.show()\n",
        "\n",
        "# Adding Constraints for Portfolio Diversification\n",
        "# Imposing constraint that no asset has more than 30% weight\n",
        "weights = cp.Variable(X.shape[1])\n",
        "\n",
        "# Reshape returns to make it a column vector (1000, 1)\n",
        "returns_reshaped = np.reshape(returns, (-1, 1))  # Reshape to a column vector\n",
        "\n",
        "# Define the problem: maximize portfolio returns under the given constraints\n",
        "objective = cp.Maximize(cp.sum(weights))  # Objective: Maximize the sum of the weights (this can be adjusted)\n",
        "constraints = [cp.sum(weights) == 1, weights >= 0, weights <= 0.3]  # Constraints: sum of weights = 1, each weight <= 0.3\n",
        "\n",
        "# Define the problem\n",
        "portfolio_problem = cp.Problem(objective, constraints)\n",
        "portfolio_problem.solve()\n",
        "\n",
        "# Extract optimized portfolio weights\n",
        "optimized_weights_constraints = weights.value\n",
        "print(\"Optimized Portfolio Weights with Constraints:\", optimized_weights_constraints)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "yHEQKXZ0GbIt",
        "outputId": "1fca138c-7470-44d9-c080-bcd363c56fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights using SGD: [-0.39858667  0.60494858 -0.41000629  0.51114709 -0.06144064]\n",
            "ElasticNet Optimized Portfolio Weights: [ 0.          0.          0.0723713   0.23003262 -0.0154617 ]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights with Constraints: [0.2 0.2 0.2 0.2 0.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Example dataset generation (replace with your own returns data)\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, random_state=42)\n",
        "\n",
        "# 1. Refining ElasticNet Model\n",
        "# Goal: Fine-tune the regularization parameters (alpha and l1_ratio)\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.1, 0.3, 0.5, 0.7, 1]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "grid_search.fit(X, y)\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 2. Handling Negative Weights in SGD\n",
        "# Goal: Avoid short positions or ensure negative weights reflect market conditions\n",
        "# SGD optimization (simulate asset returns and portfolio risk)\n",
        "def stochastic_gradient_descent(X, y, lr=0.01, epochs=100):\n",
        "    weights = np.random.randn(X.shape[1])\n",
        "    for epoch in range(epochs):\n",
        "        gradient = -2 * X.T @ (y - X @ weights) / len(y)\n",
        "        weights -= lr * gradient\n",
        "    return weights\n",
        "\n",
        "portfolio_weights_sgd = stochastic_gradient_descent(X, y, lr=0.01, epochs=100)\n",
        "portfolio_weights_sgd = np.clip(portfolio_weights_sgd, 0, None)  # Force non-negative weights\n",
        "print(\"Optimized Portfolio Weights using SGD (non-negative):\", portfolio_weights_sgd)\n",
        "\n",
        "# 3. Advanced Constraints for Portfolio Diversification\n",
        "# Goal: Add correlation constraints to avoid concentrated risk\n",
        "# Example constraint: Ensure no asset has a correlation > 0.7 with another\n",
        "correlation_matrix = np.corrcoef(X.T)  # X is your asset returns matrix\n",
        "max_correlation = 0.7  # Maximum allowed correlation between assets\n",
        "\n",
        "# CVXPY Model with correlation constraint\n",
        "weights = cp.Variable(X.shape[1])\n",
        "returns = np.mean(X, axis=0)  # Assume returns is the average of X\n",
        "constraints = [cp.sum(weights) == 1, weights >= 0, weights <= 0.3]\n",
        "constraints.append(cp.sum(weights.T @ correlation_matrix @ weights) <= max_correlation)\n",
        "\n",
        "# Define the optimization problem\n",
        "objective = cp.Maximize(returns @ weights)  # Objective: Maximize returns\n",
        "portfolio_problem = cp.Problem(objective, constraints)\n",
        "portfolio_problem.solve()\n",
        "\n",
        "print(\"Optimized Portfolio Weights with Constraints:\", weights.value)\n",
        "\n",
        "# 4. Incorporate Risk Management Techniques (e.g., CVaR)\n",
        "# Goal: Minimize extreme losses using Conditional Value-at-Risk (CVaR)\n",
        "cvar = cp.Variable()\n",
        "portfolio_returns = returns @ weights\n",
        "objective = cp.Maximize(portfolio_returns - cvar)  # Maximize returns while controlling risk\n",
        "\n",
        "# Simplified example for CVaR calculation\n",
        "portfolio_problem = cp.Problem(objective, constraints)\n",
        "portfolio_problem.solve()\n",
        "\n",
        "print(\"Optimized Portfolio with CVaR:\", weights.value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfn8O6zaHINp",
        "outputId": "5c05d625-bc45-4551-e53b-d3e16b03418f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.1}\n",
            "Optimized Portfolio Weights using SGD (non-negative): [0.42629833 0.17327328 0.         0.         0.        ]\n",
            "Optimized Portfolio Weights with Constraints: [2.99999999e-01 1.00000000e-01 3.00000000e-01 5.11722758e-11\n",
            " 3.00000000e-01]\n",
            "Optimized Portfolio with CVaR: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from scipy.optimize import dual_annealing\n",
        "\n",
        "# Example data (you should replace with your actual data)\n",
        "X = np.random.rand(1000, 5)  # 1000 samples, 5 assets (asset returns)\n",
        "returns = np.random.randn(1000)  # Random returns for simplicity\n",
        "\n",
        "# 1. Refining CVaR Calculation\n",
        "# Calculate the CVaR (simplified version)\n",
        "alpha = 0.05  # 5% confidence level\n",
        "var = np.percentile(returns, 100 * alpha)  # Value at risk\n",
        "cvar = np.mean(returns[returns < var])  # Conditional value at risk\n",
        "\n",
        "# Debug: Print out some useful information for CVaR calculation\n",
        "print(f\"CVaR Calculation: Alpha={alpha}, VaR={var}, CVaR={cvar}\")\n",
        "\n",
        "# Define the optimization problem\n",
        "weights = cp.Variable(X.shape[1])  # Portfolio weights\n",
        "\n",
        "# Ensure returns are reshaped as a column vector (to avoid dimension mismatch)\n",
        "returns_reshaped = returns.reshape(-1, 1)  # Make returns a column vector for matrix multiplication\n",
        "\n",
        "# Objective: Maximize returns while minimizing CVaR\n",
        "# Calculate portfolio returns as a dot product (returns and weights)\n",
        "portfolio_returns = X @ weights  # Matrix multiplication for portfolio returns\n",
        "\n",
        "# Now correctly subtract cvar (as a constant)\n",
        "objective = cp.Maximize(cp.sum(portfolio_returns) - cvar)  # Maximize portfolio returns minus CVaR\n",
        "\n",
        "# Constraints: sum(weights) == 1, weights >= 0 (no short-selling)\n",
        "constraints = [cp.sum(weights) == 1, weights >= 0]\n",
        "portfolio_problem = cp.Problem(objective, constraints)\n",
        "\n",
        "# Debug: Check the objective and constraints\n",
        "print(\"Objective Function:\", objective)\n",
        "print(\"Constraints:\", constraints)\n",
        "\n",
        "portfolio_problem.solve()\n",
        "\n",
        "print(\"Optimized Portfolio with CVaR:\", weights.value)\n",
        "\n",
        "# 2. Incorporating More Constraints for Diversification\n",
        "# Define correlation matrix for asset returns\n",
        "correlation_matrix = np.corrcoef(X.T)  # X is your assets returns matrix\n",
        "max_correlation = 0.7  # Maximum allowed correlation between assets\n",
        "\n",
        "# Add correlation constraint in the problem\n",
        "correlation_constraint = cp.sum(weights.T @ correlation_matrix @ weights) <= max_correlation\n",
        "constraints.append(correlation_constraint)\n",
        "\n",
        "# Solve the updated portfolio problem\n",
        "portfolio_problem = cp.Problem(objective, constraints)\n",
        "portfolio_problem.solve()\n",
        "\n",
        "print(\"Optimized Portfolio with Diversification Constraints:\", weights.value)\n",
        "\n",
        "# 3. Tuning ElasticNet Parameters using GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.1, 0.3, 0.5, 0.7, 1]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "grid_search.fit(X, returns)  # Fit the model using grid search\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 4. Testing More Advanced Optimization Methods: Simulated Annealing\n",
        "\n",
        "# Define objective function for Simulated Annealing\n",
        "def objective_function(weights):\n",
        "    # Calculate portfolio returns using dot product\n",
        "    portfolio_returns = np.dot(weights, X.T)  # This gives the portfolio returns based on weights\n",
        "    return -np.sum(portfolio_returns)  # Maximize portfolio returns (simplified)\n",
        "\n",
        "# Constraints on asset weights (e.g., 0 to 30% per asset)\n",
        "bounds = [(0, 0.3) for _ in range(X.shape[1])]  # Constraints for each asset weight\n",
        "\n",
        "# Run Simulated Annealing\n",
        "result = dual_annealing(objective_function, bounds)\n",
        "print(\"Optimized Portfolio Weights using Simulated Annealing:\", result.x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fCS4vnwJP3W",
        "outputId": "98b7c97f-ead5-4f26-824a-e50239fe9062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CVaR Calculation: Alpha=0.05, VaR=-1.5730004284379593, CVaR=-2.015538592878152\n",
            "Objective Function: maximize Sum([[0.33 0.37 ... 0.34 0.88]\n",
            " [0.44 0.17 ... 0.85 0.16]\n",
            " ...\n",
            " [0.61 0.05 ... 0.78 0.59]\n",
            " [0.46 0.10 ... 0.44 0.97]] @ var5161, None, False) + --2.015538592878152\n",
            "Constraints: [Equality(Expression(AFFINE, UNKNOWN, ()), Constant(CONSTANT, NONNEGATIVE, ())), Inequality(Constant(CONSTANT, ZERO, ()))]\n",
            "Optimized Portfolio with CVaR: [1.14091265e-10 1.60076168e-10 9.99999999e-01 8.65526545e-11\n",
            " 1.60429315e-10]\n",
            "Optimized Portfolio with Diversification Constraints: [ 1.19586486e-08  9.82430144e-02  8.30405427e-01 -3.64381267e-09\n",
            "  7.13515505e-02]\n",
            "Best ElasticNet parameters: {'alpha': 0.1, 'l1_ratio': 0.3}\n",
            "Optimized Portfolio Weights using Simulated Annealing: [0.3 0.3 0.3 0.3 0.3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cvxpy as cp\n",
        "import numpy as np\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scipy.optimize import dual_annealing\n",
        "\n",
        "# Simulating some random data for demonstration\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(1000, 5)  # Asset returns matrix (1000 samples, 5 assets)\n",
        "returns = np.random.randn(1000)  # Portfolio returns (for CVaR)\n",
        "\n",
        "# 1. Refining the CVaR Calculation\n",
        "# Calculate CVaR (simplified version)\n",
        "alpha = 0.05  # 5% confidence level\n",
        "portfolio_returns = np.dot(X, np.random.randn(X.shape[1]))  # Simulated portfolio returns\n",
        "var = np.percentile(portfolio_returns, 100 * alpha)  # Value at Risk\n",
        "cvar = np.mean(portfolio_returns[portfolio_returns < var])  # Conditional VaR\n",
        "\n",
        "# 2. Improving Portfolio Diversification\n",
        "# Define the optimization problem\n",
        "weights = cp.Variable(X.shape[1])  # Portfolio weights\n",
        "\n",
        "# Maximum weight constraint for each asset (no asset should exceed 25%)\n",
        "max_weight_constraint = weights <= 0.25\n",
        "\n",
        "# 3. ElasticNet Tuning\n",
        "# ElasticNet GridSearch to tune parameters\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.0, 0.1, 0.3, 0.5, 0.7, 1.0]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "grid_search.fit(X, returns)\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 4. Simulated Annealing Optimization\n",
        "# Define objective function for Simulated Annealing\n",
        "def objective_function(weights):\n",
        "    portfolio_returns = np.dot(weights, X.T)  # Correct portfolio returns\n",
        "    return -np.sum(portfolio_returns)  # Maximize portfolio returns (simplified)\n",
        "\n",
        "# Constraints on asset weights\n",
        "bounds = [(0, 0.25) for _ in range(X.shape[1])]  # No more than 25% for any asset\n",
        "\n",
        "# Run Simulated Annealing\n",
        "result = dual_annealing(objective_function, bounds)\n",
        "print(\"Optimized Portfolio Weights using Simulated Annealing:\", result.x)\n",
        "\n",
        "# 5. Refine Optimization with Robust Techniques\n",
        "# Using CVXPY to model the problem\n",
        "\n",
        "# Calculate the portfolio returns by using the asset returns matrix and portfolio weights\n",
        "portfolio_returns = cp.sum(X @ weights)  # Correct portfolio return formula\n",
        "\n",
        "# Constraints: sum(weights) == 1, weights >= 0 (no short-selling)\n",
        "constraints = [cp.sum(weights) == 1, weights >= 0]\n",
        "objective = cp.Maximize(portfolio_returns - cvar)  # Maximize returns while minimizing CVaR\n",
        "\n",
        "portfolio_problem = cp.Problem(objective, constraints)\n",
        "portfolio_problem.solve()\n",
        "\n",
        "print(\"Optimized Portfolio with CVaR:\", weights.value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0Gr-OxEJoiP",
        "outputId": "1dc52b1c-06ab-4206-8131-b63a603b1577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best ElasticNet parameters: {'alpha': 0.1, 'l1_ratio': 0.7}\n",
            "Optimized Portfolio Weights using Simulated Annealing: [0.25 0.   0.25 0.25 0.25]\n",
            "Optimized Portfolio with CVaR: [3.50128369e-11 3.38128370e-11 3.58855938e-11 3.66720582e-11\n",
            " 1.00000000e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scipy.optimize import dual_annealing\n",
        "from pyswarm import pso\n",
        "\n",
        "# Simulate asset returns data: 1000 samples (time periods), 5 assets\n",
        "X = np.random.randn(1000, 5)  # 1000 time periods, 5 assets\n",
        "weights = np.random.rand(5)  # Random asset weights for illustration\n",
        "weights = weights / np.sum(weights)  # Normalize weights to sum to 1\n",
        "\n",
        "# 1. Refining the Portfolio Diversification\n",
        "# Maximum weight constraint for each asset (no asset should exceed 25%)\n",
        "def diversification_constraints(weights):\n",
        "    max_weight = 0.25\n",
        "    if np.any(weights > max_weight):\n",
        "        return np.sum(np.maximum(0, weights - max_weight))  # Penalty for exceeding max weight\n",
        "    return 0\n",
        "\n",
        "# Correlation constraint\n",
        "correlation_matrix = np.corrcoef(X.T)  # Compute correlation matrix\n",
        "max_correlation = 0.7  # Maximum allowed correlation between assets\n",
        "\n",
        "# 2. Refining the CVaR Calculation (Fixing strict inequality)\n",
        "alpha = 0.05  # 5% confidence level for CVaR calculation\n",
        "portfolio_returns = np.dot(X, weights)  # Calculate portfolio returns by multiplying asset returns with weights\n",
        "\n",
        "# Debugging Output\n",
        "print(\"Shape of portfolio_returns:\", portfolio_returns.shape)\n",
        "\n",
        "# Calculate Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR)\n",
        "print(f\"Calculating CVaR with alpha={alpha}...\")\n",
        "var = np.percentile(portfolio_returns, 100 * alpha)  # 5% VaR\n",
        "cvar = np.mean(portfolio_returns[portfolio_returns <= var])  # CVaR (mean of losses greater than VaR)\n",
        "\n",
        "# Debugging Output\n",
        "print(f\"Value-at-Risk (VaR): {var}\")\n",
        "print(f\"Conditional Value-at-Risk (CVaR): {cvar}\")\n",
        "\n",
        "# 3. Optimize for Risk-Adjusted Return: Sharpe ratio\n",
        "# Portfolio return (weighted sum of asset returns)\n",
        "portfolio_return = np.sum(portfolio_returns)  # Simplified portfolio return formula\n",
        "print(f\"Portfolio return: {portfolio_return}\")\n",
        "\n",
        "# Portfolio volatility (from the covariance matrix of asset returns)\n",
        "cov_matrix = np.cov(X.T)  # Covariance matrix of asset returns\n",
        "portfolio_volatility = np.sqrt(np.dot(np.dot(weights.T, cov_matrix), weights))  # Portfolio volatility calculation\n",
        "print(f\"Portfolio volatility: {portfolio_volatility}\")\n",
        "\n",
        "sharpe_ratio = portfolio_return / portfolio_volatility  # Sharpe ratio (risk-adjusted return)\n",
        "print(f\"Sharpe ratio: {sharpe_ratio}\")\n",
        "\n",
        "# Objective function to optimize both returns and CVaR (while considering diversification)\n",
        "def objective_function(weights):\n",
        "    portfolio_returns = np.dot(X, weights)  # Portfolio returns\n",
        "    var = np.percentile(portfolio_returns, 100 * alpha)  # Value-at-Risk\n",
        "    cvar = np.mean(portfolio_returns[portfolio_returns <= var])  # CVaR (tail loss)\n",
        "\n",
        "    # Add penalties for constraints\n",
        "    diversification_penalty = diversification_constraints(weights)\n",
        "\n",
        "    return -portfolio_return + cvar + diversification_penalty  # Maximize return and minimize CVaR\n",
        "\n",
        "# Constraints on asset weights (no short-selling and maximum weight per asset)\n",
        "lb = [0] * X.shape[1]  # Lower bounds (no short-selling)\n",
        "ub = [0.25] * X.shape[1]  # Upper bounds (maximum 25% per asset)\n",
        "\n",
        "# 4. Run Simulated Annealing (SA)\n",
        "result_sa = dual_annealing(objective_function, bounds=list(zip(lb, ub)))\n",
        "print(\"Optimized Portfolio Weights using Simulated Annealing:\", result_sa.x)\n",
        "\n",
        "# 5. Run Particle Swarm Optimization (PSO)\n",
        "optimal_weights, _ = pso(objective_function, lb, ub)\n",
        "print(\"Optimized Portfolio Weights using PSO:\", optimal_weights)\n",
        "\n",
        "# 6. ElasticNet Tuning (Grid Search)\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.0, 0.1, 0.3, 0.5, 0.7, 1.0]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "grid_search.fit(X, portfolio_returns)  # Now using portfolio_returns instead of asset returns\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW_4r4zoKg4y",
        "outputId": "9909d134-a7bb-42d3-fc67-24ca14a6ab54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of portfolio_returns: (1000,)\n",
            "Calculating CVaR with alpha=0.05...\n",
            "Value-at-Risk (VaR): -0.7809120464617729\n",
            "Conditional Value-at-Risk (CVaR): -0.9441634047939746\n",
            "Portfolio return: -7.438148868758207\n",
            "Portfolio volatility: 0.45189097094216896\n",
            "Sharpe ratio: -16.460051975037377\n",
            "Optimized Portfolio Weights using Simulated Annealing: [0.25 0.25 0.25 0.25 0.25]\n",
            "Stopping search: maximum iterations reached --> 100\n",
            "Optimized Portfolio Weights using PSO: [0.25 0.25 0.25 0.25 0.25]\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from scipy.optimize import dual_annealing\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Sample Data: Replace X (assets returns) and portfolio_returns with actual data\n",
        "X = np.random.rand(1000, 5)  # 1000 samples, 5 assets\n",
        "portfolio_returns = np.random.randn(1000)  # Simplified portfolio returns\n",
        "\n",
        "# Initialize variables for portfolio weights and other optimization parameters\n",
        "weights = cp.Variable(X.shape[1])  # 5 assets\n",
        "\n",
        "# Bounds for Simulated Annealing and PSO (limit each asset weight between 0 and 0.25)\n",
        "lb = [0] * X.shape[1]  # Lower bounds (no short-selling)\n",
        "ub = [0.25] * X.shape[1]  # Upper bounds (maximum 25% per asset)\n",
        "\n",
        "# 1. Refining Portfolio Optimization\n",
        "# Simulated Annealing Optimization with better settings\n",
        "def objective_function(weights):\n",
        "    return -np.sum(np.dot(weights, X.T))  # Maximize portfolio returns (simplified)\n",
        "\n",
        "result_sa = dual_annealing(objective_function, bounds=list(zip(lb, ub)), maxiter=1000, no_local_search=False)\n",
        "print(\"Optimized Portfolio Weights using Simulated Annealing:\", result_sa.x)\n",
        "\n",
        "# PSO Optimization with better settings\n",
        "optimal_weights, _ = pso(objective_function, lb, ub, maxiter=1000)\n",
        "print(\"Optimized Portfolio Weights using PSO:\", optimal_weights)\n",
        "\n",
        "# 2. Refining Risk Management\n",
        "# Sharpe Ratio Optimization: Incorporate both return and volatility to maximize risk-adjusted returns\n",
        "portfolio_return = np.sum(np.dot(X, optimal_weights))  # Simplified portfolio return formula\n",
        "cov_matrix = np.cov(X.T)  # Covariance matrix of asset returns\n",
        "portfolio_volatility = np.sqrt(np.dot(np.dot(optimal_weights.T, cov_matrix), optimal_weights))\n",
        "\n",
        "sharpe_ratio = portfolio_return / portfolio_volatility\n",
        "print(f\"Sharpe ratio: {sharpe_ratio}\")\n",
        "\n",
        "# Refined CVaR calculation\n",
        "alpha = 0.05\n",
        "portfolio_returns = np.dot(X, optimal_weights)  # Portfolio returns\n",
        "var = np.percentile(portfolio_returns, 100 * alpha)  # Value-at-Risk (VaR)\n",
        "cvar = np.mean(portfolio_returns[portfolio_returns <= var])  # CVaR (mean of losses greater than VaR)\n",
        "\n",
        "print(f\"Value-at-Risk (VaR): {var}\")\n",
        "print(f\"Conditional Value-at-Risk (CVaR): {cvar}\")\n",
        "\n",
        "# 3. ElasticNet Tuning\n",
        "# Use GridSearchCV or RandomizedSearchCV to explore a broader range of alpha and l1_ratio values to tune the ElasticNet model\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.0, 0.1, 0.3, 0.5, 0.7, 1.0]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "grid_search.fit(X, portfolio_returns)  # Use portfolio returns instead of asset returns\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 4. Portfolio Diversification Constraints\n",
        "# Maximum weight constraint for each asset (no asset should exceed 25%)\n",
        "constraints = [cp.sum(weights) == 1, weights >= 0, weights <= 0.25]  # No weight should exceed 25%\n",
        "\n",
        "# Correlation constraint (limiting the correlation between assets)\n",
        "correlation_matrix = np.corrcoef(X.T)  # Compute correlation matrix\n",
        "max_correlation = 0.7  # Maximum allowed correlation between assets\n",
        "correlation_constraint = cp.quad_form(weights, correlation_matrix) <= max_correlation  # Adding constraint for correlation\n",
        "\n",
        "constraints.append(correlation_constraint)\n",
        "\n",
        "# Portfolio optimization with added diversification constraints\n",
        "objective = cp.Maximize(cp.sum(weights @ X.T))  # Maximize portfolio returns (sum of weighted returns)\n",
        "portfolio_problem = cp.Problem(objective, constraints)\n",
        "portfolio_problem.solve()\n",
        "\n",
        "print(\"Optimized Portfolio Weights with Diversification Constraints:\", weights.value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlmseKKZLMmq",
        "outputId": "8710e956-7080-469d-ba76-8d51b69c560e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights using Simulated Annealing: [0.25 0.25 0.25 0.25 0.25]\n",
            "Stopping search: maximum iterations reached --> 1000\n",
            "Optimized Portfolio Weights using PSO: [0.25 0.25 0.25 0.25 0.25]\n",
            "Sharpe ratio: 3848.6702895431354\n",
            "Value-at-Risk (VaR): 0.3537364610014607\n",
            "Conditional Value-at-Risk (CVaR): 0.2946151345252822\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.0}\n",
            "Optimized Portfolio Weights with Diversification Constraints: [2.50000001e-01 2.50000000e-01 2.50000000e-01 2.13175326e-10\n",
            " 2.49999999e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from scipy.optimize import dual_annealing\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Example data (X: asset returns, portfolio_returns: expected returns for portfolio)\n",
        "# Replace with actual data\n",
        "X = np.random.rand(1000, 5)  # 1000 samples, 5 assets\n",
        "portfolio_returns = np.mean(X, axis=0)  # Simplified portfolio return formula\n",
        "\n",
        "# 1. Refining Portfolio Optimization\n",
        "\n",
        "# Simulated Annealing Optimization with better settings\n",
        "def objective_function(weights):\n",
        "    portfolio_return = np.sum(np.dot(weights, X.T))  # Simplified portfolio return formula\n",
        "    return -portfolio_return  # Maximize portfolio returns\n",
        "\n",
        "lb = [0] * X.shape[1]  # Lower bounds (no short-selling)\n",
        "ub = [0.25] * X.shape[1]  # Upper bounds (maximum 25% per asset)\n",
        "\n",
        "result_sa = dual_annealing(objective_function, bounds=list(zip(lb, ub)), maxiter=10000, no_local_search=False)\n",
        "print(\"Optimized Portfolio Weights using Simulated Annealing:\", result_sa.x)\n",
        "\n",
        "# PSO Optimization with better settings\n",
        "optimal_weights, _ = pso(objective_function, lb, ub, maxiter=10000)\n",
        "print(\"Optimized Portfolio Weights using PSO:\", optimal_weights)\n",
        "\n",
        "# 2. Improving Risk Metrics\n",
        "\n",
        "# Sharpe Ratio and CVaR Optimization\n",
        "portfolio_return = np.sum(np.dot(X, optimal_weights))  # Simplified portfolio return formula\n",
        "cov_matrix = np.cov(X.T)  # Covariance matrix of asset returns\n",
        "portfolio_volatility = np.sqrt(np.dot(np.dot(optimal_weights.T, cov_matrix), optimal_weights))\n",
        "sharpe_ratio = portfolio_return / portfolio_volatility\n",
        "\n",
        "# Refined CVaR calculation\n",
        "alpha = 0.05\n",
        "portfolio_returns = np.dot(X, optimal_weights)  # Portfolio returns\n",
        "var = np.percentile(portfolio_returns, 100 * alpha)  # Value-at-Risk (VaR)\n",
        "cvar = np.mean(portfolio_returns[portfolio_returns <= var])  # CVaR (mean of losses greater than VaR)\n",
        "\n",
        "print(f\"Value-at-Risk (VaR): {var}\")\n",
        "print(f\"Conditional Value-at-Risk (CVaR): {cvar}\")\n",
        "print(f\"Sharpe ratio: {sharpe_ratio}\")\n",
        "\n",
        "# 3. Enhancing Portfolio Diversification\n",
        "\n",
        "# Diversification Constraints\n",
        "max_weight_constraint = weights <= 0.25  # No asset should exceed 25%\n",
        "constraints = [cp.sum(weights) == 1, weights >= 0, max_weight_constraint]  # Portfolio weight constraints\n",
        "\n",
        "# Correlation constraint (limiting the correlation between assets)\n",
        "correlation_matrix = np.corrcoef(X.T)  # Compute correlation matrix\n",
        "max_correlation = 0.7  # Maximum allowed correlation between assets\n",
        "correlation_constraint = cp.quad_form(weights, correlation_matrix) <= max_correlation  # Adding constraint for correlation\n",
        "\n",
        "constraints.append(correlation_constraint)\n",
        "\n",
        "# Portfolio optimization with added diversification constraints\n",
        "weights = cp.Variable(X.shape[1])  # Portfolio weights\n",
        "objective = cp.Maximize(cp.sum(weights @ X.T))  # Maximize portfolio returns (sum of weighted returns)\n",
        "portfolio_problem = cp.Problem(objective, constraints)\n",
        "portfolio_problem.solve()\n",
        "\n",
        "print(\"Optimized Portfolio Weights with Diversification Constraints:\", weights.value)\n",
        "\n",
        "# 4. ElasticNet Regularization\n",
        "\n",
        "# ElasticNet GridSearchCV to tune parameters\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.0, 0.1, 0.3, 0.5, 0.7, 1.0]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "grid_search.fit(X, portfolio_returns)  # Use portfolio returns instead of asset returns\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kycBacEJNEfC",
        "outputId": "b8c30c62-dd4a-4800-a971-b7c37e2d03ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights using Simulated Annealing: [0.25 0.25 0.25 0.25 0.25]\n",
            "Stopping search: maximum iterations reached --> 10000\n",
            "Optimized Portfolio Weights using PSO: [0.25 0.25 0.25 0.25 0.25]\n",
            "Value-at-Risk (VaR): 0.3404205180019673\n",
            "Conditional Value-at-Risk (CVaR): 0.2962414610515881\n",
            "Sharpe ratio: 3958.2342406526404\n",
            "Optimized Portfolio Weights with Diversification Constraints: None\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from scipy.optimize import dual_annealing\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Simulated asset returns (replace with your own data)\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(1000, 5)  # Simulate returns of 5 assets over 1000 time periods\n",
        "\n",
        "# Portfolio returns (replace with your own returns data)\n",
        "portfolio_returns = np.dot(X, np.random.rand(5))\n",
        "\n",
        "# Define variables\n",
        "weights = cp.Variable(X.shape[1])  # Portfolio weights for 5 assets\n",
        "\n",
        "# 1. Refining Portfolio Optimization\n",
        "\n",
        "# Simulated Annealing with better settings\n",
        "def objective_function(weights):\n",
        "    return -np.sum(np.dot(weights, X.T))  # Maximize portfolio returns (simplified)\n",
        "\n",
        "# Set bounds for portfolio weights\n",
        "lb = [0] * X.shape[1]  # Lower bounds (no short-selling)\n",
        "ub = [0.25] * X.shape[1]  # Upper bounds (maximum 25% per asset)\n",
        "\n",
        "# Simulated Annealing Optimization\n",
        "result_sa = dual_annealing(objective_function, bounds=list(zip(lb, ub)), maxiter=10000, no_local_search=False, maxfun=5000)\n",
        "print(\"Optimized Portfolio Weights using Simulated Annealing:\", result_sa.x)\n",
        "\n",
        "# PSO Optimization with better settings\n",
        "optimal_weights, _ = pso(objective_function, lb, ub, maxiter=10000, swarmsize=100)\n",
        "print(\"Optimized Portfolio Weights using PSO:\", optimal_weights)\n",
        "\n",
        "# 2. Refining Risk Metrics\n",
        "\n",
        "# Sharpe Ratio Optimization\n",
        "portfolio_return = np.sum(np.dot(X, optimal_weights))  # Portfolio return formula\n",
        "cov_matrix = np.cov(X.T)  # Covariance matrix of asset returns\n",
        "portfolio_volatility = np.sqrt(np.dot(np.dot(optimal_weights.T, cov_matrix), optimal_weights))\n",
        "sharpe_ratio = portfolio_return / portfolio_volatility\n",
        "print(f\"Sharpe ratio: {sharpe_ratio}\")\n",
        "\n",
        "# Refined CVaR Calculation\n",
        "alpha = 0.05\n",
        "portfolio_returns = np.dot(X, optimal_weights)  # Portfolio returns\n",
        "var = np.percentile(portfolio_returns, 100 * alpha)  # Value-at-Risk (VaR)\n",
        "cvar = np.mean(portfolio_returns[portfolio_returns <= var])  # CVaR (mean of losses greater than VaR)\n",
        "\n",
        "print(f\"Value-at-Risk (VaR): {var}\")\n",
        "print(f\"Conditional Value-at-Risk (CVaR): {cvar}\")\n",
        "\n",
        "# 3. Enhancing Portfolio Diversification\n",
        "\n",
        "# Diversification Constraints: Limit the maximum weight for any asset\n",
        "max_weight_constraint = weights <= 0.25  # No asset should exceed 25%\n",
        "constraints = [cp.sum(weights) == 1, weights >= 0, weights <= 0.25]  # Portfolio weight constraints\n",
        "\n",
        "# Correlation constraint (limiting the correlation between assets)\n",
        "correlation_matrix = np.corrcoef(X.T)  # Compute correlation matrix\n",
        "max_correlation = 0.7  # Maximum allowed correlation between assets\n",
        "correlation_constraint = cp.quad_form(weights, correlation_matrix) <= max_correlation  # Adding constraint for correlation\n",
        "\n",
        "constraints.append(correlation_constraint)\n",
        "\n",
        "# Portfolio optimization with added diversification constraints\n",
        "objective = cp.Maximize(cp.sum(weights @ X.T))  # Maximize portfolio returns (sum of weighted returns)\n",
        "portfolio_problem = cp.Problem(objective, constraints)\n",
        "portfolio_problem.solve()\n",
        "\n",
        "print(\"Optimized Portfolio Weights with Diversification Constraints:\", weights.value)\n",
        "\n",
        "# 4. ElasticNet Regularization Refinements\n",
        "\n",
        "# ElasticNet GridSearchCV to tune parameters\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "grid_search.fit(X, portfolio_returns)  # Use portfolio returns instead of asset returns\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 5. Advanced Risk Models (Optional)\n",
        "\n",
        "# Example: Using Extreme Value Theory (EVT) or Gaussian Mixture Models (GMM) for refining CVaR\n",
        "# Placeholder for EVT or GMM modeling\n",
        "# You can use EVT to model tail distributions and GMM for clustering data points to predict better risk assessments\n",
        "\n",
        "# 6. Sector or Asset-Class Constraints\n",
        "\n",
        "# Example constraint for limiting weight in a specific sector (if available)\n",
        "# Assume 5 sectors and a constraint that each sector's weight should not exceed 20%\n",
        "sector_weights = [0.2, 0.2, 0.2, 0.2, 0.2]  # Assume 5 sectors\n",
        "sector_constraint = cp.sum(weights) <= 1.0  # This is just an example of the constraint\n",
        "\n",
        "constraints.append(sector_constraint)\n",
        "\n",
        "# Portfolio optimization with sector-based diversification constraints\n",
        "portfolio_problem = cp.Problem(objective, constraints)\n",
        "portfolio_problem.solve()\n",
        "\n",
        "print(\"Optimized Portfolio Weights with Sector Constraints:\", weights.value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nyjI6vLNrg7",
        "outputId": "55b5bfaa-37d9-4a93-9fc2-5ac3a969c647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights using Simulated Annealing: [0.25 0.   0.25 0.25 0.25]\n",
            "Stopping search: maximum iterations reached --> 10000\n",
            "Optimized Portfolio Weights using PSO: [0.25 0.   0.25 0.25 0.25]\n",
            "Sharpe ratio: 22.590518498004975\n",
            "Value-at-Risk (VaR): -0.8176447227809162\n",
            "Conditional Value-at-Risk (CVaR): -1.0627039739793358\n",
            "Optimized Portfolio Weights with Diversification Constraints: [ 2.50000000e-01 -1.23335676e-10  2.50000000e-01  2.50000000e-01\n",
            "  2.50000000e-01]\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n",
            "Optimized Portfolio Weights with Sector Constraints: [ 2.50000000e-01 -1.41043668e-10  2.50000000e-01  2.50000000e-01\n",
            "  2.50000000e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Example data: Replace with your actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns as an example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# Portfolio Optimization with Constraints and Risk Metrics\n",
        "\n",
        "# Define the portfolio return function\n",
        "def portfolio_return(weights, X):\n",
        "    return np.sum(np.dot(X, weights))\n",
        "\n",
        "# Define the portfolio volatility (risk) function\n",
        "def portfolio_volatility(weights, X):\n",
        "    cov_matrix = np.cov(X.T)  # Covariance matrix of asset returns\n",
        "    return np.sqrt(np.dot(np.dot(weights.T, cov_matrix), weights))\n",
        "\n",
        "# Define the objective function (negative Sharpe ratio)\n",
        "def objective(weights, X):\n",
        "    portfolio_ret = portfolio_return(weights, X)\n",
        "    portfolio_vol = portfolio_volatility(weights, X)\n",
        "    # Sharpe ratio (negative because we want to maximize it)\n",
        "    return -portfolio_ret / portfolio_vol\n",
        "\n",
        "# Constraints: weights sum to 1, no short-selling, and weight limit\n",
        "def constraint_weight_sum(weights):\n",
        "    return np.sum(weights) - 1\n",
        "\n",
        "# Limit the weight of each asset (e.g., no more than 20%)\n",
        "def constraint_max_weight(weights):\n",
        "    return 0.2 - np.max(weights)\n",
        "\n",
        "# Risk metrics (CVaR, VaR)\n",
        "def calculate_risk_metrics(weights, X, alpha=0.05):\n",
        "    portfolio_returns = np.dot(X, weights)\n",
        "    var = np.percentile(portfolio_returns, 100 * alpha)  # Value-at-Risk (VaR)\n",
        "    cvar = np.mean(portfolio_returns[portfolio_returns <= var])  # CVaR (mean of losses greater than VaR)\n",
        "    return var, cvar\n",
        "\n",
        "# PSO optimization for portfolio weights\n",
        "def pso_optimization(X):\n",
        "    lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "    ub = [0.25] * n_assets  # Upper bounds (maximum 25% per asset)\n",
        "\n",
        "    # Perform PSO optimization\n",
        "    optimal_weights, _ = pso(objective, lb, ub, args=(X,), maxiter=10000, swarmsize=100)\n",
        "    return optimal_weights\n",
        "\n",
        "# Simulated Annealing with scipy's minimize\n",
        "def optimize_portfolio(X):\n",
        "    # Initial guess: equal weight allocation\n",
        "    initial_weights = np.ones(n_assets) / n_assets\n",
        "\n",
        "    # Define bounds for each asset's weight (no short-selling, and max 20% per asset)\n",
        "    bounds = [(0, 0.2) for _ in range(n_assets)]\n",
        "\n",
        "    # Constraints (weights sum to 1)\n",
        "    constraints = [{'type': 'eq', 'fun': constraint_weight_sum},\n",
        "                   {'type': 'ineq', 'fun': constraint_max_weight}]\n",
        "\n",
        "    # Run scipy's minimize function to optimize portfolio weights\n",
        "    result = minimize(objective, initial_weights, args=(X,), method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "\n",
        "    if result.success:\n",
        "        return result.x  # Optimized portfolio weights\n",
        "    else:\n",
        "        print(\"Optimization failed.\")\n",
        "        return None\n",
        "\n",
        "# Main optimization and risk analysis\n",
        "optimal_weights = optimize_portfolio(X)\n",
        "if optimal_weights is not None:\n",
        "    print(\"Optimized Portfolio Weights using Simulated Annealing:\", optimal_weights)\n",
        "\n",
        "# Risk metrics for the optimized portfolio\n",
        "var, cvar = calculate_risk_metrics(optimal_weights, X)\n",
        "print(f\"Value-at-Risk (VaR): {var}\")\n",
        "print(f\"Conditional Value-at-Risk (CVaR): {cvar}\")\n",
        "\n",
        "# Perform ElasticNet Tuning using GridSearchCV\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "grid_search.fit(X, np.dot(X, optimal_weights))  # Use portfolio returns instead of asset returns\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Diversification with sector constraints (if you have sector data)\n",
        "sector_weights = [0.2, 0.2, 0.2, 0.2, 0.2]  # Assume 5 sectors for 5 assets\n",
        "sector_constraint = np.sum(optimal_weights * sector_weights) <= 1  # Example constraint on sector exposure\n",
        "\n",
        "print(f\"Sector Diversification Constraint Met: {sector_constraint <= 1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm2jOTFFO8Ke",
        "outputId": "bcd4e41e-0dc0-4c05-ea30-dc375590e195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights using Simulated Annealing: [0.2 0.2 0.2 0.2 0.2]\n",
            "Value-at-Risk (VaR): -0.729766036370099\n",
            "Conditional Value-at-Risk (CVaR): -0.9341031527061077\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n",
            "Sector Diversification Constraint Met: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import dual_annealing\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Example data: Replace with your actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns as an example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# Define bounds for asset weights (no short-selling, max weight per asset = 0.25)\n",
        "lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "ub = [0.25] * n_assets  # Upper bounds (maximum 25% per asset)\n",
        "\n",
        "# Portfolio Optimization with Constraints and Risk Metrics\n",
        "\n",
        "# Objective function (using numpy for manual matrix operations)\n",
        "def objective_function(weights):\n",
        "    portfolio_return = np.dot(weights, X.T).sum()  # Portfolio return formula using numpy (sum of weighted returns)\n",
        "    cov_matrix = np.cov(X.T)  # Covariance matrix of asset returns\n",
        "    portfolio_volatility = np.sqrt(np.dot(np.dot(weights.T, cov_matrix), weights))  # Portfolio volatility\n",
        "    return -portfolio_return / portfolio_volatility  # Maximize Sharpe ratio (minimize negative)\n",
        "\n",
        "# Simulated Annealing optimization\n",
        "result_sa = dual_annealing(objective_function, bounds=list(zip(lb, ub)), maxiter=10000, no_local_search=False, maxfun=5000)\n",
        "print(\"Optimized Portfolio Weights using Simulated Annealing:\", result_sa.x)\n",
        "\n",
        "# PSO Optimization\n",
        "optimal_weights, _ = pso(objective_function, lb, ub, maxiter=10000, swarmsize=100)\n",
        "print(\"Optimized Portfolio Weights using PSO:\", optimal_weights)\n",
        "\n",
        "# ElasticNet Regularization Refinement with GridSearchCV\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "grid_search.fit(X, X.dot(optimal_weights))  # Use portfolio returns instead of asset returns for ElasticNet fitting\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Refined Risk Metrics: CVaR and Sharpe Ratio\n",
        "# Portfolio returns from the optimized weights\n",
        "portfolio_returns = np.dot(X, optimal_weights)  # Portfolio returns (sum of weighted asset returns)\n",
        "\n",
        "# Calculate Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR)\n",
        "alpha = 0.05\n",
        "var = np.percentile(portfolio_returns, 100 * alpha)  # Value-at-Risk (VaR)\n",
        "cvar = np.mean(portfolio_returns[portfolio_returns <= var])  # CVaR (mean of losses greater than VaR)\n",
        "\n",
        "# Calculate Sharpe ratio (risk-adjusted return)\n",
        "portfolio_return = np.sum(np.dot(optimal_weights, X.T))  # Portfolio return formula\n",
        "cov_matrix = np.cov(X.T)  # Covariance matrix of asset returns\n",
        "portfolio_volatility = np.sqrt(np.dot(np.dot(optimal_weights.T, cov_matrix), optimal_weights))\n",
        "sharpe_ratio = portfolio_return / portfolio_volatility\n",
        "\n",
        "# Print out the results\n",
        "print(f\"Value-at-Risk (VaR): {var}\")\n",
        "print(f\"Conditional Value-at-Risk (CVaR): {cvar}\")\n",
        "print(f\"Sharpe ratio: {sharpe_ratio}\")\n",
        "\n",
        "# Constraints for portfolio diversification\n",
        "max_weight_constraint = np.all(optimal_weights <= 0.2)  # Ensure no asset exceeds 20%\n",
        "print(f\"Max weight constraint met: {max_weight_constraint}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI1PVEiKPz2x",
        "outputId": "5aa94094-e6bd-441b-9fee-7abfcc3890c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights using Simulated Annealing: [0.00463028 0.         0.0332333  0.05641612 0.25      ]\n",
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Optimized Portfolio Weights using PSO: [0.00463178 0.         0.0332322  0.05641052 0.25      ]\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n",
            "Value-at-Risk (VaR): -0.3989714791339483\n",
            "Conditional Value-at-Risk (CVaR): -0.5263130587331547\n",
            "Sharpe ratio: 34.312984541734764\n",
            "Max weight constraint met: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Example data: Replace with your actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns as an example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# Portfolio optimization with constraints\n",
        "\n",
        "# Define the objective function for Sharpe ratio maximization\n",
        "def objective_function(weights, X):\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return\n",
        "    cov_matrix = np.cov(X.T)  # Covariance matrix\n",
        "    portfolio_volatility = np.sqrt(np.dot(np.dot(weights.T, cov_matrix), weights))  # Portfolio volatility\n",
        "    sharpe_ratio = portfolio_return / portfolio_volatility  # Sharpe ratio\n",
        "    return -sharpe_ratio  # We negate to maximize Sharpe ratio\n",
        "\n",
        "# 1. Address Concentration Risk (max weight constraint)\n",
        "def weight_constraint(weights):\n",
        "    return 0.2 - np.max(weights)  # No asset should exceed 20%\n",
        "\n",
        "# 2. Correlation constraint (limit correlation between assets)\n",
        "def correlation_constraint(weights, X):\n",
        "    correlation_matrix = np.corrcoef(X.T)  # Compute correlation matrix\n",
        "    portfolio_correlation = np.dot(weights.T, np.dot(correlation_matrix, weights))\n",
        "    return 0.7 - portfolio_correlation  # Maximum allowed correlation\n",
        "\n",
        "# 3. CVaR and Sortino Ratio Calculation\n",
        "def cvar_and_sortino(weights, X, alpha=0.05):\n",
        "    # Portfolio returns\n",
        "    portfolio_returns = np.dot(X, weights)\n",
        "\n",
        "    # Value-at-Risk (VaR)\n",
        "    var = np.percentile(portfolio_returns, 100 * alpha)\n",
        "\n",
        "    # Conditional Value-at-Risk (CVaR)\n",
        "    cvar = np.mean(portfolio_returns[portfolio_returns <= var])\n",
        "\n",
        "    # Sortino Ratio\n",
        "    downside_risk = np.std(np.minimum(portfolio_returns, 0))  # Downside deviation\n",
        "    sortino_ratio = np.mean(portfolio_returns) / downside_risk\n",
        "\n",
        "    return cvar, sortino_ratio\n",
        "\n",
        "# Constraints for the optimization problem\n",
        "def constraints(weights, X):\n",
        "    return [\n",
        "        {'type': 'ineq', 'fun': weight_constraint},  # Max weight constraint\n",
        "        {'type': 'ineq', 'fun': lambda w: correlation_constraint(w, X)},  # Correlation constraint\n",
        "        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}  # Sum of weights = 1\n",
        "    ]\n",
        "\n",
        "# 4. Simulated Annealing Optimization\n",
        "def optimize_with_sa(objective_function, lb, ub, X):\n",
        "    result = dual_annealing(objective_function, bounds=list(zip(lb, ub)), maxiter=10000, no_local_search=False, maxfun=5000, args=(X,))\n",
        "    print(\"Optimized Portfolio Weights using Simulated Annealing:\", result.x)\n",
        "    return result.x\n",
        "\n",
        "# 5. PSO Optimization\n",
        "def optimize_with_pso(objective_function, lb, ub, X):\n",
        "    optimal_weights, _ = pso(objective_function, lb, ub, maxiter=10000, swarmsize=100, args=(X,))\n",
        "    print(\"Optimized Portfolio Weights using PSO:\", optimal_weights)\n",
        "    return optimal_weights\n",
        "\n",
        "# ElasticNet Regularization with GridSearchCV\n",
        "def elastic_net_regularization(X, portfolio_returns):\n",
        "    parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "    grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "    grid_search.fit(X, portfolio_returns)\n",
        "    print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "    return grid_search.best_params_\n",
        "\n",
        "# Portfolio optimization with Simulated Annealing and PSO\n",
        "# Bounds for asset weights (no short-selling and max weight limit)\n",
        "lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "ub = [0.2] * n_assets  # Upper bounds (maximum 20% per asset)\n",
        "\n",
        "# Run optimization\n",
        "weights_sa = optimize_with_sa(objective_function, lb, ub, X)\n",
        "weights_pso = optimize_with_pso(objective_function, lb, ub, X)\n",
        "\n",
        "# 6. Portfolio Risk Metrics and Performance\n",
        "portfolio_return = np.sum(np.dot(X, weights_sa))  # Portfolio return\n",
        "cov_matrix = np.cov(X.T)\n",
        "portfolio_volatility = np.sqrt(np.dot(np.dot(weights_sa.T, cov_matrix), weights_sa))  # Portfolio volatility\n",
        "sharpe_ratio = portfolio_return / portfolio_volatility  # Sharpe ratio\n",
        "\n",
        "# Refined CVaR calculation (using portfolio returns)\n",
        "portfolio_returns = np.dot(X, weights_sa)\n",
        "var = np.percentile(portfolio_returns, 100 * 0.05)  # Value-at-Risk (VaR)\n",
        "cvar = np.mean(portfolio_returns[portfolio_returns <= var])  # CVaR (mean of losses greater than VaR)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Sharpe ratio: {sharpe_ratio}\")\n",
        "print(f\"Value-at-Risk (VaR): {var}\")\n",
        "print(f\"Conditional Value-at-Risk (CVaR): {cvar}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doDsAy5HXG6M",
        "outputId": "56f1b32a-6d05-4c89-82b3-39ee5b61a8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights using Simulated Annealing: [0.00370465 0.         0.02658654 0.04513267 0.2       ]\n",
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Optimized Portfolio Weights using PSO: [0.00370174 0.         0.02658325 0.04512765 0.19998725]\n",
            "Sharpe ratio: 34.31298454890837\n",
            "Value-at-Risk (VaR): -0.3191778824182497\n",
            "Conditional Value-at-Risk (CVaR): -0.4210516429665209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import dual_annealing\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Example data: Replace with your actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns as an example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# Portfolio Optimization with Constraints and Risk Metrics\n",
        "# Define the optimization problem\n",
        "\n",
        "# Initial weights - Random portfolio allocation\n",
        "weights = np.random.rand(n_assets)\n",
        "weights /= np.sum(weights)  # Normalize the weights to sum to 1\n",
        "\n",
        "# 1. Refining Diversification: Maximize the diversification by tightening the correlation limits between assets\n",
        "# Maximum weight constraint for each asset (no asset should exceed 20%)\n",
        "max_weight_constraint = 0.2  # No asset should exceed 20%\n",
        "\n",
        "# Compute correlation matrix for assets\n",
        "correlation_matrix = np.corrcoef(X.T)  # X is your asset returns matrix\n",
        "\n",
        "# Correlation constraint (limiting the correlation between assets)\n",
        "max_correlation = 0.5  # Maximum allowed correlation between assets (tightened)\n",
        "\n",
        "# 2. Improving Risk Metrics: Optimize for Sortino ratio and refine CVaR calculation with EVT or GMM\n",
        "# Portfolio return (weighted sum of asset returns)\n",
        "def calculate_portfolio_return(weights, X):\n",
        "    return np.sum(np.dot(X, weights))  # Portfolio return formula\n",
        "\n",
        "# Portfolio volatility (from the covariance matrix of asset returns)\n",
        "def calculate_portfolio_volatility(weights, X):\n",
        "    cov_matrix = np.cov(X.T)  # Covariance matrix of asset returns\n",
        "    return np.sqrt(np.dot(np.dot(weights.T, cov_matrix), weights))  # Volatility from covariance\n",
        "\n",
        "# Sortino ratio (downside risk)\n",
        "def calculate_sortino_ratio(weights, X):\n",
        "    portfolio_return = calculate_portfolio_return(weights, X)\n",
        "    downside_risk = np.std(np.minimum(np.dot(X, weights), 0))  # Downside deviation\n",
        "    return portfolio_return / downside_risk  # Sortino ratio\n",
        "\n",
        "# Refined CVaR calculation (using portfolio returns)\n",
        "def calculate_cvar(weights, X, alpha=0.05):\n",
        "    portfolio_returns = np.dot(X, weights)  # Portfolio returns\n",
        "    var = np.percentile(portfolio_returns, 100 * alpha)  # Value-at-Risk (VaR)\n",
        "    cvar = np.mean(portfolio_returns[portfolio_returns <= var])  # CVaR (mean of losses greater than VaR)\n",
        "    return cvar\n",
        "\n",
        "# 3. Simulated Annealing and PSO optimization\n",
        "\n",
        "# Objective function for Simulated Annealing and PSO (to maximize Sortino ratio)\n",
        "def objective_function(weights, X):\n",
        "    portfolio_return = calculate_portfolio_return(weights, X)\n",
        "    portfolio_volatility = calculate_portfolio_volatility(weights, X)\n",
        "    sortino_ratio = calculate_sortino_ratio(weights, X)\n",
        "    cvar = calculate_cvar(weights, X)\n",
        "\n",
        "    # Maximize Sortino ratio and minimize CVaR\n",
        "    return -(sortino_ratio - cvar)\n",
        "\n",
        "# Bounds and constraints\n",
        "lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "ub = [max_weight_constraint] * n_assets  # Upper bounds (maximum 20% per asset)\n",
        "\n",
        "# Perform Simulated Annealing optimization\n",
        "result_sa = dual_annealing(objective_function, bounds=list(zip(lb, ub)), maxiter=10000, no_local_search=False, maxfun=5000, args=(X,))\n",
        "print(\"Optimized Portfolio Weights using Simulated Annealing:\", result_sa.x)\n",
        "\n",
        "# Perform PSO optimization\n",
        "optimal_weights, _ = pso(objective_function, lb, ub, maxiter=10000, swarmsize=100, args=(X,))\n",
        "print(\"Optimized Portfolio Weights using PSO:\", optimal_weights)\n",
        "\n",
        "# 4. ElasticNet Regularization Refinement with GridSearchCV\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "grid_search.fit(X, np.dot(X, optimal_weights))  # Fit ElasticNet using the portfolio returns (X and optimal weights)\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 5. Sector Diversification Constraint (Optional)\n",
        "# Example: Sector-specific diversification constraint (if you have sector data)\n",
        "sector_weights = [0.2, 0.2, 0.2, 0.2, 0.2]  # Assume 5 sectors for 5 assets\n",
        "sector_constraint = np.sum(optimal_weights * sector_weights) <= 1  # Example constraint on sector exposure\n",
        "print(f\"Sector Diversification Constraint Met: {sector_constraint <= 1}\")\n",
        "\n",
        "# 6. Output\n",
        "# Final portfolio weights and risk-adjusted performance metrics\n",
        "final_portfolio_return = calculate_portfolio_return(optimal_weights, X)\n",
        "final_portfolio_volatility = calculate_portfolio_volatility(optimal_weights, X)\n",
        "final_sortino_ratio = calculate_sortino_ratio(optimal_weights, X)\n",
        "final_cvar = calculate_cvar(optimal_weights, X)\n",
        "\n",
        "print(f\"Final Optimized Portfolio Weights: {optimal_weights}\")\n",
        "print(f\"Final Portfolio Return: {final_portfolio_return}\")\n",
        "print(f\"Final Portfolio Volatility: {final_portfolio_volatility}\")\n",
        "print(f\"Final Sortino Ratio: {final_sortino_ratio}\")\n",
        "print(f\"Final Conditional Value-at-Risk (CVaR): {final_cvar}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q5VemWyXg7Q",
        "outputId": "660e63a3-200c-44d4-b6e2-d226706f3cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights using Simulated Annealing: [0.00567359 0.         0.02106164 0.04239879 0.2       ]\n",
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Optimized Portfolio Weights using PSO: [0.00566996 0.         0.0210648  0.0423959  0.2       ]\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n",
            "Sector Diversification Constraint Met: True\n",
            "Final Optimized Portfolio Weights: [0.00566996 0.         0.0210648  0.0423959  0.2       ]\n",
            "Final Portfolio Return: 6.9490521009643045\n",
            "Final Portfolio Volatility: 0.20261574656648448\n",
            "Final Sortino Ratio: 59.80094250156844\n",
            "Final Conditional Value-at-Risk (CVaR): -0.4191394160211103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import dual_annealing\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example data: Replace with your actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns as an example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# Portfolio Optimization with Constraints and Risk Metrics\n",
        "\n",
        "# Define objective function for optimization\n",
        "def objective_function(weights):\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return (weighted sum)\n",
        "    portfolio_volatility = np.sqrt(np.dot(np.dot(weights.T, np.cov(X.T)), weights))  # Portfolio volatility (risk)\n",
        "    return -portfolio_return / portfolio_volatility  # Maximize Sharpe ratio (minimize negative)\n",
        "\n",
        "# 1. Address Concentration Risk: Tighten maximum asset weight constraint to 15%\n",
        "lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "ub = [0.15] * n_assets  # Upper bounds (maximum 15% per asset)\n",
        "\n",
        "# Perform Simulated Annealing with more iterations\n",
        "result_sa = dual_annealing(objective_function, bounds=list(zip(lb, ub)), maxiter=15000, no_local_search=False, maxfun=8000)\n",
        "print(\"Optimized Portfolio Weights using Simulated Annealing:\", result_sa.x)\n",
        "\n",
        "# PSO Optimization with better settings (increase swarm size and iterations)\n",
        "optimal_weights, _ = pso(objective_function, lb, ub, maxiter=15000, swarmsize=100)\n",
        "print(\"Optimized Portfolio Weights using PSO:\", optimal_weights)\n",
        "\n",
        "# 2. Improve Risk Metrics: Refine CVaR using Gaussian Mixture Models (GMM) and Sortino ratio\n",
        "# Refined CVaR calculation using GMM\n",
        "portfolio_returns = np.dot(X, optimal_weights)  # Portfolio returns\n",
        "alpha = 0.05\n",
        "var = np.percentile(portfolio_returns, 100 * alpha)  # Value-at-Risk (VaR)\n",
        "cvar = np.mean(portfolio_returns[portfolio_returns <= var])  # CVaR (mean of losses greater than VaR)\n",
        "print(f\"Value-at-Risk (VaR): {var}\")\n",
        "print(f\"Conditional Value-at-Risk (CVaR): {cvar}\")\n",
        "\n",
        "# Sortino ratio: Focuses on downside risk (negative returns)\n",
        "downside_risk = np.std(np.minimum(portfolio_returns, 0))  # Downside deviation\n",
        "sortino_ratio = np.mean(portfolio_returns) / downside_risk\n",
        "print(f\"Sortino ratio: {sortino_ratio}\")\n",
        "\n",
        "# 3. ElasticNet Regularization Refinement with GridSearchCV\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "grid_search.fit(X, portfolio_returns)  # Use portfolio returns instead of asset returns\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 4. Additional Diversification Constraints: Correlation Constraint\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = np.corrcoef(X.T)  # Asset returns matrix\n",
        "\n",
        "# Add a correlation constraint to limit the correlation between assets\n",
        "max_correlation = 0.5  # Maximum allowed correlation between assets\n",
        "correlation_constraint = np.sum(correlation_matrix * np.outer(optimal_weights, optimal_weights))  # Correlation constraint\n",
        "print(f\"Correlation Constraint Met: {np.all(correlation_matrix <= max_correlation)}\")\n",
        "\n",
        "# 5. Advanced Risk Models: Gaussian Mixture Model (GMM) for better tail risk modeling\n",
        "gmm = GaussianMixture(n_components=2)\n",
        "gmm.fit(portfolio_returns.reshape(-1, 1))\n",
        "\n",
        "# Predict the tail risk using GMM (this could be used to assess extreme risk)\n",
        "gmm_predictions = gmm.predict(portfolio_returns.reshape(-1, 1))\n",
        "print(\"Gaussian Mixture Model Tail Risk:\", np.mean(gmm_predictions))\n",
        "\n",
        "# Visualization of portfolio return distribution and GMM tail prediction\n",
        "plt.hist(portfolio_returns, bins=50, alpha=0.7, label=\"Portfolio Returns\")\n",
        "plt.axvline(var, color='r', linestyle='dashed', linewidth=2, label=\"VaR (5%)\")\n",
        "plt.title(\"Portfolio Return Distribution with VaR and CVaR\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Output results\n",
        "print(f\"Optimized Portfolio Weights with Diversification Constraints: {optimal_weights}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "Ql9bQfrGYDne",
        "outputId": "9ac96773-aa45-4576-fc6d-e56a738d028c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights using Simulated Annealing: [0.00277854 0.         0.01993972 0.03384947 0.15      ]\n",
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Optimized Portfolio Weights using PSO: [0.00277342 0.         0.01989276 0.03377299 0.14966749]\n",
            "Value-at-Risk (VaR): -0.2388527571731812\n",
            "Conditional Value-at-Risk (CVaR): -0.3150876337801867\n",
            "Sortino ratio: 0.05976576370926313\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n",
            "Correlation Constraint Met: False\n",
            "Gaussian Mixture Model Tail Risk: 0.458\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights with Diversification Constraints: [0.00277342 0.         0.01989276 0.03377299 0.14966749]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.optimize as opt\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Example data: Replace with your actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns as an example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# Portfolio Optimization with Constraints and Risk Metrics\n",
        "def objective_function(weights, X):\n",
        "    portfolio_return = np.dot(weights, X.mean(axis=0))  # Simplified portfolio return formula\n",
        "    portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(np.cov(X.T), weights)))  # Portfolio volatility\n",
        "    return -portfolio_return / portfolio_volatility  # Maximize Sharpe ratio (minimize negative)\n",
        "\n",
        "# Constraints: No short-selling, sum of weights = 1, max weight per asset = 0.15\n",
        "def constraints(weights):\n",
        "    return [\n",
        "        {\"type\": \"eq\", \"fun\": lambda weights: np.sum(weights) - 1},  # Sum of weights must equal 1\n",
        "        {\"type\": \"ineq\", \"fun\": lambda weights: weights},  # No short-selling\n",
        "        {\"type\": \"ineq\", \"fun\": lambda weights: 0.2 - np.max(weights)}  # No asset can exceed 20% weight\n",
        "    ]\n",
        "\n",
        "# Bounds for asset weights (no short-selling, max weight of 20% per asset)\n",
        "bounds = [(0, 0.2) for _ in range(n_assets)]  # Lower bound 0, upper bound 0.2\n",
        "\n",
        "# Initial guess: equal distribution across assets\n",
        "initial_guess = np.ones(n_assets) / n_assets\n",
        "\n",
        "# Perform optimization using scipy.optimize.minimize\n",
        "result = opt.minimize(objective_function, initial_guess, args=(X,), bounds=bounds, constraints=constraints(initial_guess), method='SLSQP')\n",
        "\n",
        "# Extract the optimized portfolio weights\n",
        "optimized_weights = result.x\n",
        "print(f\"Optimized Portfolio Weights using SciPy: {optimized_weights}\")\n",
        "\n",
        "# 2. Improve Risk Metrics (Sharpe, Sortino, CVaR)\n",
        "portfolio_returns = np.dot(optimized_weights, X.mean(axis=0))  # Portfolio return\n",
        "portfolio_volatility = np.sqrt(np.dot(optimized_weights.T, np.dot(np.cov(X.T), optimized_weights)))  # Volatility\n",
        "\n",
        "# Sharpe Ratio\n",
        "sharpe_ratio = portfolio_returns / portfolio_volatility\n",
        "print(f\"Sharpe Ratio: {sharpe_ratio}\")\n",
        "\n",
        "# Sortino Ratio (downside risk)\n",
        "downside_risk = np.std(np.minimum(np.dot(X, optimized_weights), 0))  # Downside deviation\n",
        "sortino_ratio = portfolio_returns / downside_risk\n",
        "print(f\"Sortino Ratio: {sortino_ratio}\")\n",
        "\n",
        "# Refined CVaR calculation\n",
        "alpha = 0.05\n",
        "portfolio_returns_all = np.dot(X, optimized_weights)  # Portfolio returns for all assets\n",
        "var = np.percentile(portfolio_returns_all, 100 * alpha)  # Value-at-Risk (VaR)\n",
        "cvar = np.mean(portfolio_returns_all[portfolio_returns_all <= var])  # CVaR (mean of losses greater than VaR)\n",
        "\n",
        "print(f\"Value-at-Risk (VaR): {var}\")\n",
        "print(f\"Conditional Value-at-Risk (CVaR): {cvar}\")\n",
        "\n",
        "# 3. Gaussian Mixture Model (GMM) for Tail Risk\n",
        "gmm = GaussianMixture(n_components=2)  # Fit a GMM with 2 components\n",
        "gmm.fit(portfolio_returns_all.reshape(-1, 1))  # Fit to portfolio returns\n",
        "tail_risk = gmm.score_samples(portfolio_returns_all.reshape(-1, 1))  # Calculate tail risk (log-likelihood)\n",
        "print(f\"Gaussian Mixture Model Tail Risk: {tail_risk.mean()}\")\n",
        "\n",
        "# 4. ElasticNet Regularization Refinement with GridSearchCV\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "grid_search.fit(X, portfolio_returns_all)  # Use portfolio returns instead of asset returns\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 5. Further Diversification (Sector Constraints or Correlation Constraints)\n",
        "# Example: Sector-specific diversification constraint (if you have sector data)\n",
        "sector_weights = [0.2, 0.2, 0.2, 0.2, 0.2]  # Assume 5 sectors for 5 assets\n",
        "sector_constraint = np.sum(optimized_weights * sector_weights)  # Example constraint on sector exposure\n",
        "print(f\"Sector Diversification Constraint: {sector_constraint <= 1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcJAVvA0Yip3",
        "outputId": "3ecb564f-b927-48ba-f04b-d834a9be016c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights using SciPy: [0.2 0.2 0.2 0.2 0.2]\n",
            "Sharpe Ratio: 0.01255346540074495\n",
            "Sortino Ratio: 0.02137556841044008\n",
            "Value-at-Risk (VaR): -0.729766036370099\n",
            "Conditional Value-at-Risk (CVaR): -0.9341031527061077\n",
            "Gaussian Mixture Model Tail Risk: -0.6138690037966058\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n",
            "Sector Diversification Constraint: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import dual_annealing\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Example data: Replace with actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns as an example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# Constraints: max weight per asset, minimum correlation between assets\n",
        "max_weight = 0.15  # Maximum weight per asset\n",
        "\n",
        "# Portfolio Optimization with Constraints and Risk Metrics\n",
        "weights = np.ones(n_assets) / n_assets  # Initialize equal weights\n",
        "\n",
        "# 1. Tighten Maximum Weight Constraint and Correlation Constraints\n",
        "correlation_matrix = np.corrcoef(X.T)  # Asset returns matrix\n",
        "max_correlation = 0.5  # Maximum allowed correlation between assets\n",
        "\n",
        "# Ensure max weight constraint is enforced\n",
        "max_weight_constraint = weights <= max_weight\n",
        "\n",
        "# Portfolio return and risk calculations\n",
        "def portfolio_return(weights, X):\n",
        "    return np.dot(X, weights)  # Portfolio return over all periods (matrix multiplication)\n",
        "\n",
        "def portfolio_volatility(weights, X):\n",
        "    cov_matrix = np.cov(X.T)\n",
        "    return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))  # Portfolio volatility\n",
        "\n",
        "def cvar_calculation(weights, X, alpha=0.05):\n",
        "    portfolio_returns = np.dot(X, weights)\n",
        "    var = np.percentile(portfolio_returns, 100 * alpha)  # Value-at-Risk (VaR)\n",
        "    cvar = np.mean(portfolio_returns[portfolio_returns <= var])  # CVaR\n",
        "    return cvar\n",
        "\n",
        "# 2. Improve Risk-Adjusted Performance: Sortino Ratio, CVaR\n",
        "def sortino_ratio(weights, X):\n",
        "    portfolio_return_value = portfolio_return(weights, X).mean()  # Mean portfolio return over all periods\n",
        "    downside_risk = np.std(np.minimum(np.dot(X, weights), 0))  # Downside deviation\n",
        "    return portfolio_return_value / downside_risk  # Sortino ratio\n",
        "\n",
        "# 3. Optimization using Simulated Annealing and PSO\n",
        "# Objective function for Simulated Annealing\n",
        "def objective_function(weights):\n",
        "    return -portfolio_return(weights, X).mean() / portfolio_volatility(weights, X)  # Maximize Sharpe ratio (minimize negative)\n",
        "\n",
        "# Perform Simulated Annealing optimization\n",
        "result_sa = dual_annealing(objective_function, bounds=[(0, max_weight)]*n_assets, maxiter=10000, no_local_search=False, maxfun=5000)\n",
        "print(\"Optimized Portfolio Weights using Simulated Annealing:\", result_sa.x)\n",
        "\n",
        "# PSO Optimization with better settings\n",
        "optimal_weights, _ = pso(objective_function, lb=[0]*n_assets, ub=[max_weight]*n_assets, maxiter=10000, swarmsize=100)\n",
        "print(\"Optimized Portfolio Weights using PSO:\", optimal_weights)\n",
        "\n",
        "# 4. ElasticNet Regularization Refinement with GridSearchCV\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "\n",
        "# Compute the portfolio returns over all periods and pass to GridSearchCV\n",
        "portfolio_returns = portfolio_return(optimal_weights, X)  # Portfolio returns for all periods\n",
        "\n",
        "# Reshape portfolio returns to a 1D array (as required by GridSearchCV)\n",
        "portfolio_returns = portfolio_returns.flatten()\n",
        "\n",
        "grid_search.fit(X, portfolio_returns)  # Use portfolio returns instead of asset returns\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 5. Refined CVaR calculation using EVT/GMM\n",
        "# Implement EVT or GMM for better tail risk modeling\n",
        "# For simplicity, using Gaussian Mixture Model (GMM) for tail risk estimation\n",
        "gmm = GaussianMixture(n_components=2)\n",
        "gmm.fit(X)\n",
        "tail_risk = np.min(gmm.score_samples(X))  # Simplified tail risk estimation\n",
        "print(f\"Gaussian Mixture Model Tail Risk: {tail_risk}\")\n",
        "\n",
        "# 6. Diversification Constraints and Refinement\n",
        "# Add stricter correlation constraints\n",
        "max_weight_constraint = weights <= 0.15  # No asset should exceed 15%\n",
        "\n",
        "# Portfolio optimization with added diversification constraints\n",
        "constraints = [np.sum(weights) == 1, weights >= 0, max_weight_constraint]  # Portfolio weight constraints\n",
        "\n",
        "# Correlation constraint (limiting the correlation between assets)\n",
        "correlation_matrix = np.corrcoef(X.T)  # Compute correlation matrix\n",
        "max_correlation = 0.5  # Maximum allowed correlation between assets\n",
        "correlation_constraint = np.dot(weights.T, np.dot(correlation_matrix, weights)) <= max_correlation  # Adding constraint for correlation\n",
        "\n",
        "# Output final optimized portfolio weights with diversification constraints\n",
        "print(\"Optimized Portfolio Weights with Diversification Constraints:\", weights)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZVnRXffY8x7",
        "outputId": "44223558-cae0-434a-b4cd-eed54c5011cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights using Simulated Annealing: [0.00277745 0.         0.0199397  0.03384985 0.15      ]\n",
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Optimized Portfolio Weights using PSO: [0.00113792 0.         0.00777852 0.01329372 0.05838448]\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n",
            "Gaussian Mixture Model Tail Risk: -16.544109863664616\n",
            "Optimized Portfolio Weights with Diversification Constraints: [0.2 0.2 0.2 0.2 0.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import dual_annealing\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Example data: Replace with actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns as an example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# Constraints: max weight per asset, minimum correlation between assets\n",
        "max_weight = 0.15  # Maximum weight per asset\n",
        "max_correlation = 0.5  # Maximum allowed correlation between assets\n",
        "\n",
        "# Portfolio Optimization with Constraints and Risk Metrics\n",
        "weights = np.ones(n_assets) / n_assets  # Initialize equal weights\n",
        "\n",
        "# 1. Tighten Maximum Weight Constraint and Correlation Constraints\n",
        "correlation_matrix = np.corrcoef(X.T)  # Asset returns matrix\n",
        "\n",
        "# Ensure max weight constraint is enforced\n",
        "max_weight_constraint = weights <= max_weight\n",
        "\n",
        "# Portfolio return and risk calculations\n",
        "def portfolio_return(weights, X):\n",
        "    return np.dot(X, weights)  # Portfolio return over all periods (matrix multiplication)\n",
        "\n",
        "def portfolio_volatility(weights, X):\n",
        "    cov_matrix = np.cov(X.T)\n",
        "    return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))  # Portfolio volatility\n",
        "\n",
        "def cvar_calculation(weights, X, alpha=0.05):\n",
        "    portfolio_returns = np.dot(X, weights)\n",
        "    var = np.percentile(portfolio_returns, 100 * alpha)  # Value-at-Risk (VaR)\n",
        "    cvar = np.mean(portfolio_returns[portfolio_returns <= var])  # CVaR\n",
        "    return cvar\n",
        "\n",
        "# 2. Improve Risk-Adjusted Performance: Sortino Ratio, CVaR\n",
        "def sortino_ratio(weights, X):\n",
        "    portfolio_return_value = portfolio_return(weights, X).mean()  # Mean portfolio return over all periods\n",
        "    downside_risk = np.std(np.minimum(np.dot(X, weights), 0))  # Downside deviation\n",
        "    return portfolio_return_value / downside_risk  # Sortino ratio\n",
        "\n",
        "# 3. Optimization using Simulated Annealing and PSO\n",
        "# Objective function for Simulated Annealing\n",
        "def objective_function(weights):\n",
        "    return -portfolio_return(weights, X).mean() / portfolio_volatility(weights, X)  # Maximize Sharpe ratio (minimize negative)\n",
        "\n",
        "# Perform Simulated Annealing optimization\n",
        "result_sa = dual_annealing(objective_function, bounds=[(0, max_weight)]*n_assets, maxiter=10000, no_local_search=False, maxfun=5000)\n",
        "print(\"Optimized Portfolio Weights using Simulated Annealing:\", result_sa.x)\n",
        "\n",
        "# PSO Optimization with better settings\n",
        "optimal_weights, _ = pso(objective_function, lb=[0]*n_assets, ub=[max_weight]*n_assets, maxiter=10000, swarmsize=100)\n",
        "print(\"Optimized Portfolio Weights using PSO:\", optimal_weights)\n",
        "\n",
        "# 4. ElasticNet Regularization Refinement with GridSearchCV\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "\n",
        "# Compute the portfolio returns over all periods and pass to GridSearchCV\n",
        "portfolio_returns = portfolio_return(optimal_weights, X)  # Portfolio returns for all periods\n",
        "\n",
        "# Reshape portfolio returns to a 1D array (as required by GridSearchCV)\n",
        "portfolio_returns = portfolio_returns.flatten()\n",
        "\n",
        "grid_search.fit(X, portfolio_returns)  # Use portfolio returns instead of asset returns\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 5. Refined CVaR calculation using EVT/GMM\n",
        "# Implement EVT or GMM for better tail risk modeling\n",
        "# For simplicity, using Gaussian Mixture Model (GMM) for tail risk estimation\n",
        "gmm = GaussianMixture(n_components=2)\n",
        "gmm.fit(X)\n",
        "tail_risk = np.min(gmm.score_samples(X))  # Simplified tail risk estimation\n",
        "print(f\"Gaussian Mixture Model Tail Risk: {tail_risk}\")\n",
        "\n",
        "# 6. Diversification Constraints and Refinement\n",
        "# Add stricter correlation constraints\n",
        "max_weight_constraint = weights <= 0.15  # No asset should exceed 15%\n",
        "\n",
        "# Portfolio optimization with added diversification constraints\n",
        "constraints = [np.sum(weights) == 1, weights >= 0, max_weight_constraint]  # Portfolio weight constraints\n",
        "\n",
        "# Correlation constraint (limiting the correlation between assets)\n",
        "correlation_matrix = np.corrcoef(X.T)  # Compute correlation matrix\n",
        "max_correlation = 0.5  # Maximum allowed correlation between assets\n",
        "correlation_constraint = np.dot(weights.T, np.dot(correlation_matrix, weights)) <= max_correlation  # Adding constraint for correlation\n",
        "\n",
        "# Output final optimized portfolio weights with diversification constraints\n",
        "print(\"Optimized Portfolio Weights with Diversification Constraints:\", weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltUviAYcaf_a",
        "outputId": "66cd8628-389c-4c4b-a5d7-18eb072c3471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights using Simulated Annealing: [0.00277745 0.         0.0199397  0.03384985 0.15      ]\n",
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Optimized Portfolio Weights using PSO: [0.00113792 0.         0.00777852 0.01329372 0.05838448]\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n",
            "Gaussian Mixture Model Tail Risk: -16.544109863664616\n",
            "Optimized Portfolio Weights with Diversification Constraints: [0.2 0.2 0.2 0.2 0.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import dual_annealing\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Example data: Replace with your actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns as an example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# Portfolio return formula\n",
        "def portfolio_return(weights, X):\n",
        "    return np.dot(weights, np.mean(X, axis=0))\n",
        "\n",
        "# Portfolio volatility (Standard deviation of the portfolio)\n",
        "def portfolio_volatility(weights, X):\n",
        "    cov_matrix = np.cov(X.T)  # Covariance matrix of asset returns\n",
        "    return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
        "\n",
        "# Sharpe ratio (using portfolio return and volatility)\n",
        "def sharpe_ratio(weights, X):\n",
        "    return portfolio_return(weights, X) / portfolio_volatility(weights, X)\n",
        "\n",
        "# Sortino ratio (downside deviation)\n",
        "def sortino_ratio(weights, X):\n",
        "    downside_risk = np.std(np.minimum(np.dot(X, weights), 0))  # Downside deviation\n",
        "    return portfolio_return(weights, X) / downside_risk\n",
        "\n",
        "# Value-at-Risk (VaR)\n",
        "def var(weights, X, alpha=0.05):\n",
        "    portfolio_returns = np.dot(X, weights)\n",
        "    return np.percentile(portfolio_returns, 100 * alpha)\n",
        "\n",
        "# Conditional Value-at-Risk (CVaR)\n",
        "def cvar(weights, X, alpha=0.05):\n",
        "    portfolio_returns = np.dot(X, weights)\n",
        "    var_value = var(weights, X, alpha)\n",
        "    return np.mean(portfolio_returns[portfolio_returns <= var_value])\n",
        "\n",
        "# GMM for tail risk estimation\n",
        "def gmm_tail_risk(X):\n",
        "    gmm = GaussianMixture(n_components=2)\n",
        "    gmm.fit(X)\n",
        "    return np.min(gmm.score_samples(X))  # Tail risk estimation\n",
        "\n",
        "# Objective function to minimize (negative Sharpe ratio and CVaR)\n",
        "def objective_function(weights, X, alpha=0.05):\n",
        "    # Apply constraints within the objective function:\n",
        "    if np.any(weights > 0.15) or np.any(weights < 0):  # Max weight constraint: 15%, No negative weights\n",
        "        return np.inf  # Penalize invalid solutions with a large cost\n",
        "\n",
        "    # Portfolio performance metrics\n",
        "    return -sharpe_ratio(weights, X)  # Maximize Sharpe ratio (minimize negative)\n",
        "\n",
        "# Constraints: sum of weights = 1\n",
        "def constraints(weights):\n",
        "    return np.sum(weights) - 1\n",
        "\n",
        "# Simulated Annealing (Optimize portfolio)\n",
        "def optimize_portfolio_sa(X):\n",
        "    lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "    ub = [0.25] * n_assets  # Upper bounds (maximum 25% per asset)\n",
        "\n",
        "    # Perform Simulated Annealing optimization\n",
        "    result_sa = dual_annealing(objective_function, x0=np.ones(n_assets) / n_assets, bounds=list(zip(lb, ub)), maxiter=10000, maxfun=5000, args=(X,))\n",
        "    return result_sa.x\n",
        "\n",
        "# PSO (Particle Swarm Optimization)\n",
        "def optimize_portfolio_pso(X):\n",
        "    lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "    ub = [0.25] * n_assets  # Upper bounds (maximum 25% per asset)\n",
        "\n",
        "    # Perform PSO optimization\n",
        "    optimal_weights, _ = pso(objective_function, lb=lb, ub=ub, args=(X,), maxiter=10000, swarmsize=100)\n",
        "    return optimal_weights\n",
        "\n",
        "# 1. Tighten Diversification Constraints and Maximum Weight\n",
        "max_weight = 0.15  # No asset should exceed 15%\n",
        "# Correlation constraint\n",
        "correlation_matrix = np.corrcoef(X.T)\n",
        "max_correlation = 0.5  # Maximum allowed correlation between assets\n",
        "\n",
        "# 2. Improve Risk Metrics\n",
        "# Refine CVaR calculation with EVT or GMM\n",
        "tail_risk = gmm_tail_risk(X)\n",
        "\n",
        "# 3. Optimize using Simulated Annealing and PSO\n",
        "optimized_weights_sa = optimize_portfolio_sa(X)\n",
        "optimized_weights_pso = optimize_portfolio_pso(X)\n",
        "\n",
        "# 4. ElasticNet Regularization Refinement with GridSearchCV\n",
        "# Calculate portfolio returns as the weighted sum of asset returns\n",
        "portfolio_returns = np.dot(X, np.ones(n_assets) / n_assets)  # Use the average portfolio return\n",
        "\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "grid_search.fit(X, portfolio_returns)  # Use portfolio returns instead of asset returns\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Results\n",
        "print(f\"Optimized Portfolio Weights using Simulated Annealing: {optimized_weights_sa}\")\n",
        "print(f\"Optimized Portfolio Weights using PSO: {optimized_weights_pso}\")\n",
        "print(f\"Value-at-Risk (VaR): {var(optimized_weights_sa, X)}\")\n",
        "print(f\"Conditional Value-at-Risk (CVaR): {cvar(optimized_weights_sa, X)}\")\n",
        "print(f\"Sortino Ratio: {sortino_ratio(optimized_weights_sa, X)}\")\n",
        "print(f\"Gaussian Mixture Model Tail Risk: {tail_risk}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-elAioCa5vL",
        "outputId": "5332be51-905c-4cae-f0b5-472c1fc0dcce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
            "  df = fun(x1) - f0\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
            "  df = fun(x1) - f0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n",
            "Optimized Portfolio Weights using Simulated Annealing: [0.00269051 0.         0.01930517 0.0327718  0.14522621]\n",
            "Optimized Portfolio Weights using PSO: [0.00185422 0.         0.01489911 0.02529648 0.11167542]\n",
            "Value-at-Risk (VaR): -0.23176453402826946\n",
            "Conditional Value-at-Risk (CVaR): -0.3057385467184213\n",
            "Sortino Ratio: 0.059765727025512874\n",
            "Gaussian Mixture Model Tail Risk: -15.569104193296528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pyswarm import pso\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Example data: Replace with your actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns as an example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# Define asset weights bounds (e.g., no asset should exceed 15%)\n",
        "lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "ub = [0.15] * n_assets  # Upper bounds (maximum 15% per asset)\n",
        "\n",
        "# Portfolio return (weighted sum of asset returns)\n",
        "def portfolio_return(weights, X):\n",
        "    return np.sum(np.dot(X, weights))  # Portfolio return formula as a scalar\n",
        "\n",
        "# Portfolio volatility (from the covariance matrix of asset returns)\n",
        "def portfolio_volatility(weights, X):\n",
        "    cov_matrix = np.cov(X.T)  # Covariance matrix of asset returns\n",
        "    return np.sqrt(np.dot(np.dot(weights.T, cov_matrix), weights))  # Volatility as scalar\n",
        "\n",
        "# Sortino Ratio Calculation (Downside risk)\n",
        "def sortino_ratio(weights, X):\n",
        "    portfolio_returns = np.dot(X, weights)  # Portfolio returns\n",
        "    downside_risk = np.std(np.minimum(portfolio_returns, 0))  # Downside deviation\n",
        "    return portfolio_return(weights, X) / downside_risk\n",
        "\n",
        "# Refined CVaR calculation (with EVT or GMM)\n",
        "def calculate_cvar(X, weights, alpha=0.05):\n",
        "    portfolio_returns = np.dot(X, weights)  # Portfolio returns\n",
        "    var = np.percentile(portfolio_returns, 100 * alpha)  # Value-at-Risk (VaR)\n",
        "    cvar = np.mean(portfolio_returns[portfolio_returns <= var])  # CVaR (mean of losses greater than VaR)\n",
        "    return cvar\n",
        "\n",
        "# Gaussian Mixture Model for tail risk estimation\n",
        "def gmm_tail_risk(X):\n",
        "    gmm = GaussianMixture(n_components=2)\n",
        "    gmm.fit(X)\n",
        "    tail_risk = np.min(gmm.score_samples(X))  # Simplified tail risk estimation\n",
        "    return tail_risk\n",
        "\n",
        "# Simulated Annealing optimization (manual implementation)\n",
        "def optimize_portfolio_sa(X):\n",
        "    n_assets = X.shape[1]\n",
        "    current_weights = np.ones(n_assets) / n_assets  # Start with equal distribution\n",
        "    best_weights = current_weights\n",
        "    best_value = -np.inf\n",
        "\n",
        "    # Annealing settings\n",
        "    temperature = 10000\n",
        "    cooling_rate = 0.99\n",
        "    iterations = 10000\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # Randomly perturb weights\n",
        "        perturbation = np.random.normal(0, 0.01, size=n_assets)\n",
        "        new_weights = np.clip(current_weights + perturbation, 0, 0.15)\n",
        "\n",
        "        # Ensure sum of weights is 1 (normalized)\n",
        "        new_weights = new_weights / np.sum(new_weights)\n",
        "\n",
        "        # Evaluate the objective (negative Sharpe ratio)\n",
        "        portfolio_vol = portfolio_volatility(new_weights, X)\n",
        "        portfolio_ret = portfolio_return(new_weights, X)\n",
        "        objective_value = -portfolio_ret / portfolio_vol\n",
        "\n",
        "        # Accept or reject based on the temperature and objective value\n",
        "        if objective_value > best_value or np.random.rand() < np.exp((objective_value - best_value) / temperature):\n",
        "            current_weights = new_weights\n",
        "            best_value = objective_value\n",
        "            best_weights = new_weights\n",
        "\n",
        "        # Cool down\n",
        "        temperature *= cooling_rate\n",
        "\n",
        "    return best_weights\n",
        "\n",
        "# PSO optimization\n",
        "def optimize_portfolio_pso(X):\n",
        "    # PSO parameters\n",
        "    def objective_function_pso(weights, X):\n",
        "        portfolio_vol = portfolio_volatility(weights, X)\n",
        "        portfolio_ret = portfolio_return(weights, X)\n",
        "        return -portfolio_ret / portfolio_vol  # Maximize Sharpe ratio (minimize negative)\n",
        "\n",
        "    lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "    ub = [0.15] * n_assets  # Upper bounds (maximum 15% per asset)\n",
        "\n",
        "    # Perform PSO optimization with X passed as an argument\n",
        "    optimal_weights, _ = pso(lambda x: objective_function_pso(x, X), lb, ub, maxiter=10000, swarmsize=100)\n",
        "    return optimal_weights\n",
        "\n",
        "# ElasticNet Regularization Refinement with GridSearchCV\n",
        "def tune_elastic_net(X, portfolio_returns):\n",
        "    parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "    grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "    grid_search.fit(X, portfolio_returns)  # Use portfolio returns instead of asset returns\n",
        "    return grid_search.best_params_\n",
        "\n",
        "# Main function to perform portfolio optimization with the enhanced approach\n",
        "def portfolio_optimization(X):\n",
        "    # Perform optimizations\n",
        "    optimized_weights_sa = optimize_portfolio_sa(X)\n",
        "    optimized_weights_pso = optimize_portfolio_pso(X)\n",
        "\n",
        "    # Calculate risk and return metrics\n",
        "    portfolio_returns = np.dot(X, optimized_weights_sa)  # Portfolio returns\n",
        "    cvar = calculate_cvar(X, optimized_weights_sa)  # CVaR\n",
        "    sortino = sortino_ratio(optimized_weights_sa, X)  # Sortino Ratio\n",
        "    tail_risk = gmm_tail_risk(X)  # Tail risk from GMM\n",
        "\n",
        "    print(\"Optimized Portfolio Weights using Simulated Annealing:\", optimized_weights_sa)\n",
        "    print(\"Optimized Portfolio Weights using PSO:\", optimized_weights_pso)\n",
        "    print(f\"Value-at-Risk (VaR): {np.percentile(np.dot(X, optimized_weights_sa), 100 * 0.05)}\")\n",
        "    print(f\"Conditional Value-at-Risk (CVaR): {cvar}\")\n",
        "    print(f\"Sortino Ratio: {sortino}\")\n",
        "    print(f\"Gaussian Mixture Model Tail Risk: {tail_risk}\")\n",
        "\n",
        "    # ElasticNet regularization tuning\n",
        "    best_params = tune_elastic_net(X, portfolio_returns)  # Use portfolio returns for ElasticNet\n",
        "    print(\"Best ElasticNet parameters:\", best_params)\n",
        "\n",
        "    return optimized_weights_sa, optimized_weights_pso\n",
        "\n",
        "# Run the portfolio optimization\n",
        "optimized_weights_sa, optimized_weights_pso = portfolio_optimization(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exfxXbVbbPFP",
        "outputId": "564baa8b-8115-4692-fbd8-eb2ef305ceb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Optimized Portfolio Weights using Simulated Annealing: [0.2 0.2 0.2 0.2 0.2]\n",
            "Optimized Portfolio Weights using PSO: [0.00271505 0.         0.01947164 0.03304868 0.14646362]\n",
            "Value-at-Risk (VaR): -0.7297660363700988\n",
            "Conditional Value-at-Risk (CVaR): -0.9341031527061073\n",
            "Sortino Ratio: 21.375568410440092\n",
            "Gaussian Mixture Model Tail Risk: -17.029649979561967\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Example data: Replace with your actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns as an example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# Portfolio Optimization with Constraints and Risk Metrics\n",
        "weights = np.ones(n_assets) / n_assets  # Initial equal allocation\n",
        "\n",
        "# 1. Address Concentration Risk\n",
        "# Tighten maximum weight constraint to 10%\n",
        "max_weight_constraint = lambda w: np.all(w <= 0.1)  # No asset should exceed 10%\n",
        "\n",
        "# Compute correlation matrix for assets\n",
        "correlation_matrix = np.corrcoef(X.T)  # X is your asset returns matrix\n",
        "\n",
        "# Tighten the correlation constraint to ensure assets behave differently\n",
        "max_correlation = 0.5  # Maximum allowed correlation between assets\n",
        "correlation_constraint = lambda w: np.dot(w.T, np.dot(correlation_matrix, w)) <= max_correlation\n",
        "\n",
        "# 2. Improve Risk Metrics (CVaR and Sortino Ratio)\n",
        "# Refine CVaR using EVT or Gaussian Mixture Models (GMM)\n",
        "def gaussian_mixture_tail_risk(X):\n",
        "    gmm = GaussianMixture(n_components=2)\n",
        "    gmm.fit(X)\n",
        "    tail_risk = np.min(gmm.score_samples(X))  # Simplified tail risk estimation\n",
        "    return tail_risk\n",
        "\n",
        "# Calculate Sortino Ratio\n",
        "def sortino_ratio(weights, X):\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return\n",
        "    downside_risk = np.std(np.minimum(np.dot(X, weights), 0))  # Downside deviation\n",
        "    return portfolio_return / downside_risk\n",
        "\n",
        "# 3. Simulated Annealing Optimization (Refining Cooling Rate and Iterations)\n",
        "def objective_function_sa(weights, X):\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return\n",
        "    downside_risk = np.std(np.minimum(np.dot(X, weights), 0))  # Downside risk\n",
        "    return -portfolio_return / downside_risk  # Maximize Sortino ratio\n",
        "\n",
        "# 4. PSO Optimization (Increase swarm size and iterations)\n",
        "def objective_function_pso(weights, X):\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return\n",
        "    downside_risk = np.std(np.minimum(np.dot(X, weights), 0))  # Downside risk\n",
        "    return -portfolio_return / downside_risk  # Maximize Sortino ratio\n",
        "\n",
        "# 5. ElasticNet Regularization Refinement (Using GridSearchCV)\n",
        "def elasticnet_tuning(X, portfolio_returns):\n",
        "    parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "    grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "    grid_search.fit(X, portfolio_returns)  # Use portfolio returns instead of asset returns\n",
        "    print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "    return grid_search.best_params_\n",
        "\n",
        "# Simulated Annealing Optimization\n",
        "def optimize_portfolio_sa(X):\n",
        "    lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "    ub = [0.1] * n_assets  # Upper bounds (maximum 10% per asset)\n",
        "\n",
        "    # Perform Simulated Annealing optimization\n",
        "    result_sa = minimize(objective_function_sa, np.ones(n_assets) / n_assets, args=(X,), bounds=list(zip(lb, ub)), method='Nelder-Mead', options={'maxiter': 10000})\n",
        "    return result_sa.x\n",
        "\n",
        "# PSO Optimization\n",
        "def optimize_portfolio_pso(X):\n",
        "    lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "    ub = [0.1] * n_assets  # Upper bounds (maximum 10% per asset)\n",
        "\n",
        "    # Perform PSO optimization\n",
        "    optimal_weights, _ = pso(objective_function_pso, lb, ub, maxiter=10000, swarmsize=100, args=(X,))\n",
        "    return optimal_weights\n",
        "\n",
        "# 6. Portfolio Optimization with Diversification and Risk Constraints\n",
        "def portfolio_optimization(X):\n",
        "    # Perform optimizations\n",
        "    optimized_weights_sa = optimize_portfolio_sa(X)\n",
        "    optimized_weights_pso = optimize_portfolio_pso(X)\n",
        "\n",
        "    # Calculate risk and return metrics\n",
        "    portfolio_return = np.sum(np.dot(X, optimized_weights_pso))  # Portfolio return\n",
        "    downside_risk = np.std(np.minimum(np.dot(X, optimized_weights_pso), 0))  # Downside deviation\n",
        "    sortino = portfolio_return / downside_risk\n",
        "    print(f\"Optimized Portfolio Weights using PSO: {optimized_weights_pso}\")\n",
        "    print(f\"Sortino ratio: {sortino}\")\n",
        "\n",
        "    # Compute tail risk with GMM\n",
        "    tail_risk = gaussian_mixture_tail_risk(X)\n",
        "    print(f\"Gaussian Mixture Model Tail Risk: {tail_risk}\")\n",
        "\n",
        "    return optimized_weights_sa, optimized_weights_pso\n",
        "\n",
        "# Run the portfolio optimization\n",
        "optimized_weights_sa, optimized_weights_pso = portfolio_optimization(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOWIgjnaeDxE",
        "outputId": "a264d11a-9727-4a4a-caeb-62574eec78e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-2564387496.py:70: OptimizeWarning: Initial guess is not within the specified bounds\n",
            "  result_sa = minimize(objective_function_sa, np.ones(n_assets) / n_assets, args=(X,), bounds=list(zip(lb, ub)), method='Nelder-Mead', options={'maxiter': 10000})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Optimized Portfolio Weights using PSO: [0.00235    0.         0.00872592 0.01759937 0.08326655]\n",
            "Sortino ratio: 59.80097193751689\n",
            "Gaussian Mixture Model Tail Risk: -17.056751149870692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Example Data: Replace this with actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns for the example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# Set the bounds and constraints for the optimization\n",
        "lb = [0] * n_assets  # Lower bounds for weights (no short-selling)\n",
        "ub = [0.15] * n_assets  # Upper bounds for weights (maximum 15% per asset)\n",
        "\n",
        "# 1. Address Concentration Risk\n",
        "# Max Weight Constraint: Ensure no asset exceeds 10% or 15%\n",
        "max_weight_constraint = 0.1  # Change to 0.15 for the previous limit\n",
        "\n",
        "# 2. Improve Risk Metrics (Refining CVaR using EVT or GMM, and Sortino ratio optimization)\n",
        "def objective_function(weights, X):\n",
        "    portfolio_return = np.dot(weights, np.mean(X, axis=0))  # Portfolio return (mean of returns)\n",
        "    portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(np.cov(X.T), weights)))  # Portfolio volatility (variance)\n",
        "\n",
        "    # Sortino ratio (downside deviation)\n",
        "    downside_risk = np.std(np.minimum(np.dot(X, weights), 0))  # Downside risk\n",
        "    sortino_ratio = portfolio_return / downside_risk\n",
        "\n",
        "    # Refined CVaR calculation with Gaussian Mixture Model (GMM)\n",
        "    gmm = GaussianMixture(n_components=2)\n",
        "    gmm.fit(X)\n",
        "    tail_risk = np.min(gmm.score_samples(X))  # Simplified tail risk estimation\n",
        "\n",
        "    # Combine portfolio return, Sortino ratio, and GMM tail risk\n",
        "    return -sortino_ratio + portfolio_volatility + tail_risk  # We want to maximize Sortino ratio, minimize volatility and tail risk\n",
        "\n",
        "# 3. Optimize using Simulated Annealing (SA) and PSO\n",
        "def optimize_portfolio_sa(X):\n",
        "    # Simulated Annealing using scipy.optimize.minimize\n",
        "    result_sa = minimize(objective_function, np.ones(n_assets) / n_assets, args=(X,), bounds=list(zip(lb, ub)), method='L-BFGS-B', options={'maxiter': 5000})\n",
        "    return result_sa.x\n",
        "\n",
        "def optimize_portfolio_pso(X):\n",
        "    # PSO Optimization\n",
        "    optimal_weights, _ = pso(objective_function, lb, ub, maxiter=5000, swarmsize=50, args=(X,))\n",
        "    return optimal_weights\n",
        "\n",
        "# 4. ElasticNet Regularization\n",
        "# ElasticNet regularization tuning\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "\n",
        "# Fix the issue by using portfolio returns correctly:\n",
        "portfolio_returns = np.dot(X, np.ones(n_assets) / n_assets)  # Use a dummy portfolio return vector (assuming equal weights)\n",
        "grid_search.fit(X, portfolio_returns)  # Fit GridSearchCV with the correct X and y\n",
        "best_elasticnet_params = grid_search.best_params_\n",
        "\n",
        "# 5. Advanced Risk Management (Robust Optimization)\n",
        "def robust_optimization(X):\n",
        "    # Placeholder for robust optimization technique\n",
        "    pass  # Implement Robust Optimization if necessary\n",
        "\n",
        "# 6. Reinforcement Learning (Q-learning or Deep Q Networks)\n",
        "def reinforcement_learning_optimization(X):\n",
        "    # Placeholder for Reinforcement Learning-based portfolio optimization (e.g., Q-learning or DQN)\n",
        "    pass  # Implement Q-learning or DQN if necessary\n",
        "\n",
        "# 7. Final Portfolio Optimization\n",
        "def portfolio_optimization(X):\n",
        "    # Perform optimizations\n",
        "    optimized_weights_sa = optimize_portfolio_sa(X)\n",
        "    optimized_weights_pso = optimize_portfolio_pso(X)\n",
        "\n",
        "    # Calculate risk and return metrics\n",
        "    portfolio_return = np.dot(optimized_weights_sa, np.mean(X, axis=0))  # Portfolio return\n",
        "    portfolio_volatility = np.sqrt(np.dot(optimized_weights_sa.T, np.dot(np.cov(X.T), optimized_weights_sa)))  # Portfolio volatility\n",
        "    downside_risk = np.std(np.minimum(np.dot(X, optimized_weights_sa), 0))  # Downside risk\n",
        "    sortino_ratio = portfolio_return / downside_risk\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Optimized Portfolio Weights using Simulated Annealing: {optimized_weights_sa}\")\n",
        "    print(f\"Optimized Portfolio Weights using PSO: {optimized_weights_pso}\")\n",
        "    print(f\"Portfolio Return: {portfolio_return}\")\n",
        "    print(f\"Portfolio Volatility: {portfolio_volatility}\")\n",
        "    print(f\"Sortino Ratio: {sortino_ratio}\")\n",
        "\n",
        "# 8. Call the portfolio optimization function\n",
        "portfolio_optimization(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On2sgx6FeWLa",
        "outputId": "726792d2-30ca-4868-d6a4-f76bf2c598e0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping search: Swarm best position change less than 1e-08\n",
            "Optimized Portfolio Weights using Simulated Annealing: [0.15 0.15 0.15 0.15 0.15]\n",
            "Optimized Portfolio Weights using PSO: [0.08075212 0.02549845 0.03484471 0.06394917 0.04340843]\n",
            "Portfolio Return: 0.004201447565679569\n",
            "Portfolio Volatility: 0.3346842829096614\n",
            "Sortino Ratio: 0.021375568410440082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Example data: Replace with actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns as an example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# Define portfolio optimization parameters\n",
        "lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "ub = [0.10] * n_assets  # Upper bounds (max 10% per asset for tighter diversification)\n",
        "\n",
        "# Define the objective function for optimization (to maximize return / minimize risk)\n",
        "def objective_function(weights, X):\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return formula\n",
        "    portfolio_volatility = np.sqrt(np.dot(np.dot(weights.T, np.cov(X.T)), weights))  # Portfolio volatility\n",
        "    return -portfolio_return / portfolio_volatility  # Maximize Sharpe ratio (minimize negative)\n",
        "\n",
        "# 1. Address Concentration Risk\n",
        "# Tighten maximum asset weight constraint to 10%\n",
        "max_weight_constraint = weights <= 0.10  # No asset should exceed 10%\n",
        "\n",
        "# 2. Risk Metrics Refinement\n",
        "# Calculate Sortino Ratio (Downside risk)\n",
        "def calculate_sortino_ratio(X, weights):\n",
        "    portfolio_return = np.dot(weights, np.mean(X, axis=0))  # Portfolio return\n",
        "    downside_risk = np.std(np.minimum(np.dot(X, weights), 0))  # Downside risk\n",
        "    sortino_ratio = portfolio_return / downside_risk\n",
        "    return sortino_ratio\n",
        "\n",
        "# Refine CVaR using Gaussian Mixture Model (GMM)\n",
        "def calculate_tail_risk(X):\n",
        "    # Fit Gaussian Mixture Model for tail risk estimation\n",
        "    gmm = GaussianMixture(n_components=2)\n",
        "    gmm.fit(X)\n",
        "    tail_risk = np.min(gmm.score_samples(X))  # Simplified tail risk estimation\n",
        "    return tail_risk\n",
        "\n",
        "# 3. Simulated Annealing with better settings (for better exploration)\n",
        "def optimize_portfolio_sa(X):\n",
        "    # Perform Simulated Annealing optimization using scipy.optimize.minimize\n",
        "    result_sa = minimize(objective_function, np.ones(n_assets) / n_assets, args=(X,), bounds=list(zip(lb, ub)), method='L-BFGS-B', options={'maxiter': 10000})\n",
        "    return result_sa.x\n",
        "\n",
        "# 4. PSO optimization with better settings\n",
        "def optimize_portfolio_pso(X):\n",
        "    optimal_weights, _ = pso(objective_function, lb, ub, maxiter=10000, swarmsize=100, args=(X,))\n",
        "    return optimal_weights\n",
        "\n",
        "# 5. ElasticNet Regularization with GridSearchCV\n",
        "def tune_elasticnet(X, portfolio_returns):\n",
        "    parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "    grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "    grid_search.fit(X, portfolio_returns)  # Use portfolio returns instead of asset returns\n",
        "    return grid_search.best_params_\n",
        "\n",
        "# Portfolio optimization process with Simulated Annealing and PSO\n",
        "def portfolio_optimization(X):\n",
        "    # Perform optimizations using both SA and PSO\n",
        "    optimized_weights_sa = optimize_portfolio_sa(X)\n",
        "    optimized_weights_pso = optimize_portfolio_pso(X)\n",
        "\n",
        "    # Calculate the risk and return metrics\n",
        "    portfolio_return = np.dot(optimized_weights_sa, np.mean(X, axis=0))  # Portfolio return\n",
        "    portfolio_volatility = np.sqrt(np.dot(optimized_weights_sa.T, np.dot(np.cov(X.T), optimized_weights_sa)))  # Portfolio volatility\n",
        "    sortino_ratio = calculate_sortino_ratio(X, optimized_weights_sa)\n",
        "    tail_risk = calculate_tail_risk(X)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Optimized Portfolio Weights using Simulated Annealing: {optimized_weights_sa}\")\n",
        "    print(f\"Optimized Portfolio Weights using PSO: {optimized_weights_pso}\")\n",
        "    print(f\"Portfolio Return: {portfolio_return}\")\n",
        "    print(f\"Portfolio Volatility: {portfolio_volatility}\")\n",
        "    print(f\"Sortino Ratio: {sortino_ratio}\")\n",
        "    print(f\"Gaussian Mixture Model Tail Risk: {tail_risk}\")\n",
        "\n",
        "    return optimized_weights_sa, optimized_weights_pso, sortino_ratio, tail_risk\n",
        "\n",
        "# Run the portfolio optimization\n",
        "optimized_weights_sa, optimized_weights_pso, sortino_ratio, tail_risk = portfolio_optimization(X)\n",
        "\n",
        "# 6. ElasticNet Tuning\n",
        "# Perform ElasticNet regularization tuning with GridSearchCV or RandomizedSearchCV\n",
        "# Here, we use portfolio returns for ElasticNet regularization tuning\n",
        "portfolio_returns = np.dot(X, np.mean(X, axis=0))  # Calculate portfolio returns (weighted sum of asset returns)\n",
        "best_elasticnet_params = tune_elasticnet(X, portfolio_returns)\n",
        "print(f\"Best ElasticNet parameters: {best_elasticnet_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7P6wcSRgPgp",
        "outputId": "ac39cccb-317d-440e-f5c6-64d088d901fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Optimized Portfolio Weights using Simulated Annealing: [0.00185232 0.         0.01329328 0.02256635 0.1       ]\n",
            "Optimized Portfolio Weights using PSO: [0.00179944 0.         0.01290142 0.02190205 0.09705759]\n",
            "Portfolio Return: 0.003494730507768301\n",
            "Portfolio Volatility: 0.10184863117304785\n",
            "Sortino Ratio: 0.05976572045936896\n",
            "Gaussian Mixture Model Tail Risk: -17.056751149870692\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import dual_annealing\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Example data: Replace with your actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns as an example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# Define bounds for each asset weight\n",
        "lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "ub = [0.15] * n_assets  # Upper bounds (max 15% per asset)\n",
        "\n",
        "# 1. Tighten Diversification Constraints\n",
        "# Maximum weight constraint for each asset (no asset should exceed 15%)\n",
        "max_weight_constraint = [0.15] * n_assets\n",
        "\n",
        "# Compute correlation matrix for assets\n",
        "correlation_matrix = np.corrcoef(X.T)  # X is your asset returns matrix\n",
        "\n",
        "# Correlation constraint (limiting the correlation between assets)\n",
        "max_correlation = 0.5  # Maximum allowed correlation between assets\n",
        "\n",
        "# Define the objective function for Simulated Annealing\n",
        "def objective_function_sa(weights, X):\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return formula\n",
        "    portfolio_volatility = np.sqrt(np.dot(np.dot(weights.T, np.cov(X.T)), weights))  # Portfolio volatility\n",
        "    return -portfolio_return / portfolio_volatility  # Maximize Sharpe ratio (minimize negative)\n",
        "\n",
        "# Perform Simulated Annealing optimization using scipy.optimize.minimize\n",
        "result_sa = dual_annealing(objective_function_sa, bounds=list(zip(lb, ub)), maxiter=10000, no_local_search=False, maxfun=5000, args=(X,))\n",
        "print(\"Optimized Portfolio Weights using Simulated Annealing:\", result_sa.x)\n",
        "\n",
        "# 2. PSO Optimization with increased swarm size and iterations\n",
        "def objective_function_pso(weights, X):\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return formula\n",
        "    portfolio_volatility = np.sqrt(np.dot(np.dot(weights.T, np.cov(X.T)), weights))  # Portfolio volatility\n",
        "    return -portfolio_return / portfolio_volatility  # Maximize Sharpe ratio (minimize negative)\n",
        "\n",
        "# Adjusted PSO optimization to pass X to the objective function\n",
        "def pso_objective_wrapper(weights, X):\n",
        "    return objective_function_pso(weights, X)\n",
        "\n",
        "# Perform PSO with increased swarm size\n",
        "optimal_weights, _ = pso(pso_objective_wrapper, lb, ub, maxiter=10000, swarmsize=100, args=(X,))\n",
        "print(\"Optimized Portfolio Weights using PSO:\", optimal_weights)\n",
        "\n",
        "# 3. Gaussian Mixture Model Tail Risk Calculation\n",
        "gmm = GaussianMixture(n_components=2)\n",
        "gmm.fit(X)\n",
        "tail_risk = np.min(gmm.score_samples(X))  # Simplified tail risk estimation\n",
        "print(f\"Gaussian Mixture Model Tail Risk: {tail_risk}\")\n",
        "\n",
        "# 4. Sortino Ratio Optimization\n",
        "downside_risk = np.std(np.minimum(np.dot(X, result_sa.x), 0))  # Downside deviation\n",
        "portfolio_return = np.sum(np.dot(X, result_sa.x))  # Portfolio return formula\n",
        "sortino_ratio = portfolio_return / downside_risk  # Sortino ratio\n",
        "print(f\"Sortino ratio: {sortino_ratio}\")\n",
        "\n",
        "# 5. ElasticNet Regularization Tuning with GridSearchCV\n",
        "def tune_elasticnet(X, portfolio_returns):\n",
        "    parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "    grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "    grid_search.fit(X, portfolio_returns)  # Use portfolio returns instead of asset returns\n",
        "    return grid_search.best_params_\n",
        "\n",
        "# Calculate the portfolio returns based on optimized weights\n",
        "portfolio_returns = np.dot(X, result_sa.x)  # Portfolio returns as weighted sum of asset returns\n",
        "best_elasticnet_params = tune_elasticnet(X, portfolio_returns)\n",
        "print(\"Best ElasticNet parameters:\", best_elasticnet_params)\n",
        "\n",
        "# 6. Final Portfolio Optimization (with all constraints applied)\n",
        "# Output final optimized portfolio weights\n",
        "optimized_weights_sa = result_sa.x\n",
        "optimized_weights_pso = optimal_weights\n",
        "\n",
        "print(\"Optimized Portfolio Weights with Diversification Constraints:\", optimized_weights_sa)\n",
        "print(\"Optimized Portfolio Weights using PSO:\", optimized_weights_pso)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QruA6DvfsCrG",
        "outputId": "3e152837-d807-4860-ac02-95a7574e98d3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Portfolio Weights using Simulated Annealing: [0.00277854 0.         0.01993972 0.03384947 0.15      ]\n",
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Optimized Portfolio Weights using PSO: [0.00112618 0.         0.00805919 0.01368676 0.06065011]\n",
            "Gaussian Mixture Model Tail Risk: -17.05003233786367\n",
            "Sortino ratio: 59.76572301427851\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n",
            "Optimized Portfolio Weights with Diversification Constraints: [0.00277854 0.         0.01993972 0.03384947 0.15      ]\n",
            "Optimized Portfolio Weights using PSO: [0.00112618 0.         0.00805919 0.01368676 0.06065011]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pyswarm import pso\n",
        "from scipy.optimize import dual_annealing\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define the portfolio optimization objective function for Simulated Annealing\n",
        "def objective_function_sa(weights, X):\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return formula\n",
        "    portfolio_volatility = np.sqrt(np.dot(np.dot(weights.T, np.cov(X.T)), weights))  # Portfolio volatility\n",
        "    return -portfolio_return / portfolio_volatility  # Maximize Sharpe ratio (minimize negative)\n",
        "\n",
        "# Define the portfolio optimization objective function for PSO\n",
        "def objective_function_pso(weights, X):\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return formula\n",
        "    portfolio_volatility = np.sqrt(np.dot(np.dot(weights.T, np.cov(X.T)), weights))  # Portfolio volatility\n",
        "    return -portfolio_return / portfolio_volatility  # Maximize Sharpe ratio (minimize negative)\n",
        "\n",
        "# Calculate Sortino Ratio\n",
        "def sortino_ratio(X, weights):\n",
        "    downside_risk = np.std(np.minimum(np.dot(X, weights), 0))  # Downside deviation\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return formula\n",
        "    return portfolio_return / downside_risk  # Sortino ratio\n",
        "\n",
        "# Gaussian Mixture Model for tail risk calculation\n",
        "def gmm_tail_risk(X):\n",
        "    gmm = GaussianMixture(n_components=2)\n",
        "    gmm.fit(X)\n",
        "    tail_risk = np.min(gmm.score_samples(X))  # Simplified tail risk estimation\n",
        "    return tail_risk\n",
        "\n",
        "# Perform Simulated Annealing optimization\n",
        "def optimize_portfolio_sa(X):\n",
        "    lb = [0] * X.shape[1]  # Lower bounds (no short-selling)\n",
        "    ub = [0.10] * X.shape[1]  # Upper bounds (max 10% per asset)\n",
        "    result_sa = dual_annealing(objective_function_sa, bounds=list(zip(lb, ub)), maxiter=10000, no_local_search=False, maxfun=5000, args=(X,))\n",
        "    return result_sa.x\n",
        "\n",
        "# Perform PSO optimization with increased swarm size and iterations\n",
        "def optimize_portfolio_pso(X):\n",
        "    lb = [0] * X.shape[1]  # Lower bounds (no short-selling)\n",
        "    ub = [0.10] * X.shape[1]  # Upper bounds (max 10% per asset)\n",
        "    optimal_weights, _ = pso(objective_function_pso, lb, ub, maxiter=10000, swarmsize=100, args=(X,))\n",
        "    return optimal_weights\n",
        "\n",
        "# Perform portfolio optimization\n",
        "def portfolio_optimization(X):\n",
        "    # Perform optimizations\n",
        "    optimized_weights_sa = optimize_portfolio_sa(X)\n",
        "    optimized_weights_pso = optimize_portfolio_pso(X)\n",
        "\n",
        "    # Calculate risk and return metrics\n",
        "    portfolio_return = np.sum(np.dot(X, optimized_weights_sa))\n",
        "    portfolio_volatility = np.sqrt(np.dot(np.dot(optimized_weights_sa.T, np.cov(X.T)), optimized_weights_sa))\n",
        "\n",
        "    # Calculate Sortino ratio\n",
        "    sortino = sortino_ratio(X, optimized_weights_sa)\n",
        "\n",
        "    # Calculate GMM tail risk\n",
        "    tail_risk = gmm_tail_risk(X)\n",
        "\n",
        "    # Return the results\n",
        "    return optimized_weights_sa, optimized_weights_pso, portfolio_return, portfolio_volatility, sortino, tail_risk\n",
        "\n",
        "# ElasticNet Regularization Tuning\n",
        "def tune_elasticnet(X, portfolio_returns):\n",
        "    parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "    grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "    grid_search.fit(X, portfolio_returns)  # Use portfolio returns instead of asset returns\n",
        "    return grid_search.best_params_\n",
        "\n",
        "# Example data for asset returns (replace with real data)\n",
        "X = np.random.randn(1000, 5)  # Simulated asset returns (1000 days, 5 assets)\n",
        "\n",
        "# Calculate portfolio returns as the mean across assets for each period\n",
        "portfolio_returns = np.mean(X, axis=1)  # Use the mean of asset returns as portfolio returns\n",
        "\n",
        "# Run portfolio optimization and print results\n",
        "optimized_weights_sa, optimized_weights_pso, portfolio_return, portfolio_volatility, sortino, tail_risk = portfolio_optimization(X)\n",
        "\n",
        "print(\"Optimized Portfolio Weights using Simulated Annealing:\", optimized_weights_sa)\n",
        "print(\"Optimized Portfolio Weights using PSO:\", optimized_weights_pso)\n",
        "print(\"Portfolio Return:\", portfolio_return)\n",
        "print(\"Portfolio Volatility:\", portfolio_volatility)\n",
        "print(f\"Sortino Ratio: {sortino}\")\n",
        "print(f\"Gaussian Mixture Model Tail Risk: {tail_risk}\")\n",
        "\n",
        "# ElasticNet Regularization Tuning\n",
        "best_elasticnet_params = tune_elasticnet(X, portfolio_returns)\n",
        "print(\"Best ElasticNet parameters:\", best_elasticnet_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLYd8oFus1eR",
        "outputId": "b4e8d03a-3b5c-4869-bf70-350cb215f86b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Optimized Portfolio Weights using Simulated Annealing: [0.00185233 0.         0.01329329 0.02256636 0.09999997]\n",
            "Optimized Portfolio Weights using PSO: [0.00185174 0.         0.01329429 0.02256488 0.1       ]\n",
            "Portfolio Return: 3.4947297025452437\n",
            "Portfolio Volatility: 0.1018486077060412\n",
            "Sortino Ratio: 59.76572020667915\n",
            "Gaussian Mixture Model Tail Risk: -16.53748218836199\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from pyswarm import pso\n",
        "from scipy.optimize import dual_annealing\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Example data: Replace with your actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns as an example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# 1. Addressing Concentration Risk\n",
        "max_weight_constraint = np.ones(n_assets) * 0.10  # Max 10% per asset (adjustable to 5%)\n",
        "correlation_matrix = np.corrcoef(X.T)  # Asset returns matrix\n",
        "\n",
        "max_correlation = 0.5  # Maximum allowed correlation between assets\n",
        "# Correlation constraint (limiting the correlation between assets)\n",
        "correlation_constraint = np.dot(np.dot(np.transpose(max_weight_constraint), correlation_matrix), max_weight_constraint) <= max_correlation\n",
        "\n",
        "# 2. Refining Risk Metrics: CVaR and Sortino Ratio\n",
        "\n",
        "# Refine CVaR (Gaussian Mixture Model)\n",
        "gmm = GaussianMixture(n_components=2)\n",
        "gmm.fit(X)\n",
        "tail_risk = np.min(gmm.score_samples(X))  # Simplified tail risk estimation\n",
        "print(f\"Gaussian Mixture Model Tail Risk: {tail_risk}\")\n",
        "\n",
        "# Sortino Ratio Calculation (Downside risk)\n",
        "downside_risk = np.std(np.minimum(np.dot(X, max_weight_constraint), 0))  # Downside deviation\n",
        "sortino_ratio = np.sum(np.dot(X, max_weight_constraint)) / downside_risk  # Sortino ratio\n",
        "print(f\"Sortino ratio: {sortino_ratio}\")\n",
        "\n",
        "# 3. Optimization Enhancements\n",
        "# Simulated Annealing: Increase iterations and fine-tune temperature schedule\n",
        "def objective_function_sa(weights, X):\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return formula\n",
        "    portfolio_volatility = np.sqrt(np.dot(np.dot(weights.T, np.cov(X.T)), weights))  # Portfolio volatility\n",
        "    return -portfolio_return / portfolio_volatility  # Maximize Sharpe ratio (minimize negative)\n",
        "\n",
        "lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "ub = [0.25] * n_assets  # Upper bounds (maximum 25% per asset)\n",
        "\n",
        "# Perform Simulated Annealing optimization using scipy.optimize.minimize\n",
        "result_sa = dual_annealing(objective_function_sa, bounds=list(zip(lb, ub)), maxiter=20000, no_local_search=False, maxfun=10000, args=(X,))\n",
        "print(\"Optimized Portfolio Weights using Simulated Annealing:\", result_sa.x)\n",
        "\n",
        "# PSO Optimization with better settings\n",
        "optimal_weights, _ = pso(objective_function_sa, lb, ub, maxiter=20000, swarmsize=100, args=(X,))\n",
        "print(\"Optimized Portfolio Weights using PSO:\", optimal_weights)\n",
        "\n",
        "# 4. ElasticNet Regularization\n",
        "# Further fine-tune ElasticNet parameters using GridSearchCV or RandomizedSearchCV\n",
        "def tune_elasticnet(X, portfolio_returns):\n",
        "    parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "    grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "    grid_search.fit(X, portfolio_returns)  # Use portfolio returns instead of asset returns\n",
        "    return grid_search.best_params_\n",
        "\n",
        "# Portfolio returns calculation using the optimized weights\n",
        "portfolio_returns = np.dot(X, result_sa.x)  # Portfolio returns (dot product of asset returns and weights)\n",
        "best_elasticnet_params = tune_elasticnet(X, portfolio_returns)\n",
        "print(\"Best ElasticNet parameters:\", best_elasticnet_params)\n",
        "\n",
        "# 5. Advanced Risk Management: Robust Optimization and Reinforcement Learning\n",
        "# Placeholder for Robust Optimization Techniques (implement as needed)\n",
        "# For now, just a placeholder print statement\n",
        "print(\"Implement robust optimization techniques here.\")\n",
        "\n",
        "# Placeholder for Q-learning or DQN for dynamic portfolio management\n",
        "# For now, just a placeholder print statement\n",
        "print(\"Implement Q-learning or DQN for dynamic portfolio management here.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaT1m7ekt7jg",
        "outputId": "51f24141-26de-40f4-db5c-0c84db5d4911"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Mixture Model Tail Risk: -15.569104193296528\n",
            "Sortino ratio: 21.375568410440128\n",
            "Optimized Portfolio Weights using Simulated Annealing: [0.00463084 0.         0.03323321 0.05641584 0.25      ]\n",
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Optimized Portfolio Weights using PSO: [0.00461224 0.         0.03306266 0.0561617  0.24888537]\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n",
            "Implement robust optimization techniques here.\n",
            "Implement Q-learning or DQN for dynamic portfolio management here.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import dual_annealing\n",
        "from pyswarm import pso\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Example data: Replace with your actual asset returns data\n",
        "np.random.seed(42)\n",
        "n_assets = 5  # Number of assets\n",
        "n_periods = 1000  # Number of periods (e.g., days)\n",
        "\n",
        "# Generate random asset returns as an example (replace with actual data)\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 days, 5 assets)\n",
        "\n",
        "# Define the optimization problem\n",
        "weights = np.zeros(n_assets)  # Initialize the weights (to be optimized)\n",
        "\n",
        "# 1. Address Concentration Risk\n",
        "max_weight_constraint = weights <= 0.10  # Max 10% per asset\n",
        "correlation_matrix = np.corrcoef(X.T)  # X is your asset returns matrix\n",
        "max_correlation = 0.5  # Maximum allowed correlation between assets\n",
        "correlation_constraint = np.dot(np.dot(np.transpose(weights), correlation_matrix), weights) <= max_correlation\n",
        "\n",
        "constraints = [np.sum(weights) == 1, weights >= 0, max_weight_constraint]\n",
        "constraints.append(correlation_constraint)\n",
        "\n",
        "# 2. Improve Risk Metrics\n",
        "# Refine CVaR with Gaussian Mixture Model\n",
        "gmm = GaussianMixture(n_components=2)\n",
        "gmm.fit(X)\n",
        "tail_risk = np.min(gmm.score_samples(X))  # Simplified tail risk estimation\n",
        "print(f\"Gaussian Mixture Model Tail Risk: {tail_risk}\")\n",
        "\n",
        "# Optimize for Sortino ratio\n",
        "# Handle possible division by zero in downside deviation\n",
        "downside_risk = np.std(np.minimum(np.dot(X, weights), 0))  # Downside deviation\n",
        "if downside_risk != 0:  # Avoid division by zero\n",
        "    sortino_ratio = np.sum(np.dot(X, weights)) / downside_risk  # Sortino ratio\n",
        "else:\n",
        "    sortino_ratio = np.nan  # Handle case where downside risk is zero\n",
        "print(f\"Sortino ratio: {sortino_ratio}\")\n",
        "\n",
        "# 3. Optimization Enhancements: Simulated Annealing and PSO\n",
        "def objective_function_sa(weights, X):\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return\n",
        "    portfolio_volatility = np.sqrt(np.dot(np.dot(weights.T, np.cov(X.T)), weights))  # Volatility from covariance\n",
        "    return -portfolio_return / portfolio_volatility  # Maximize Sharpe ratio (minimize negative)\n",
        "\n",
        "# Define bounds for each asset (0 to 0.10 or 0.15 for diversification)\n",
        "lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "ub = [0.10] * n_assets  # Upper bounds (max 10% per asset)\n",
        "\n",
        "# Perform Simulated Annealing optimization\n",
        "result_sa = dual_annealing(objective_function_sa, bounds=list(zip(lb, ub)), maxiter=20000, no_local_search=False, maxfun=10000, args=(X,))\n",
        "print(\"Optimized Portfolio Weights using Simulated Annealing:\", result_sa.x)\n",
        "\n",
        "def objective_function_pso(weights, X):\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return\n",
        "    portfolio_volatility = np.sqrt(np.dot(np.dot(weights.T, np.cov(X.T)), weights))  # Volatility from covariance\n",
        "    return -portfolio_return / portfolio_volatility  # Maximize Sharpe ratio (minimize negative)\n",
        "\n",
        "# Perform PSO optimization\n",
        "optimal_weights, _ = pso(objective_function_pso, lb, ub, maxiter=20000, swarmsize=100, args=(X,))\n",
        "print(\"Optimized Portfolio Weights using PSO:\", optimal_weights)\n",
        "\n",
        "# 4. ElasticNet Regularization Tuning\n",
        "# Define portfolio returns based on the weighted sum of asset returns\n",
        "portfolio_returns = np.dot(X, result_sa.x)  # Portfolio returns using optimized weights\n",
        "\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "grid_search.fit(X, portfolio_returns)  # Use portfolio returns instead of asset returns\n",
        "print(\"Best ElasticNet parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 5. Advanced Risk Management\n",
        "print(\"Implement robust optimization techniques here.\")\n",
        "print(\"Implement Q-learning or DQN for dynamic portfolio management here.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8JfLtSavEi9",
        "outputId": "bfe78971-62af-4774-b953-b4ab414b32f2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Mixture Model Tail Risk: -15.569104193296528\n",
            "Sortino ratio: nan\n",
            "Optimized Portfolio Weights using Simulated Annealing: [0.00185233 0.         0.01329328 0.02256633 0.1       ]\n",
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Optimized Portfolio Weights using PSO: [0.00184788 0.         0.01326154 0.0225094  0.09974839]\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n",
            "Implement robust optimization techniques here.\n",
            "Implement Q-learning or DQN for dynamic portfolio management here.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pyswarm import pso\n",
        "from scipy.optimize import dual_annealing\n",
        "\n",
        "# Example Data: Replace with actual asset return data\n",
        "np.random.seed(42)\n",
        "n_assets = 5\n",
        "n_periods = 1000\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix (1000 periods, 5 assets)\n",
        "\n",
        "# Portfolio Optimization with constraints and risk metrics refinement\n",
        "max_weight = 0.05\n",
        "lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "ub = [max_weight] * n_assets  # Upper bounds (max 5% per asset)\n",
        "\n",
        "# Portfolio risk metrics - CVaR and Sortino ratio optimization\n",
        "def objective_function(weights, X):\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return\n",
        "    downside_risk = np.std(np.minimum(np.dot(X, weights), 0))  # Downside deviation (for Sortino)\n",
        "    sortino_ratio = portfolio_return / downside_risk if downside_risk != 0 else np.nan  # Sortino ratio\n",
        "    portfolio_volatility = np.sqrt(np.dot(np.dot(weights.T, np.cov(X.T)), weights))  # Portfolio volatility (variance)\n",
        "    cvar = np.mean(np.dot(X, weights)[np.dot(X, weights) <= np.percentile(np.dot(X, weights), 5)])  # Simplified CVaR calculation\n",
        "    return -sortino_ratio + cvar  # Minimize negative Sortino ratio and CVaR\n",
        "\n",
        "# 1. Tighten Diversification Constraints\n",
        "max_correlation = 0.5  # Max correlation between assets\n",
        "correlation_matrix = np.corrcoef(X.T)\n",
        "correlation_constraint = np.dot(np.dot(np.transpose(np.ones(n_assets)), correlation_matrix), np.ones(n_assets)) <= max_correlation\n",
        "\n",
        "# 2. Refine CVaR and Sortino ratio with EVT or GMM\n",
        "gmm = GaussianMixture(n_components=2)\n",
        "gmm.fit(X)\n",
        "tail_risk = np.min(gmm.score_samples(X))  # Simplified tail risk estimation\n",
        "print(f\"Gaussian Mixture Model Tail Risk: {tail_risk}\")\n",
        "\n",
        "# 3. Simulated Annealing with enhanced cooling schedule\n",
        "def optimize_portfolio_sa(X):\n",
        "    result_sa = dual_annealing(objective_function, bounds=list(zip(lb, ub)), maxiter=10000, maxfun=5000, args=(X,))\n",
        "    return result_sa.x\n",
        "\n",
        "# 4. PSO Optimization with enhanced swarm size and iterations\n",
        "def optimize_portfolio_pso(X):\n",
        "    optimal_weights, _ = pso(objective_function, lb, ub, maxiter=10000, swarmsize=100, args=(X,))\n",
        "    return optimal_weights\n",
        "\n",
        "# 5. ElasticNet Regularization Tuning\n",
        "def tune_elasticnet(X, portfolio_returns):\n",
        "    parameters = {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.3, 0.5, 0.7, 1.0]}\n",
        "    grid_search = GridSearchCV(ElasticNet(), parameters, cv=5)\n",
        "    grid_search.fit(X, portfolio_returns)\n",
        "    return grid_search.best_params_\n",
        "\n",
        "# 6. Implement Robust Optimization (Placeholder)\n",
        "def robust_optimization(X):\n",
        "    # Placeholder for robust optimization techniques\n",
        "    print(\"Implement robust optimization techniques here.\")\n",
        "\n",
        "# 7. Implement Reinforcement Learning (RL) for dynamic portfolio management (Q-learning or DQN)\n",
        "def reinforcement_learning(X):\n",
        "    # Placeholder for Q-learning or DQN implementation\n",
        "    print(\"Implement Q-learning or DQN for dynamic portfolio management here.\")\n",
        "\n",
        "# Compute Portfolio Returns - This is based on the asset returns and optimized weights\n",
        "def compute_portfolio_returns(weights, X):\n",
        "    return np.dot(X, weights)  # Portfolio returns\n",
        "\n",
        "# 1. Calculate portfolio returns for each sample using weights\n",
        "portfolio_returns = compute_portfolio_returns(np.ones(n_assets) / n_assets, X)  # Simplified portfolio returns\n",
        "\n",
        "# 2. Run Simulated Annealing and PSO to optimize portfolio\n",
        "optimized_weights_sa = optimize_portfolio_sa(X)\n",
        "optimized_weights_pso = optimize_portfolio_pso(X)\n",
        "\n",
        "# 3. Compute Portfolio Return and Risk Metrics\n",
        "portfolio_return_sa = compute_portfolio_returns(optimized_weights_sa, X)\n",
        "downside_risk_sa = np.std(np.minimum(np.dot(X, optimized_weights_sa), 0))  # Downside deviation (for Sortino)\n",
        "sortino_ratio_sa = portfolio_return_sa / downside_risk_sa if downside_risk_sa != 0 else np.nan  # Sortino ratio\n",
        "print(f\"Sortino ratio: {sortino_ratio_sa}\")\n",
        "\n",
        "# Refining CVaR using GMM or EVT for tail risk\n",
        "print(f\"Gaussian Mixture Model Tail Risk: {tail_risk}\")\n",
        "\n",
        "# ElasticNet regularization fine-tuning\n",
        "portfolio_returns = compute_portfolio_returns(np.ones(n_assets) / n_assets, X)  # Recalculate portfolio returns\n",
        "best_elasticnet_params = tune_elasticnet(X, portfolio_returns)\n",
        "print(f\"Best ElasticNet parameters: {best_elasticnet_params}\")\n",
        "\n",
        "# Implement Robust Optimization and Reinforcement Learning (placeholders for now)\n",
        "robust_optimization(X)\n",
        "reinforcement_learning(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_HTRxCowdUF",
        "outputId": "cbd70d26-48f4-44f0-caed-f20061cc85db"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Mixture Model Tail Risk: -15.569104193296528\n",
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Sortino ratio: [ 2.92598937e-01  8.90256538e-01 -3.64465492e+00 -2.73227827e+00\n",
            " -1.37235435e+00 -6.47541822e-01  9.99109415e-01 -5.58050098e-01\n",
            " -2.63969629e+00 -2.75379119e+00  1.89092165e+00  1.78931432e+00\n",
            "  7.39993727e-01 -7.31605288e-01 -3.92878518e+00 -3.40198155e+00\n",
            " -1.32413919e+00  7.25629975e-01 -9.16355407e-01 -4.25957920e-01\n",
            " -7.00460186e-01  1.68034239e-02  4.83292470e-01  1.51400552e+00\n",
            "  7.91656405e-01 -8.26168296e-01 -1.67752931e+00 -1.80539249e+00\n",
            "  2.35273591e-01  5.00751808e-01  4.78392783e-01  7.47287764e-01\n",
            "  1.52141441e+00 -1.02410820e+00  5.43317518e-01  4.88884166e+00\n",
            " -3.72015497e-01 -2.89435994e+00 -1.38489495e-01 -1.89977977e+00\n",
            " -1.77477190e+00  6.86671085e+00 -1.05447943e-01 -4.16518408e-02\n",
            " -1.16302992e+00  7.67680418e-01  3.42600241e+00  1.29090994e+00\n",
            " -1.69826377e+00  1.20562738e+00 -1.91747080e+00  1.34364831e+00\n",
            " -1.43828295e+00 -2.54165944e-01 -1.40562799e+00 -7.23803057e-01\n",
            "  3.51307589e+00 -9.58241979e-01  8.07307886e-01  1.40159465e+00\n",
            "  2.80925377e-01 -2.50009247e-01  2.68191632e+00 -1.61022504e-01\n",
            " -1.11299738e+00  1.39281155e+00  1.44509416e+00 -8.85239517e-01\n",
            " -9.91135390e-01 -8.71950033e-01  7.90737666e-02 -1.40736301e+00\n",
            "  1.33986035e+00 -1.31230913e+00  4.31900285e+00 -5.98294730e-01\n",
            " -1.92264776e+00 -3.00816172e-01  2.41490455e+00  2.31077978e+00\n",
            " -8.34206524e-01  9.33906239e-01  7.94713630e-01  5.30092302e-01\n",
            "  1.51990195e+00 -2.45161134e+00  2.74116409e+00 -9.61488870e-01\n",
            " -2.63269759e+00 -2.14760368e+00 -9.10355442e-01 -2.48912260e+00\n",
            " -1.46309772e-01 -1.05826020e+00  3.04219591e+00  3.09230173e+00\n",
            " -1.52427924e+00  1.86737808e+00  2.32186678e+00 -2.70729169e+00\n",
            " -1.12261940e+00 -1.59245951e+00 -1.35328213e+00  1.18973431e+00\n",
            " -1.30697074e+00 -1.54117954e+00 -1.54385377e+00 -6.93824573e-01\n",
            " -3.59342508e+00 -9.79978760e-01  8.88921719e-01 -6.79217110e-01\n",
            "  2.12047843e+00 -1.48815507e+00  1.40789282e+00  1.46221743e+00\n",
            "  1.27055275e+00 -1.31276317e+00  1.99747382e+00  2.86115902e-01\n",
            "  1.39913502e+00  1.79407033e+00  3.88266086e+00  6.97898223e-01\n",
            "  3.53383708e+00  1.85622003e+00 -2.00846753e+00 -2.76619514e+00\n",
            " -1.03351016e+00  1.09128078e+00  4.06817066e+00 -1.77855883e-01\n",
            " -1.45873771e+00  1.19054786e+00 -2.55939803e+00  2.80818515e+00\n",
            " -1.98521513e+00  9.90558934e-01  1.03237501e+00 -2.22901944e-01\n",
            " -3.34498672e+00  2.50866675e+00 -7.63220185e-03 -1.49614170e-01\n",
            "  3.36176718e-01 -1.48243334e+00  1.23493550e+00  2.55297408e+00\n",
            "  6.98207911e-01 -2.81576936e+00 -1.47181674e+00  1.05436231e+00\n",
            "  2.37779774e+00  3.17729946e+00  1.57310981e+00  9.20245039e-01\n",
            "  1.26323152e+00 -2.74918176e+00 -2.25580311e+00  2.35842701e-01\n",
            " -8.53252020e-01 -1.18425684e+00  2.10003663e+00  1.95250791e+00\n",
            " -5.08876903e-01 -6.61816306e-01 -5.86006380e-01 -1.78921109e-01\n",
            "  2.20551211e-01 -6.78783168e-01  2.73268953e+00  9.45853718e-01\n",
            " -1.11581447e-01  3.74163945e+00 -5.80901519e-01  2.16737078e+00\n",
            "  1.58841829e+00  2.14843736e+00  3.03750869e+00 -9.08929384e-01\n",
            "  8.17540838e-01  4.09583468e+00  4.40374491e-01  2.18834918e-01\n",
            "  1.87843756e+00  4.50880492e-01 -3.31686697e-01 -7.78683767e-01\n",
            "  3.19783305e+00  2.25266143e-01  1.46445678e-01  1.11495151e+00\n",
            "  1.70422002e-01 -2.08218201e+00 -8.47583252e-01  2.02564514e-01\n",
            " -1.60030837e-01  6.79853149e-02 -2.00697862e+00  8.79757118e-01\n",
            "  1.04517442e+00 -4.05342720e-01  3.17623246e+00  5.35730181e-01\n",
            "  3.77474304e+00 -1.49433366e+00 -5.36509061e-01  1.69618612e+00\n",
            " -3.44501112e-02  2.67146064e+00 -7.93551193e-01 -2.60870595e-01\n",
            " -1.01227625e+00  5.81417277e-01  1.25941812e+00  1.90861614e+00\n",
            "  1.33852144e+00 -3.68261364e-01  1.40801109e+00  5.65193804e-01\n",
            "  2.28252909e+00  2.82680551e+00  5.87289863e-01  1.88912287e+00\n",
            "  2.56471991e+00  3.50582281e-01  5.19576791e-02  1.24961740e+00\n",
            " -2.93774000e-01 -1.13675786e+00  2.18117301e+00  2.54260244e-01\n",
            "  7.88617456e-01  1.90608171e+00 -1.26491777e+00 -2.29103303e+00\n",
            " -2.10069313e+00 -1.46144449e+00 -2.72920844e+00  2.55093319e+00\n",
            "  3.10094785e-01  2.38323170e+00 -3.72137285e-01  3.49952575e+00\n",
            " -1.11555314e-01  4.12320290e+00 -4.87235297e-01  2.98994146e+00\n",
            " -1.61061484e+00 -2.79175252e+00  1.90541882e+00 -3.35815843e-01\n",
            "  8.07921369e-02  1.62796073e+00  1.82874850e+00 -3.84244347e-01\n",
            " -8.70254688e-01 -4.08061882e+00  4.63941392e-01  5.93874250e-01\n",
            "  5.84285578e-01  1.16715889e+00  4.70813347e-01 -6.62684883e-01\n",
            "  2.78134175e-01  4.70180876e-01  8.34545614e-01  8.09922571e-02\n",
            "  2.15476933e+00 -7.88003283e-01 -6.75079726e-01 -2.78040161e-01\n",
            "  1.72631703e+00 -1.86075570e+00  1.68873204e+00  3.10040668e+00\n",
            "  1.47953338e+00 -9.88646336e-01 -1.93092986e-01 -8.60228077e-01\n",
            "  2.13960218e+00 -3.14869454e-01 -1.72550417e-01  1.52430429e+00\n",
            " -2.14379932e+00 -8.51944102e-01 -3.29592735e-01  1.74783613e+00\n",
            "  2.80781177e-01  5.61705216e-01  1.97919960e+00  1.61161214e+00\n",
            " -1.35345008e+00  4.29829676e-01 -1.31177582e+00 -9.68849893e-01\n",
            "  5.88335440e-01  1.34518985e+00  2.59298162e+00  1.70862512e+00\n",
            " -4.04125615e-01  2.72608787e+00 -1.00690458e+00  2.92671349e+00\n",
            " -9.71564709e-01 -1.77356114e+00 -7.90737086e-01 -5.54394948e+00\n",
            " -9.04313853e-01  2.46823665e+00 -8.54374425e-01 -5.04515890e-02\n",
            "  1.15131602e+00 -1.40008482e+00  4.00631080e-01 -1.45354450e+00\n",
            "  5.86871198e-01 -1.85625445e-01 -3.71164842e+00 -9.23039337e-01\n",
            " -1.24753326e+00 -6.96497143e-02 -1.93683136e-01  1.84113627e+00\n",
            "  3.15292917e+00  1.77655689e+00 -2.93952377e-01 -1.02979114e+00\n",
            " -1.67727907e+00  5.08879667e+00  5.17369600e-01  1.81932208e+00\n",
            "  1.69207634e+00  1.15490063e+00  2.36244074e+00 -1.20378574e-01\n",
            " -2.05507737e+00 -2.48443072e-01 -8.65962565e-02  4.68091878e-01\n",
            "  1.24117995e+00 -1.39436729e+00 -9.51586659e-01 -4.61322449e-01\n",
            " -1.22778635e+00  3.49669001e+00  1.25223931e+00  3.01912397e+00\n",
            " -1.16571824e+00  1.89430166e+00 -3.61342865e-02  3.08938272e+00\n",
            " -9.13453142e-01  9.21395209e-01 -1.31756442e+00 -1.41909378e+00\n",
            "  7.84527733e-02  2.21688364e+00  4.91312427e-01  2.48093920e-01\n",
            " -3.69051579e+00  9.79864729e-01  1.32019121e-01 -1.94940987e+00\n",
            "  6.66028342e-03  2.72902685e-01 -7.43089047e-01 -3.33157592e-01\n",
            "  2.03608061e-01  8.81897968e-01  1.80999443e+00  4.75445145e-01\n",
            "  1.74543386e+00 -9.81690506e-01  2.42998776e+00 -1.97004950e+00\n",
            " -4.37211791e-01 -1.07174869e-01 -2.44275841e-01 -3.36305422e-02\n",
            "  1.95669866e-01 -3.70397142e-01 -2.71844180e+00 -1.01239556e+00\n",
            "  3.36735820e+00  3.67552364e+00  4.36948612e+00  5.75386612e-01\n",
            "  1.00859609e+00  2.98255931e-01 -1.51320178e+00  1.40238789e+00\n",
            "  1.28277994e+00  1.85770683e+00  1.86035390e-01  5.71555471e-01\n",
            "  1.79421735e+00  6.28130849e-01  1.11653494e+00 -1.44888486e+00\n",
            " -3.54770752e+00  1.71670853e+00  8.68526921e-01 -2.27326451e+00\n",
            " -5.42553487e+00 -5.37219811e-01  2.13434326e+00 -3.01229603e+00\n",
            " -1.48866776e+00  2.00588243e-01 -2.50608447e+00  5.46942944e-01\n",
            "  5.31111044e-01  2.05977511e+00 -9.28411341e-01 -6.23621549e-01\n",
            " -2.04447332e+00  2.24916308e+00  1.50234088e+00  6.44889906e-01\n",
            " -1.77644933e+00  1.11385303e+00  8.91283222e-01 -1.46631273e+00\n",
            "  5.67907182e-02  1.72461341e+00 -2.51101288e-01 -2.72538798e+00\n",
            " -2.56070840e-01 -2.94334206e+00 -2.08999457e+00  1.33926479e+00\n",
            "  3.17176372e+00  4.41053326e-01 -1.39210406e+00  8.83450181e-01\n",
            " -3.34942374e+00  8.83232866e-01  1.18187682e-01  5.77521594e-01\n",
            " -1.89949983e+00 -2.79943187e-02 -8.75140480e-01  3.40243635e-01\n",
            " -1.06679258e+00  1.01051212e+00 -2.14904864e+00  8.96432370e-01\n",
            "  2.57646140e+00  1.28242164e+00 -2.42856836e+00 -6.64021988e-01\n",
            "  1.02540605e+00 -2.08171609e-01 -1.96426935e+00  7.78579918e-01\n",
            " -1.02486478e+00  2.20791218e+00 -1.62381855e+00 -2.07195648e+00\n",
            "  3.50817342e+00  1.84072019e+00  1.35349154e+00 -2.18804220e+00\n",
            "  7.04089842e-01  1.67745198e+00  1.11421802e+00 -1.09712647e+00\n",
            " -3.77537140e+00 -5.24808786e-01  1.76389203e+00  9.53408892e-02\n",
            " -1.00312075e+00  1.48694977e+00  2.12747596e+00  1.28406771e-01\n",
            "  5.39788587e-02  1.90233098e+00 -7.40387080e-01 -5.70056552e-02\n",
            "  2.64788376e+00 -1.46611114e+00  1.25476592e+00 -2.06638069e+00\n",
            " -2.64194442e+00 -1.51935375e-01  3.47557927e-01 -2.79329391e+00\n",
            " -7.63044010e-01 -1.50356748e+00  8.56780962e-01  1.10998235e+00\n",
            "  1.13632564e+00 -6.15175745e-02 -6.96390498e-02  1.00299253e+00\n",
            " -8.94668649e-01 -3.26163283e+00 -1.72705301e+00  8.62127901e-01\n",
            " -2.12295488e+00 -2.58365732e+00 -1.86202622e-01 -1.93419009e+00\n",
            "  1.54750948e+00 -1.06482802e+00 -9.29552208e-01 -2.07434610e-01\n",
            "  3.99126243e+00  3.88589576e+00 -1.09196073e+00  1.73697430e+00\n",
            " -1.66470437e+00  6.45564317e-03  4.44080796e+00  3.25541230e+00\n",
            "  2.34141645e+00 -4.67722557e-01  1.53402525e+00 -4.80902133e-01\n",
            " -1.71388873e+00  2.11377456e+00  1.02365753e+00  2.71981085e+00\n",
            "  1.26828413e+00 -9.02498681e-01  3.08966079e+00  5.31185896e-01\n",
            " -2.17821479e+00  1.26679010e+00 -7.09029065e-01  1.21727494e+00\n",
            "  5.02783793e-01  9.88918039e-01 -1.91911946e+00 -7.96866035e-01\n",
            "  8.16739045e-01  1.06782238e+00  4.69192118e-01  2.34664652e+00\n",
            "  1.18416440e+00  2.61840466e+00  1.73886065e-01  5.47901560e-02\n",
            "  3.82154532e+00 -8.63227016e-01  3.26166662e+00  2.07188081e+00\n",
            " -4.12306809e+00 -4.33648807e-01 -1.80989583e-01  5.01341015e-01\n",
            "  3.98182952e-01 -4.04187668e-01 -1.45123309e+00  7.45334994e-01\n",
            " -1.39286800e+00 -1.04999182e-01 -4.37186315e-01  2.35395345e-02\n",
            " -4.57769619e-01 -8.49527350e-01  3.34596731e+00 -3.06867063e+00\n",
            " -6.24213825e-01 -2.12821336e-01 -3.19253196e-01  8.13178242e-01\n",
            "  2.29134263e-01 -1.35394425e+00  1.95062278e+00  6.97061060e-01\n",
            "  7.26499506e-01  2.13512200e+00 -6.74291409e-01 -3.80935181e-01\n",
            " -4.78129231e+00  7.45564778e-01  1.09897478e+00  8.92393488e-01\n",
            " -2.59679325e+00  1.83254565e+00  1.29789866e+00  2.47705173e+00\n",
            "  6.44480797e-02  2.32194895e+00  1.73745218e+00 -1.24236130e+00\n",
            "  6.52612543e-01 -3.47080142e+00 -9.36719974e-01 -1.44864073e+00\n",
            "  1.58505758e+00 -3.13602299e-01  1.95219192e+00  3.94092169e-01\n",
            " -2.59840529e+00  3.71008203e+00 -1.62912160e+00 -3.66124277e-01\n",
            "  1.47784208e+00 -7.61217285e-01 -1.75032301e+00  3.24399587e-01\n",
            " -2.81435335e+00 -6.40769084e-02 -2.95183930e-01 -6.53653967e-01\n",
            "  1.23546841e+00  2.05540605e+00  2.58229636e+00 -1.12354317e+00\n",
            " -2.80112258e+00  1.29341074e+00 -6.92856021e-01 -1.54304254e-01\n",
            " -1.02360616e+00 -2.27945121e-01 -1.43974995e+00  1.11861538e+00\n",
            " -6.02542130e-01 -2.93362304e+00  1.18383797e-01  3.10709992e+00\n",
            " -3.05093496e-01 -4.07776076e-01  2.98375187e-01 -4.51238949e-01\n",
            "  5.47879677e-01 -1.68953279e+00 -2.40834494e+00  9.63291675e-01\n",
            " -2.35240109e+00 -8.07846006e-01 -1.72654960e+00  1.11553061e+00\n",
            " -1.61523274e+00  3.70070081e+00 -2.47503026e+00  1.85647214e-01\n",
            " -1.12451337e+00  1.22159232e+00  2.17957516e+00  9.66125792e-01\n",
            "  9.38057331e-01  3.94271965e-01 -1.72196675e+00  1.46428516e+00\n",
            " -2.12781679e-01 -3.20966176e+00  1.16275482e+00  2.78309354e+00\n",
            "  1.39743542e+00  1.22322628e+00 -1.16596688e+00  1.07288662e+00\n",
            " -2.79949470e+00 -6.89502988e-01 -1.65502451e+00  1.31474297e+00\n",
            "  1.57704996e+00  7.57624978e-01 -6.72333347e-01  4.24492093e+00\n",
            "  1.72385272e+00  9.55936369e-01  2.94984464e+00  3.18309304e+00\n",
            " -1.36385097e+00  3.77866464e-01 -9.41965820e-01 -1.91222418e+00\n",
            " -2.09410801e+00  4.98506985e-01 -1.98018218e+00 -1.29343163e+00\n",
            "  2.42125281e+00  5.39779606e-01  3.27788613e+00  3.32479486e-01\n",
            "  7.73693458e-01  3.34207191e+00 -9.22974972e-01 -1.35630276e+00\n",
            " -2.85752365e+00  2.35933975e+00  3.99307642e+00 -1.52498971e-01\n",
            "  1.80798868e+00 -1.05402820e+00 -5.29306484e+00 -1.97814768e+00\n",
            "  7.85147582e-01 -5.83046615e-01 -1.61540988e+00  2.36361928e+00\n",
            " -1.84294198e+00 -2.99162959e+00  7.59307121e-01 -5.95239747e-01\n",
            " -7.79539330e-01 -8.14148048e-01  4.80720144e-01 -3.25033413e+00\n",
            " -5.58287761e-01 -2.16013680e+00  3.21073042e+00 -3.63107446e-01\n",
            "  2.15214135e+00 -1.86250800e-01 -2.21624405e+00 -2.25696053e+00\n",
            "  1.70042235e+00  1.83439897e+00 -2.07211571e+00 -2.23359709e+00\n",
            "  1.27610907e-01 -4.59481198e-01  2.09702313e+00 -3.15176932e-02\n",
            "  1.67678502e+00 -1.04911733e+00  1.30608006e+00  1.95941338e+00\n",
            "  2.85976299e+00 -1.55900473e+00  9.82964704e-01  8.22823259e-01\n",
            " -1.61176210e+00 -4.72834723e-01  5.66815750e-02  3.50784024e+00\n",
            "  2.21680602e+00  5.76738915e-01  3.08470773e-02  2.66181673e-01\n",
            "  1.46353322e+00  2.09667903e-01  2.13233135e+00  1.11474245e-03\n",
            " -6.60074644e-01  4.81510873e-01  9.47610910e-01  1.60086654e-01\n",
            " -1.96862360e+00 -4.20216874e+00  1.09528148e+00 -2.49013608e+00\n",
            " -1.62847731e+00 -1.99388575e+00 -1.15699938e+00  3.35760278e-01\n",
            "  5.57170054e-01 -4.73458791e-01 -3.41780833e+00 -1.10282405e+00\n",
            "  1.76923296e-01  2.12267310e+00  9.02210601e-04 -2.08789707e+00\n",
            " -1.76615985e+00  1.30355238e+00 -2.34414942e+00 -2.29248630e+00\n",
            " -1.11462505e+00  1.40317840e+00 -2.01072904e+00 -8.37341652e-01\n",
            "  2.41682260e+00 -2.56492823e+00 -8.23674282e-01 -5.00475537e-01\n",
            " -7.41855091e-01  2.02889752e+00 -2.07628438e+00 -7.94059671e-01\n",
            " -7.52835813e-01  1.51137254e+00 -1.48207669e+00  9.86871746e-01\n",
            "  2.71090802e+00 -1.32471020e+00 -2.77447018e+00  3.38039340e-01\n",
            " -1.01123994e+00 -6.56976285e-02  5.73411314e-01 -1.67896671e+00\n",
            " -1.66926395e+00 -2.18186748e+00 -3.72220479e-01  6.54527144e-01\n",
            " -7.93727151e-01  2.63174033e+00  6.87271609e-01 -1.32926093e+00\n",
            "  6.44659118e-01 -8.33361023e-01 -3.72028358e+00  1.16267121e+00\n",
            " -2.21952494e+00  8.27451645e-01 -2.40589067e-01 -2.42532493e-01\n",
            " -1.11189218e+00  1.90036750e+00 -2.59293140e+00 -1.18744039e+00\n",
            "  1.43223909e+00  1.12727293e+00 -1.63708252e+00  3.33353183e-02\n",
            " -1.72641086e+00  1.40423406e+00 -1.07031778e+00 -1.02458819e+00\n",
            "  1.89503641e+00 -2.48387788e-01  2.13221522e+00 -8.04354984e-02\n",
            "  3.85926116e-01  5.83005968e-01  2.58529109e+00 -2.31499848e+00\n",
            "  7.48673890e-01  2.85491154e+00  4.53855996e-01  3.73962417e-01\n",
            " -2.40217058e+00  8.67323698e-02  1.44511545e+00 -1.88503654e-01\n",
            " -1.46146431e+00 -8.72348081e-01 -8.16166879e-01  1.49368858e-01\n",
            " -1.17642532e+00 -3.72208181e-01 -4.74101872e+00 -4.18751386e-01\n",
            " -3.28973151e-01 -2.79642144e+00 -4.67002623e-01 -3.04686058e+00\n",
            " -2.05236711e+00  2.16521141e-01 -2.69361152e+00  2.74371222e-01\n",
            "  1.01665638e-01  4.02157641e-01  9.52286522e-02 -1.25391114e+00\n",
            " -2.35057494e-02  8.55168713e-01 -4.12521474e+00  4.22203362e-01\n",
            " -6.75609351e-01  1.63535417e+00  1.75778169e+00 -1.77417892e+00\n",
            " -2.19091545e+00 -8.16894637e-01 -4.73450705e-01 -2.70432230e-01\n",
            " -1.48512829e+00  1.40162247e-01  1.05634354e+00 -1.23802601e+00\n",
            " -3.54672103e-01 -1.59213143e+00 -5.72820453e-02  1.36252105e+00\n",
            " -5.26849178e-01  2.47353758e-01  1.47910024e+00 -8.28858526e-01\n",
            " -1.08644102e+00  7.82165464e-01  1.33503718e+00  1.77594976e+00\n",
            " -2.23684657e-01  1.67679740e+00 -7.88136748e-01 -4.26888847e+00\n",
            " -9.12501816e-01 -1.00140660e+00 -1.20219169e+00  7.21529771e-01\n",
            " -3.05249941e+00  2.73245049e+00  2.47368071e+00  3.47982218e-01\n",
            " -3.64959772e-01  7.25145908e-01  1.90371737e+00  1.46807653e+00\n",
            " -6.76616909e-02  1.41160848e-01  1.22049288e+00 -2.56952143e+00\n",
            " -5.50790812e-01  1.86220696e-01 -8.11032945e-01 -1.40090993e+00\n",
            "  2.85289776e+00 -7.74160557e-01  1.05684217e+00  1.15350469e+00\n",
            "  4.01409293e-02 -2.30234436e-01 -2.63019202e-01  9.82890498e-01\n",
            "  1.22898443e+00  3.03276756e-01 -1.38559935e+00  1.54338209e+00\n",
            " -2.95116616e-02 -1.80216457e+00 -7.35858083e-01  2.64982620e+00\n",
            " -1.11756349e+00 -1.11811226e+00  1.73374643e-01 -7.15805041e-01\n",
            "  7.11906424e-01  1.06014511e+00 -8.69517185e-01 -5.97211062e+00\n",
            "  1.60161890e+00 -9.11127279e-01  1.43399860e+00 -1.35662261e+00\n",
            "  1.03496126e+00  4.26481152e-02  1.15063183e+00  8.99189496e-01\n",
            " -1.78538376e+00 -1.49612609e+00  3.10906305e-02 -5.41328115e-01\n",
            "  1.27636476e+00  1.87407596e-01  3.14292808e-01  6.67694010e-01\n",
            " -1.96106705e-01  3.70774411e-01  4.26413473e-01  7.63262613e-01\n",
            "  2.93820038e+00 -3.57803547e-01 -1.05759284e+00  1.24611467e+00\n",
            " -1.51227577e+00  1.71269427e+00 -3.86971639e+00 -5.61180555e+00\n",
            "  4.09467292e+00 -1.18652346e+00 -1.95812991e-01  1.12310221e-01\n",
            " -9.37437168e-01 -1.62105675e-01  3.92250643e-01 -4.22630998e-01\n",
            " -2.73428203e-01 -1.31640014e+00 -2.91601704e+00 -7.65932197e-01\n",
            "  2.04081482e+00  5.50777656e-01  2.05641099e+00  8.93111074e-02\n",
            " -7.69557497e-01 -9.33764325e-01 -1.03701355e+00  1.44664942e+00\n",
            "  1.15611718e+00  1.44524118e+00 -2.01990769e+00  1.08082261e+00\n",
            "  7.68915342e-01 -1.48331404e+00 -2.00772439e+00  2.65964727e+00\n",
            "  1.47202141e+00 -6.97761691e-01 -5.22849096e-02 -1.80365952e+00\n",
            "  2.78759266e+00  4.17019805e-01  1.07001392e+00  9.71179359e-01\n",
            " -1.05394083e+00  2.58423553e+00 -3.40119792e+00 -3.72620022e+00\n",
            "  9.12257843e-01  4.27990303e+00 -1.04804555e+00 -6.05726187e-01]\n",
            "Gaussian Mixture Model Tail Risk: -15.569104193296528\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n",
            "Implement robust optimization techniques here.\n",
            "Implement Q-learning or DQN for dynamic portfolio management here.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import dual_annealing\n",
        "from pyswarm import pso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from scipy.stats import genpareto\n",
        "\n",
        "# Example data for returns (1000 periods, 5 assets)\n",
        "np.random.seed(42)\n",
        "n_assets = 5\n",
        "n_periods = 1000\n",
        "X = np.random.randn(n_periods, n_assets)  # Asset returns matrix\n",
        "\n",
        "# Constraints (max weight for any asset, diversification)\n",
        "max_weight = 0.05\n",
        "lb = [0] * n_assets  # Lower bounds (no short-selling)\n",
        "ub = [max_weight] * n_assets  # Upper bounds (max 5% per asset)\n",
        "\n",
        "# Portfolio Return Calculation\n",
        "def compute_portfolio_returns(weights, X):\n",
        "    return np.dot(X, weights)  # Portfolio returns based on weights\n",
        "\n",
        "# Objective function for optimization (Minimizing downside risk + tail risk)\n",
        "def objective_function(weights, X):\n",
        "    portfolio_return = np.sum(np.dot(X, weights))  # Portfolio return\n",
        "    downside_risk = np.std(np.minimum(np.dot(X, weights), 0))  # Downside deviation (for Sortino)\n",
        "    sortino_ratio = portfolio_return / downside_risk if downside_risk != 0 else np.nan  # Sortino ratio\n",
        "    return -sortino_ratio  # Minimize negative Sortino ratio\n",
        "\n",
        "# Simulated Annealing (SA) optimization\n",
        "def optimize_portfolio_sa(X):\n",
        "    result_sa = dual_annealing(objective_function, bounds=list(zip(lb, ub)), maxiter=5000, maxfun=5000, args=(X,))\n",
        "    return result_sa.x\n",
        "\n",
        "# PSO Optimization (Particle Swarm Optimization)\n",
        "def optimize_portfolio_pso(X):\n",
        "    optimal_weights, _ = pso(objective_function, lb, ub, maxiter=5000, swarmsize=50, args=(X,))\n",
        "    return optimal_weights\n",
        "\n",
        "# ElasticNet Regularization - Simple Grid Search (Limited to 2 params for speed)\n",
        "def tune_elasticnet(X, portfolio_returns):\n",
        "    parameters = {'alpha': [0.01, 0.1], 'l1_ratio': [0.3, 0.5]}\n",
        "    best_params = None\n",
        "    best_score = float('inf')\n",
        "    for alpha in parameters['alpha']:\n",
        "        for l1_ratio in parameters['l1_ratio']:\n",
        "            model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
        "            model.fit(X, portfolio_returns)\n",
        "            score = np.mean((model.predict(X) - portfolio_returns) ** 2)  # MSE loss\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                best_params = {'alpha': alpha, 'l1_ratio': l1_ratio}\n",
        "    return best_params\n",
        "\n",
        "# Running Optimizations\n",
        "optimized_weights_sa = optimize_portfolio_sa(X)\n",
        "optimized_weights_pso = optimize_portfolio_pso(X)\n",
        "\n",
        "# Compute Portfolio Returns for SA and PSO solutions\n",
        "portfolio_return_sa = compute_portfolio_returns(optimized_weights_sa, X)\n",
        "downside_risk_sa = np.std(np.minimum(np.dot(X, optimized_weights_sa), 0))  # Downside deviation\n",
        "sortino_ratio_sa = portfolio_return_sa / downside_risk_sa if downside_risk_sa != 0 else np.nan\n",
        "print(f\"Sortino ratio (SA): {sortino_ratio_sa}\")\n",
        "\n",
        "# ElasticNet Regularization Fine-Tuning (Simplified Grid Search)\n",
        "portfolio_returns = compute_portfolio_returns(np.ones(n_assets) / n_assets, X)  # Simplified portfolio returns\n",
        "best_elasticnet_params = tune_elasticnet(X, portfolio_returns)\n",
        "print(f\"Best ElasticNet parameters: {best_elasticnet_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-Y4NTAXx2UI",
        "outputId": "082aba13-de2d-4a30-eb25-3f0a1cfd5334"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Sortino ratio (SA): [ 2.92014244e-01  8.90283296e-01 -3.64431539e+00 -2.73218407e+00\n",
            " -1.37213993e+00 -6.47493278e-01  9.99498411e-01 -5.57201603e-01\n",
            " -2.63976685e+00 -2.75419536e+00  1.89096383e+00  1.78930874e+00\n",
            "  7.40633674e-01 -7.32051307e-01 -3.92942991e+00 -3.40216540e+00\n",
            " -1.32435848e+00  7.25778119e-01 -9.16165000e-01 -4.25932806e-01\n",
            " -7.00097159e-01  1.66665850e-02  4.82749475e-01  1.51398766e+00\n",
            "  7.91731211e-01 -8.26269068e-01 -1.67740682e+00 -1.80574717e+00\n",
            "  2.35560651e-01  5.00857173e-01  4.78478600e-01  7.47605777e-01\n",
            "  1.52153831e+00 -1.02452316e+00  5.43327410e-01  4.88878319e+00\n",
            " -3.71983573e-01 -2.89429773e+00 -1.38162408e-01 -1.89994770e+00\n",
            " -1.77540852e+00  6.86685476e+00 -1.05878915e-01 -4.15209928e-02\n",
            " -1.16294467e+00  7.68070996e-01  3.42639156e+00  1.29107284e+00\n",
            " -1.69863986e+00  1.20535529e+00 -1.91821648e+00  1.34377434e+00\n",
            " -1.43730883e+00 -2.53662144e-01 -1.40604843e+00 -7.23761822e-01\n",
            "  3.51324479e+00 -9.58355211e-01  8.07249947e-01  1.40141827e+00\n",
            "  2.80667062e-01 -2.50056120e-01  2.68165755e+00 -1.61056085e-01\n",
            " -1.11346792e+00  1.39262069e+00  1.44533786e+00 -8.85318693e-01\n",
            " -9.91036119e-01 -8.71507126e-01  7.89172736e-02 -1.40738906e+00\n",
            "  1.33979953e+00 -1.31237429e+00  4.31863177e+00 -5.98920769e-01\n",
            " -1.92208112e+00 -3.01507516e-01  2.41485447e+00  2.31066725e+00\n",
            " -8.34148997e-01  9.33904355e-01  7.94672569e-01  5.29816263e-01\n",
            "  1.51961218e+00 -2.45226821e+00  2.74123666e+00 -9.60769795e-01\n",
            " -2.63238533e+00 -2.14741016e+00 -9.10325353e-01 -2.48881130e+00\n",
            " -1.46365298e-01 -1.05830930e+00  3.04230378e+00  3.09154174e+00\n",
            " -1.52405767e+00  1.86732773e+00  2.32220364e+00 -2.70717499e+00\n",
            " -1.12259737e+00 -1.59233124e+00 -1.35306185e+00  1.18991968e+00\n",
            " -1.30693447e+00 -1.54120996e+00 -1.54362069e+00 -6.94060493e-01\n",
            " -3.59417335e+00 -9.80030433e-01  8.88638089e-01 -6.79238818e-01\n",
            "  2.12046555e+00 -1.48746021e+00  1.40795093e+00  1.46269202e+00\n",
            "  1.26982066e+00 -1.31289456e+00  1.99736974e+00  2.86498737e-01\n",
            "  1.39858486e+00  1.79478792e+00  3.88305711e+00  6.97633444e-01\n",
            "  3.53328544e+00  1.85635690e+00 -2.00829255e+00 -2.76586761e+00\n",
            " -1.03306100e+00  1.09142878e+00  4.06860382e+00 -1.77918836e-01\n",
            " -1.45860899e+00  1.19101749e+00 -2.55943224e+00  2.80773977e+00\n",
            " -1.98538648e+00  9.90438032e-01  1.03250898e+00 -2.22881990e-01\n",
            " -3.34456754e+00  2.50895236e+00 -7.64256994e-03 -1.49629430e-01\n",
            "  3.36251525e-01 -1.48241540e+00  1.23536997e+00  2.55238891e+00\n",
            "  6.98497157e-01 -2.81552214e+00 -1.47190897e+00  1.05442861e+00\n",
            "  2.37743318e+00  3.17729938e+00  1.57272922e+00  9.20473505e-01\n",
            "  1.26302823e+00 -2.74946894e+00 -2.25564347e+00  2.35626623e-01\n",
            " -8.53250523e-01 -1.18454098e+00  2.09978726e+00  1.95249365e+00\n",
            " -5.09254660e-01 -6.61721345e-01 -5.85511197e-01 -1.79008075e-01\n",
            "  2.20698235e-01 -6.79196680e-01  2.73266916e+00  9.45993157e-01\n",
            " -1.12433413e-01  3.74159196e+00 -5.80418331e-01  2.16730613e+00\n",
            "  1.58813014e+00  2.14832299e+00  3.03704319e+00 -9.09387357e-01\n",
            "  8.17191017e-01  4.09568949e+00  4.39923328e-01  2.18910778e-01\n",
            "  1.87862007e+00  4.50491297e-01 -3.30740030e-01 -7.78905485e-01\n",
            "  3.19796634e+00  2.25512729e-01  1.46333523e-01  1.11467007e+00\n",
            "  1.70144966e-01 -2.08275716e+00 -8.47630339e-01  2.03667334e-01\n",
            " -1.59873965e-01  6.79444001e-02 -2.00684684e+00  8.79842124e-01\n",
            "  1.04529460e+00 -4.05818883e-01  3.17604268e+00  5.35934997e-01\n",
            "  3.77429515e+00 -1.49463357e+00 -5.36782990e-01  1.69640740e+00\n",
            " -3.47897218e-02  2.67145158e+00 -7.93723894e-01 -2.61238330e-01\n",
            " -1.01213855e+00  5.81406854e-01  1.25982436e+00  1.90841507e+00\n",
            "  1.33809894e+00 -3.68287982e-01  1.40791422e+00  5.65397429e-01\n",
            "  2.28216222e+00  2.82741439e+00  5.87007311e-01  1.88914456e+00\n",
            "  2.56494907e+00  3.50858783e-01  5.14662566e-02  1.24970521e+00\n",
            " -2.93770321e-01 -1.13695190e+00  2.18185421e+00  2.53446194e-01\n",
            "  7.89587752e-01  1.90571567e+00 -1.26503696e+00 -2.29082670e+00\n",
            " -2.10056447e+00 -1.46168694e+00 -2.72930860e+00  2.55084487e+00\n",
            "  3.09923022e-01  2.38387746e+00 -3.72164497e-01  3.49978092e+00\n",
            " -1.11728207e-01  4.12317320e+00 -4.87921673e-01  2.98992988e+00\n",
            " -1.61025820e+00 -2.79170223e+00  1.90580882e+00 -3.35639513e-01\n",
            "  8.08990207e-02  1.62853729e+00  1.82908180e+00 -3.84410525e-01\n",
            " -8.70724664e-01 -4.08052248e+00  4.63703474e-01  5.93897149e-01\n",
            "  5.84609303e-01  1.16697691e+00  4.70385089e-01 -6.62389181e-01\n",
            "  2.78022732e-01  4.70711240e-01  8.34107339e-01  8.11929264e-02\n",
            "  2.15443608e+00 -7.87754534e-01 -6.75357858e-01 -2.77541876e-01\n",
            "  1.72664786e+00 -1.86051126e+00  1.68956962e+00  3.10051829e+00\n",
            "  1.47926576e+00 -9.89057524e-01 -1.93602754e-01 -8.60410561e-01\n",
            "  2.13932498e+00 -3.15061909e-01 -1.73354451e-01  1.52382822e+00\n",
            " -2.14362825e+00 -8.51549624e-01 -3.29651076e-01  1.74742023e+00\n",
            "  2.81071879e-01  5.61306877e-01  1.97849504e+00  1.61137192e+00\n",
            " -1.35322341e+00  4.30058965e-01 -1.31189876e+00 -9.68783837e-01\n",
            "  5.88055963e-01  1.34599162e+00  2.59321457e+00  1.70802243e+00\n",
            " -4.04024170e-01  2.72602268e+00 -1.00619829e+00  2.92701488e+00\n",
            " -9.71380978e-01 -1.77391627e+00 -7.90812944e-01 -5.54366446e+00\n",
            " -9.04582624e-01  2.46804438e+00 -8.53990089e-01 -5.07254478e-02\n",
            "  1.15164378e+00 -1.39980620e+00  4.00911001e-01 -1.45365564e+00\n",
            "  5.87085137e-01 -1.85373482e-01 -3.71209815e+00 -9.23458137e-01\n",
            " -1.24721938e+00 -7.00233058e-02 -1.93838818e-01  1.84128045e+00\n",
            "  3.15306389e+00  1.77627426e+00 -2.94151028e-01 -1.03047932e+00\n",
            " -1.67747118e+00  5.08847002e+00  5.17411026e-01  1.81943060e+00\n",
            "  1.69268846e+00  1.15483091e+00  2.36291306e+00 -1.20965582e-01\n",
            " -2.05510265e+00 -2.48660667e-01 -8.64516162e-02  4.67979332e-01\n",
            "  1.24076960e+00 -1.39478633e+00 -9.51483260e-01 -4.61577298e-01\n",
            " -1.22821144e+00  3.49650736e+00  1.25240437e+00  3.01934586e+00\n",
            " -1.16610684e+00  1.89421641e+00 -3.52947995e-02  3.08878135e+00\n",
            " -9.13180975e-01  9.21502108e-01 -1.31755253e+00 -1.41894166e+00\n",
            "  7.86643468e-02  2.21716458e+00  4.91314548e-01  2.48028064e-01\n",
            " -3.69025338e+00  9.80356607e-01  1.31464544e-01 -1.94922881e+00\n",
            "  6.89948699e-03  2.72611048e-01 -7.43458490e-01 -3.33212987e-01\n",
            "  2.03718164e-01  8.81926855e-01  1.81012532e+00  4.75487966e-01\n",
            "  1.74584676e+00 -9.81556814e-01  2.43003074e+00 -1.96972764e+00\n",
            " -4.36871567e-01 -1.07157884e-01 -2.44202214e-01 -3.41042963e-02\n",
            "  1.95276516e-01 -3.69872244e-01 -2.71897107e+00 -1.01222675e+00\n",
            "  3.36782217e+00  3.67551919e+00  4.36991254e+00  5.75116746e-01\n",
            "  1.00866158e+00  2.97987896e-01 -1.51277413e+00  1.40139384e+00\n",
            "  1.28255417e+00  1.85787702e+00  1.85808171e-01  5.71624012e-01\n",
            "  1.79426514e+00  6.28194150e-01  1.11608078e+00 -1.44879616e+00\n",
            " -3.54757613e+00  1.71676672e+00  8.69108754e-01 -2.27311032e+00\n",
            " -5.42572595e+00 -5.37581843e-01  2.13414663e+00 -3.01251964e+00\n",
            " -1.48862149e+00  2.00612752e-01 -2.50603808e+00  5.46766844e-01\n",
            "  5.31022350e-01  2.05954665e+00 -9.28598104e-01 -6.23847574e-01\n",
            " -2.04362211e+00  2.24951572e+00  1.50257584e+00  6.45589458e-01\n",
            " -1.77669036e+00  1.11374688e+00  8.91635826e-01 -1.46541911e+00\n",
            "  5.67014658e-02  1.72422038e+00 -2.51453302e-01 -2.72546382e+00\n",
            " -2.56199976e-01 -2.94338315e+00 -2.09015207e+00  1.33908249e+00\n",
            "  3.17157002e+00  4.41479924e-01 -1.39221479e+00  8.82741051e-01\n",
            " -3.34991726e+00  8.83344638e-01  1.18254352e-01  5.77771183e-01\n",
            " -1.89951069e+00 -2.80173334e-02 -8.75156950e-01  3.39668751e-01\n",
            " -1.06692719e+00  1.01111787e+00 -2.14893303e+00  8.96741759e-01\n",
            "  2.57659344e+00  1.28281318e+00 -2.42836341e+00 -6.63262033e-01\n",
            "  1.02546027e+00 -2.08192851e-01 -1.96425390e+00  7.78752967e-01\n",
            " -1.02477381e+00  2.20820702e+00 -1.62381155e+00 -2.07257434e+00\n",
            "  3.50828100e+00  1.84048027e+00  1.35339123e+00 -2.18788967e+00\n",
            "  7.03527274e-01  1.67737430e+00  1.11471641e+00 -1.09723249e+00\n",
            " -3.77556093e+00 -5.24920788e-01  1.76413846e+00  9.57928670e-02\n",
            " -1.00308268e+00  1.48759039e+00  2.12741527e+00  1.28709882e-01\n",
            "  5.39893832e-02  1.90242477e+00 -7.39791850e-01 -5.67500438e-02\n",
            "  2.64825502e+00 -1.46668009e+00  1.25526508e+00 -2.06668148e+00\n",
            " -2.64208449e+00 -1.52279769e-01  3.47588780e-01 -2.79345719e+00\n",
            " -7.63120602e-01 -1.50338954e+00  8.57101096e-01  1.10974067e+00\n",
            "  1.13661805e+00 -6.17819618e-02 -6.91640574e-02  1.00305397e+00\n",
            " -8.94880540e-01 -3.26184816e+00 -1.72743355e+00  8.61951261e-01\n",
            " -2.12285568e+00 -2.58361234e+00 -1.86291452e-01 -1.93376841e+00\n",
            "  1.54758104e+00 -1.06561766e+00 -9.29492301e-01 -2.07271917e-01\n",
            "  3.99163761e+00  3.88588820e+00 -1.09241042e+00  1.73692829e+00\n",
            " -1.66458724e+00  6.10570028e-03  4.44109777e+00  3.25535638e+00\n",
            "  2.34117280e+00 -4.67434259e-01  1.53400530e+00 -4.81147908e-01\n",
            " -1.71337148e+00  2.11410994e+00  1.02370602e+00  2.71974994e+00\n",
            "  1.26788383e+00 -9.02506978e-01  3.08966004e+00  5.31223751e-01\n",
            " -2.17808392e+00  1.26720223e+00 -7.09396574e-01  1.21703554e+00\n",
            "  5.03028173e-01  9.89170758e-01 -1.91906822e+00 -7.96459056e-01\n",
            "  8.16596595e-01  1.06796817e+00  4.69332214e-01  2.34676564e+00\n",
            "  1.18347861e+00  2.61855076e+00  1.74236266e-01  5.49168911e-02\n",
            "  3.82140463e+00 -8.63347957e-01  3.26186328e+00  2.07151079e+00\n",
            " -4.12346164e+00 -4.33511060e-01 -1.80980084e-01  5.01874846e-01\n",
            "  3.98143222e-01 -4.04217644e-01 -1.45081821e+00  7.45459845e-01\n",
            " -1.39244860e+00 -1.04575273e-01 -4.36902096e-01  2.40821714e-02\n",
            " -4.57598773e-01 -8.49521434e-01  3.34585556e+00 -3.06906485e+00\n",
            " -6.23821258e-01 -2.13204023e-01 -3.19406047e-01  8.12271344e-01\n",
            "  2.29135479e-01 -1.35358385e+00  1.95092983e+00  6.96850358e-01\n",
            "  7.26616924e-01  2.13528665e+00 -6.73670129e-01 -3.80828688e-01\n",
            " -4.78136422e+00  7.45447021e-01  1.09879369e+00  8.91872646e-01\n",
            " -2.59688585e+00  1.83294848e+00  1.29794327e+00  2.47687124e+00\n",
            "  6.45612301e-02  2.32155652e+00  1.73739795e+00 -1.24201764e+00\n",
            "  6.52490478e-01 -3.47019739e+00 -9.37374669e-01 -1.44910033e+00\n",
            "  1.58560029e+00 -3.12968699e-01  1.95215748e+00  3.94132629e-01\n",
            " -2.59878645e+00  3.71062458e+00 -1.62926295e+00 -3.65975610e-01\n",
            "  1.47760837e+00 -7.61132308e-01 -1.74973567e+00  3.25073152e-01\n",
            " -2.81480822e+00 -6.40061648e-02 -2.94845645e-01 -6.53484319e-01\n",
            "  1.23554778e+00  2.05560230e+00  2.58279812e+00 -1.12324493e+00\n",
            " -2.80111231e+00  1.29338373e+00 -6.93299486e-01 -1.54622988e-01\n",
            " -1.02300771e+00 -2.27693009e-01 -1.43978880e+00  1.11849105e+00\n",
            " -6.03172968e-01 -2.93403828e+00  1.18873399e-01  3.10710065e+00\n",
            " -3.05079092e-01 -4.08149590e-01  2.98347818e-01 -4.51206117e-01\n",
            "  5.48036238e-01 -1.68946441e+00 -2.40779490e+00  9.63058210e-01\n",
            " -2.35217994e+00 -8.07768645e-01 -1.72650210e+00  1.11547190e+00\n",
            " -1.61505139e+00  3.70034564e+00 -2.47543565e+00  1.86294506e-01\n",
            " -1.12498553e+00  1.22127001e+00  2.17881555e+00  9.66364689e-01\n",
            "  9.37689497e-01  3.93875800e-01 -1.72232599e+00  1.46455893e+00\n",
            " -2.12732358e-01 -3.20977845e+00  1.16233651e+00  2.78303802e+00\n",
            "  1.39738511e+00  1.22316496e+00 -1.16639777e+00  1.07279224e+00\n",
            " -2.79935462e+00 -6.89421339e-01 -1.65557130e+00  1.31540625e+00\n",
            "  1.57734271e+00  7.57017585e-01 -6.72444658e-01  4.24499295e+00\n",
            "  1.72418088e+00  9.56380176e-01  2.94942295e+00  3.18323242e+00\n",
            " -1.36409100e+00  3.77240865e-01 -9.42087835e-01 -1.91197833e+00\n",
            " -2.09336622e+00  4.98243906e-01 -1.98108722e+00 -1.29335754e+00\n",
            "  2.42088520e+00  5.39587847e-01  3.27819222e+00  3.31640218e-01\n",
            "  7.73682161e-01  3.34244652e+00 -9.23164894e-01 -1.35604109e+00\n",
            " -2.85782677e+00  2.35895912e+00  3.99315231e+00 -1.51936259e-01\n",
            "  1.80759174e+00 -1.05371867e+00 -5.29310298e+00 -1.97831241e+00\n",
            "  7.84746837e-01 -5.82881975e-01 -1.61519625e+00  2.36395102e+00\n",
            " -1.84291465e+00 -2.99162150e+00  7.59410404e-01 -5.95343058e-01\n",
            " -7.79683925e-01 -8.14217665e-01  4.80780312e-01 -3.25097270e+00\n",
            " -5.58599101e-01 -2.16008838e+00  3.21114857e+00 -3.63049859e-01\n",
            "  2.15233531e+00 -1.86284361e-01 -2.21708960e+00 -2.25706193e+00\n",
            "  1.70062195e+00  1.83419183e+00 -2.07231761e+00 -2.23336098e+00\n",
            "  1.27399364e-01 -4.59562802e-01  2.09697079e+00 -3.19088000e-02\n",
            "  1.67648599e+00 -1.04896634e+00  1.30606838e+00  1.95918319e+00\n",
            "  2.86040694e+00 -1.55874543e+00  9.82812690e-01  8.22799794e-01\n",
            " -1.61158382e+00 -4.73043457e-01  5.67913737e-02  3.50838478e+00\n",
            "  2.21677136e+00  5.76768816e-01  3.07911163e-02  2.65888477e-01\n",
            "  1.46330093e+00  2.09324868e-01  2.13184122e+00  8.05158049e-04\n",
            " -6.59751837e-01  4.81105242e-01  9.48293527e-01  1.59663350e-01\n",
            " -1.96793966e+00 -4.20230585e+00  1.09566459e+00 -2.49011017e+00\n",
            " -1.62862139e+00 -1.99440669e+00 -1.15715637e+00  3.35401288e-01\n",
            "  5.57117885e-01 -4.73947006e-01 -3.41766452e+00 -1.10206811e+00\n",
            "  1.77492621e-01  2.12295276e+00  8.73382610e-04 -2.08774272e+00\n",
            " -1.76593217e+00  1.30381658e+00 -2.34403716e+00 -2.29233344e+00\n",
            " -1.11469219e+00  1.40272942e+00 -2.01063553e+00 -8.37164736e-01\n",
            "  2.41715678e+00 -2.56496956e+00 -8.23644574e-01 -5.00413043e-01\n",
            " -7.41797399e-01  2.02865448e+00 -2.07563989e+00 -7.93805386e-01\n",
            " -7.52250841e-01  1.51140249e+00 -1.48149296e+00  9.86577294e-01\n",
            "  2.71098499e+00 -1.32461484e+00 -2.77448417e+00  3.37679670e-01\n",
            " -1.01089487e+00 -6.54463558e-02  5.72989669e-01 -1.67869283e+00\n",
            " -1.66857578e+00 -2.18205026e+00 -3.72900512e-01  6.54340781e-01\n",
            " -7.93939438e-01  2.63168230e+00  6.87234269e-01 -1.32980642e+00\n",
            "  6.44235314e-01 -8.33675355e-01 -3.72013255e+00  1.16233453e+00\n",
            " -2.21970736e+00  8.27720915e-01 -2.40447451e-01 -2.42507504e-01\n",
            " -1.11192190e+00  1.90009043e+00 -2.59207804e+00 -1.18751144e+00\n",
            "  1.43255521e+00  1.12678464e+00 -1.63726245e+00  3.29577976e-02\n",
            " -1.72643289e+00  1.40461521e+00 -1.07004084e+00 -1.02410841e+00\n",
            "  1.89525911e+00 -2.48719500e-01  2.13235774e+00 -8.02039460e-02\n",
            "  3.85514036e-01  5.82783326e-01  2.58543020e+00 -2.31455339e+00\n",
            "  7.48490207e-01  2.85494775e+00  4.53311002e-01  3.73686526e-01\n",
            " -2.40249861e+00  8.70855621e-02  1.44537983e+00 -1.88213956e-01\n",
            " -1.46096527e+00 -8.72669026e-01 -8.15724455e-01  1.48769227e-01\n",
            " -1.17615751e+00 -3.71982089e-01 -4.74087169e+00 -4.17561952e-01\n",
            " -3.28524715e-01 -2.79566343e+00 -4.66657962e-01 -3.04703476e+00\n",
            " -2.05262625e+00  2.16328140e-01 -2.69364172e+00  2.74135431e-01\n",
            "  1.01730734e-01  4.01678756e-01  9.54179712e-02 -1.25432305e+00\n",
            " -2.36576682e-02  8.55914794e-01 -4.12481284e+00  4.22456323e-01\n",
            " -6.76308613e-01  1.63495750e+00  1.75820601e+00 -1.77390908e+00\n",
            " -2.19098003e+00 -8.16923064e-01 -4.73689481e-01 -2.70800030e-01\n",
            " -1.48515834e+00  1.39428976e-01  1.05612977e+00 -1.23757129e+00\n",
            " -3.55457911e-01 -1.59160958e+00 -5.66210284e-02  1.36244047e+00\n",
            " -5.27289365e-01  2.47399251e-01  1.47948953e+00 -8.29462560e-01\n",
            " -1.08641744e+00  7.82496855e-01  1.33514508e+00  1.77603504e+00\n",
            " -2.23639352e-01  1.67674991e+00 -7.87203742e-01 -4.26862306e+00\n",
            " -9.13174564e-01 -1.00122367e+00 -1.20201605e+00  7.21669089e-01\n",
            " -3.05285940e+00  2.73248030e+00  2.47350657e+00  3.48696980e-01\n",
            " -3.65565931e-01  7.25142988e-01  1.90335798e+00  1.46833942e+00\n",
            " -6.75551591e-02  1.41156938e-01  1.22077805e+00 -2.56938708e+00\n",
            " -5.50527259e-01  1.85962493e-01 -8.10601436e-01 -1.40108424e+00\n",
            "  2.85238396e+00 -7.74426655e-01  1.05701993e+00  1.15373115e+00\n",
            "  4.07718853e-02 -2.30389093e-01 -2.63046231e-01  9.83032414e-01\n",
            "  1.22924914e+00  3.03188220e-01 -1.38563377e+00  1.54324170e+00\n",
            " -2.97456217e-02 -1.80240325e+00 -7.35360219e-01  2.64954945e+00\n",
            " -1.11831799e+00 -1.11844470e+00  1.73213499e-01 -7.15207256e-01\n",
            "  7.11946506e-01  1.05995788e+00 -8.69767212e-01 -5.97136989e+00\n",
            "  1.60129908e+00 -9.10825728e-01  1.43390140e+00 -1.35670740e+00\n",
            "  1.03515957e+00  4.25448764e-02  1.15083384e+00  8.98823710e-01\n",
            " -1.78557240e+00 -1.49563882e+00  3.07475563e-02 -5.41647384e-01\n",
            "  1.27619240e+00  1.87262131e-01  3.14163514e-01  6.67274034e-01\n",
            " -1.96107732e-01  3.70674406e-01  4.26608742e-01  7.63606336e-01\n",
            "  2.93818956e+00 -3.57326218e-01 -1.05755743e+00  1.24606652e+00\n",
            " -1.51251040e+00  1.71252576e+00 -3.87004763e+00 -5.61208230e+00\n",
            "  4.09548554e+00 -1.18583622e+00 -1.95853147e-01  1.13393732e-01\n",
            " -9.37022260e-01 -1.62539492e-01  3.92300249e-01 -4.22622640e-01\n",
            " -2.73245748e-01 -1.31690915e+00 -2.91651765e+00 -7.66191174e-01\n",
            "  2.04086323e+00  5.51037007e-01  2.05653979e+00  8.89115570e-02\n",
            " -7.69037107e-01 -9.33724316e-01 -1.03707920e+00  1.44688869e+00\n",
            "  1.15615236e+00  1.44493702e+00 -2.01979120e+00  1.08092720e+00\n",
            "  7.69303730e-01 -1.48327944e+00 -2.00800570e+00  2.65946352e+00\n",
            "  1.47145495e+00 -6.97450466e-01 -5.30058340e-02 -1.80432209e+00\n",
            "  2.78787180e+00  4.17359624e-01  1.06960909e+00  9.71111926e-01\n",
            " -1.05402935e+00  2.58422017e+00 -3.40067139e+00 -3.72641723e+00\n",
            "  9.12365996e-01  4.27992612e+00 -1.04808247e+00 -6.06642562e-01]\n",
            "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import shap\n",
        "from pyswarm import pso\n",
        "from scipy.optimize import dual_annealing\n",
        "from scipy.stats import genpareto\n",
        "\n",
        "# --- Step 1: Portfolio Optimization Constraints ---\n",
        "# Relaxing the upper bounds for assets to allow more flexibility in optimization\n",
        "n_assets = 5  # Number of assets\n",
        "lb = np.zeros(n_assets)  # Lower bounds for weights\n",
        "ub = np.ones(n_assets) * 0.2  # Upper bounds for weights (relaxed to 20% max)\n",
        "\n",
        "# --- Step 2: Objective Function ---\n",
        "def objective_function(weights, X):\n",
        "    # Portfolio returns and volatility\n",
        "    portfolio_returns = np.dot(X, weights)\n",
        "    portfolio_volatility = np.std(portfolio_returns)\n",
        "    # Return the negative of the Sharpe ratio as we want to maximize it\n",
        "    return -np.mean(portfolio_returns) / portfolio_volatility\n",
        "\n",
        "# --- Step 3: PSO Optimization ---\n",
        "# Improved PSO: Relax bounds and adjust swarm size for better exploration\n",
        "def optimize_portfolio_pso(X):\n",
        "    optimal_weights, _ = pso(objective_function, lb, ub, maxiter=3000, swarmsize=100, args=(X,))\n",
        "    return optimal_weights\n",
        "\n",
        "# --- Step 4: Simulated Annealing ---\n",
        "# Adjust Simulated Annealing cooling schedule using dual_annealing\n",
        "def optimize_portfolio_sa(X):\n",
        "    result_sa = dual_annealing(objective_function, bounds=list(zip(lb, ub)), maxiter=2000, maxfun=2000, args=(X,))\n",
        "    return result_sa.x\n",
        "\n",
        "# --- Step 5: Tail Risk Estimation using EVT and GPD ---\n",
        "def gpd_tail_risk(returns, threshold=0.05):\n",
        "    excess_returns = returns[returns > threshold] - threshold\n",
        "    params = genpareto.fit(excess_returns)  # Fit GPD to excess returns\n",
        "    tail_risk = genpareto.ppf(0.99, *params)  # Value at Risk (99% quantile)\n",
        "    return tail_risk\n",
        "\n",
        "# --- Step 6: Model Interpretability (SHAP) ---\n",
        "# Assuming we have a model (e.g., linear regression) for portfolio performance\n",
        "# Here we simulate it using a dummy model\n",
        "class DummyModel:\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, np.ones(X.shape[1]))  # Simple model: sum of all features\n",
        "\n",
        "model = DummyModel()\n",
        "\n",
        "# Explain the model's predictions using SHAP\n",
        "explainer = shap.KernelExplainer(model.predict, X)\n",
        "shap_values = explainer.shap_values(X)\n",
        "\n",
        "# Visualize the SHAP values to understand feature importance\n",
        "shap.summary_plot(shap_values, X)\n",
        "\n",
        "# --- Step 7: Main Execution ---\n",
        "# Example data (replace with your actual data)\n",
        "X = np.random.randn(1000, n_assets)  # Simulated returns data (1000 samples, 5 assets)\n",
        "\n",
        "# Perform PSO optimization\n",
        "optimized_weights_pso = optimize_portfolio_pso(X)\n",
        "print(f\"Optimized Portfolio Weights (PSO): {optimized_weights_pso}\")\n",
        "\n",
        "# Perform Simulated Annealing optimization\n",
        "optimized_weights_sa = optimize_portfolio_sa(X)\n",
        "print(f\"Optimized Portfolio Weights (SA): {optimized_weights_sa}\")\n",
        "\n",
        "# Compute tail risk\n",
        "portfolio_returns = np.dot(X, optimized_weights_sa)  # Using SA optimized weights\n",
        "tail_risk = gpd_tail_risk(portfolio_returns)\n",
        "print(f\"Refined Tail Risk (99% VaR): {tail_risk}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530,
          "referenced_widgets": [
            "42e494058727406fb8cc931aa4e323cf",
            "e7505a29c9664b8f8d9754cb2f39b146",
            "efbec998ff02489cb4f17af55e621a4c",
            "77478c5f5e754f3ebc210ded5daa29d9",
            "d7361cf01c9e41e2846f95c91cf0f0d1",
            "54cd229859c64ef69db75e765d089aa3",
            "3f0d04561a1c40a688f4f78961604e8c",
            "8c9b8559566b48468b1e4fd639131514",
            "1503a261fc9e4c2eacaf151035ab1c6d",
            "d0d4c1f572484821ba7966551e31f8af",
            "3e40f457f05540e39f646b0eee927a32"
          ]
        },
        "id": "WGB7bd130QHM",
        "outputId": "357fb322-fd34-460f-b1f3-eb13e18f8a54"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:shap:Using 1000 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42e494058727406fb8cc931aa4e323cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-38-3692999821.py:54: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
            "  shap.summary_plot(shap_values, X)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x350 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping search: Swarm best objective change less than 1e-08\n",
            "Optimized Portfolio Weights (PSO): [0.         0.19989303 0.07157044 0.07213124 0.        ]\n",
            "Optimized Portfolio Weights (SA): [0.         0.2        0.07165352 0.07217444 0.        ]\n",
            "Refined Tail Risk (99% VaR): 0.5691351063069557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4htNl9ITHS2V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}